{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 219/219 [00:00<00:00, 6831.57 examples/s]\n",
      "Map: 100%|██████████| 910/910 [00:00<00:00, 6816.49 examples/s]\n",
      "Casting the dataset: 100%|██████████| 219/219 [00:00<00:00, 35427.05 examples/s]\n",
      "Casting the dataset: 100%|██████████| 910/910 [00:00<00:00, 24316.35 examples/s]\n",
      "Map: 100%|██████████| 219/219 [00:09<00:00, 23.08 examples/s]\n",
      "Map: 100%|██████████| 910/910 [00:37<00:00, 24.23 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 08:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.641200</td>\n",
       "      <td>2.364693</td>\n",
       "      <td>0.026504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.312400</td>\n",
       "      <td>2.192280</td>\n",
       "      <td>0.188904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.214800</td>\n",
       "      <td>1.904316</td>\n",
       "      <td>0.237766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.668300</td>\n",
       "      <td>1.597488</td>\n",
       "      <td>0.386181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.368200</td>\n",
       "      <td>1.518604</td>\n",
       "      <td>0.439288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.368400</td>\n",
       "      <td>1.268645</td>\n",
       "      <td>0.519682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>1.157767</td>\n",
       "      <td>0.597784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.751600</td>\n",
       "      <td>1.188004</td>\n",
       "      <td>0.597264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training SFT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation data: 100%|██████████| 910/910 [04:21<00:00,  3.48sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6077\n",
      "F1 Score: 0.5843\n",
      "AUC: 0.9315\n",
      "Average Inference Time per Sample: 1.1966 seconds\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from datasets.dataset_dict import DatasetDict, IterableDatasetDict\n",
    "from datasets.iterable_dataset import IterableDataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from datasets import ClassLabel\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from transformers import logging as transformers_logging\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from huggingface_hub import HfFolder\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "import torch\n",
    "import glob\n",
    "# Load checkpoint\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from datasets.dataset_dict import DatasetDict, IterableDatasetDict\n",
    "from datasets.iterable_dataset import IterableDataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from datasets import ClassLabel\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "# Data preparation\n",
    "import torch\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "transformers_logging.set_verbosity_error()\n",
    "\n",
    "# ========================== CMD Argument Parser ==========================\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Train a model using CPT (Continual Pretraining Training)\")\n",
    "#     parser.add_argument(\"--per_device_train_batch_size\", type=int, default=8, help=\"Batch size per device during training\")\n",
    "#     parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=8, help=\"Batch size per device during evaluation\")\n",
    "#     parser.add_argument(\"--num_train_epochs\", type=int, default=1, help=\"Number of training epochs\")\n",
    "#     parser.add_argument(\"--learning_rate\", type=float, default=1e-6, help=\"Learning rate for training\")\n",
    "#     parser.add_argument(\"--project_root\", type=str, default=\"/Users/lujun.li/projects/mt_luxembourgish\", help=\"Path to project root\")\n",
    "#     parser.add_argument(\"--training_dataset_path\", type=str, default=\"data/processed/dataset_merged_llama_fake_targets.jsonl\", help=\"Path to training dataset\")\n",
    "#     parser.add_argument(\"--model_path\", type=str, default=\"/home/llama/Personal_Directories/srb/binary_classfication/Llama-3.2-3B-Instruct\", help=\"Path to model\")\n",
    "#     parser.add_argument(\"--resume_from_checkpoint\", type=bool, default=False, help=\"Resume training from checkpoint\")\n",
    "#     parser.add_argument(\"--resume_checkpoint_path\", type=str, default=None, help=\"Path to checkpoint to resume training from\")\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# args = parse_args()\n",
    "\n",
    "# print(\"Arguments passed:\")\n",
    "# print(f\"Train Batch Size: {args.per_device_train_batch_size}\")\n",
    "# print(f\"Eval Batch Size: {args.per_device_eval_batch_size}\")\n",
    "# print(f\"Number of Epochs: {args.num_train_epochs}\")\n",
    "# print(f\"Learning Rate: {args.learning_rate}\")\n",
    "# print(f\"Project Root: {args.project_root}\")\n",
    "# print(f\"Training Dataset Path: {args.training_dataset_path}\")\n",
    "# print(f\"Model path: {args.model_path}\")\n",
    "# print(f\"Resume from checkpoint: {args.resume_from_checkpoint}\")\n",
    "# print(f\"Resume checkpoint path: {args.resume_checkpoint_path}\")\n",
    "\n",
    "\n",
    "# per_device_train_batch_size = args.per_device_train_batch_size  # Batch size for training per device\n",
    "# per_device_eval_batch_size = args.per_device_eval_batch_size  # Batch size for evaluation per device\n",
    "# num_train_epochs = args.num_train_epochs  # Number of epochs for training\n",
    "# learning_rate = args.learning_rate # Learning rate for the optimizer\n",
    "# project_root = args.project_root\n",
    "# training_dataset_path = args.training_dataset_path\n",
    "# model_path = args.model_path\n",
    "# resume_from_checkpoint = args.resume_from_checkpoint\n",
    "# resume_checkpoint_path = args.resume_checkpoint_path\n",
    "\n",
    "# Data preparation\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "num_train_epochs = 3\n",
    "learning_rate = 5e-5\n",
    "project_root = \"/home/snt/projects_lujun/agentCLS\"\n",
    "training_dataset_path = \"assets/training_dataset/LDD_split.json\"\n",
    "model_path = \"answerdotai/ModernBERT-base\"\n",
    "resume_from_checkpoint = False\n",
    "resume_checkpoint_path = None\n",
    "\n",
    "train_dataset_path = os.path.abspath(os.path.join(project_root, training_dataset_path))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "train_seed = 3407\n",
    "train_ratio = 0.01\n",
    "logging_steps = 10\n",
    "eval_steps = 10\n",
    "eval_strategy = \"steps\"\n",
    "save_strategy = \"epoch\"\n",
    "save_total_limit = 2\n",
    "logging_strategy = \"steps\"\n",
    "max_grad_norm = 0.3\n",
    "input_dataset_name = train_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
    "model_name = model_path.split(\"/\")[-1]\n",
    "max_length = 4096\n",
    "\n",
    "if resume_from_checkpoint and resume_checkpoint_path is None:\n",
    "    raise ValueError(\"Please provide a checkpoint path to resume training from\")\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    output_dir = resume_checkpoint_path\n",
    "else:\n",
    "    current_time = datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
    "    output_dir = f\"{project_root}/assets/logs/{input_dataset_name}_{train_ratio}_{model_name}_output_{current_time}\"\n",
    "\n",
    "dataset = pd.read_json(train_dataset_path, lines=True)\n",
    "dataset.rename(columns={\"cls_label\": \"labels\"}, inplace=True)\n",
    "\n",
    "# Compute sample counts for each group based on 'labels' and 'split'\n",
    "sample_counts = dataset.groupby(['labels', 'split']).size() * train_ratio\n",
    "\n",
    "filtered_train_data = dataset.groupby('labels', group_keys=False).apply(\n",
    "    lambda x: x[x['split'] == 'train'].iloc[:int(sample_counts.loc[x.name, 'train'])]\n",
    ")\n",
    "\n",
    "filtered_validation_data = dataset.groupby('labels', group_keys=False).apply(\n",
    "    lambda x: x[x['split'] == 'validation'].iloc[:int(sample_counts.loc[x.name, 'validation'])*10]\n",
    ")\n",
    "\n",
    "filtered_train = filtered_train_data.reset_index(drop=True)\n",
    "filtered_validation = filtered_validation_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(filtered_train)\n",
    "val_dataset = Dataset.from_pandas(filtered_validation)\n",
    "\n",
    "# Prepare model labels - useful for inference\n",
    "labels =  set(train_dataset['labels'])\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x: {\"labels\": label2id[x[\"labels\"]]})\n",
    "val_dataset = val_dataset.map(lambda x: {\"labels\": label2id[x[\"labels\"]]})\n",
    "\n",
    "labels_ids = list(set(train_dataset['labels']))\n",
    "class_label = ClassLabel(num_classes=len(labels_ids), names=labels_ids)\n",
    "train_dataset = train_dataset.cast_column(\"labels\", class_label)\n",
    "val_dataset = val_dataset.cast_column(\"labels\", class_label)\n",
    "\n",
    "\n",
    "def tokenize(examples):\n",
    "    # Tokenize the content and add the corresponding labels to the output\n",
    "    tokenized_inputs = tokenizer(examples[\"content\"], padding=True, max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
    "    # tokenized_inputs[\"label\"] = examples[\"labels\"] \n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "keep_columns = [\"labels\", \"input_ids\", \"attention_mask\"]\n",
    "\n",
    "# Remove all other columns from the dataset\n",
    "tokenized_train_dataset = train_dataset.map(tokenize, batched=True, remove_columns=[col for col in train_dataset.column_names if col not in keep_columns])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize, batched=True, remove_columns=[col for col in val_dataset.column_names if col not in keep_columns])\n",
    "train_dataset.features.keys()\n",
    "    \n",
    "\n",
    "## Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels, label2id=label2id, id2label=id2label,)\n",
    "\n",
    "# Metric helper method\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    score = f1_score(\n",
    "            labels, predictions, labels=labels, pos_label=1, average=\"weighted\"\n",
    "        )\n",
    "    \n",
    "    return {\"f1\": float(score) if score == 1 else score}\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Define training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir= output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        learning_rate=5e-5,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        bf16=True,\n",
    "        optim=\"adamw_torch_fused\", \n",
    "        logging_strategy=logging_strategy,\n",
    "        logging_steps=logging_steps,\n",
    "        eval_strategy=eval_strategy,\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=save_strategy,\n",
    "        # save_total_limit=save_total_limit,\n",
    "        load_best_model_at_end=False,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        # group_by_length=True,\n",
    "        # use_mps_device=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        # push to hub parameters\n",
    "        # push_to_hub=True,\n",
    "        # hub_strategy=\"every_save\",\n",
    "        # hub_token=HfFolder.get_token(),\n",
    "        report_to=\"tensorboard\",\n",
    "        disable_tqdm=False,\n",
    "        seed = train_seed\n",
    "    )\n",
    "    \n",
    "    # Create a Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] \n",
    "    )\n",
    "    trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    print(\"Finished training SFT.\")\n",
    "    return trainer_stats\n",
    "\n",
    "def evaluate():\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    def get_last_checkpoints(output_dir):\n",
    "        checkpoints = os.listdir(output_dir)\n",
    "        checkpoints = [c for c in checkpoints if \"checkpoint\" in c]\n",
    "        checkpoints = [int(c.split(\"-\")[-1]) for c in checkpoints]\n",
    "        last_checkpoint = max(checkpoints)\n",
    "        return f\"{output_dir}/checkpoint-{last_checkpoint}\"\n",
    "    \n",
    "    checkpoints_path  = get_last_checkpoints(output_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoints_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    validation_results = []\n",
    "\n",
    "    # Initialize lists to store true and predicted labels\n",
    "    true_label_one_hot_list = []\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    # Start time for measuring inference efficiency\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate through validation dataset and make predictions\n",
    "    for input in tqdm(val_dataset, desc=\"Processing validation data\", unit=\"sample\"):\n",
    "        sample = input['content']\n",
    "        true_label_idx = int(input['labels'])\n",
    "        tokenized_input = tokenizer(sample, padding=True, max_length=max_length, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        model_output = model(**tokenized_input)\n",
    "        logits = model_output.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
    "        predicted_label = id2label[str(predicted_class_idx)]\n",
    "        \n",
    "        true_label = id2label[str(true_label_idx)]\n",
    "        true_label_one_hot = np.zeros(probabilities.size(-1))\n",
    "        true_label_one_hot[true_label_idx] = 1\n",
    "\n",
    "        true_labels.append(true_label)\n",
    "        true_label_one_hot_list.append(true_label_one_hot)\n",
    "        predicted_labels.append(predicted_label)\n",
    "        all_probs.append(probabilities.detach().cpu().numpy())  # Store the raw probabilities for AUC\n",
    "        result = {\n",
    "            'content': sample,\n",
    "            'true_label': true_label,\n",
    "            'predicted_label': predicted_label,\n",
    "            'true_label_one_hot': true_label_one_hot.tolist(),\n",
    "            'predicted_class_idx': predicted_class_idx,\n",
    "            'probabilities': probabilities.detach().cpu().numpy().tolist()  # Convert to list for JSON serialization\n",
    "        }\n",
    "        validation_results.append(result)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
    "    df_validation_results = pd.DataFrame(validation_results)\n",
    "    jsonl_file_path = os.path.join(checkpoints_path, f'validation_results_{timestamp}.jsonl')\n",
    "    df_validation_results.to_json(jsonl_file_path, orient='records', lines=True)\n",
    "\n",
    "\n",
    "    # Calculate accuracy, F1 score, and AUC\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')  # weighted F1 score\n",
    "    auc = roc_auc_score(np.array(true_label_one_hot_list),  np.squeeze(np.array(all_probs), axis=1), multi_class='ovr', average='weighted')  # for multi-class AUC\n",
    "\n",
    "    # Calculate inference time (average time per sample)\n",
    "    end_time = time.time()\n",
    "    inference_time = (end_time - start_time) / len(train_dataset)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"Average Inference Time per Sample: {inference_time:.4f} seconds\")\n",
    "    return \n",
    "        \n",
    "\n",
    "trainer_stats = None\n",
    "# trainer_stats = train()\n",
    "\n",
    "def main():\n",
    "    trainer_stats = train()\n",
    "    evaluate()\n",
    "    return trainer_stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainer_stats = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 315/315 [00:00<00:00, 9997.77 examples/s]\n",
      "Filter: 100%|██████████| 315/315 [00:00<00:00, 11308.79 examples/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/snt/projects_lujun/agentCLS/assets/logs/LDD_split_0.001_ModernBERT-base_output_03_11_14_30_16'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m     last_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(checkpoints)\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoint-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_checkpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 57\u001b[0m checkpoints_path  \u001b[38;5;241m=\u001b[39m \u001b[43mget_last_checkpoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoints_path)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     60\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n",
      "Cell \u001b[0;32mIn[2], line 51\u001b[0m, in \u001b[0;36mget_last_checkpoints\u001b[0;34m(output_dir)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_last_checkpoints\u001b[39m(output_dir):\n\u001b[0;32m---> 51\u001b[0m     checkpoints \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     checkpoints \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m checkpoints \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m c]\n\u001b[1;32m     53\u001b[0m     checkpoints \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(c\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m checkpoints]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/snt/projects_lujun/agentCLS/assets/logs/LDD_split_0.001_ModernBERT-base_output_03_11_14_30_16'"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from datasets.dataset_dict import DatasetDict, IterableDatasetDict\n",
    "from datasets.iterable_dataset import IterableDataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from datasets import ClassLabel\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "# Data preparation\n",
    "import torch\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "input_dataset_path = \"/home/snt/projects_lujun/agentCLS/assets/LDD_split.json\"\n",
    "model_path = \"answerdotai/ModernBERT-base\"\n",
    "train_ratio = 0.01\n",
    "checkpoint_dir = \"/home/snt/projects_lujun/agentCLS/assets/logs/LDD_split_0.001_ModernBERT-base_output_03_11_14_30_16\"\n",
    "\n",
    "input_dataset_name = input_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
    "model_name = model_path.split(\"/\")[-1]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dataset = pd.read_json(input_dataset_path, lines=True)\n",
    "dataset.rename(columns={\"cls_label\": \"labels\"}, inplace=True)\n",
    "sample_counts = dataset.groupby('labels').size() * train_ratio\n",
    "filtered_data = dataset.groupby('labels', group_keys=False).apply(lambda x: x.iloc[:int(sample_counts[x.name])])\n",
    "dataset = filtered_data.reset_index(drop=True)\n",
    "dataset = Dataset.from_pandas(dataset)\n",
    "\n",
    "train_dataset = dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "val_dataset = dataset.filter(lambda x: x[\"split\"] == \"validation\")\n",
    "\n",
    "# Prepare model labels - useful for inference\n",
    "labels =  set(train_dataset['labels'])\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "def get_last_checkpoints(output_dir):\n",
    "    checkpoints = os.listdir(output_dir)\n",
    "    checkpoints = [c for c in checkpoints if \"checkpoint\" in c]\n",
    "    checkpoints = [int(c.split(\"-\")[-1]) for c in checkpoints]\n",
    "    last_checkpoint = max(checkpoints)\n",
    "    return f\"{output_dir}/checkpoint-{last_checkpoint}\"\n",
    "\n",
    "checkpoints_path  = get_last_checkpoints(checkpoint_dir)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoints_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Initialize lists to store true and predicted labels\n",
    "true_label_one_hot_list = []\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "all_probs = []\n",
    "\n",
    "# Start time for measuring inference efficiency\n",
    "start_time = time.time()\n",
    "\n",
    "# Iterate through validation dataset and make predictions\n",
    "for input in tqdm(train_dataset, desc=\"Processing validation data\", unit=\"sample\"):\n",
    "    sample = input['content']\n",
    "    true_label = input['labels']\n",
    "    tokenized_input = tokenizer(sample, padding=True, max_length=max_length, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    model_output = model(**tokenized_input)\n",
    "    logits = model_output.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
    "    predicted_label = id2label[str(predicted_class_idx)]\n",
    "    \n",
    "    true_label_idx = int(label2id[true_label])\n",
    "    true_label_one_hot = np.zeros(probabilities.size(-1))\n",
    "    true_label_one_hot[true_label_idx] = 1\n",
    "\n",
    "    true_labels.append(true_label)\n",
    "    true_label_one_hot_list.append(true_label_one_hot)\n",
    "    predicted_labels.append(predicted_label)\n",
    "    all_probs.append(probabilities.detach().cpu().numpy())  # Store the raw probabilities for AUC\n",
    "\n",
    "# Calculate accuracy, F1 score, and AUC\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')  # weighted F1 score\n",
    "auc = roc_auc_score(np.array(true_label_one_hot_list),  np.squeeze(np.array(all_probs)), multi_class='ovr', average='weighted')  # for multi-class AUC\n",
    "\n",
    "# Calculate inference time (average time per sample)\n",
    "end_time = time.time()\n",
    "inference_time = (end_time - start_time) / len(train_dataset)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Average Inference Time per Sample: {inference_time:.4f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
