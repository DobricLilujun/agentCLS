{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 55000/55000 [01:13<00:00, 746.03 examples/s] \n",
      "Generating test split: 100%|██████████| 5000/5000 [00:42<00:00, 118.65 examples/s]\n",
      "Generating validation split: 100%|██████████| 5000/5000 [00:39<00:00, 125.78 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'celex_id': '32006D0213', 'text': 'COMMISSION DECISION\\nof 6 March 2006\\nestablishing the classes of reaction-to-fire performance for certain construction products as regards wood flooring and solid wood panelling and cladding\\n(notified under document number C(2006) 655)\\n(Text with EEA relevance)\\n(2006/213/EC)\\nTHE COMMISSION OF THE EUROPEAN COMMUNITIES,\\nHaving regard to the Treaty establishing the European Community,\\nHaving regard to Directive 89/106/EEC of 21 December 1988, on the approximation of laws, regulations and administrative provisions of the Member States relating to construction products (1), and in particular Article 20(2) thereof,\\nWhereas:\\n(1)\\nDirective 89/106/EEC envisages that in order to take account of different levels of protection for construction works at national, regional or local level, it may be necessary to establish in the interpretative documents classes corresponding to the performance of products in respect of each essential requirement. Those documents have been published as the ‘Communication of the Commission with regard to the interpretative documents of Directive 89/106/EEC’ (2).\\n(2)\\nWith respect to the essential requirement of safety in the event of fire, interpretative document No 2 lists a number of interrelated measures which together define the fire safety strategy to be variously developed in the Member States.\\n(3)\\nInterpretative document No 2 identifies one of those measures as the limitation of the generation and spread of fire and smoke within a given area by limiting the potential of construction products to contribute to the full development of a fire.\\n(4)\\nThe level of that limitation may be expressed only in terms of the different levels of reaction-to-fire performance of the products in their end-use application;\\n(5)\\nBy way of harmonised solution, a system of classes was adopted in Commission Decision 2000/147/EC of 8 February 2000 implementing Council Directive 89/106/EEC as regards the classification of the reaction-to-fire performance of construction products (3).\\n(6)\\nIn the case of wood flooring and solid wood panelling and cladding it is necessary to use the classification established in Decision 2000/147/EC.\\n(7)\\nThe reaction-to-fire performance of many construction products and/or materials, within the classification provided for in Decision 2000/147/EC, is well established and sufficiently well known to fire regulators in Member States that they do not require testing for this particular performance characteristic.\\n(8)\\nThe measures provided for in this Decision are in accordance with the opinion of the Standing Committee on Construction,\\nHAS ADOPTED THIS DECISION:\\nArticle 1\\nThe construction products and/or materials which satisfy all the requirements of the performance characteristic ‘reaction to fire’ without need for further testing are set out in the Annex.\\nArticle 2\\nThe specific classes to be applied to different construction products and/or materials, within the reaction-to-fire classification adopted in Decision 2000/147/EC, are set out in the Annex to this Decision.\\nArticle 3\\nProducts shall be considered in relation to their end-use application, where relevant.\\nArticle 4\\nThis Decision is addressed to the Member States.\\nDone at Brussels, 6 March 2006.', 'labels': [1, 20, 7, 3, 0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('multi_eurlex', 'en', trust_remote_code=True)\n",
    "\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celex_id :  32003R1330\n",
      "celex_id\n",
      "text :  Commission Regulation (EC) No 1330/2003\n",
      "of 25 July 2003\n",
      "establishing the standard import values for determining the entry price of certain fruit and vegetables\n",
      "THE COMMISSION OF THE EUROPEAN COMMUNITIES,\n",
      "Having regard to the Treaty establishing the European Community,\n",
      "Having regard to Commission Regulation (EC) No 3223/94 of 21 December 1994 on detailed rules for the application of the import arrangements for fruit and vegetables(1), as last amended by Regulation (EC) No 1947/2002(2), and in particular Article 4(1) thereof,\n",
      "Whereas:\n",
      "(1) Regulation (EC) No 3223/94 lays down, pursuant to the outcome of the Uruguay Round multilateral trade negotiations, the criteria whereby the Commission fixes the standard values for imports from third countries, in respect of the products and periods stipulated in the Annex thereto.\n",
      "(2) In compliance with the above criteria, the standard import values must be fixed at the levels set out in the Annex to this Regulation,\n",
      "HAS ADOPTED THIS REGULATION:\n",
      "Article 1\n",
      "The standard import values referred to in Article 4 of Regulation (EC) No 3223/94 shall be fixed as indicated in the Annex hereto.\n",
      "Article 2\n",
      "This Regulation shall enter into force on 26 July 2003.\n",
      "This Regulation shall be binding in its entirety and directly applicable in all Member States.\n",
      "Done at Brussels, 25 July 2003.\n",
      "text\n",
      "labels :  [2, 17]\n",
      "labels\n"
     ]
    }
   ],
   "source": [
    "for key in dataset['train'][1].keys():\n",
    "    print(key,\": \", dataset['train'][1][key])\n",
    "    print (key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data From LDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read 33388 documents\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd  # Import pandas for DataFrame manipulation\n",
    "\n",
    "\n",
    "def read_folder_structure(root_dir='.'):\n",
    "    data = []  # Initialize an empty list to store document information\n",
    "    index = 0  # Initialize a unique index for each document\n",
    "    \n",
    "    # Mapping of class labels to their descriptions\n",
    "    cls_descriptions = {\n",
    "        \"cs.AI\": \"Artificial Intelligence\",\n",
    "        \"cs.CE\": \"Computational Engineering\",\n",
    "        \"cs.CV\": \"Computer Vision\",\n",
    "        \"cs.DS\": \"Data Structures\",\n",
    "        \"cs.IT\": \"Information Theory\",\n",
    "        \"cs.NE\": \"Neural and Evolutionary\",\n",
    "        \"cs.PL\": \"Programming Languages\",\n",
    "        \"cs.SY\": \"Systems and Control\",\n",
    "        \"math.AC\": \"Commutative Algebra\",\n",
    "        \"math.GR\": \"Group Theory\",\n",
    "        \"math.ST\": \"Statistics Theory\"\n",
    "    }\n",
    "    \n",
    "    # Traverse all folders in the root directory\n",
    "    for cls_label in os.listdir(root_dir):\n",
    "        cls_path = os.path.join(root_dir, cls_label)\n",
    "        \n",
    "        if os.path.isdir(cls_path):  # Check if the item is a directory\n",
    "            # Traverse all .txt files in the current folder\n",
    "            for filename in os.listdir(cls_path):\n",
    "                if filename.endswith('.txt'):  # Process only .txt files\n",
    "                    file_id = filename[:-4]  # Remove the .txt extension to get the unique file ID\n",
    "                    file_path = os.path.join(cls_path, filename)\n",
    "                    \n",
    "                    # Read the content of the file\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                    \n",
    "                    # Append structured data to the list\n",
    "                    data.append({\n",
    "                        'index': index,         # Unique index for each document\n",
    "                        'cls_label': cls_label, # Class label (folder name)\n",
    "                        'description': cls_descriptions.get(cls_label, \"\"),  # Class description\n",
    "                        'file_id': file_id,     # Unique file ID (filename without extension)\n",
    "                        'content': content      # File content\n",
    "                    })\n",
    "                    \n",
    "                    index += 1  # Increment the unique index\n",
    "    \n",
    "    # Convert the list of dictionaries into a Pandas DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "dataset_df = read_folder_structure(\"/home/snt/projects_lujun/agentCLS/assets/LDD/Long-document-dataset/extracted_dataset\")\n",
    "print(f\"Successfully read {len(dataset_df)} documents\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cls_label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "description",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "file_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "799cce6b-efc1-4d98-bc67-84a2e091c09e",
       "rows": [
        [
         "0",
         "0",
         "cs.CE",
         "Computational Engineering",
         "0902.0763v1.pdf",
         "Genetic algorithm based optimization and post\noptimality analysis of multi-pass face milling\nSourabh K. Saha*\nDepartment of Mechanical Engineering\nIIT Kanpur\nKanpur, 208016, India\n\nAbstract:\nThis paper presents an optimization technique for the multi-pass face milling process.\nGenetic algorithm (GA) is used to obtain the optimum cutting parameters by minimizing\nthe unit production cost for a given amount of material removal. Cutting speed, feed and\ndepth of cut for the finish and rough passes are the cutting parameters. An equal depth of\ncut for roughing passes has been considered. A lookup table containing the feasible\ncombinations of depth of cut in finish and rough passes is generated so as to reduce the\nnumber of variables by one. The resulting mixed integer nonlinear optimization problem\nis solved in a single step using GA. The entire technique is demonstrated in a case study.\nPost optimality analysis of the example problem is done to develop a strategy for\noptimizing without running GA again for different values of total depth of cut.\nKeywords: Multi-pass face milling; Optimization; Genetic algorithm; Mixed integer\nnonlinear problem; Post optimality analysis.\n\n1. Introduction\nOptimization of cutting parameters in machining processes has been an important area of\nresearch. Starting from Taylor [1], there has been an increasing interest in machining\nprocess optimization [2-19]. Selection of optimum cutting conditions can lead to a\nsubstantial reduction in the operating costs. Commonly used objective functions are the\nminimization of production cost, the maximization of production rate and the\nmaximization of profit rate. Abuelnaga and El-Dardiry [4] discussed a number of\ntraditional optimization methods and highlighted the relative advantages and\ndisadvantages of the methods for solving the problems of machining economics. These\nobjectives can be represented in terms of the machining parameters such as cutting speed,\nfeed, depth of cut and the number of passes. During selection of the parameters, care\nmust be taken to ensure that the essential constraints are satisfied. Cutting force, power,\nsurface finish and tool life are some of the commonly considered constraints [5-8]. Initial\nresearch in machining process optimization focused on single pass operations [1-4].\nHowever, due to restrictions on the amount of material that can be removed in a single\npass, multi-pass operations are required [6-10]. Machining is done in two stages in multipass operations. Majority of material removal takes place in a series of rough passes in\nthe first stage. In the next stage a small amount of material is removed by a single finish\npass. Typically, two distinct approaches have been used for optimization of multi-pass\noperations. One is the equal depth of strategy in which the depth of cut in all the rough\n*Corresponding Author email: sourabh.k.saha@gmail.com\n\n\fpasses is considered to be same [11, 12]. The other is the unequal depth of cut strategy.\nTypically such problems are optimized in two steps [13-15]. First, the optimum cutting\nspeed and feed are obtained for all the possible depths of cut. Then, the total depth of cut\nis distributed optimally among the different rough and finish passes. Jawahir and Wang\n[16] have summarized the recent contributions to the field of machining process\nmodeling and optimization. They have also presented a machining process optimization\nmethod in which many process performances such as surface roughness, cutting force,\ntool life and material removal rate have been combined into a single objective using\nweight factors.\nMany researchers have dealt with the problem of machining parameters for turning\nprocess. Relatively less research has been done in optimization of multi-point cutting\noperations such as milling. Recently there have been quite a few reports in literature on\noptimization of milling operations [12, 14-19]. Sonmez et al. [12] used dynamic\nprogramming to obtain the optimum number of passes and geometric programming for\nobtaining the optimum cutting conditions. Their work showed that the performance of\nmulti-pass milling operations is better than that of single pass operations. Shunmugam et\nal. [14] minimized the production cost for rough passes and finish passes in two stages. In\nthe first stage, separate minimum costs for an individual rough and finish pass were\ndetermined and tabulated for various fixed values of depth of cut selected from a series of\ndepths. In the second stage, genetic algorithm (GA) was used for finding the optimum\nnumber of rough passes and allocation of total stock in each of the rough passes and the\nfinal pass to achieve a minimum total production cost. An and Chen [15] used a similar\ntechnique for first stage and integer programming for the second stage for solving the\nsame problem. The two stage strategy for optimization leads to a large number of\ncomputations and increases the computational time when there are a large number of\npasses to consider. Wang et al. [17] have presented a new methodology involving the use\nof GA for the selection of cutting conditions for multi-pass face milling operations based\non a comprehensive criterion of integrating the contributing effects of all major\nmachining performance measures. They first used Taguchi method for design of\nexperiments to predict machining performance measures and then utilized genetic\nalgorithms to optimize the cutting conditions. In their method, optimization of parameters\nin the rough and finish passes have been done simultaneously instead of the two stage\nprocedure. Since Genetic Algorithms suffer from the problem of premature convergence\nto local optimum, some researchers have focused on using Hybrid Genetic Algorithms in\nwhich a local search based optimization procedure is used in conjunction with GA for\noptimizing machining operations [18, 19]. Wang et al. [19] used genetic simulated\nannealing (GSA) for multi-pass end milling operation to minimize the production time.\nGSA combines the properties of genetic algorithms (GA) with those of Simulated\nAnnealing (SA) which is a local search based procedure. It was shown by them that the\nhybrid method produced results superior to those obtained by using GA alone.\nIn the present work, binary coded genetic algorithm (GA) with an elitist replacement\nstrategy has been used for minimization of unit production cost in a multi-pass face\nmilling operation. For a given amount of material removal (quantified by the total depth\nof cut), the optimum values for the cutting parameters have been obtained in a single step\n\n\fby using a lookup table method. The lookup table is generated by considering all the\nfeasible combinations of depths of cut in the rough and finish passes for a given total\ndepth of cut. All rough passes have been considered to be of equal depth of cut. Using\nGA, the cutting speed, feed and depth of cut in the finish and rough passes have been\noptimized simultaneously. The face milling operation and the cutting process model have\nbeen discussed in Section 2. In Section 3, the optimization technique based on GA has\nbeen explained. The technique has been demonstrated in a case study discussed in\nSection 4. Results obtained from the proposed technique have been compared with\nreported values of Shunmugam et al. [14] and An and Chen [15]. A numerical test has\nbeen performed to verify the convergence of the GA to the global optimum. Subsequently\na constraint sensitivity analysis has been performed. This analysis is helpful in deciding\non the relative importance of the constraints when the constraint limits are flexible.\nThrough an analysis of the optimization results, an estimation strategy has been\ndeveloped for quickly obtaining the optimum values without running GA, when the total\ndepth of cut is changed. Although a multi-pass face milling process has been chosen as an\nexample in this work, the lookup table based GA technique discussed here can be easily\napplied to other multi-pass machining processes.\n\n2. Problem Formulation\n2.1.\n\nProcess Schematic\n\nIn face milling, the cutter is mounted on a spindle having an axis of rotation\nperpendicular to the work piece surface. The milled surface results from the action of\ncutting edges located on the periphery and face of the cutter.\nIn multi-pass milling the entire material is not removed in a single pass but in a series of\npasses which involves multiple rough passes and a final finishing pass.\nThe schematic representation of the cutting process is shown in Fig. 1.\nThe cutting parameters can be chosen to be different in each of the individual passes.\nHowever for simplification, all rough passes have been considered to be identical. Thus\nwe have two sets of cutting parameters (speed, feed, and depth of cut), one for all the\nrough passes and one for the finish pass. Along with this, we have the number of rough\npasses as an additional variable. In this paper, for a given total depth of cut the unit\nproduction cost has been minimized to obtain the optimum values of the cutting\nparameters.\nIn rough machining, the length of the cutter travel is given by\n\nLtr = L + a p + er\n\n(1)\n\nWhere L is the length of the work piece, ap is the approach and er,s is the extra-travel of\nthe cutter at the ends, normally taken to be 2–5 mm. The approach distance for\nsymmetrical milling is given as\n\n\f2\n\nap =\n\n2\n\nD\n⎛D⎞ ⎛B⎞\n− ⎜ ⎟ − ⎜ ⎟ Lts = L + D + es\n2\n⎝ 2⎠ ⎝2⎠\n(2)\n\nIn finish machining,\nLts = L + D + es\n\n(3)\n\nas the cutter has to completely clear the work piece length [20].\n\n2.2.\n\nCutting Process Model\n\n2.2.1. Decision Variables\nUnit production cost depends on the cutting parameters used for machining. The cutting\nparameters which can be controlled are the cutting speed (Vs, Vr), feed (fr, fs) and depth\nof cut (ds, dr) for the finish and rough passes respectively and the number of rough passes\n(n). All these parameters are chosen as the decision variables.\n\n2.2.2. Objective Function\nThe unit production cost (UC) can be represented as:\n\nUC= CM + CI + CR + CT\n(4)\nWhere\nCM, CI, CR and CT are actual machining cost, machine idle cost, tool replacement cost\nand tool cost respectively [6]. These can be represented as:\nCM = k0 (tms + ntmr )\nWhere\n(d − d s )\nn= t\ndr\nπ DLts\ntms =\n1000Vs f s Z\nπ DLtr\ntmr =\n1000Vr f r Z\n\n(5)\n\n(6)\nCI = k0tl\ntl = t p + ti\ntl = t p + n(h1 Ltr + h2 ) + (h1 Lts + h2 )\n\nCI = ko [t p + n(h1 Ltr + h2 ) + (h1 Lts + h2 )]\n\n(7)\n\n\ftms\nt\n+ n(kote Z mr )\nTs\nTr\nt\nt\nCT = kt Z ms + n(kt Z mr )\nTs\nTr\nCR = ko te Z\n\n(8)\n(9)\n\nThe tool life in finish and rough passes (Ts and Tr respectively) for face milling as\ndiscussed by Nefedov [20] can be expressed as:\nC K D qv\nT l = xv v yvv sv pv\nVd f B Z\nWhere, Cv and Kv are constants, D is the cutter diameter, B is the width of cut, Z is the\nnumber of teeth in the cutter; l, pv, qv, sv, xv, yv are constant exponents.\nFor a given cutter and fixed width of cut, the tool life for finish and rough conditions can\nbe simplified in terms of the cutting parameters as:\nC\nTs = n1 no2 n3\nVs d s f s\nC\nTr = n1 no2 n3\n(10)\nVr d r f r\n⎛ C K D qv ⎞\nWhere the constant Co = ⎜ v sv v pv ⎟\n⎝ B Z ⎠\n\n1\n\nl\n\nx\ny\n1\nand, n1 = , n2 = v , n3 = v\nl\nl\nl\n\nThus, the objective can be stated as\nMinimize UC = UCs + nUCr + kot p\nWhere\nas\n+ bsVs n1 −1d s n2 f s n3 −1 + cs\nVs f s\na\n(11)\nUCr = r + brVr n1 −1d r n2 f r n3 −1 + cr\nVr f r\nas, bs, cs, ar, br and cr are constants with units such that when ‘V’ is in m/min, ‘f’ in\nmm/tooth and ‘d’ in mm gives UC in $/piece.\nUCs =\n\n2.2.3. Constraints\nVariable Bounds\nThe parameters are allowed to vary in a limited range of values determined by the cutting\ntool manufacturer and the machine specifications.\n-\n\nCutting Velocity\nVs ,min ≤ Vs ≤ Vs ,max\nVr ,min ≤ Vr ≤ Vr ,max\n\n(12)\n\n\f-\n\nFeed\n\nf s ,min ≤ f s ≤ f s ,max\nf r ,min ≤ f r ≤ f r ,max\n\n(13)\n-\n\nDepth of cut\nd s ,min ≤ d s ≤ d s ,max in steps of d s,step\nd r ,min ≤ d r ≤ d r ,max in steps of d r,step\n\n(14)\nInequality constraints\n- Force constraint\nFor milling process cutting force is given by\np\nC f K f B s f Z f d n4 f n5\nF=\nq\nD f\nWhere, Cf, Kf are constants; sf, pf, qf, n4, n5 are constant exponents. Representing the\nforce in terms of only the machining conditions:\n\nF = C1d n4 f n5\n\n(15)\ns\n\nWhere the constant C1 =\n\nCf K f B Z\nD\n\nf\n\npf\n\nqf\n\nTherefore,\nFs = C1d sn4 f sn5 ≤ Fmax\n\nFr = C1d rn4 f rn5 ≤ Fmax\nWhere Fmax is obtained from machine tool limitations.\n\n(16)\n\n-\n\nCutting power constraint\nFV\n≤ Pmax\n(17)\nP=\n6120η\nNeglecting contribution of feed force to power (due to low values of feed rate as\ncompared to the cutting velocity) and using expression for force F from Eq. 15\nC1\nP = C2Vd n4 f n5 , where C2 =\n6120η\nHence,\nC2Vs d sn4 f sn5 ≤ Pmax\nC2Vr d rn4 f rn5 ≤ Pmax\n\n(18)\n-\n\nSurface finish constraint\n\nFollowing Boothroyd [21], the surface finish for milling process can be expressed by:\n\n\ff2\nre\nAnd the surface requirement constraint is:\nRa ≤ Rmax\nRa = 0.0321\n\n(19)\n\nThis can be reformulated in terms of feed as:\n\nfs ≤\n\nRs ,max re\n\nfr ≤\n\nRr ,max re\n\n0.0321\n0.0321\n\nThis constraint can therefore be merged with the variable bounds for feed as:\nRs ,max re\n)\nf s ,min ≤ f s ≤ min( f s ,max ,\n0.0321\nf r ,min ≤ f r ≤ min( f r ,max ,\n\nRr ,max re\n0.0321\n\n)\n\n(20)\nEquality Constraint:\nTotal depth of cut\nThe depth of cut in finish pass (ds) and rough passes (dr) are related to the total depth of\ncut (dt) by:\n\ndt = d s + nd r where n is a positive integer\n\n(21)\n\n3. GA Implementation\nThe multi-pass milling model used here is based on 7 process variables, speed, feed and\ndepth of cut for finish pass and rough passes and the number of rough passes (Vs, fs, ds,\nVr, fr, dr and n). Out of these only 6 are independent since ds, dr and n are related by Eq.\n21. The number of variables can be further reduced to 5 by using both the constraints Eq.\n14 and Eq. 21. This is done by generating a lookup table which consists of all possible\nfinish and rough depth of cut (ds, dr respectively) pairs for a given total depth of cut (dt).\nSince ds and dr can take only discrete values from a given set as given by Eq. 14, the\nnumber of pairs of (ds,dr) which can be used to remove dt is limited and is obtained by\nusing Eq. 21. The 5th variable then denotes the position of a (ds,dr) pair in this lookup\ntable of feasible (ds,dr) pairs, with Vs, Vr, fs, fr as the first 4 variables. This converts the\nproblem into a mixed integer nonlinear problem (MINLP) with four continuous variables\nand one discrete variable.\n\n\fBinary coded genetic algorithm with an elitist strategy for replacement is used in this\ntechnique. It operates on the principle of the “survival of the fittest” [22, 23]. With\nMINLP problems, GA generally suffers from the prematurity problem and may require\nmany runs to avoid the trap of local minima [24]. However, the problem being\ninvestigated is a special case of the general MINLP problem as the constraints are\nindependent of the integer variable (position in the lookup table). With a suitable fine\ntuning of the parameters for the genetic operators it is possible to obtain the global\noptimum results using the proposed technique. A block diagram of the technique is\npresented in Fig. 2. The technique is described below.\n\nBinary Coding\n\n3.1.\n\nIn binary coded GA, a binary string is used as solution string to represent real values of a\nvariable. The length of the string depends on the precision required [22]. To obtain the\nactual values of the variables from their binary encoding the following relationship is\nused:\n( x UB − x LB )\nxi = xi LB + DV i li i\n2 −1\nwhere\nxiU , LB is upper and lower bound of i th variable\nli is length of i th string\nDV is the Decimal Value of the string\nDV =\n\nj =li −1\n\n∑b2\nj =0\n\nj\n\nj\n\n, b j is the bit at position j of the binary string\n\n(22)\n\n3.2.\n\nSolution Representation\n\nVs, Vr, fs, fr are the first 4 variables and the 5th variable is the position of the feasible (ds,\ndr) pair in the lookup table. The first 4 variables are represented by 15 bit binary strings\nwhereas the length of the binary string for 5th variable depends on the table size. All the\nvariable strings are combined to form a single binary string representing the member.\n\n3.3.\n\nInitialization\n\nThe GA is performed using a population size of N=750. Based on the schema theory for\nbinary coded GA, the population size N is chosen by maximizing the rate of gain of the\nnumber of schemata represented by binary string of a particular length [22]. In the current\nstudy N is chosen to be 750 as it is found that the gain saturates around this value. For\ninitialization 750 different binary strings are created randomly. To create a string, for\neach bit position a random number is generated in the range [0, 1.0]. The bit is assigned a\nvalue ‘0’ if the number turns out to be less than or equal to 0.5 and assigned ‘1’\notherwise.\n\n\f3.4.\n\nEvaluation\n\n3.4.1. Fitness Value\nTo obtain the fitness value corresponding to a member string, first the solution string is\nbroken down into its blocks corresponding to each variable. The variable values are then\nobtained using the binary coding formula Eq. 22. The decoded value for the first four\nblocks directly gives the values for Vs, fs, Vr and fr respectively. The depths of cut values\nare decoded by using the fifth variable (as the position in the lookup table) to pick up the\ncorresponding (ds,dr) pair from the lookup table. The objective function is then evaluated\nusing Eq. 11. The objective function value is used directly as the fitness value for the\nmember.\n\n3.4.2. Constraints Handling\nTo handle constraints, each solution is assigned a constraint violation (CV) value such\nthat all feasible solutions have a zero CV value. For the infeasible solutions it has a non\nzero value. CV is obtained as:\nJ\n\nK\n\nj =1\n\nk =1\n\nCV = ∑ 〈 g j ( x)〉 + ∑ | hk ( x) |\nwhere\n⎧−α if α <0\n〈α 〉 = ⎨\n⎩0 if α >0\nSuch that the constraints are normalized and in the form:\ng j ( x) ≥ 0, j = 1, 2,..., J\nhk ( x) = 0, k = 1, 2,..., K\n\n(23)\nIn this technique, only the inequality constraints are used for CV evaluation (Eq. 16, Eq.\n18 and Eq. 20) as the equality constraint (Eq. 21) is automatically satisfied when the\nlookup table for feasible (ds,dr) pairs is generated.\n\n3.5.\n\nSelection\n\nBinary Tournament selection has been used for selection. The selection has been done\nbased upon three possible cases. For the case when both the solutions are feasible, the\none with lower function value (i.e. better fitness) is selected. When both the solutions are\n\n\funfeasible, the one with lower constraint violation is selected. When one of the solutions\nis feasible and the other infeasible, the feasible solution is selected. For picking up two\nsolutions for comparison, the population is divided into two parts midway and then a\nsolution occupying same position from each part is picked. The population obtained after\nselection operation is termed as P’.\n\n3.6.\n\nGenetic Operators\n\nThe crossover operation is performed on the population P’ obtained after selection. A two\npoint crossover has been used with a high crossover probability pc =0.8 . A two point\ncrossover with the high crossover probability is used as it helps the diversity preservation\nbetter than the single point one [23]. After crossover, mutation of the population is\nperformed. Bit wise mutation operator has been used with a mutation\nprobability pm =0.05 . In the present study it is found that mutation probability lower than\n0.05 leads to premature convergence to a local optimum point due to loss of diversity in\nthe population. Details of the fine tuning tests performed for obtaining the values of\ngenetic parameters are discussed in section 4.1.\n\n3.7.\n\nReplacement Strategy: Elitist\n\nOnce crossover and mutation is performed over the selected population, the intermediate\npopulation is merged with the original population and then the best N out of the 2N\nmembers are chosen to entirely replace the original population. This process ensures that\nthe best members of the merged population are not lost and better solutions move into the\nnext generation. Though all members in a population can expect to have a lifetime of one\ngeneration, members with a higher fitness can have a longer lifetime when elitist\nstrategies are used [25]. This leads to a higher rate of convergence of the algorithm.\nIn this work, the best N solutions are identified through the following process:\nFirst, the original population and new populations (obtained after selection, crossover,\nand mutation) are merged and separated into two distinct portions, a feasible one and an\ninfeasible one. Next, the following cases are considered:\nCase 1: Number of feasible solutions > N\nThe feasible part of the merged population is sorted in increasing order of the objective\nfunction value. As we are interested in minimizing the objective function, the top N\nmembers are then picked to replace the original population.\nCase 2: Number of feasible solutions < N\nAll the feasible solutions are picked to form the initial population for next generation. But\nin order to maintain a constant size population some infeasible solutions must also be\nincluded. For this the infeasible portion of the merged population is sorted in increasing\norder of the constraint violation (CV) value. As many of these as required to complete the\npopulation are then picked for the next generation.\n\n\fCase 3: Number of feasible solutions = N\nAll the feasible solutions are picked to replace the original population.\n\nConvergence Criterion\n\n3.8.\n\nAll the steps from selection to replacement are repeated for a fixed number of\ngenerations. The appropriate number of generations is obtained by observing the number\nof generations for a few trial runs when the difference between the best member and the\npopulation average falls below a threshold value. This value is then used for all other\nfuture runs. In the current study the GA is run for 100 generations.\n\n4. Case Study\nIn this section, the proposed method has been illustrated in a case study. The data for the\nexample is provided in Table 1, which is same as that used by Shunmugam et al. [14] and\nAn and Chen [15]. Section 4.5 compares the performance of the proposed optimization\nscheme with the two stage optimization method of Shunmugam et al. and An and Chen.\nShunmugam et al. used GA to solve the problem in two stages. In their method, an\n“unequal depth of cut” strategy is adopted. In the first stage, solutions corresponding to\nseparate minimum costs for the individual rough and finish passes are determined and\ntabulated for various fixed values of the depth of cut. In the second stage, the optimum\nnumber of passes and the optimum subdivision of the depths of cut for different passes\nare obtained using GA. An and Chen used a similar method for the first stage and an\ninteger programming based method for the second stage.\n\n4.1.\n\nSelection of GA parameter values\n\nGA is performed with a population size of N=750. The value of N is based on the schema\ntheory for binary coded GA [22]. For binary coded GA, the number of schemata (S)\nrepresented by a population of size ‘μ’ consisting of binary strings of length ‘l’ is given\nby:\ni μ\nl\n⎛ l ⎞ i ⎧⎪ ⎡ ⎛ 1 ⎞ ⎤ ⎫⎪\nS ( μ , l ) = ∑ ⎜ ⎟ 2 ⎨1 − ⎢1 − ⎜ ⎟ ⎥ ⎬\n(24)\ni =0 ⎝ i ⎠\n⎪⎩ ⎢⎣ ⎝ 2 ⎠ ⎥⎦ ⎪⎭\nThe population size (N) is chosen so that all points in the search space are represented by\nthe initial set of schemata. For this purpose the rate of gain in the number of represented\nschemata with an increase in population size is maximized. The rate of gain is given by:\nS ( μ , l ) − 2l\nG=\n(25)\n\nμ\n\n\fFor a binary string length of 65 we calculated G by varying μ in the range [1 5000]. It is\nfound that the maximum value of G is obtained at μ=5000 (Gmax=3.4885e19). Since this\nvalue is very high, a practical value of N is chosen corresponding to the point where\nG=0.999Gmax. This is obtained at μ=750. Hence, a population size (N) of 750 is chosen.\nFor cross-over probability (pc), a trial value of 0.8 is chosen. With these values of N and\npc and a high value of mutation probability (pm=0.1), GA is run and evolution of the\npopulation average fitness and best fitness value is observed. For one value of total depth\nof cut (dt=6 mm), results are shown in Fig. 3. From this plot it can be seen that the\naverage fitness value approaches the population best as the generation increases. GA is\nrun several times for the same conditions and the generation number corresponding to the\ngeneration when the difference between population average and population best falls\nbelow 0.25% is observed. It is found to be in the 80-90 range. A conservative value of\n100 is hence chosen for the number of generations needed for convergence. It is expected\nthat with lower values of mutation probability, convergence would be faster.\nTo determine the value of pm, variation of success rate with change in pm was observed.\nThe success rate here is defined as the percentage of runs for which the GA converged to\nthe global optimum after 100 generations. At each value of pm, 20 GA runs were\nperformed with different initial populations to determine the success rate. From Fig. 4 it\ncan be seen that for pm=0.05 onwards, the success rate is 100%. Hence, pm=0.05 has been\nselected for solving the problem.\n\n4.2.\n\nObtaining look-up table\n\nFor a total depth of cut (dt), several combinations of rough and finish cut depths (dr and ds\nrespectively) can be obtained which give the same total depth of cut, subject to the\nconstraint Eq. 21. This is done by first making all combinations of (ds,dr) pairs from the\nd − ds\nrange of possible values. Then, for each pair it is checked whether the quantity t\nis\ndr\nan integer. The integer value represents the number of rough passes. This condition is\nequivalent to Eq. 21. A feasible solution is one for which this condition is true. The\nfeasible (ds,dr) pairs for dt=6mm are shown in Table 2. This set of feasible (ds,dr) pairs\nforms the lookup table discussed in section 3. Each feasible solution is identified by the\ncorresponding pair number in the lookup table. Instead of considering dr and ds as two\ndistinct variables, this pair number is considered as the variable for GA. This reduces the\nnumber of variables by one. The number of feasible (ds,dr) pairs depends on the value of\ndt and for each dt a new lookup table is generated which satisfies the constraint Eq. 21 for\nthe corresponding dt.\n\n4.3.\n\nComputation Results\n\nGA was performed on the above problem with different total depths of cut (dt). The\nalgorithms were coded in Visual C++ and run on a Pentium 4 PC. After 100 generations\nof GA run, the member with lowest objective function value in the population was\nchosen as the optimum value. The corresponding optimum machining parameters for\ndifferent total depths of cut are tabulated in Table 3. The tool life for rough and finish\n\n\fpasses can be obtained from the optimum machining conditions by using the tool life\nequation (Eq. 10). The zero values for constraint violation (CV) suggest that all the\nsolutions are feasible and do not violate any of the constraints.\n\n4.4.\n\nVerification of global optimum\n\nFor the problem of depth of cut distribution into multiple passes, if we disregard the\noptimization criterion of minimizing the unit cost, then we can have several ways of\ndistributing the total depth of cut into ‘n’ number of rough passes and a single finish pass.\nAll such choices of (ds,dr) pairs are tabulated in the look-up table before starting with the\nGA optimization. Now, if we select any one of these set of rough and finish depth of cut,\nwe can separately optimize the cutting velocity and feed for the rough and finish passes.\nThe unit cost for finish pass (UCs) and unit cost for rough pass (UCr) can be separately\nminimized. As the depths of cut are already known, only two variables are involved\n(cutting speed and feed). Thus for each pair of feasible (dr,ds) pair, a pair of cutting\nvelocity and feed can be obtained for minimum UCs and UCr. In this way, we can have\nseveral optimum solutions for unit cost UC (Eq. 11) corresponding to each feasible set of\n(dr,ds). However, not all of these unit costs will have the same value. The solution\ncorresponding to lowest unit cost among these is then called as the global optimum and\nall other solutions are the respective local optimum.\nTo verify whether the results obtained from GA are the global optimums, the local\nminimum UC corresponding to each combination of feasible (dr,ds) pair for dt= 6mm is\ncalculated. Optimization of UCs and UCr has been done analytically by obtaining the\nderivates of the functions UCs and UCr in terms of the cutting velocity and feed and using\nthe classical conditions for minimization of two variable functions. UC is then calculated\nby using Eq. 11. The local optimum values so obtained are shown in Table 4. It can be\nseen that the solution obtained from the GA runs for dt= 6mm shown in Table 3\n(UC=1.4108 $/piece) matches closely with the global optimum (UC=1.4102 $/piece) as\nshown in Table 4.\n\n4.5.\n\nComparison with other schemes\n\nA comparison of the proposed scheme with the results of An and Chen [15] and\nShunmugam et al. [14] is made in Table 5. The table shows that the minimum unit\nproduction cost obtained in the current study are lower than those reported by An and\nChen and Shnumugam et al. For dt=8 mm the GA results are better than those of An and\nChen by 5.2% and better by 14% than those of Shunmugam et al. It is to be noted that,\nwhile calculating the cutting speeds at each pass, both Shunmugam et al. and An and\nChen have used an a priori value of the tool life (Tr=Ts=240 min.) to reduce the number\nof variables in the first stage. Because of this assumption about tool life, non-optimum\ncutting velocities are obtained by them for the cases where the cutting velocity obtained\nby their method does not violate the power constraint. In our proposed single stage\nmethod, the tool life values are obtained a posteriori, after the optimum conditions are\nobtained. From Fig. 5 it is evident that for the optimum conditions, the tool life for rough\npasses (Tr) is not constant and shows wide variations with changes in total depth of cut\n(dt). The reason for this is discussed in section 4.7 with reference to the relationship\n\n\famong the cutting parameters for the optimum conditions. The tool life for finish pass\nshows very small variations with changing dt, however the average value for optimum\nconditions is found to be 220 min. as opposed to the Ts=240 min. value used by\nShunmugam et al. It is interesting to note that for the optimum conditions, the tool life in\nrough passes is much higher than the tool life in the finish pass. This can be explained by\nthe higher values of the cutting velocity for finish pass for optimum conditions (Table 3).\nDue to high feed in the rough passes, the cutting velocities of rough passes are limited by\nthe power constraint (Eq. 18) to lower values.\n\n4.6.\n\nPost optimality constraint sensitivity analysis\n\nA study was made to observe how the optimal values change when the force and power\nconstraints are changed. Optimal unit production costs were obtained by multiplying a\nfactor to the maximum allowable force (Fmax) and maximum allowable power (Pmax). The\nresults were used to compare the sensitivity of the solution with respect to these\nconstraints. As expected, the optimal costs are lower when the constraints are relaxed, i.e.\nwhen the maximum allowable force or power limits are higher. From Fig. 6 it can be\nobserved that the optimal cost is more sensitive to the power constraint. From the slope it\ncan be seen that the rate of change of the optimal cost with changes in Pmax is higher than\nthat for the rate of change of optimal cost with the changes in Fmax. This information is\nespecially useful in the cases where estimated or approximate values for the maximum\nforce and power limits are used and a range of values for Fmax and Pmax are possible.\nBased on Fig. 6, given a choice of relaxing the limits, the power constraint must be\nrelaxed first in order to obtain lower optimal costs. Such constraint sensitivity analysis\ncan be easily done when a GA based method is used for optimizing the speed and feed\nalong with the depth of cut. It is difficult to do the same using classical methods such as\nthe integer programming method used by An and Chen [15] or the two stage method used\nby Shunmugam et al. [14].\n\n4.7.\nAnalysis of relationship in decision space for optimum\nconditions\nTo investigate the relationship among the optimum cutting conditions, optimal cost\nvalues and total depth of cut (dt), the optimum cutting conditions and the optimal cost\nwere obtained for different dt. The optimal costs are shown in Fig. 7. A similar stepped\nform nature can be seen when the number of rough passes (n) for the optimum conditions\nis plotted versus dt (Fig. 8). This stepped nature can be explained if the effect of increase\nin ‘n’ on optimal costs is more than the effect of increase in the depth of rough cut (dr).\nThis suggests that for optimum conditions, the number of rough passes must be as low as\npossible subject to the available (ds,dr) combinations for a given dt.\nWhen the depth of cut in rough passes (dr) is plotted against the total depth of cut we\nagain find a stepped form plot (Fig. 9). For the same number of rough passes, the optimal\ncost increases with the increase in dr.\nWhen the feed rate for the rough pass (fr) is plotted against the total depth of cut we find\na stepped plot but with the opposite trend (Fig. 10). For the same ‘n’, optimal cost\n\n\fincreases with decreasing feed rate. This figure explains the nature of the Tr versus dt plot\n(Fig. 5) since the tool life increases with decrease in the feed rate. Although the depth of\ncut (dr) increases, the effect of increased feed rate is more dominant because of the higher\nexponent of feed rate in extended Taylor’s tool life equation (Eq. 10).\n\n4.8.\n\nStrategy for estimating the optimum conditions\n\nFrom Table 3 it is evident that for the optimum conditions, feed rate for finish pass and\ncutting velocity for finish and rough passes remain almost unchanged with changes in the\ntotal depth of cut (dt). The feed rate for finish pass is restricted to a fixed value because of\nthe surface roughness constraint (Eq. 20). Similarly, the cutting velocities in finish and\nrough passes are restricted to fixed values by the power constraint (Eq. 18). This\ninformation along with the relationships among the decision variables for optimum\nconditions discovered in Section 4.7 can be used to develop an optimization strategy\nspecific to this problem. This strategy allows us to quickly estimate the optimum\nconditions when the total depth of cut is changed without running the GA again for the\nnew dt.\nTo start with, we can easily determine the number of rough passes (n) in the optimum\ncondition by finding out the smallest possible integer value for ‘n’, given the limits for ds\nand dr.\nn =[\n\ndt − d s ,max\nd r ,max\n\n], where [] is the greatest integer operator\n\n(26)\n\nFrom the possible (ds,dr) combinations for the given dt, the one which has ‘n’ number of\nrough passes is chosen. From Section 4.7, for the optimal conditions the depth of cut in\nrough pass should be the lowest one if multiple (ds,dr) combinations are possible for the\nsame ‘n’. Thus, we have the parameters ‘n’, ‘ds’ and ‘dr’.\nNext, we use the average values from Table 3 for the feed rate of finish pass (fs=0.279\nmm/tooth), the cutting velocity of finish pass (Vs=123.2 m/min) and rough passes\n(Vr=60.35 m/min).\nFinally, it is known that the cost of a single pass operation decreases with the increase of\nthe feed rate [6]. Hence for the rough pass feed rate (fr) the maximum possible value is\nchosen subject to constraints given by Eq. 16 and Eq. 20, i.e.\n\n{\n\nf r =min f r,max , (R r,max re )/0.0321, f r*\n\n}\n\n(27)\n\nWhere fr* is obtained from the upper limit of the constraint equation (Eq. 16).\nThe above procedure is used for estimating the optimum cost for dt=11.5 mm. The\noptimum values obtained by this strategy and by the GA run are tabulated in Table 6. It\ncan be seen that the cost obtained by this strategy (UC=2.2194 $/piece) is in close\nagreement with the cost obtained by GA (UC= 2.1995 $/piece). The value obtained by\nthe estimation strategy is poorer from the value obtained from GA by not more than 1%.\n\n\fThis optimization strategy can be used to quickly obtain an estimate of the optimum\ncutting conditions when the total depth of cut is changed. However, it is to be noted that\nthis strategy is applicable only to the specific problem under consideration and does not\nwork if the parameters of the problem such as number of teeth of cutter or length of work\npiece are changed. If such a strategy is required for the new problem, then the\nrelationships among the optimum conditions must be re-evaluated after running the GA\nbased method for different dt values.\n\n5. Conclusion\nIn this paper an optimization methodology is proposed for the optimization of the multipass face milling process. Binary coded genetic algorithm (GA) is used to minimize the\nunit production cost along with the satisfaction of several nonlinear constraints. Equal\ndepth of cut is used for the roughing passes and the relationship between total depth of\ncut (dt), depth of cut for finish pass (ds) and for rough pass (dr) is represented by a single\nvariable denoting the position of the (ds,dr) pair in the lookup table consisting of all\nfeasible (ds,dr) pairs. This transforms the problem into a mixed integer nonlinear\noptimization problem. An elitist binary coded GA is used to solve the problem and the\nentire technique is demonstrated in a case study. The minimum unit production costs\nobtained by the current technique are better than those reported in literature for the same\nproblem [14, 15]. The numerical test shows that GA does not get stuck to the local\noptimums and is successful in converging to the global optimum. On performing post\noptimality analysis for constraint sensitivity it is found that the optimum point is more\nsensitive to the power constraint as compared to the force constraint. Based on the\nanalysis of the optimum results, relationships among the decision variables are identified.\nThese relationships are used to develop a strategy to quickly estimate the optimum\ncutting conditions without running the GA, when the total depth of cut (dt) is changed.\nThe prediction of the estimation strategy is found to match closely to the results obtained\nfrom GA.\n\n6. Scope for future development\nIn the present work, all rough passes have been considered to have the same depth of cut.\nHowever, it still needs to be investigated whether a strategy of unequal depth of cut for\nrough passes can provide better results using this technique. It would be an interesting\nproblem to identify the conditions under which an unequal depth of cut would be better\nthan the equal depth of cut strategy and vice versa.\nThe main focus of the present paper is on showing the effectiveness of the proposed\noptimization methodology. Therefore, simple relations have been used for the estimation\nof cutting forces and surface roughness. However, even with more realistic relations the\nsame optimization technique can be used. Additional constraints may also be easily\nintroduced to make the optimization problem more realistic. The lookup table based\ntechnique employed in this paper can be extended for optimization of other multi-pass\nmachining processes.\n\n\fNomenclature\nB\nC0\nC1, C2\nD\ndr\nds\ndt\ner, es\nF\nFmax\nfr\nfs\nh1\nh2\nko\nkt\nL\nLtr\nLts\nN\nn\nn1, n2, n3\nn4, n5\nP\nPmax\npc, pm\nRr,max, Rs,max\nre\nTr,Ts\nte\nti\ntl\ntmr, tms\ntp\nUC\nUCr, UCs\nVr\nVs\nZ\n\nWidth of work piece (mm)\nConstant in tool life equation for face milling\nConstants in cutting force and power equations respectively\nCutter diameter (mm)\nDepth of cut for the rough passes (mm)\nDepth of cut for finish pass (mm)\nTotal depth of cut (mm)\nExtra travel of cutter at the ends (mm)\nCutting force (kgf)\nMaximum allowable cutting force (kgf)\nFeed for the rough passes (mm/tooth)\nFeed for the finish pass (mm/tooth)\nConstant related to tool travel time (min/mm)\nConstant related to tool approach/depart time (min)\nLabor cost per unit time including overhead cost ($/min)\nCost of cutting tool per edge ($/cutting edge)\nLength of work piece (mm)\nCutting travel length for rough pass (mm)\nCutting travel length for finish pass (mm)\nPopulation size for GA runs\nNumber of rough passes\nExponent of cutting velocity, depth of cut and feed in tool life equation\nExponent of depth of cut and feed in the empirical relation for cutting\nforce\nCutting power (kW)\nMaximum power (kW)\nCrossover and mutation probability in GA runs\nMaximum allowable surface roughness (Ra value) for rough and finish\npasses (mm)\nCutter nose radius (mm)\nTool life for rough and finish passes (min)\nTool exchange time (min/cutting edge)\nIdle tool motion time (min)\nMachine idling time (min)\nActual machining time for rough and finish passes (min)\nPreparation time (min/piece)\nTotal machining cost per unit ($/piece)\nMachining cost per unit for rough and finish passes ($/piece)\nCutting velocity for rough passes (m/min)\nCutting velocity for finish pass (m/min)\nNumber of teeth in the cutter\n\n\fReferences\n1. Taylor FW (1906) On the art of cutting metals. J Eng Ind Trans ASME 28:31-350\n2. Gilbert WW (1950) Economics of machining. Machining theory and practice.\nAmerican Society of Metals, Cleveland, Ohio, pp 465–485\n3. Petropoulos PG (1973) Optimal selection of machining rate variables by\ngeometric programming. Int J Prod Res 11(4):305–314\n4. Abuelnaga AM, El-Dardiry EA (1984) Optimization methods for metal cutting.\nInt J Mach Tool Des Res 24(1):11–18\n5. Ermer DS (1971) Optimization of the constrained machining economics problem\nby geometric programming. J Eng Ind 93:1067–1072\n6. Shin YC, Joo YS (1992) Optimization of machining conditions with practical\nconstraints. Int J Prod Res 30(12):2907–2919\n7. Al-Ahmari AMA (2001) Mathematical model for determining machining\nparameters in multipass turning operations with constrains. Int J Prod Res\n39:3367-3376\n8. Sankar RS, Asokan P, Saravanan R, Kumanan S, Prabhaharan G (2007) Selection\nof machining parameters for constrained machining problem using evolutionary\ncomputation. Int J Adv Manu Tech 32 (9-10): 892-901\n9. Lambert BK, Walvekar AG (1978) Optimization of multipass machining\noperations. Int J Prod Res 16(4):259–265\n10. Yellowley I, Gunn EA (1989) Optimal subdivision of cut in multi-pass machining\noperations. Int J Prod Res 27(9):1573-1588\n11. Chen MC, Tsai DM (1996) A simulated annealing approach for optimization of\nmulti-pass turning operations. Int J Prod Res 34(10):2803-2825\n12. Sönmez AI, Baykasoǧlu A, Dereli T, Filiz IH (1999) Dynamic optimization of\nmultipass milling operations via geometric programming. Int J Mach Tool\nManufact 39(2):297-320\n13. Gupta R, Batra JL, Lal GK (1995) Determination of optimal subdivision of depth\nof cut in multipass turning with constraints. Int J Prod Res 33(9):2555–2565\n14. Shunmugam MS, Reddy SVB, Narendran TT (2000) Selection of optimal\nconditions in multi-pass face-milling using genetic algorithm. Int J Mach Tools\nManuf 40:401–414\n\n\f15. An L, Chen M (2003) On Optimization of Machining Parameters. Control and\nAutomation ICCA '03. Proceedings. 4th International Conference on pp.839-843,\n10-12 June 2003\n16. Jawahir, I.S. and Wang, X (2007) Development of hybrid predictive models and\noptimization techniques for machining operations, J Mater Process Tech 185:4659\n17. Wang X, Kardekar A, Jawahir IS, Performance-based optimization of multi-pass\nface-milling operations using genetic algorithms. Intelligent Computation in\nManufacturing Engineering Conference, Naples, Italy, June 2004.\n18. Sheta A, Turabieh H, Vasant, P (2007) Hybrid Optimization Genetic Algorithms\n(HOGA) with Interactive Evolution to Solve Constraint Optimization Problems\nfor Production systems. Int J Computational Sc Vol. 1, No. 4:395-406.\n19. Wang ZG, Rahman M, Wong YS, Sun J (2005) Optimization of multi-pass\nmilling using parallel genetic algorithm and parallel genetic simulated annealing.\nInt J Mach Tools Manuf 45(15):1726-1734\n20. Nefedov N, Osipov K (1987) Typical Examples and Problems in Metal Cutting\nand Tool Design. Mir Publishers, Moscow\n21. Boothroyd G (1985) Fundamentals of Metal Machining and Machine Tools.\nMcGraw-Hill, New Delhi\n22. Holland JH (1975) Adaptation in Natural and Artificial Systems. University of\nMichigan Press, Second edition: MIT Press, 1992\n23. Mitchell M (1998) An Introduction to Genetic Algorithms. MIT Press,\nCambridge, MA\n24. Yokota T, Gen M, Li YX (1996) Genetic algorithm for non-linear mixed integer\nprogramming problems and its applications. Computers and Industrial\nEngineering 30(4):905-917\n25. DeJong K (1975) An analysis of the behavior of a class of genetic adaptive\nsystems. Ph.D. thesis, University of Michigan, USA\n\nFigure Captions\nFig. 1: (a) Cutting process in face milling showing the cutter path in (b) rough pass and\n(c) finish pass\nFig. 2: Block diagram of the proposed genetic algorithm\n\n\fFig 3: Evolution of population best and population average value of unit cost (UC) with\ngeneration\nFig 4: Variation of success rate of convergence of GA with mutation probability\nFig. 5: Variation of tool life in rough and finish passes for optimum cutting conditions\nwith changes in the total depth of cut (dt)\nFig. 6: Effect of changing the force and power constraints on the optimum values of unit\nproduction cost (at dt=6mm)\nFig. 7: Variation of optimum unit production cost with total depth of cut (dt)\nFig. 8: Variation of number of rough passes for optimum conditions with changes in the\ntotal depth of cut (dt)\nFig. 9: Variation of depth of cut of rough passes for optimum conditions with changes in\nthe total depth of cut (dt)\nFig. 10: Variation of the feed for rough passes for optimum conditions with changes in\nthe total depth of cut (dt)\n\nTable Captions\nTable 1: Data used for the example problem\nTable 2: Look-up table consisting of the feasible combinations of finish and rough depth\nof cut (ds,dr) pairs for a total depth of cut (dt) of 6 mm\nTable 3: Cutting parameter values, total machining cost and tool life corresponding to\noptimum operation point for different total depths of cut\nTable 4: Local optimum unit costs corresponding to all the feasible combinations of\nfinish and rough depths of cut (feasible (ds,dr) pairs ) for a total depth of cut of 6 mm\nTable 5: Comparison of the results obtained from the proposed GA based technique with\nthe results of Shunmugam et al. [14] and An and Chen [15]\nTable 5: Comparison of the optimization results obtained from GA and the estimation\nstrategy developed in Sec. 4.8 for the total depth of cut (dt) of 11.5mm\n\n\fFigures\nFigure 1\n\n\fFigure 2\n\nBegin\n\nGenerate lookup table for feasible (ds,dr) pairs\n\nCreate initial population, Po\n\nEvaluate Po\n\nFor i =1 to 100\n\nPerform selection, crossover, and\nmutation on population Pi to obtain Pi’’\n\nEvaluate Pi’’\n\nObtain Pi+1 by choosing the best N\nmembers from Pi and Pi’’\n\nPick the best solution from the converged\npopulation as the optimum solution\n\nEnd\n\n\fFigure 3\n\n\fFigure 4\n\n\fFigure 5\n\n\fFigure 6\n\n\fFigure 7\n\n\fFigure 8\n\n\fFigure 9\n\n\fFigure 10\n\n\fTables:\n\nTable 1\nL= 400 mm, Lts=403 mm, Ltr=260.55 mm, B=100 mm, D=160 mm, Z=16, re=1.0 mm\ner=es=3 mm, h1=7x10-4 min/mm, h2=0.3 min\nko=0.5 $/min, kt=2.5 $/cutting edge, te=1.5 min/cutting edge tp=0.75 min/piece\nVs,min=Vr,min=50 m/min, Vs,max=Vr,max=300 m/min, fs,min=fr,min=0.1 mm/tooth,\nfs,max=fr,max=0.6 mm/tooth, ds,min=0.5 mm, ds,max=2 mm, dr,min=1 mm, dr,max=4 mm,\nds,step=dr,step=0.1 mm\nFmax=815.77 kgf, Pmax=10 kW, Rs,max=0.0025 mm, Rr,max=0.025 mm, η=0.8,\nCv=445, l=0.32, xv=0.15, yv=0.35, pv=0, qv=0.2, sv=0.2, Kv=1.0, Cf=54.5, sf=1.0, pf=1.0,\nqf=1.0, Kf=1.0\nC0=253337816.7,C1=545, C2=0.111315, n1=3.125, n2=0.46875, n3=1.09375, n4=0.9,\nn5=0.74\nas=6.330309, ar=4.09271 bs=1.680135x10-6, br=2.598712x10-6, cs=0.29105, cr=0.2411925\n\n\fTable 2\nPair\nnumber\n\nds\n\ndr\n\nmm\n\nmm\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n1\n1\n1\n1.5\n1.5\n2\n2\n2\n2\n\n0.5\n1\n2.5\n0.5\n1.5\n0.5\n1\n2\n4\n\n\fTable 3\n\ndt\nmm\n\nVs\n\nVr\n\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n\n122.23\n122.38\n124.46\n123.80\n123.40\n122.10\n124.00\n122.92\n123.70\n123.89\n122.07\n\nm/min\n60.12\n60.00\n60.03\n63.27\n60.13\n60.03\n60.05\n60.19\n60.13\n60.02\n60.03\n\nfs\nfr\nmm/tooth\n0.2791\n0.2791\n0.2790\n0.2790\n0.2791\n0.2790\n0.2789\n0.2791\n0.2789\n0.2791\n0.2790\n\n0.3187\n0.5658\n0.4355\n0.3499\n0.3187\n0.4533\n0.3890\n0.3499\n0.3187\n0.4037\n0.3757\n\nds\ndr\nmm\n\nn\n\n2\n2\n1.8\n2\n2\n2\n1.8\n1.9\n2\n1.8\n2\n\n1\n2\n2\n2\n2\n3\n3\n3\n3\n4\n4\n\n4\n2.5\n3.1\n3.5\n4\n3\n3.4\n3.7\n4\n3.3\n3.5\n\nCV\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\nUC\n$/piece\n\nTs\n\n1.4108\n1.6914\n1.7615\n1.8276\n1.8830\n2.1606\n2.2328\n2.2940\n2.3553\n2.6396\n2.6956\n\n222\n221\n220\n213\n216\n223\n223\n223\n214\n224\n223\n\nTr\nmin\n1274\n853\n1025\n1044\n1274\n997\n1110\n1189\n1274\n1082\n1138\n\n\fTable 4\nds\n\ndr\n\nmm\n\nmm\n\n1\n1\n1\n1.5\n1.5\n2\n2\n2\n2\n\n0.5\n1\n2.5\n0.5\n1.5\n0.5\n1\n2\n4\n\nn\n\n10\n5\n2\n9\n3\n8\n4\n2\n1\n\nUCs\n\nUCr\n\nUC\n\n$/piece\n\n$/piece\n\n$/piece\n\n0.53660\n0.53660\n0.53660\n0.55920\n0.55920\n0.57098\n0.57098\n0.57098\n0.57098\n\n0.40794\n0.34594\n0.38123\n0.40794\n0.34733\n0.40794\n0.34594\n0.36078\n0.46420\n\n4.9910\n2.6413\n1.6741\n4.6057\n1.9762\n4.2095\n2.3297\n1.6675\n1.4102\n\n\fTable 5\nTotal depth of cut\n(dt in mm)\n6\n8\n\nOptimum unit production cost, UC ($/piece)\nCurrent Study\nAn and Chen [15]\nShunmugam et\nal.[14]\n1.4858\n--1.4108\n1.7615\n1.8523\n2.0086\n\n\fTable 6\n\nMethod\n\ndt\nmm\n\nGA\n11.5\nEstimation\nStrategy (Sec 4.8) 11.5\n\nVs\n\nVr\nm/min\n\nfs\nfr\nmm/tooth\n\nds\n\ndr\n\nn\n\nUC,min\n$/piece\n\nmm\n\n123.24\n\n60.73\n\n0.2791\n\n0.4125\n\n1.9\n\n3.2\n\n3\n\n2.1995\n\n123.2\n\n60.35\n\n0.279\n\n0.424\n\n1.9\n\n3.2\n\n3\n\n2.2194\n\n\f"
        ],
        [
         "1",
         "1",
         "cs.CE",
         "Computational Engineering",
         "1708.08551v1.pdf",
         "Deep Learning for Accelerated Reliability Analysis of\nInfrastructure Networks\nMohammad Amin Nabian, Hadi Meidani∗\n\narXiv:1708.08551v1 [cs.CE] 28 Aug 2017\n\nDepartment of Civil and Environmental Engineering, University of Illinois at Urbana-Champaign, Urbana,\nIllinois, USA.\n\nAbstract\nNatural disasters can have catastrophic impacts on the functionality of infrastructure systems and\ncause severe physical and socio-economic losses. Given budget constraints, it is crucial to optimize\ndecisions regarding mitigation, preparedness, response, and recovery practices for these systems.\nThis requires accurate and efficient means to evaluate the infrastructure system reliability. While\nnumerous research efforts have addressed and quantified the impact of natural disasters on infrastructure systems, typically using the Monte Carlo approach, they still suffer from high computational cost and, thus, are of limited applicability to large systems. This paper presents a deep\nlearning framework for accelerating infrastructure system reliability analysis. In particular, two\ndistinct deep neural network surrogates are constructed and studied: (1) A classifier surrogate\nwhich speeds up the connectivity determination of networks, and (2) An end-to-end surrogate that\nreplaces a number of components such as roadway status realization, connectivity determination,\nand connectivity averaging. The proposed approach is applied to a simulation-based study of the\ntwo-terminal connectivity of a California transportation network subject to extreme probabilistic\nearthquake events. Numerical results highlight the effectiveness of the proposed approach in accelerating the transportation system two-terminal reliability analysis with extremely high prediction\naccuracy.\nKeywords: Reliability Analysis, Infrastructure Systems, Surrogates, Neural Networks, Deep\nLearning, Uncertainty Quantification, Natural Disasters.\n\n∗ Corresponding\n\nauthor\nEmail address: meidani@illinois.edu (Hadi Meidani)\n\nPreprint submitted to Computer-Aided Civil and Infrastructure Engineering\n\nAugust 30, 2017\n\n\f1. Introduction\nThe hazard reliability for an infrastructure system is defined to be the degree of assurance\nthat the system will continue to successfully operate at a desired level of performance during a\ncertain period of time and in a specified environment in the aftermath of a hazard [1]. Assessment\nof the impact of natural disasters on infrastructure systems is of importance toward four main\nobjectives: (1) Planning for actions that eliminate or reduce the long-term risk to human life and\ninfrastructure systems (e.g.[2]); (2) Disaster preparation or adjustment, which aims to reduce the\nrisk of damages and injuries while enabling the capability to cope with the temporary disruption\nof the infrastructure systems (e.g.[3]); (3) Development of effective emergency response strategies\n(e.g.[4]); and (4) Post-disaster recovery planning (e.g.[5]). These four are, respectively, known as\nthe mitigation, preparedness, response, and recovery practices.\nA variety of analytical [6], simulation [7–11], and optimization [12] approaches are proposed\nin the literature for hazard reliability analysis of infrastructure systems. A comprehensive literature review on transportation infrastructure system performance in disasters is provided in [13].\nSimulation-based reliability assessment of large infrastructure systems are often computationally\nintractable or expensive due to the large number of network components, complex network topology, statistical dependence between component failures, and uncertainties in the hazard models.\nThis will impose limitations on design optimization or sensitivity analysis of these systems. Alternatively, a more efficient response assessment for large infrastructure systems can be made possible\nby using approximate surrogates [14].\nSurrogates are fast models that approximately describe the relationship between the system\ninputs and outputs and serve as a substitute for more expensive simulation tools. If the response\nevaluated by the reference expensive model is denoted by f (x), a surrgate seeks to provide a global\napproximate function f˜ (x). This is typically done by using a set of inputs xi ∈ Dd , i = {1, 2, ..., M },\nand the corresponding ‘exact’ system outputs f (xi ). There are several types of surrogate techniques\nto choose from. Among the most popular ones are polynomial functions (e.g.[15]), radial basis\nfunctions (e.g.[16]), Kriging (e.g.[17]), support vector machines (SVMs) (e.g.[7, 18]), and neural\nnetwork (e.g.[19]).\nUniversal approximation theorem in the mathematical theory of artificial neural networks rigorously proves that the standard multilayer, feed-forward, neural networks consist of one or more\nhidden layers with sufficiently many hidden units and, with arbitrary non-constant activation func2\n\n\ftions, can approximate any Borel-measurable function in a finite-dimensional space up to any\narbitrary degree of accuracy [20, 21]. This signifies that any failure in function approximation with\nsufficient accuracy by a multilayer network must be due to insufficient number of hidden units,\ninadequate learning, or lack of a deterministic input-output map [20]. Although this theorem states\nthat single-hidden-layer neural networks are already universal approximations, implementation of\nmultiple layers will improve the performance of the neural network [22]. With the cutting-edge\nneural network architectures and advanced training algorithms, deep learning has recently been\nsuccessfully used to solve elusive problems [22] and have won several machine learning contests [23].\nDeep learning consists of the development of computational models using multiple processing layers\nin order to learn data representations with multiple abstraction levels [22, 24, 25].\nThe goal of this paper is to propose a general framework to accelerate reliability analysis of\ninfrastructure systems. In this paper, we demonstrate how one can achieve this goal using deep\nneural network surrogates in the context of two-terminal reliability assessment of transportation\nnetworks subject to extreme earthquake events. Two distinct deep neural network surrogates are\nconstructed and studied: a classifier surrogate, which speeds up the two-terminal connectivity\nevaluation for a given network topology, and an end-to-end surrogate that replaces the entire\nMonte Carlo simulation and can be used to immediately calculate the average (expected) twoterminal connectivity given the failure probability of network components. Although the idea of\nusing artificial neural networks in reliability analysis of structures and infrastructure systems has\nbeen previously studied (e.g. [26–33]), the major contributions of this work are as follows: (1)\nNeural network surrogates with multiple hidden layers were used to enhance the performance of\nsurrogate-based two-terminal reliability analysis; (2) An end-to-end surrogate was proposed, which\nbypasses the sample-based calculations module that typically requires prohibitively large number\nof Monte Carlo simulations; and (3) In training the end-to-end surrogate, instead of using exact\ntraining data, we propose to use the predictions of the classifier surrogate to drastically reduce the\ncomputational time. We will numerically show that the proposed end-to-end surrogate is capable\nof accelerating the two-terminal reliability analysis of transportation networks by more than four\norders of magnitude, and how such acceleration can substantially facilitate sensitivity analysis and\npotentially other planning procedures for large networks.\nThe remainder of this paper is organized as follows. A general simulation-based framework\nfor two-terminal reliability analysis of transportation networks subject to earthquake events is\n\n3\n\n\fdescribed in Section 2. Next, Section 3 presents the proposed surrogate-based analysis of twoterminal reliability using deep neural networks. Finally, the accuracy and efficiency of the proposed\nsurrogate-based analysis is demonstrated through a case study for the San Jose-Mountain view\ntransportation network in Section 4.\n\n2. Two-Terminal Reliability Analysis\nThis section explains a general framework for two-terminal reliability analysis. First, the twoterminal connectivity of a network is introduced. Next, ground motion prediction equations are\nintroduced, which enable the prediction of ground motion intensity measures at the location of\nnetwork components. Given these predictions, it is then illustrated how one can evaluate the vulnerability of network components by the use of fragility analysis. Finally, a Monte Carlo simulation\nprocedure is described for the analysis of system-level response.\n2.1. Two-Terminal Connectivity\nConsider a transportation network represented by a graph G = (V , E), where V is the set\nof nodes and E is the set of links (i.e. roadways). In the aftermath of an earthquake, the links\nconnecting pairs of nodes eij ∈ E, may stop functioning primarily due to bridge failures. The\ntwo-terminal connectivity is defined as follows. Given a source node vs ∈ V and a terminal node\nvt ∈ V, the two-terminal connectivity is the condition where at least a connection exists between\nthe source and terminal nodes. A pair of adjacent nodes (vi , vj ) are disconnected if there is at least\none failed bridge on the link eij . In this work, it is assumed that bridges are the only components\nof the transportation network that are vulnerable to and get impacted by seismic hazards. This\nassumption is very common in the literature (e.g. [8]). The two-terminal connectivity problem is\nrelevant when, for instance, the accessibility from a major attraction point to a major hospital, or\nfrom a feedstock to demand zones, is to be maintained during an emergency [34].\n2.2. Ground Motion Prediction\nFor engineering applications, the evaluation of earthquake ground motions is generally performed\nusing empirical Ground Motion Prediction Equations (GMPE) [35, 36]. GMPEs are statistical\nmodels that provide a means to predict the ground motion intensity measures, such as peak ground\nmotions or response spectra, as a function of earthquake magnitude, source-to-site distance, fault\n4\n\n\fmechanism, local site conditions, etc. GMPEs are generally constructed based on empirical data\nand are empirical regression models of recorded data. A summary of all the empirical GMPEs for\nestimation of earthquake Peak Ground Acceleration (PGA) and elastic response spectral ordinates\npublished between 1964 and 2016 is provided in [37].\nIn this work, to determine the ground motion (specifically its spectral acceleration Sa ) at a\nbridge site, the Graizer-Kalkan 2015 (GK15) GMPE [38, 39] is adopted. GK15 consists of predictive\nequations for spectral acceleration and PGA that are derived based on physical simulations and\nempirical data, which are applicable to earthquakes of moment magnitude M between 5.0 and 8.0,\nat closest distances to fault rupture plane R ranging from 0 to 200 km, at sites having Vs30 in\nthe range of 200 to 1,300 m/s, and for spectral periods T of 0.01−5 s. In GK15, the PGA, herein\ndenoted by aPGA , is calculated as a multiplication of a series of functions, and in natural logarithmic\nscale, is given by\nln (aPGA ) = ln (G1 ) + ln (G2 ) + ln (G3 ) + ln (G4 ) + ln (G5 ) + σln(aPGA ) ,\n\n(1)\n\nwhere G1 represents a scaling function for magnitude and style faulting, G2 is a model for ground\nmotion attenuation, G3 is a model for adjustment to the attenuation rate in order to take into\naccount the regional anelastic attenuation, G4 represents the site amplification model, and G5 represents a model for basin scaling. σln(aPGA ) is the residual variability, which accounts for unexplained\nvariability in the ground motion data used for the calibration of GMPE. In seismic hazard analysis,\nreducing this residual variability is of a high priority, since at large values of aPGA , the probabilities\nof exceedance go up rapidly with σln(aPGA ) . As will be shown in the numerical examples, the twoterminal connectivity of San Jose-Mountain View transportation network is significantly affected\nby this residual variability [38].\nThe form of GK15 for the 5% damped Sa response ordinates is\nSa = aPGA µ (M, R, Vs30 , Bdepth ) ,\n\n(2)\n\nwhere the spectral shape µ is parameterized by M , R, Vs30 , and basin depth under the site Bdepth .\nFor the analysis of bridge fragility, as described in the next subsection, spectral accelerations at\n0.3 s and 1.0 s are used.\n\n5\n\n\f2.3. Bridge Fragility Analysis\nThere are several well-established ways for the analysis of structural response to natural hazards. In this study, the HAZUS-MH fragility model [40], developed by the Federal Emergency\nManagement Agency (FEMA), is implemented for the calculation of transportation network bridge\nresponse to earthquake ground shaking. HAZUS-MH is a standardized methodology for the estimation of potential physical, economic, and social losses from earthquakes, hurricanes, and floods.\nFor a given level of ground motion, fragility curves or damage functions for bridges are modeled as\nfunctions with log-normal distributions that yield the probability of reaching or exceeding different\ndamage states. Individual fragility curves are parametrized by a median value of ground motion or\nground failure, and an associated standard deviation.\nThe required inputs needed to estimate the damages to a bridge in HAZUS-MH fragility model\nare geographical location of the bridge (latitude and longitude), spectral accelerations at 0.3 s and\n1.0 s at the bridge location, peak ground acceleration, soil type, and bridge classification. Bridges are\nclassified into 28 primary types based on several structural characteristics, such as seismic design,\nstructure type, number of spans, and pier type. Five damage states are considered for bridges,\nwhich are none, slight, moderate, extensive, and complete damage states. Extensive damage for\nbridges is defined by shear failure, degradation of columns with no collapse, differential settlement\nat connections, large residual movement at connections, and shear key failure at abutments. In this\nstudy, it is assumed that the bridges will stop functioning at the onset of extensive damage state\nimmediately after an earthquake event.\nFor each of the bridge classes, a total of four different fragility curves are constructed from the\ncombination of two log-normal distributions for ground shaking and ground failure. Afterward,\nspecific fragility curves for individual bridges are constructed by updating the generic curves based\non the bridge characteristics. The output of fragility analysis for each bridge is four different curves\nthat represent the probability of that bridge exceeding a damage state for a given level of ground\nmotion. These fragility curves are then used, as illustrated in the next subsection, in order to\ncalculate the system-level response of transportation network to an earthquake via a simulationbased study of two-terminal connectivity.\n\n6\n\n\f2.4. Two-Terminal Reliability Analysis\nIn order to estimate the system-level network response to an earthquake affecting it components,\ne.g. roadways, Monte Carlo Simulation (MCS) may be used. MCS is a straight-forward, easy\nto implement, approach ideally suited to parallel computing. For calculation of the two-terminal\nconnectivity in this study, network realizations are drawn by randomly removing roadways according\nto their survival probabilities, given by Equation 4. Specifically, the damage state for each roadway\nis modeled as a Bernoulli random variable with the following distribution\n\nxi =\n\n\n1,\n\nwith probability pi ,\n\n0,\n\nwith probability 1 − pi ,\n\n(3)\n\nwhere {0, 1} denotes the survived and failed states, respectively, with a survival probability of pi .\nA roadway with at least one failed bridge will be removed. Therefore, the survival probability pi\nof roadway i with k bridges of IDs {i1 , . . . , ik } is calculated in logarithmic scale as\n\nln (pi ) =\n\nk\nX\n\u0002\n\u0001\u0003\nln pij ,\n\n(4)\n\nj=1\n\nwhere pij is the survival probability of bridge ij .\nLet vs ∈ V and vt ∈ V denote, respectively, the source and terminal nodes predetermined by\nthe stakeholder. For a network realization using the j th MC sample, the two-terminal connectivity\nis assessed by evaluating whether there is any connection between the source and terminal\n\n\u0010\n\n(j)\n\n(j)\n\n(j)\n\ngj x1 , x2 , ..., x`\n\n\u0011\n\n=\n\n\n1,\n\nif s,t are connected,\n\n0,\n\notherwise,\n\n(5)\n\nin which ` is the total number of roadways in the network. This procedure is repeated by drawing\nmore network realizations until convergence of the quantity of interest (QoI) is achieved. The DepthFirst Search (DFS) algorithm [41, 42] with a linear-time computational complexity of O (|V | + |E|)\nis utilized herein for the evaluation of two-terminal connectivity. For a given set of failure probabilities for bridges, and for a given MCS with N network realizations, the expected two-terminal\n\n7\n\n\fconnectivity Pc is estimated by\n\nPc =\n\nN\n1 X\n(j)\n(j)\n(j)\ngj (x1 , x2 , ..., x` ),\nN j=1\n\n(6)\n\nIn order to accelerate the two-terminal connectivity computations, the Monte Carlo calculations\nare performed in parallel [43] where different processors evaluate the network connectivity for different network realizations. In the next section, we explain the approach to train and use fast and\naccurate deep learning surrogates in place of Monte Carlo-based DFS (exact) calculations.\n\n3. Surrogate Model for Two-Terminal Connectivity\n3.1. Deep Neural Networks\nFor notation brevity, single hidden layer neural networks are introduced first, since its subsequent generalization to multiple hidden layers, which makes a neural network deep, will be straightforward. Given the d-dimensional row vector x ∈ Dd as model input, the k-dimensional output of\na standard single hidden layer neural network is in the form of\ny = σ (xW1 + b1 ) W2 + b2 ,\n\n(7)\n\nin which W1 and W2 are weight matrices of size d × q and q × k, respectively, b1 and b2 are biases\nof size 1 × q and 1 × k, respectively. The function σ (·) is an element-wise non-linearity, commonly\nknown as the activation function. In deep neural networks, the output of each activation function is\ntransformed by a new weight matrix and a new bias, and is then fed to another activation function.\nFor each new set of weight matrix and bias that is added to (7), a new hidden layer is added to the\nneural network. The capacity of neural networks can be easily increased by adding more hidden\nlayers or more units to each hidden layer.\nPopular choices of activation functions are Sigmoid, hyperbolic tangent (Tanh), and rectified\nlinear unit (RELU). The RELU activation function has the form of f (θ) = max (0, θ). RELUs\nare getting increasingly popular in deep learning applications as, compared to Sigmoid and Tanh\nactivations, they are faster and do not suffer from the vanishing gradient problem.\nIn order to calibrate the weight matrices and biases for a regression problem, we may use a\n\n8\n\n\fEuclidean loss function as follows\n\nEMSE (X, Y) =\n\nM\n1 X\n2\nky − ŷi k ,\n2M i=1 i\n\n(8)\n\nwhere EMSE is the mean squared error, X = {x1 , x2 , ..., xM } is the set of M observed inputs,\nY = {y1 , y2 , ..., yM } is the set of M observed outputs, and {ŷ1 , ŷ2 , ..., ŷM } is the set of neural\nnetwork output (model prediction) corresponding to the set of inputs X. For a binary classification\ntask, we may use a binary cross-entropy loss function in the form of\n\nEBCE (X, Y) = −\n\nM\nX\n{yi log (ŷi ) + (1 − yi ) log (1 − ŷi )},\n\n(9)\n\ni=1\n\nwhere EBCE is the binary cross-entropy. Minimizing the loss function with respect to model parameters (W1 , W2 , · · · , b1 , b2 ) will yield the calibrated model parameters (W1∗ , W2∗ , · · · , b∗1 , b∗2 ).\nFor instance, for a binary cross-entropy loss function, we have\n(W1∗ , W2∗ , · · · , b∗1 , b∗2 , · · · ) =\n\nargmax EBCE (X, Y) ,\n\n(10)\n\n(W1 ,··· ,b1 ··· )\n\nMinimizing the loss function is usually performed using backpropagation [22, 44]. It consists of\na two-phase cycle; forward pass and backward pass. A forward pass takes the input to the network\nand propagates it through the layers of the network, one by one, to calculate the network output.\nA backward pass starts from the network output and propagates towards the input layer while\ncalculating the gradients, layer by layer, using the chain rule.\n3.2. Deep Neural Networks for Two-Terminal Reliability Analysis\nThe step-by-step procedure for construction of DNN surrogates that can be used to accelerate\ntwo-terminal reliability analysis of transportation systems is elaborated in this section. Two different surrogate models are developed in this study. The first model is hereinafter referred to as\nthe classifier surrogate. It replaces the DFS algorithm to determine whether a particular source-toterminal connection exists. It does so for each MC sample, i.e. for each realized roadway failure and\nits corresponding topology. The input to this model is therefore a deterministic network topology\nin the form of a binary vector, and the output is a binary variable indicating the connection.\n\n9\n\n\fFigure 1: Workflow for calculation of the expected two-terminal connectivity, using exact (DFS) connectivity check\n(top), classifier surrogate (middle), and end-to-end surrogate (bottom).\n\nThe second surrogate model, which we refer to as the end-to-end surrogate, is designed to\nreplace the topology realization, connectivity determination, and connectivity averaging modules\n(see Figure 1). It is used to immediately evaluate the average (expected) two-terminal connectivity\ngiven the roadway failure probabilities. It bypasses roadway status realizations from the failure\nprobabilities, and thus saves computational time. Figures 2 and 3 show the proposed frameworks\nfor constructing the classifier and end-to-end surrogates, respectively, and how these surrogates are\nutilized in the evaluation of expected two-terminal connectivity.\n3.3. Surrogate Performance Measures\nIn order to evaluate the accuracy of a DNN surrogate model, a number of performance measures\nare used in this study. They include QoI prediction accuracy αQoI , binary classification accuracy\nαbinary , sensitivity or True Positive Rate (TPR), and specificity or True Negative Rate (TNR) [45].\nThe last three measures are applicable to binary classification only.\nThe QoI prediction accuracy for connectivity is calculated as\nPc − P̂c\nαQoI = 1 −\n\nPc\n\n,\n\n(11)\n\nwhere Pc and P̂c are the two-terminal connectivity calculated respectively using exact (DFS) con10\n\n\fStart\n\nI.Generate data\n\nIII.Perform analysis using the surrogate\n\nDraw an earthquake realization\nby sampling, e.g., random magnitude, etc.\n\nDraw an earthquake realization\nby sampling, e.g., random magnitude, etc.\n\nCalculate ground motion\nIMs at bridge locations\nusing a GMPE\n\nCalculate ground motion\nIMs at bridge locations\nusing a GMPE\n\nPerform bridge fragility analysis\nto calculate failure probabilities\n\nPerform bridge fragility analysis\nto calculate failure probabilities\n\nDraw a network realization\nby sampling from roadway failure states\n\nDraw a network realization\nby sampling from roadway failure states\n\nEvaluate network connectivity\nusing DFS algorithm\n\nEvaluate network connectivity\nusing the trained deep\nneural network surrogate\n\nNo\n\nSufficient data?\n\nCalculate expected twoterminal connectivity\n\nYes\n\nII.Train the surrogate\nConverged?\n\nSplit data into training,\nevaluation, and test data\n\nNo\n\nYes\n\nDesign a deep neural network\nby specifying number of hidden layers,\ndimensionality, and activation function\n\nStop\n\nTrain the surrogate\nusing training data\nEvaluate surrogate accuracy\nusing evaluation data\nNo\n\nSufficient accuracy\non evaluation data?\n\nYes\n\nFigure 2: Framework for constructing the classifier surrogate and utilizing it for Monte Carlo-based two-terminal\n1\nreliability analysis. In this procedure, the classifier surrogate\nwill replace the DFS algorithm.\n\n11\n\n\fStart\n\nGenerate training data\nby performing multiple analysis\nusing classifier surrogate, as\noutlined in Fig.2, part III.\nTrain the surrogate\nas outlined in Fig.2, part II.\nDraw an earthquake realization\nby sampling, e.g., random magnitude, etc.\nCalculate ground motion IMs at\nbridge locations and perform\nbridge fragility analysis\nCalculate expected two-terminal\nconnectivity\nNo\n\nConverged?\nYes\nStop\n\nFigure 3: Framework for constructing the end-to-end surrogate and utilizing it for two-terminal reliability analysis\n1\nwith end-to-end surrogate replacing Monte Carlo simulations.\n\n12\n\n\fnectivity check and the surrogate. The binary classification accuracy is calculated as\nαbinary =\n\nTP + TN\n,\nTP + FP + TN + FN\n\n(12)\n\nwhere TP (True Positive) is the number of times the surrogate correctly predicts network survival,\nTN (True Negative) is the number of times the surrogate correctly predicts network failure, FP\n(False Positive) is the number of times the surrogate incorrectly predicts network survival, and FN\n(False Negative) is the number of times the surrogate incorrectly predicts network failure, satisfying\nT P +T N +F P +F N = N . As more specific measures, True Positive Rate (TPR) and True Negative\nRate (TNR) are calculated as\n\nTPR =\n\nTP\n,\nTP + FN\n\n(13)\n\nTNR =\n\nTN\n.\nTN + FP\n\n(14)\n\n4. Case Study for the San Jose-Mountain View Transportation Network\nThe surrogate-based two-terminal connectivity analysis procedure described in Section 3.2 is\napplied to the transportation network that connects San Jose, CA to Mountain View, CA in the\nUnited States. This network is located in a region of high seismic activity. A sketch of this\nnetwork is provided in Figure 4. The network consists of 39 bridges, 12 nodes, and 18 roadway\nlinks (out of which 14 have at least one bridge). Throughout the numerical examples presented in\nthis section, the network is considered to be impacted by the 1989 Loma Prieta earthquake with\nvarying magnitudes. The geographical coordinates for the epicenter of Loma Prieta earthquake are\n37.04◦ N, 121.88◦ W.\nA number of assumptions and choices were made throughout this section. First, bridges are\nassumed to be the only network components vulnerable to earthquakes, as commonly considered\nin the literature (e.g. [8]). Second, the network is considered to be an undirected graph since the\nadjacent bridges on two different sides of the road share the same or very similar properties. Also,\naccording to the HAZUS-MH soil classification, the soil for the study area is determined to be of\ntype D.\n13\n\n\fFigure 4: Layout of the San Jose-Mountain View transportation network. Numbers near each bridge are the ID of\nthat bridge. Multiple IDs shows that there are multiple bridges there but only one is shown.\n\nThe NetworkX Python library [46] was used for network connectivity evaluation using DFS\nalgorithm, and the Keras deep learning library [47] was used for construction of classifier and endto-end surrogates. The Python source codes for all the simulations presented in this section are\nmade available on GitHub [48]. Computations in sections 4.1, 4.2 are conducted on a quad core 2.5\nGHz MacBook Pro with 16 GB of RAM. Computations in sections 4.3, 4.4 are conducted on the\nComet cluster from XSEDE (Extreme Science and Engineering Discovery Environment) resources\n[50], with 24 2.5 GHz CPU cores and 128 GB of DRAM.\n4.1. Classifier Surrogate Training and Prediction\nFollowing the framework represented in Figure 2, given a network realization, the classifier\nsurrogate indicates whether a source-to-terminal connection exists. The input and output to this\nsurrogate are a binary vector of roadway conditions (failed or survived) and a binary connectivity\nindicator, respectively. In order to generate training and evaluation data sets, a total of 10,000\nsamples of earthquake magnitude M , denoted by {mi }10000\nare drawn according to\ni=1\nmi = 8.0 − θi ,\n\n14\n\n(15)\n\n\fFigure 5: The probability density function used to draw samples from earthquake magnitude in order to generate\nsurrogate training and evaluation data sets.\n\nwhere θi is a random sample drawn from a truncated exponential distribution with a shape parameter and lower and upper bounds of 15, 0, and 1.5, receptively. Ninety percent of the samples are\nused for training, and the rest is left for surrogate evaluation. A sketch for the probability distribution of M is provided in Figure 5. Training and evaluation samples are preferred to be drawn from\nan exponential distribution, and not a uniform distribution. This is due to the non-linear relationship between earthquake moment magnitude and energy release [49], leading to larger sensitivity of\nfailure probabilities to magnitude perturbations when the nominal magnitude is larger; hence, the\nexponentially increasing distribution of training samples.\nThe classifier surrogate consists of 7 hidden layers with different dimensionalities (see Figure 6).\nRELU activation is adopted for hidden layers 1 through 6, while the Sigmoid activation is used in\nthe last hidden layer. The Adam optimization algorithm [50]is used to minimize the binary crossentropy loss function (Equation 9). For 150 epochs and a batch size of 64, it took 83.05 seconds to\ntrain the classifier surrogate.\nIn order to evaluate the predictive performance of the trained classifier surrogate, we consider\nfive different scenarios with earthquake magnitudes 6.7, 7.0, 7.3, 7.6, and 7.9 Mw , and for each\nwe use the trained surrogate for two-terminal connectivity evaluation. The surrogate-based results\nare compared versus exact connectivity results obtained using DFS algorithm. Figure 7 shows the\nsurvival probabilities for the 39 bridges subject to the five earthquake scenarios. For each earthquake\nevent, given these survival probabilities and by using Equation 4 for calculating roadway failure\n\n15\n\n\fOutput\nDimension=1\n\nInput layer\nDimension=14\n\nHidden layer 1\nDimension=128\nActivation: RELU\n\nHidden layer 2\nDimension=64\nActivation: RELU\n\nHidden layer 3\nDimension=32\nActivation: RELU\n\nHidden layer 4\nDimension=16\nActivation: RELU\n\nHidden layer 5\nDimension=8\nActivation: RELU\n\nHidden layer 6\nDimension=4\nActivation: RELU\n\nHidden layer 7\nDimension=2\nActivation:\nSigmoid\n\nFigure 6: Architecture of the classifier surrogate. This surrogate\nis to be used instead of the DFS algorithm in order\n1\nto accelerate network connectivity evaluation given a network realization. The model consists of 7 hidden layers with\ndifferent dimensionalities. The input and output to this model are, respectively, a binary vector of roadway failure\nstates and a scalar that represents the expected two-terminal connectivity.\n\nFigure 7: Bridge survival probabilities for different earthquake events considered in example 1.\n\nprobabilities, a total of 100,000 network realizations are generated. The two-terminal connectivity\nof each one of these network realizations is determined using the classifier surrogate and the DFS\nalgorithm, and the resulting expected connectivities are compared in Figure 8. It is evident from\nthe convergence plots that the surrogate and DFS results are in close agreement. The estimated\nexpected values for two-terminal connectivity, as well as computational times, are compared in Table\n1, and surrogate performance measures are reported in Table 2. Compared to exact connectivity\ncheck using DFS, the classifier surrogate predictions are about one order of magnitude faster, with\naccuracies of more than 99.9%.\n\n16\n\n\f(a)\n\n(b)\n\n(c)\n\n(d)\n\n(e)\nFigure 8: Convergence plots for prediction of two-terminal connectivity for different earthquakes of magnitude (a)\n6.7 Mw, (b) 7.0 Mw, (c) 7.3 Mw, (d) 7.6 Mw, (e) 7.9 Mw. The x-axis is in logarithmic scale.\n\n17\n\n\fTable 1: Two-terminal connectivity predictions and computational times for different earthquake magnitudes using\nDFS and classifier surrogate.\n\nMagnitude\n\nPc\n\nP̂c\n\nDFS time (s)\n\n6.7\n7.0\n7.3\n7.6\n7.9\n\n0.9769\n0.9256\n0.8635\n0.7679\n0.6263\n\n0.9769\n0.9255\n0.8633\n0.7679\n0.6264\n\n5.23\n5.40\n6.05\n5.71\n5.63\n\nSurrogate\ntime (s)\n0.62\n0.62\n0.63\n0.62\n0.65\n\nTable 2: Classifier surrogate accuracy indicators for different earthquake magnitudes\n\nMagnitude\n6.7\n7.0\n7.3\n7.6\n7.9\n\nαbinary\n1.0000\n0.9998\n0.9995\n0.9995\n0.9995\n\nTPR\n1.0000\n0.9998\n0.9996\n0.9997\n0.9996\n\nTNR\n0.9996\n0.9989\n0.9990\n0.9988\n0.9993\n\nNext, we investigate the performance of the classifier surrogate in network connectivity prediction for a probabilistic earthquake event, i.e. for earthquakes with probabilistic magnitudes.\nFollowing [34, 51], it is assumed that the earthquake magnitude follows a truncated exponential\ndistribution with the following pdf\n\nfM (m) =\n\n\n\n\n\nβ exp [−β(m − mmin )]\n,\n1 − exp [−β(mmax − mmin )]\n\n\n 0,\n\nm ` 6 m 6 mu ,\n\n(16)\n\notherwise,\n\nwhere mmin and mmax are the minimum and maximum of random magnitudes, which are set to 6.8\nand 7.5, respectively. β is the shape parameter and is set to 0.76 [34] .\nA total of 10,000,000 network realizations are generated, each corresponding to a random sample\nfrom the probabilistic magnitude and a random sample from roadway failure states. For each of\nthese network realizations, the two-terminal connectivity is evaluated using DFS algorithm and the\nclassifier surrogate, and the resulting expected connectivities are compared in Figure 9. Expected\ntwo-terminal connectivity using DFS and classifier surrogate is, respectively, 0.9002 and 0.9001. The\nDFS and surrogate computational time are, respectively, 603.97 and 53.89 seconds, and αbinary ,\nTPR, and TNR are 0.9997, 0.9998, and 0.9989, respectively. Once again, classifier surrogate results\nare in close agreement with DFS results with accuracies of more than 99.9%, however, are achieved\n\n18\n\n\fFigure 9: Convergence plot for prediction of two-terminal connectivity for a probabilistic earthquake event. The\nx-axis is in logarithmic scale.\n\none order of magnitude faster.\n4.2. Uncertainty-Aware Two-Terminal Reliability Analysis Using Classifier Surrogate\nIn this section, we investigate an additional layer of uncertainty in two-terminal reliability\nassessment. Specifically, we consider the residual variability in GMPE, which is due to the model\nfitting error. To quantify the impact of this residual variability, we start off by the probabilistic\nearthquake event defined in Equation 16, and then consider aPGA and Sa at each bridge location\nto be normally distributed random variables with mean values calculated using equations 1 and 2,\nand standard deviations reported in [38]. To study the classifier surrogate performance in this case,\nfollowing the procedure represented in Figure 2, 10,000,000 network realizations are generated by\nconsecutive sampling from earthquake magnitudes, aPGA and Sa at bridge locations, and roadway\nfailure states according to their failure probabilities. Figure 10 shows good agreement between\nexpected two-terminal connectivities using DFS and the classifier surrogate. This is while the\nsurrogate evaluation, compared to DFS, is about one order of magnitude faster, i.e. 574.48 vs.\n53.08 seconds. The estimated expected two-terminal connectivity using DFS and classifier surrogate\nare, respectively, 0.6853 and 0.6853, and αbinary , TPR, and TNR are 0.9990, 0.9993, and 0.9985,\nrespectively.\n\n19\n\n\fFigure 10: Convergence plot for prediction of two-terminal connectivity for a probabilistic earthquake event. The\nresidual variability in GMPE is taken into account. The x-axis is in logarithmic scale.\n\nIt should be noted that the evaluated two-terminal reliability in this section (0.69) is smaller than\nthe one evaluated in the previous section (0.90). This highlights the importance of the additional\nuncertainties in ground motion intensity measures (e.g. aPGA and Sa ), which is usually ignored in\nreliability studies (e.g. [7, 8]) and can lead to overestimation of the network reliability.\n4.3. End-to-End Surrogate Training and Prediction\nAs mentioned earlier, two terminal connectivity calculations can be substantially accelerated\nby using an end-to-end surrogate, which replaces the entire MCS as outlined in Figure 3. To\nnumerically demonstrate this, we need to first train the end-to-end surrogate. The training data\ncan be generated using the DFS algorithm. Alternatively, we will use the previously-developed\nclassifier surrogate to produce the training data. It should be noted that this training data set is\nnot exact, but according to the results in the previous sections the error is expected to be negligible\nand the computational speed up is expected to be substantial.\nFigure 11 shows the architecture of the end-to-end surrogate, with the input being the vector of\nroadway failure probabilities and the output the expected two-terminal connectivity. The surrogate\nconsists of 5 hidden layers with different dimensionalities. Sigmoid activation is adopted for hidden\nlayers. The Adam optimizer is used to minimize the Euclidean loss function (Equation 8). To\ngenerate training and evaluation data, a total of 3,000 magnitude samples are drawn, and for each\n20\n\n\fInput layer\nDimension=14\n\nHidden layer 1\nDimension=256\nActivation:\nSigmoid\n\nHidden layer 2\nDimension=128\nActivation:\nSigmoid\n\nHidden layer 3\nDimension=64\nActivation:\nSigmoid\n\nHidden layer 4\nDimension=8\nActivation:\nSigmoid\n\nHidden layer 5\nDimension=4\nActivation:\nSigmoid\n\nOutput\nDimension=1\n\n1 surrogate is to be used instead of the MCS in order to\nFigure 11: Architecture of the end-to-end surrogate. This\naccelerate the evaluation of expected two-terminal connectivity given the bridge failure probabilities. The model\nconsists of 5 hidden layers with different dimensionalities. The input and output to this model are, respectively, a\nvector of roadway failure probabilities and a scalar that represents the expected two-terminal connectivity.\n\nmagnitude sample, 100,000 topology samples are drawn whose two-terminal connectivities were\nevaluated using the classifier surrogate. For a batch size of 64 and 2,000 epochs, the end-to-end\nsurrogate training time (including generation of training and evaluation data and model calibration)\nwas 351.99 seconds.\nUsing the trained end-to-end surrogate, we study the two-terminal connectivity of the San JoseMountain View transportation network subject to a probabilistic earthquake event. Similar to\nSection 4.1, it is assumed that the earthquake magnitude follows a truncated exponential distribution. The lower and upper bounds for the magnitude variability are set to 6.8 and 7.5, respectively.\nWithout loss of generality, the GMPE residual variabilities were ignored for simplicity. To test the\nsurrogate, 10,000 magnitude samples are drawn and for each sample, the expected two-terminal\nconnectivity is calculated using the end-to-end surrogate. As the reference case, for each earthquake\nrealization, a total of 100,000 topology realizations are drawn and their connectivity is evaluated\nusing DFS algorithm, and the results are compared in Figure 12. As another way of demonstrating\nthe surrogate accuracy, Figure 13 compares the DFS and surrogate predictions of connectivity for\neach earthquake realization. The expected two-terminal connectivity was estimated to be 0.9001\nusing both approaches while the computational times for DFS and end-to-end surrogate were found\nto be 7,857.92 and 0.71 seconds, respectively.\n4.4. One-at-a-time Sensitivity Analysis Using End-to-End Surrogate\nIn this section, we demonstrate the application of the proposed end-to-end surrogate in maintenance planning. In particular, we consider the optimal seismic retrofitting of bridges [52] where\ndecision makers seek to improve the two-terminal reliability of the network. In this case, typically\nin the face of budget constraints, it is crucial to identify the bridges that are most influential on twoterminal reliability and prioritize them for repair. To this end, a one-at-a-time (OAT) sensitivity\nanalysis can be performed [53]. It involves considering amplifications on the survival probabili-\n\n21\n\n\fFigure 12: Convergence plot for prediction of two-terminal connectivity for a probabilistic earthquake event. The\nend-to-end surrogate estimates are calculated with no MCS.\n\nFigure 13: A comparison between the DFS and end-to-end surrogate predictions of the two-terminal connectivity for\neach earthquake realization. The surrogate predictions are calculated with no Monte Carlo simulation.\n\n22\n\n\fties, one bridge at a time, while keeping the other bridges’ survival probabilities at their nominal\nvalues. These amplifications should reflect the expected outcomes of repair plans for each bridge.\nWe assume that these retrofit plans will result in an amplification rate of 10% for every bridge.\nConsidering this rate, the expected two-terminal connectivities are then calculated using the DFS\nalgorithm and end-to-end surrogate, for the nominal and “retrofitted” networks. Here we consider\na probabilistic earthquake event as defined in Equation 16 with a magnitude ranging between 7.3\nand 7.9 Mw . Table 3 shows the OAT sensitivity analysis results. For brevity, only the results for\nthe three most and least sensitive components are shown. For the DFS results, for each earthquake\nrealization, a total of 100,000 topology realizations are drawn. With no amplification, the expected\ntwo-terminal connectivity probability of the network subject to this probabilistic earthquake event\nis found to be 0.7641 using DFS and 0.7643 using the end-to-end surrogate. This table also highlights the substantial computational savings that the end-to-end surrogate can offer in repetitive\nprocesses, e.g. optimization, sensitivity analysis, or real-time risk-informed decision making.\nTable 3: Sensitivity analysis summary for selected bridges\n\nRank\n\nBridge\nID\n\n1\n2\n3\n..\n.\n37\n38\n39\n\n0\n13\n25\n..\n.\n14\n10\n26\n\nImprovement in\nconnectivity (%)\n(DFS estimate)\n8.22\n7.80\n2.17\n..\n.\n0.07\n0.01\n0.00\n\nImprovement in\nconnectivity (%)\n(surrogate estimate)\n8.19\n7.78\n2.14\n..\n.\n0.06\n0.01\n0.00\n\nDFS time\n(s)\n\nSurrogate\ntime (s)\n\n8361.22\n7855.76\n8334.18\n..\n.\n8089.66\n8131.76\n8079.06\n\n0.75\n0.72\n0.74\n..\n.\n0.75\n0.74\n0.75\n\n5. Conclusion\nApproximations and uncertainties inherent in infrastructure systems reliability analysis on one\nhand and the associated computational challenge on the other hand motivate the utilization of\nfast and sufficiently accurate surrogates that can replace one or more computational modules in\nthe analysis pipeline. The resulting surrogate-based reliability analysis can then facilitate optimal\nplanning and management of infrastructure systems subject to natural hazards. In this paper, we\nstudied the surrogates that are trained based on deep learning, and using a case study, highlighted\nhow they can offer fast computation of infrastructure response with high accuracy. An important\n\n23\n\n\fadvantage of using deep learning in building surrogates for nonlinear system responses is its capability for automatic feature engineering/detection. This will remove the need to manually identify\nfeatures for a given data, and make the approach broadly applicable to various nonlinear responses.\nThe proposed surrogate-based reliability analysis framework can be further extended by augmenting the training data to improve the prediction accuracy. An example of data augmentation for\nimproving TNR can be created as follows. For each topology realization with no source-to-terminal\nconnectivity, we can generate multiple additional topology realizations by randomly (according to\nroadway failure probabilities) letting the survived roadways fail. These additional network realizations will not incur extra computational burden, as they are already known to be corresponding to\na “no-connectivity” condition. Another extension to further improve the computational efficiency\nis to make use of graphic processing units (GPUs) in deep neural network surrogate training and\nprediction. Deep learning generally involves large matrix multiplications that are substantially\nparallelizable using GPUs, leading to significant acceleration.\n\nAcknowledgement\nThis work used the Extreme Science and Engineering Discovery Environment (XSEDE), which\nis supported by National Science Foundation grant number ACI-1053575.\n\nReferences\nReferences\n[1] S. Zacks, Introduction to reliability analysis: probability models and statistical methods,\nSpringer Science & Business Media, 2012.\n[2] D. Godschalk, Natural hazard mitigation: Recasting disaster policy and planning, Island Press,\n1999.\n[3] D. Paton, Disaster preparedness: a social-cognitive perspective, Disaster Prevention and Management: An International Journal 12 (3) (2003) 210–216.\n[4] M. Perry, Natural disaster management planning: A study of logistics managers responding\nto the tsunami, International Journal of Physical Distribution & Logistics Management 37 (5)\n(2007) 409–433.\n24\n\n\f[5] C. E. Adie, et al., Holistic disaster recovery: Ideas for building local sustainability after a\nnatural disaster, DIANE Publishing, 2001.\n[6] Y.-M. Wang, J. Liu, T. M. Elhag, An integrated ahp–dea methodology for bridge risk assessment, Computers & industrial engineering 54 (3) (2008) 513–525.\n[7] R. Stern, J. Song, D. Work, Accelerated monte carlo system reliability analysis through\nmachine-learning-based surrogate models of network connectivity, Reliability Engineering &\nSystem Safety 164 (2017) 1–9.\n[8] P. Bocchini, D. M. Frangopol, A stochastic computational framework for the joint transportation network fragility analysis and traffic flow distribution under extreme events, Probabilistic\nEngineering Mechanics 26 (2) (2011) 182–193.\n[9] L. Chang, A. S. Elnashai, B. F. Spencer, J. Song, Y. Ouyang, Transportations systems modeling\nand applications in earthquake engineering, Tech. rep., DTIC Document (2010).\n[10] P. Bocchini, D. M. Frangopol, Generalized bridge network performance analysis with correlation and time-variant reliability, Structural Safety 33 (2) (2011) 155–164.\n[11] M. A. Nabian, H. Meidani, Uncertainty quantification and pca-based model reduction for\nparallel monte carlo analysis of infrastructure system reliability, Tech. rep. (2017).\n[12] C. Liu, Y. Fan, F. Ordóñez, A two-stage stochastic programming model for transportation\nnetwork protection, Computers & Operations Research 36 (5) (2009) 1582–1590.\n[13] R. Faturechi, E. Miller-Hooks, Measuring the performance of transportation infrastructure\nsystems in disasters: A comprehensive review, Journal of infrastructure systems 21 (1) (2014)\n04014025.\n[14] S. Koziel, L. Leifsson, Surrogate-based modeling and optimization, Applications in Engineering.\n[15] N. V. Queipo, R. T. Haftka, W. Shyy, T. Goel, R. Vaidyanathan, P. K. Tucker, Surrogate-based\nanalysis and optimization, Progress in aerospace sciences 41 (1) (2005) 1–28.\n[16] S. M. Wild, R. G. Regis, C. A. Shoemaker, Orbit: Optimization by radial basis function\ninterpolation in trust-regions, SIAM Journal on Scientific Computing 30 (6) (2008) 3197–3219.\n\n25\n\n\f[17] J. P. Kleijnen, Kriging metamodeling in simulation: A review, European journal of operational\nresearch 192 (3) (2009) 707–716.\n[18] N. Tabatabaee, M. Ziyadi, Y. Shafahi, Two-stage support vector classifier and recurrent neural\nnetwork predictor for pavement performance modeling, Journal of Infrastructure Systems 19 (3)\n(2012) 266–274.\n[19] M. Ziyadi, I. L. Al-Qadi, Efficient surrogate method for predicting pavement response to various\ntire configurations, Neural Computing and Applications (2016) 1–13.\n[20] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks are universal approximators, Neural networks 2 (5) (1989) 359–366.\n[21] K. Hornik, Approximation capabilities of multilayer feedforward networks, Neural networks\n4 (2) (1991) 251–257.\n[22] Y. LeCun, Y. Bengio, G. Hinton, Deep learning, Nature 521 (7553) (2015) 436–444.\n[23] J. Schmidhuber, Deep learning in neural networks: An overview, Neural networks 61 (2015)\n85–117.\n[24] I. Goodfellow, Y. Bengio, A. Courville, Deep learning, MIT Press, 2016.\n[25] H. B. Demuth, M. H. Beale, O. De Jess, M. T. Hagan, Neural network design, Martin Hagan,\n2014.\n[26] J. Cheng, Q. Li, R.-c. Xiao, A new artificial neural network-based response surface method for\nstructural reliability analysis, Probabilistic Engineering Mechanics 23 (1) (2008) 51–63.\n[27] M. Papadrakakis, N. D. Lagaros, Reliability-based structural optimization using neural networks and monte carlo simulation, Computer methods in applied mechanics and engineering\n191 (32) (2002) 3491–3507.\n[28] H. M. Gomes, A. M. Awruch, Comparison of response surface and neural network with other\nmethods for structural reliability analysis, Structural safety 26 (1) (2004) 49–67.\n[29] J. E. Hurtado, D. A. Alvarez, Neural-network-based reliability analysis: a comparative study,\nComputer methods in applied mechanics and engineering 191 (1) (2001) 113–132.\n26\n\n\f[30] C. Srivaree-ratana, A. Konak, A. E. Smith, Estimation of all-terminal network reliability using\nan artificial neural network, Computers & Operations Research 29 (7) (2002) 849–868.\n[31] A. H. Elhewy, E. Mesbahi, Y. Pu, Reliability analysis of structures using neural network\nmethod, Probabilistic Engineering Mechanics 21 (1) (2006) 44–53.\n[32] J. B. Cardoso, J. R. de Almeida, J. M. Dias, P. G. Coelho, Structural reliability analysis using\nmonte carlo simulation and neural networks, Advances in Engineering Software 39 (6) (2008)\n505–513.\n[33] J. Zhang, R. O. Foschi, Performance-based design and seismic reliability analysis using designed\nexperiments and neural networks, Probabilistic Engineering Mechanics 19 (3) (2004) 259–267.\n[34] W.-H. Kang, J. Song, P. Gardoni, Matrix-based system reliability method and applications to\nbridge networks, Reliability Engineering & System Safety 93 (11) (2008) 1584–1593.\n[35] J. P. Stewart, J. Douglas, M. Javanbarg, Y. Bozorgnia, N. A. Abrahamson, D. M. Boore,\nK. W. Campbell, E. Delavaud, M. Erdik, P. J. Stafford, Selection of ground motion prediction\nequations for the global earthquake model, Earthquake Spectra 31 (1) (2015) 19–45.\n[36] J. J. Bommer, J. Douglas, F. Scherbaum, F. Cotton, H. Bungum, D. Fäh, On the selection of\nground-motion prediction equations for seismic hazard analysis, Seismological Research Letters\n81 (5) (2010) 783–793.\n[37] J. Douglas, Ground-motion prediction equations 1964-2016, Pacific Earthquake Engineering\nResearch Center Berkeley, CA, 2017.\n[38] V. Graizer, E. Kalkan, Summary of the gk15 ground-motion prediction equation for horizontal pga and 5% damped psa from shallow crustal continental earthquakes, Bulletin of the\nSeismological Society of America 106 (2) (2016) 687–707.\n[39] V. Graizer, Update of the graizer–kalkan groundmotion prediction equations for shallow crustal\ncontinental earthquakes, US Geol. Surv. Open-File Rept 1009 (2015) 98.\n[40] FEMA, Hazus-mh mr3: Technical manual (2008).\n[41] R. Tarjan, Depth-first search and linear graph algorithms, SIAM journal on computing 1 (2)\n(1972) 146–160.\n27\n\n\f[42] R. E. Korf, Depth-first iterative-deepening: An optimal admissible tree search, Artificial intelligence 27 (1) (1985) 97–109.\n[43] J. S. Rosenthal, Parallel computing and monte carlo algorithms, Far east journal of theoretical\nstatistics 4 (2) (2000) 207–236.\n[44] W. Jin, Z. J. Li, L. S. Wei, H. Zhen, The improvements of bp neural network learning algorithm,\nin: Signal Processing Proceedings, 2000. WCCC-ICSP 2000. 5th International Conference on,\nVol. 3, IEEE, 2000, pp. 1647–1649.\n[45] P. Baldi, S. Brunak, Y. Chauvin, C. A. Andersen, H. Nielsen, Assessing the accuracy of\nprediction algorithms for classification: an overview, Bioinformatics 16 (5) (2000) 412–424.\n[46] D. A. Schult, P. Swart, Exploring network structure, dynamics, and function using networkx,\nin: Proceedings of the 7th Python in Science Conferences (SciPy 2008), Vol. 2008, 2008, pp.\n11–16.\n[47] F. Chollet, Keras, https://github.com/fchollet/keras (2015).\n[48] M. A. Nabian, UIUC-UQ/Deep-Learning-for-Reliability-Analysis: Deep Learning for Accelerated Reliability Analysis of Infrastructure Systems (Aug. 2017). doi:10.5281/zenodo.846898.\nURL https://doi.org/10.5281/zenodo.846898\n[49] H. Kanamori, The energy release in great earthquakes, Journal of geophysical research 82 (20)\n(1977) 2981–2987.\n[50] D. Kingma, J. Ba, Adam:\n\nA method for stochastic optimization, arXiv preprint\n\narXiv:1412.6980.\n[51] P. Cosentino, V. Ficarra, D. Luzio, Truncated exponential frequency-magnitude relationship in\nearthquake statistics, Bulletin of the Seismological Society of America 67 (6) (1977) 1615–1623.\n[52] G. Buckle, I. Friedland, J. Mander, G. Martin, R. Nutt, M. Power, Seismic retrofitting manual\nfor highway structures, MCEER, Buffalo, 2006.\n[53] V. Komkov, K. K. Choi, E. J. Haug, Design sensitivity analysis of structural systems, Vol. 177,\nAcademic press, 1986.\n\n28\n\n\f"
        ],
        [
         "2",
         "2",
         "cs.CE",
         "Computational Engineering",
         "0705.1759v1.pdf",
         "FINITE ELEMENT MODEL UPDATING USING RESPONSE SURFACE METHOD\nTshilidzi Marwala\nSchool of Electrical and Information Engineering\nUniversity of the Witwatersrand\nP/Bag 3, Wits, 2050, South Africa\nt.marwala@ee.wits.ac.za\nThis paper proposes the response surface method for finite element model updating. The response\nsurface method is implemented by approximating the finite element model surface response equation\nby a multi-layer perceptron. The updated parameters of the finite element model were calculated using\ngenetic algorithm by optimizing the surface response equation. The proposed method was compared\nto the existing methods that use simulated annealing or genetic algorithm together with a full finite\nelement model for finite element model updating. The proposed method was tested on an unsymmetrical H-shaped structure. It was observed that the proposed method gave the updated natural frequencies and mode shapes that were of the same order of accuracy as those given by simulated annealing\nand genetic algorithm. Furthermore, it was observed that the response surface method achieved these\nresults at a computational speed that was more than 2.5 times as fast as the genetic algorithm and a full\nfinite element model and 24 times faster than the simulated annealing.\nproximation model, which traditionally is a polynomial\nand therefore is less expensive to evaluate. This makes\nRSM very useful to FE model updating because optimizing the FE model to match measured data to FE model\ngenerated data is a computationally expensive exercise.\nFurthermore, the calculation of the gradients that are\nessential when traditional optimization methods, such as\nconjugate gradient methods, are used is computationally\nexpensive and often encounters numerical problems\nsuch as ill-conditioning. RSM tends to be immune to\nsuch problems when used for FE model updating. This\nis largely because RSM solves a crude approximation of\nthe FE model rather than the full FE model which is of\nhigh dimensional order. The multi-layer perceptron\n(MLP)6 is used to approximate the response equation.\nThe RSM is particularly useful for optimizing systems\nthat are evolving as a function of time, a situation that is\nprevalent in model-based fault diagnostics found in the\nmanufacturing sector. To date, RSM has been used extensively to optimize complex models and processes7,8.\nIn summary, the RSM is used because of the following\nreasons: (1) the ease of implementation that includes low\ncomputational time; (2) the suitability of the approach to\nthe manufacturing sector where model-based methods\nare often used to monitor structures that evolve as a\nfunction of time.\nFE model updating has been used widely to detect\ndamage in structures9. When implementing FE updating\nmethods for damage identification, it is assumed that the\nFE model is a true dynamic representation of the structure and this is achieved through FE model updating.\nThis means that changing any physical parameter of an\nelement in the FE model is equivalent to introducing\ndamage in that region. There are two approaches that\nare used in FE updating: direct methods and iterative\n\nIntroduction\nFinite element (FE) models are widely used to predict\nthe dynamic characteristics of aerospace structures.\nThese models often give results that differ from the\nmeasured results and therefore need to be updated to\nmatch the measured data. FE model updating entails\ntuning the model so that it can better reflect the measured data from the physical structure being modeled1.\nOne fundamental characteristic of an FE model is that it\ncan never be a true reflection of the physical structure\nbut it will forever be an approximation. FE model updating fundamentally implies that we are identifying a\nbetter approximation model of the physical structure\nthan the original model. The aim of this paper is to introduce updating of finite element models using Response Surface Method (RSM)2. Thus far, the RSM\nmethod has not been used to solve the FE updating problem1. This new approach to FE model updating is compared to methods that use simulated annealing (SA) or\ngenetic algorithm (GA) together with full FE models for\nFE model updating. FE model updating methods have\nbeen implemented using different types of optimization\nmethods such as genetic algorithm and conjugate gradient methods3-5. Levin and Lieven5 proposed the use of\nSA and GA for FE updating.\nRSM is an approximate optimization method that\nlooks at various design variables and their responses and\nidentify the combination of design variables that give the\nbest response. The best response, in this paper, is defined as the one that gives the minimum distance between the measured data and the data predicted by the\nFE model. RSM attempts to replace implicit functions\nof the original design optimization problem with an ap*\n\nAssociate Professor\n\n1\n\n\fmethods1. Direct methods, which use the modal properties, are computationally efficient to implement and reproduce the measured modal data exactly. Furthermore,\nthey do not take into account the physical parameters\nthat are updated. Consequently, even though the FE\nmodel is able to predict measured quantities, the updated\nmodel is limited in the following ways: it may lack the\nconnectivity of nodes - connectivity of nodes is a phenomenon that occurs naturally in finite element modeling because of the physical reality that the structure is\nconnected; the updated matrices are populated instead\nof banded - the fact that structural elements are only\nconnected to their neighbors ensures that the mass and\nstiffness matrices are diagonally dominated with few\ncouplings between elements that are far apart; and there\nis a possible loss of symmetry of the systems matrices.\nIterative procedures use changes in physical parameters\nto update FE models and produce models that are physically realistic. Iterative methods that use modal properties and the RSM for FE model updating are implemented in this paper. The FE models are updated so that\nthe measured modal properties match the FE model predicted modal properties. The proposed RSM updating\nmethod is tested on an unsymmetrical H-shaped structure.\n\nand stiffness matrices. The frequency response functions (FRFs) are defined as the ratio of the Fourier transformed response to the Fourier transformed force. The\nFRFs may be expressed in receptance and inertance\nform. On the one hand, receptance expression of the\nFRF is defined as the ratio of the Fourier transformed\ndisplacement to the Fourier transformed force. On the\nother hand, inertance expression of the FRF is defined as\nthe ratio of the Fourier transformed acceleration to the\nFourier transformed force. The inertance FRF (H) may\nbe written in terms of the modal properties by using the\nmodal summation equation as follows10:\nN\n− ω 2φkiφli\nH kl ( ω ) = ∑\n(3)\n2\n2\ni =1 − ω + 2ζ i ω i ωj + ω i\nEquation 3 is an FRF due to excitation at position k and\nresponse measurement at position l, ω is the frequency\npoint, ω i is the ith natural frequency, N is the number of\nmodes and ζi is the damping ratio of mode i. The excitation and response of the structure and Fourier transform method10 can be used to calculate the FRFs.\nThrough equation 3 and a technique called modal analysis10, the natural frequencies and mode shapes can be\nindirectly calculated from the measured FRFs. The modal properties of a dynamic system depend on the mass\nand stiffness matrices of the system as indicated by\nequation 2. Therefore, the measured modal properties\ncan be reproduced by the FE model if the correct mass\nand stiffness matrices are identified.\nFE model updating is achieved by identifying the\ncorrect mass and stiffness matrices. The correct mass\nand stiffness matrices, in the light of the measured data,\ncan be obtained by identifying the correct moduli of\nelasticity for various sections of the structure under consideration1. In this paper, to correctly identify the\nmoduli of elasticity of the structure, the following cost\nfunction that measures the distance between measured\ndata and FE model calculated data, is minimized:\n\nMathematical Background\nIn this study, modal properties, i.e. natural frequencies\nand mode shapes, are used as a basis for FE model updating. For this reason these parameters are described in\nthis section. Modal properties are related to the physical\nproperties of the structure. All elastic structures may be\ndescribed in terms of their distributed mass, damping\nand stiffness matrices in the time domain through the\nfollowing expression10:\n\n[ M ]{ X ' ' } + [ C ]{ X ' } + [ K ]}{ X } = { F }\n(1)\nwhere [M], [C] and [K] are the mass, damping and\nstiffness matrices respectively, and {X}, {X′} and {X′′}\nare the displacement, velocity and acceleration vectors\nrespectively while {F} is the applied force vector. If\nequation 1 is transformed into the modal domain to form\nan eigenvalue equation for the ith mode, then10:\n\n ω m − ω calc\nE = ∑ γ i  i m i\nωi\ni =i\n\nN\n\nN\n\n(\n\n2\n\n\n ...\n\n\n\n+ β ∑ 1 − diag( MAC({ φ }icalc ,{ φ }im ))\n\n(4)\n\n)\n\ni\n\n2\n\n( −ω i [ M ] + jω i [ C ] + [ K ]){ φ }i = { 0 }\n(2)\nth\nwhere j = − 1 , ω i is the i complex eigenvalue, with\nits imaginary part corresponding to the natural frequency\nωi, { 0 } is the null vector and { φ }i is the ith complex\nmode shape vector with the real part corresponding to\nthe normalized mode shape {φ}i. From equation 2, it\nmay be deduced that the changes in the mass and stiffness matrices cause changes in the modal properties of\nthe structure. Therefore, the modal properties can be\nidentified through the identification of the correct mass\n\nHere m is for measured, calc is for calculated, N is the\nnumber of modes; γ i is the weighting factor that measures the relative distance between the initial estimated\nnatural frequencies for mode i and the target frequency\nof the same mode; the parameter β is the weighting\nfunction on the mode shapes; the MAC is the modal\nassurance criterion11; and the diag(MAC)i stands for the\nith diagonal element of the MAC matrix. The MAC is a\nmeasure of the correlation between two sets of mode\nshapes of the same dimension. In equation 4 the first\n\n2\n\n\fpart has a function of ensuring that the natural frequencies predicted by the FE model are as close to the measured ones as possible while the second term ensures that\nthe mode shapes between measurements and those predicted by the FE model are correlated. When two sets of\nmode shapes are perfectly correlated then the MAC\nmatrix is an identity matrix. The updated model is\nevaluated by comparing the natural frequencies and\nmode shapes from the FE models before and after updating to the measured ones.\n\nInitial Conditions\nUpdating\nParameters\n\nUpdating\nObjective\n\nUpdating Space\n\nGeneration of surface\nresponse data using\nthe FE model\n\nResponse Surface Method\n\nFunctional\napproximation\n\nRSM method is a procedure that operates by generating a response for a given input. The inputs are the parameters to be updated and the response is the error between the measured data and the FE model generated\ndata. Then an approximation model of the input parameters and the response, called a response surface\nequation, is constructed. As a consequence of this, the\noptimization method operates on the surface response.\nThis equation is usually simple and not computationally\nintensive as opposed to a full FE model. RSM has other\nadvantages such as the ease of implementation through\nparallel computation and the ease at which parameter\nsensitivity can be calculated.\nThe proposed RSM consists of these essential components: (1) the response surface approximation equation; and (2) the optimization procedure. There are\nmany techniques that have been used for response surface approximation such as polynomial approximation12\nand neural networks13. A multi-layer perceptron is used\nas a response surface approximation equation6. Further\nunderstanding of different approaches to response surface approximation may be found in the literature14-19.\nIn this paper, MLP is used because it has been successfully used to solve complicated regression problems.\nThe details of the MLP are described in the next section.\nThe second component of the RSM is the optimization\nof the response surface. There are many types of optimization methods that can be used to optimize the response surface equation and these include the gradient\nbased methods20 and evolutionary computation methods21. The gradient based methods have a shortcoming\nof identifying local optimum solutions while evolutionary computing methods are better able to identify global\noptimum solution. As a result of the global optimum\nadvantage of evolutionary methods, in this study the GA\nis used to optimize the response surface equation. The\nmanner in which the RSM is implemented is shown in\nFigure 1.\nIn this figure it shown that the RSM is implemented\nby following these steps:\n1) Setting initial conditions which are: updating parameters, updating objective, which is in equation 4,\n\nGlobal\noptimization\n\nFunctional\nevaluation at\nthe optimum\nsolution on the\nFE model\n\nUpdating\ncriteria\nsatisfied?\n\nN\n\nReplace the worst\nsurface response\ndata with the\noptimum point and\nits FE response\n\nY\nStop\n\nFigure 1. The flowchart of the RSM. Here N stands for no\nand Y stands for yes.\nand updating space.\n2) The FE model is then used to generate sample response surface data\n3) MLP is used to approximate the response surface\napproximation equation from the data generated in\nStep 2.\n4) GA is used to find a global optimum solution.\n5) The new optimum solution is used to evaluate the\nresponse from the full FE model.\n6) If the optimum solution does not satisfy the objective, then the new optimum and the corresponding\nFE model calculated response replaces the candidate\nwith the worst response in data set generated in Step\n2 and then steps 3 to 5 are repeated. If the objective\nis satisfied then stop and the optimum solution becomes the ultimate solution.\nStep 6 ensures that the simulation always operates in\nthe region of the optimum solution. The next section\n\n3\n\n\fsic dimensionality of the data. Models of this form can\napproximate any continuous function to arbitrary accuracy if the number of hidden units M is sufficiently large.\nThe relationship between the output y, representing error\nbetween the model and measured data, and input, x,\nrepresenting updating parameters may be written as\nfollows6:\n\ndescribes an MLP, which is used for functional approximation.\n\nMulti-layer Perceptron\nMulti-layer perceptron is a type of neural networks\nwhich used in the present study. This section gives the\nover-view of the MLP in the context of functional approximation. The MLP is viewed in this paper as parameterized graphs that make probabilistic assumptions\nabout data. Learning algorithms are viewed as methods\nfor finding parameter values that look probable in the\nlight of the data. Supervised learning is used to identify\nthe mapping function between the updating parameters\n(x) and the response (y). The response is calculated using equation 4. The reason why the MLP is used is because it provides a distributed representation with respect to the input space due to cross-coupling between\ninput, hidden and output layers. The MLP architecture\ncontains a hyperbolic tangent basis function in the hidden units and linear basis functions in the output units6.\nA schematic illustration of the MLP is shown in Figure\n2.\nThis network architecture contains hidden units and\noutput units and has one hidden layer. The bias parameters in the first layer are shown as weights from an extra\n\nM\n\n d\n\ny k =  ∑ wkj( 2 ) tanh ∑ w (ji1 ) x i + w (j 01 )  + wk( 02 ) \n(5)\n i =1\n\n j =1\n\nHere, w (ji1 ) and w(ji2 ) indicate weights in the first and second layers, respectively, going from input i to hidden\nunit j, M is the number of input units, d is the number of\noutput units while w (j 01 ) indicates the bias for the hidden\nunit j. Training the neural network identifies the weights\nin equations 5 and a cost function must be chosen to\nidentify these weights. A cost function is a mathematical\nrepresentation of the overall objective of the problem.\nThe main objective, this is used to construct a cost\nfunction, is to identify a set of neural network weights\ngiven updating parameters and the error between the FE\nmodel and the measured data. If the training set\nD = { x k , t k }kN=1 is used and assuming that the targets t\nare sampled independently given the inputs xk and the\nweight parameters, wkj, the cost function, E, may be\nwritten as follows using the sum-of-square error func6\ntion :\n\nOutput Units\n\nN\n\nzM\n\nThe sum-of-square error function is chosen because it\n6\nhas been found to be suited to regression problems . In\nequation 6, N is the number of training examples and K\nis the number of output units. In this paper, N is equal to\n150, while K is equal to 1.\nBefore the MLP is trained, the network architecture\nneeds to be constructed by choosing the number of hidden units, M. If M is too small, the MLP will be insufficiently flexible and will give poor generalization of the\ndata because of high bias. However, if M is too large,\nthe neural network will be unnecessarily flexible and\nwill give poor generalization due to a phenomenon\nknown as over-fitting caused by high variance. In this\nstudy, we choose M such that the number of weights is at\nmost fewer than the number of response data. This is in\nline with the basic mathematical principle which states\nthat in order to solve a set of equations with n variables\nyou need at least n independent data points. The next\nsection describes the GA, which is a method that is used\nto solve for the optimum solution of the response surface\napproximation equation.\n\nHidden Units\n\nbias\nxd\n\n(6)\n\nn =1 k =1\n\nz0\nz1\n\nK\n\nE = ∑ ∑ { t nk − y nk } 2\n\nyc\n\ny1\n\nx1\n\nx0\n\nInput Units\n\nFigure 2. Feed-forward network having two layers of\nadaptive weights\ninput having a fixed value of x0=1. The bias parameters\nin the second layer are shown as weights from an extra\nhidden unit, with the activation fixed at z0=1. The model\nin Figure 2 is able to take into account the intrinsic di-\n\nGenetic Algorithms\nGA was inspired by Darwin’s theory of natural evolution. Genetic algorithm is a simulation of natural evolu-\n\n4\n\n\ftion where the law of the survival of the fittest is applied\nto a population of individuals. This natural optimization\nmethod is used to optimize either the response surface\napproximation equation or the error between the FE\nmodel and the measured data. GA is implemented by\ngenerating a population and creating a new population\nby performing the following procedures: (1) crossover;\n(2) mutation; (3) and reproduction. The details of these\nprocedures can be found in Holland21 and Goldberg22.\nThe crossover operator mixes genetic information in the\npopulation by cutting pairs of chromosomes at random\npoints along their length and exchanging over the cut\nsections. This has a potential of joining successful operators together. Arithmetic crossover technique22 is\nused in this paper. Arithmetic crossover takes two parents and performs an interpolation along the line formed\nby the two parents. For example if two parents p1 and\np2 undergo crossover, then a random number a which\nlies in the interval [0,1] is generated and the new offsprings formed are p1(a-1) and pa.\n\nThe next section describes simulated annealing which\nis used to update an FE model using a FE model.\n\nSimulated Annealing\nSimulated Annealing is a Monte Carlo method that is\nused to investigate the equations of state and frozen\nstates of n degrees of freedom system23. SA was inspired by the process of annealing where objects, such as\nmetals, re-crystallize or liquids freeze. In the annealing\nprocess the object is heated until it is molten, then it is\nslowly cooled down such that the metal at any given\ntime is approximately in thermodynamic equilibrium.\nAs the temperature of the object is lowered, the system\nbecomes more ordered and approaches a frozen state at\nT=0. If the cooling process is conducted insufficiently\nor the initial temperature of the object is not sufficiently\nhigh, the system may become quenched forming defects\nor freezing out in metastable states. This indicates that\nthe system is trapped in a local minimum energy state.\nThe process that is followed to simulate the annealing\nprocess was proposed by Metropolis et al.24 and it involves choosing the initial state with energy Eold (see\nequation 4) and temperature T and holding T constant\nand perturbing the initial configuration and computing\nEnew at the new state. If Enew is lower than Eold, then accept the new state, otherwise if the opposite is the case\nthen accept this state with a probability of exp -(dE/T)\nwhere dE is the change in energy. This process can be\nmathematically represented as follows:\n\nMutation is a process that introduces to a population,\nnew information. Non-uniform mutation22 was used and\nit changes one of the parameters of the parent based on a\nnon-uniform probability distribution. The Gaussian distribution starts with a high variance and narrows to a\npoint distribution as the current generation approaches\nthe maximum generation.\nReproduction takes successful chromosomes and reproduces them in accordance to their fitness functions.\nIn this study normalized geometric selection method was\nused22. This method is a ranking selection function\nwhich is based on the normalized geometric distribution.\nUsing this method the least fit members of the population are gradually driven out of the population. The basic genetic algorithm was implemented in this paper as\nfollows:\n\nif E new < E old accept state E new\n E − E old  (7)\n\nelse accept E new with probability exp new\nT\n\n\nThis processes is repeated such that the sampling statistics for the current temperature is adequate, and then\nthe temperature is decreased and the process is repeated\nuntil a frozen state where T=0 is achieved.\nSA was first applied to optimization problems by\nKirkpatrick, et al. 23. The current state is the current\nupdating solution, the energy equation is the objective\nfunction in equation 4, and the ground state is the global\noptimum solution.\n\n1) Randomly create an initial population of a certain\nsize.\n2) Evaluate all of the individuals in the population using the objective function in equation 4.\n3) Use the normalized geometric selection method to\nselect a new population from the old population\nbased on the fitness of the individuals as given by the\nobjective function.\n\nExample: Asymmetrical H-structure\nAn unsymmetrical H-shaped aluminum structure\nshown in Figure 3 was used to validate the proposed\nmethod. This structure was also used by Marwala and\nHeyns4 as well as Marwala25. This structure had three\nthin cuts of 1mm that went half-way through the crosssection of the beam. These cuts were introduced to elements 3, 4 and 5. The structure with these cuts was used\nso that the initial FE model gives data that are far from\nthe measured data and, thereby test the proposed proce-\n\n4) Apply some genetic operators, non-uniform mutation\nand arithmetic crossover, to members of the population to create new solutions.\n5) Repeat steps 2-6, which is termed one generation,\nuntil a certain fixed number of generations has been\nachieved\n\n5\n\n\fdure on a difficult FE model updating problem. The\nstructure was suspended using elastic rubber bands. The\nstructure was excited using an electromagnetic shaker\nand the response was measured using an accelerometer.\nThe structure was divided into 12 elements. It was excited at a position indicated by double-arrows, in Figure\n3, and acceleration was measured at 15 positions indicated by single-arrows in Figure 3. The structure was\ntested freely-suspended, and a set of 15 frequency response functions were calculated. A roving accelerometer was used for the testing. The mass of the accelerometer was found to be negligible compared to the mass\n\nwas run 150 times to generate the data for functional\napproximation. The MLP implemented had 12 input\nvariables corresponding to the 12 elements in the FE\nmodel, 8 hidden units and one output unit corresponding\nto the error in equation 4. As described before, the MLP\nhad a hyperbolic tangent activation function in the hidden layer and linear activation function in the output\nlayer. The RSM functional approximation via the MLP\nwas evaluated 10 times (iterations) each time using the\nGA to calculate the optimum point and evaluating this\noptimum point on the FE model and then storing the\nprevious optimum point in the data set for the current\n\nA\n200mm\n\n400mm\nB\n\n600mm\ny\nCross-section indicated by line AB\n9.8mm\nx\n32.2mm\n\nFigure 3. Irregular H-shaped structure\nof the structure. The number of measured coordinates functional approximation. The scaled conjugate gradiwas 15.\nent method was used to train the MLP, primarily beThereafter, the finite element model was constructed cause of its computational efficiency27. The initial funcusing the Structural Dynamics Toolbox26. The FE tional approximation was obtained by training the MLP\nmodel used Euler-Bernoulli beam elements. The FE for 150 training cycles and on a subsequent functional\nmodel contained 12 elements. The moduli of elasticity of these elements were used as updating parameTable 1. Results showing measured frequencies, the initial freters. When the FE updating was implemented the\nquencies and the frequencies obtained when the FE model is\nmoduli of elasticity was restricted to vary from\nupdated using the RSM, SA and GA\n10\n10\n-2\n6.00x10 to 8.00x10 N.m . The weighting factors,\nMeasured\nInitial\nFrequencies Frequencies Frequencies\nin the first term in equation 4, were calculated for\nFreq\nFreq\nfrom RSM\nfrom SA\nfrom GA\neach mode as the square of the error between the\n(Hz)\n(Hz)\nUpdated\nUpdated\nUpdated\nmeasured natural frequency and the natural frequency\nModel\n(Hz)\nModel\n(Hz)\nModel (Hz)\ncalculated from the initial model and the weighting\n53.9\n56.2\n52.2\n54.0\n53.9\nfunction for the second term in equation 4 was set to\n117.3\n127.1\n118.4\n118.8\n120.1\n0.75. When the RSM, SA and GA were implemented\n228.4\n209.4\n209.7\n211.3\nfor model updating the results shown in Table 1 were 208.4\n254.0\n263.4\n251.1\n253.8\n253.4\nobtained.\n445.1\n452.4\n432.7\n435.8\n438.6\nOn implementing the proposed RSM, the FE model\n\n6\n\n\fand that from the initial model was 8.4%. When the\nRSM was used, this error was reduced to 0.9% while\nusing SA it was reduced to 1.3% and using the GA it\nwas reduced to 2.4%. The error of the third natural frequencies between the measured data and the initial FE\nmodel was 9.6%. When the RSM was used, this error\nwas reduced to 0.5% while using SA reduced it to 0.6%\nand using the GA and a full FE model reduced it to\n1.4%. The error between the fourth measured natural\nfrequency and that from the initial model was 3.7%.\nWhen the RSM was used for FE updating, this error was\nreduced to 1.1% while using the SA reduced it to 0.1%\nand using the GA and a full FE model reduced it to\n0.2%. The error between the fifth measured natural frequency and that from the initial model was 1.6%. When\nthe RSM was used, this error was increased to 2.8%\nwhile using SA increased it to 2.1% and using the GA\nand a full FE model the error was reduced to 1.5%.\nOverall, the SA gave the best results with an average\nerror, calculated over all the five natural frequencies, of\n0.9% followed by the GA with an average error of 1.1%\nand then RSM with an average error of 1.7%. All the\nthree methods on average improved when compared to\nthe average error between the initial FE model and the\nmeasured data, which was 5.5%.\nThe updated FE models implemented were\n9\nalso\nvalidated on the mode shapes they pre8\ndicted. To make this assessment possible the\n7\nMAC11 was used. The mean of the diagonal of\n6\nthe MAC vector was used to compare the mode\n5\nshapes predicted by the updated and initial FE\nmodels to the measured mode shapes. The av4\nerage MAC calculated between the mode shapes\n3\nfrom an initial FE model and the measured\n2\nmode shapes was 0.8394. When the average\n1\nMAC was calculated between the measured data\n0\nand data obtained from the updated FE models,\nit was observed that the RSM, SA and GA up1 2 3 4 5 6 7 8 9 10 11 12\ndated FE model gave the improved average of\nElement\nthe diagonal of the MAC matrix of 0.8413,\nInitial\nRSM\nGA\nSA\n0.8430 and 0.8419, respectively. Therefore, the\nFigure 4. Graph showing the initial moduli of elasticity and the SA gave the best MAC followed by the GA\nmoduli of elasticity obtained when the FE model is updated using which was followed by the RSM. However,\nthe RSM, GA and SA. Here e10 indicates 10 to the power 10 and these differences in accuracies of the MAC and\nthe units are Nm-2\nnatural frequencies were not significant.\nThe computational time taken to run the complete\nRSM method was 46 CPU seconds, while\nmeasured natural frequency and that from the initial FE\nthe\nSA\nand\na\nfull\nFE model took 19 CPU minutes to run\nmodel, which was obtained when the modulus of elasticand\nthe\nGA\nand\na\nfull FE model took 117 CPU seconds.\n10\n-2\nity of 7.00x10 N.m was assumed, was 4.3%. When\nThe\nRSM\nwas\nfound\nto be faster than the GA which was\nthe RSM was used for FE updating, this error was rein\nturn\nmuch\nfaster\nthan\nthe SA which was faster that the\nduced to 3.1% while using SA it was reduced to 0.2%\nGA.\nOn\nimplementing\nthe\nRSM, 160 FE model evaluaand using the GA approach it was reduced to 0%. The\ntions\nwere\nmade,\nwhile\non\nimplementing the SA 19485\nerror between the second measured natural frequency\nFE model evaluations were made and on implementing\nModulus of Elasticity (e10)\n\napproximation, where the data set had the previous optimum solution added to it, used 5 training cycles. On\nusing the RSM, the MLP was only initialized once. The\nGA was implemented on a population size of 50 and 200\ngenerations. The normalized geometric distribution was\nimplemented with a probability of selecting the best\ncandidate set to 8%, mutation rate of 0.3% and crossover rate of 60%.\nWhen SA and a full FE model was implemented for\nFE updating, the scale of the cooling schedule was set to\n4 and the number of individual annealing runs was set to\n3. When the simulation was run, the first run involved\n7008 FE model calculations, in the second run 6546 FE\nmodel calculations and in the third run 5931 FE model\ncalculations were made.\nOn implementing the GA and a full FE model, the\nsame options as those that were used in the implementation of the RSM were used. The results showing the\nmoduli of elasticity of the initial FE model, RSM updated FE model, SA updated FE model and GA updated\nFE model are shown in Figure 4. Table 1 shows the\nmeasured natural frequencies, initial natural frequencies\nand natural frequencies obtained by the RSM, SA and\nGA updated FE models. The error between the first\n\n7\n\n\fthe GA 10000 FE model calculations were made. In this\npaper, a simple FE model with 39 degrees of freedom is\nupdated. It can, therefore, be concluded that if the FE\nmodel had several thousand degrees of freedom, the\nRSM will be substantially faster than the other methods.\nThis conclusion should be understood in the light of the\nfact that FE models usually have many degrees of freedom.\n\nchanges in their vibration characteristics: a literature\nreview” Los Alamos National Laboratory Report LA13070-MS, 1996.\n10\nEwins, D.J., Modal testing: theory and practice, Research Studies Press, Letchworth, U.K, 1995.\n11\nAllemang, R.J. and Brown, D.L., “A correlation coefficient for modal vector analysis” Proceedings of the 1st\nInternational Modal Analysis Conference, 1-18, 1982.\n12\nSacks, J., Welch, W.J., Mitchell, T.J., and Wynn,\nH.P., “Design and analysis of computer experiments”\nStatistical Science, 4(4), 1989, pp. 409-435.\n13\nVaraajan, S., Chen, W., and Pelka, C.J., “Robust\nconcept exploration of propulsion systems with enhanced model approximation” Engineering Optimization, 32(3), 2000, pp. 309-334.\n14\nGiunta, A. A., and Watson, L. T., “A Comparison of\nApproximation Modeling Techniques: Polynomial Versus Interpolating Models,” AIAA-98-4758, American\nInstitute of Aeronautics and Astronautics, Inc., 1998,\npp.392-401.\n15\nKoch, P. N., Simpson, T. W., Allen, J. K., and Mistree, F., 1999, “Statistical Approximations for Multidisciplinary Design Optimization: The Problem of Size,”\nJournal of Aircraft, 36(1), 1999, pp. 275-286.\n16\nJin, R., Chen, W., and Simpson, T., 2000, “Comparative Studies of Metamodeling Techniques under\nMultiple\nModeling\nCriteria,”\n8th\nAIAA/NASA/USAF/ISSMO Symposium on Multidisciplinary Analysis and Optimization, Long Beach, CA, September 6-8, 2000.\n17\nLin, Y., Krishnapur, K., Allen, J. K., and Mistree,\nF., 2000, “Robust Concept Exploration in Engineering\nDesign: Metamodeling Techniques and Goal Formulations,” Proceedings of the 2000 ASME Design Engineering Technical Conferences, DETC2000/DAC14283, September 10-14, 2000, Baltimore, Maryland.\n18\nWang, G.G., 2003, “Adaptive response surface\nmethod using inherited Latin hypercube design points,”\nTransactions of the ASME, Journal of Mechanical Design,125, pp. 210-220.\n19\nSimpson, T. W., Peplinski, J. D., Koch, P. N., and\nAllen, J. K., “Metamodels for Computer-based Engineering Design: Survey and Recommendations,” Engineering with Computers, 17, 2001, pp. 129-150.\n20\nFletcher, R., Practical Methods of Optimization.\n2nd edition, New York: Wiley, 1987.\n21\nHolland, J, Adaptation in Natural and Artificial Systems, Ann Arbor: University of Michigan Press, 1975.\n22\nGoldberg, D.E., Genetic algorithms in search, optimization and machine learning, Addison-Wesley, Reading, MA, 1989.\n23\nMetropolis, N, Rosenbluth, A.W., Rosenbluth,\nM.N., Teller, A.H., and Teller, E., “Equations of state\n\nConclusion\nIn this study, RSM is proposed for FE model updating. The proposed RSM was implemented within the\nframework of the MLP for functional approximation and\nGA for optimization of the MLP response surface function. This procedure was compared to the GA and SA.\nWhen these techniques were tested on the unsymmetrical\nH-shaped structure, it was observed that the RSM was\nfaster than the SA and GA without much compromise on\nthe accuracy of the predicted modal properties.\n\nAcknowledgment\nThe author would like to thank Stefan Heyns, the now\nNational Research Foundation as well as the AECI, Ltd\nfor their assistance in this work.\n\nReferences\n1\n\nFriswell, M.I., and Mottershead, J.E., Finite element\nmodel updating in structural dynamics, Kluwer Academic Publishers Group, Norwell, MA, 1995, pp. 1-286.\n2\nMontgomery, D.C., Design and analysis of experiments, 4th Edition, John Wiley and Sons, NY, 1995,\nChapter 14.\n3\nMarwala, T., “Finite element model updating using\nwavelet data and genetic algorithm” Journal of Aircraft,\n39(4), 2002, pp. 709-711.\n4\nMarwala, T., and Heyns, P.S., “A multiple criterion\nmethod for detecting damage on structures” AIAA Journal, 195(2), 1998, pp. 1494-1501.\n5\nLevin, R.I. and Lieven, N.A.J, “Dynamic finite element model updating using simulated annealing and\ngenetic algorithms” Mechanical Systems and Signal\nProcessing, 12(1), pp. 91-120.\n6\nBishop, C.M., Neural Networks for Pattern Recognition. Oxford: Clarendon, 1996.\n7\nLee, S.H., Kim, H.Y., and Oh, S.I., “Cylindrical tube\noptimization using response surface method based on\nstochastic process” Journal of Materials Processing\nTechnology, 130-131(20), 2002, pp. 490-496.\n8\nEdwards, I.M., and Jutan, A., “Optimization and control using response surface methods” Computers &\nChemical Engineering, 21(4), 1997, pp. 441-453.\n9\nDoebling, S.W., Farrar, C.R., Prime, M.B., and\nShevitz, D.W., “Damage identification and health monitoring of structural and mechanical systems from\n\n8\n\n\f26\n\ncalculations by fast computing machines,” Journal of\nChemical Physics, 21, 1953, pp. 1087-1092.\n24\nKirkpatrick, S., Gelatt, C.D., and Vecchi, M.P.,\n“Optimization by simulated annealing,” Science, 220,\n1983, pp. 671-680.\n25\nMarwala, T, A multiple criterion updating method\nfor damage detection on structures. University of Pretoria Masters Thesis, 1997.\n\nBalmès, E., Structural Dynamics Toolbox User’s\nManual Version 2.1, Scientific Software Group, Sèvres,\nFrance, 1997.\n27\nMøller, M. “A scaled conjugate gradient algorithm\nfor fast supervised learning” Neural Networks, vol. 6,\n1993, pp. 525-533.\n\n9\n\n\f"
        ],
        [
         "3",
         "3",
         "cs.CE",
         "Computational Engineering",
         "1801.03018v1.pdf",
         "Predict Forex Trend via Convolutional Neural Networks\n\narXiv:1801.03018v1 [cs.CE] 9 Jan 2018\n\nYun-Cheng Tsai,1∗ Jun-Hao Chen,2 Jun-Jie Wang3\n1 Center\n\nfor General Education\nof Computer Science and Information Engineering\n1,2 National Taiwan University, Taipei 10617, Taiwan\n3 National Taipei University, New Taipei City 23741, Taiwan\n\n2,3 Department\n\nDeep learning is an effective approach to solving image recognition problems. People draw\nintuitive conclusions from trading charts; this study uses the characteristics of deep learning\nto train computers in imitating this kind of intuition in the context of trading charts.\nThe three steps involved are as follows:\n1. Before training, we pre-process the input data from quantitative data to images.\n2. We use a convolutional neural network (CNN), a type of deep learning, to train our\ntrading model.\n3. We evaluate the model’s performance in terms of the accuracy of classification.\nA trading model is obtained with this approach to help devise trading strategies. The main\napplication is designed to help clients automatically obtain personalized trading strategies.\nKeywords: Deep Learning, Convolutional Neural Network (CNN), Geometric Brownian\nMotion (GBM), Forex (FX), Trading Strategies.\n\n1\n\nIntroduction\n\nHuman beings are visual animals; the eyes are the most compact structure of all the sensory organs,\nand the visual intelligence of the human brain is rich in content. Exercise, behaviour, and thinking\nactivities use visual sensory data as their greatest source of information. The more flexible and talented\nwe become, the more we rely on visual intelligence.\nWhat general business and decision makers desire after analysis is not the data itself, but the value.\nTherefore, it is important that data analyses be intuitive; in this way, the visualization of financial data\ncan be more easily accepted: they can ”see the story” and thus interpret the data more easily.\n∗\n\nThe author was supported in part by the Ministry of Science and Technology of Taiwan under grant 106-3114-E-001-005.\nTo whom correspondence should be addressed; E-mail: pecutsai@ntu.edu.tw\n\n1\n\n\fAlthough visualization analysis can benefit decision makers, many traditional statistical or machine\nlearning methods for predicting currency movements use quantitative models. These methods do not\nconsider visualization. We attempt to make good use of the advantages of visualization and comprehensively enhance the efficiency of intelligence analysis. For example, most traders use charts to analyse\nand predict currency movement trends, which carry obvious economic benefits. However, in this visualization, analysis is artificial. We intend to teach machines to achieve visualization like a human brain;\nwe then hope to use the machine to visually analyse huge financial data.\nConvolutional neural networks (CNNs) are widely used in pattern and image recognition problems.\nIn these applications, the best possible correction detection rates (CDRs) have been achieved using\nCNNs. For example, CNNs have achieved a CDR of 99.77% using the Modified National Institute\nof Standards and Technology (MNIST) database of handwritten digits, a CDR of 97.47% with the New\nYork University Object Recognition Benchmark (NORB) dataset of 3D objects, and a CDR of 97.6% on\nover 5600 images of more than 10 objects. CNNs not only give the best performance compared to other\ndetection algorithms but also outperform humans in cases such as classifying objects into fine-grained\ncategories such as the particular breed of dogs or species of birds.\nThe two main reasons for choosing a CNN model to predict currency movements are as follows:\n1. CNN models are good at detecting patterns in images such as lines. We expect that this property\ncan also be used to detect the trend of trading charts.\n2. CNNs can detect relationships among images that humans cannot find easily; the structure of\nneural networks can help detect complicated relationships among features.\nCNN is a graph-based model, which is different from quantitative models. People do not need to consider\nall possible features that affect currency movements using quantitative models alone.\nCompared to a quantitative model, a CNN model contains many filters that are similar to the eyes\nof a human being and can extract the features of images. As the convolution layer goes deeper, a CNN\nmodel can also extract more detailed features from the image, just like human visualization.\nPredicting currency movement trends is a time-series problem. Many people look for the Holy Grail\nof prediction, which in fact does not exist. We cannot predict the future in the real world; however,\nwe can define the small world to evaluate our prediction approach. In order to realize the idea, we use\na geometric Brownian motion (GBM) to model the currency movements. We believe that these prices\nfollow, at least approximately, as a subset of real-world rules that we can derive from the historical data\nand our knowledge of prices.\nThe three steps involved are as follows:\n1. Before training, pre-process the training data from quantitative data to images. Our input images\ninclude price, Moving Average 5, Moving Average 10, and Moving Average 20 information.\n2. Use a CNN to train our trading models.\n3. Evaluate the models in terms of the accuracy of classification.\n2\n\n\fWhen we control our small world, we use the CNN model to classify the weekly currency movements\nby separating price series into three groups: rising trend, down-trend, and non-movement groups. The\nremainder of this paper is organized as follows. A review of the literature is given in the next section. In\nSection 3, we present our methodology. Then, a description of the empirical data employed in our study\nis provided in Section 4. Section 5 presents the conclusion of our study.\n\n2\n\nPreliminary\n\nWe use a graph-based model to train a predictive model, rather than using common quantitative\nmethods such as recurrent neural networks (RNNs). In other words, we want to model the thoughts of\npeople rather than the rule-based decisions, which can be clearly stated by the people.\nResearch on using CNNs to predict financial asset prices is limited; most researchers prefer the\nquantitative-based models. However, there are still some researchers attempting to study it.\nDi Persio et al. [2] tried to compare different artificial neural network (ANN) approaches to predict\nstock market indices in classification-based models. They compared three common neural network models, namely multilayer perceptron (MLP), CNN, and long short-term memory (LSTM). They found that\na novel architecture based on a combination of wavelets and CNNs reaches an 83% accuracy rate on\nforeign exchange rates, outperforming the RNNs by 4%.\nDistinct from our work, Di Persio et al. [2] designed their CNN architecture by using a 1-dimensional\nconvolution layer and a 1-dimensional pooling layer. The 1-dimensional convolution layer considers only\nthe price data, which means this convolution layer still captures the quantitative information.\nSimilar to our work, Ashwin Siripurapu used convolution networks to predict movements in stock\nprices with a series of time-series pixel images. The input images to the model are the graphs of high\nand low prices for a 30-min window of time. The input graphs to the model are saved in an RGB color\nspace to highlight the different lines of the stock prices.\nSiripurapu used three kinds of input images. For the first input, he used only the high and low prices,\nand for the second one, he added the volume data together with the high and low prices. For the third\none, he used the correlation feature representation of the top ten companies share of the Standard and\nPoor’s 500 index basket. In the experiment, Siripurapu used two different architectures of conventional\nnetworks, called full and reduced models. The full model had five pairs of convolution-ReLU-pooling\nlayers and was further connected to a fully connected layer. The reduced model reduced the pooling\nlayers in the first two pairs. Although the performance does not exceed 0 for an out-of-sample R square,\nit still gives us many ideas for using pixel images as the input data to a CNN model.\nPeople like to think intuitively when viewing trading charts; many of them cannot clearly explain\nhow to make their decisions and how to achieve better performance. We focus on this direction by using\npixel images as inputs to enable the computer refine the features from it. However, beyond learning, we\nwant to teach the computer to simulate, and thus predict the behaviour of people as they trade on the\ntrading charts; that is, make a model which can learn the trading strategies of the people.\nDefine St as the price of the financial asset at time t. The risk-neutralized version of stock price’s\n\n3\n\n\flog-normal diffusion process is\ndSt = r St dt + σ St dWt ,\n\n(1)\n\nwhere r is the risk-free rate, σ is the constant volatility price process of the financial asset, and the random\nvariable Wt is a standard Brownian motion [1]. St is said to follow a geometric Brownian motion (GBM)\nprocess because it satisfies the above stochastic differential equation (SDE). For an initial value S0 , the\nequation (1), has the analytic solution:\n\u0012\n\u0013\nσ2\nSt = S0 exp r −\nt + σ Wt .\n2\nFrom equation (1), it has the following discrete solution [4]:\n\u0012\n\u0013\n√\nσ2\nXt = Xt−1 + r −\n∆t + σ ∆t Bt ,\n2\n\n(2)\n\nwhere Xt ≡ ln(St ) is the log-price, ∆t ≡ T /n is the length of a time step in the time interval [0, T ]\ndivided into n subintervals, Bt ∼ N (0, 1) is i.i.d. normal random variable and σ is the annualized\nconstant volatility.\nThe CNN is one of the best graph-based models in recent years. Many new architectures of CNNs\nconstantly appeared very fast, but the most original architecture was proposed by K. Fukushima in 1980.\nK. Fukushima proposed a model called Neocognitron, which is generally seen as the model that inspires\nthe CNN on the computation side [3].\nNeocognitron is a neural network designed to simulate the human visual cortex. It consists of two\ntypes of layers, called feature extractor layers and structured connection layers. The feature extractor\nlayers, also called S-layers, simulate the cell in the primary visual cortex, helping human beings to\nperform feature extraction. The structured connection layers, also called C-layers, simulate the complex\ncell in the higher pathway of the visual cortex, providing the model with its shifted invariant property.\nInspired by the Neocognitron and the concept of back propagation, the most generally classic modern\nCNN, LeNet, was proposed by LeCun et al. in 1990. The potential of the modern convolution architecture can be seen in LeNet (1990), consisting of a convolution layer, a subsampling layer, and a full\nconnection layer [5].\nAs the concept of rectified linear unit (ReLU) and drop out were presented in recent years, a new\nconvolution-based model, AlexNet, proposed by Hinton and Alex Krizhevsky, appeared and beat the previous champion of the ImageNet Challenge, with over 15M labelled high resolution images and roughly\n22,000 categories. There are three main differences between LeNet and AlexNet:\n1. The ReLU is used as the activation function in AlexNet. It introduces a non-linearity transform\nafter convolution, which helps the computer to simulate human vision more accurately. The ReLU\nis also a non-saturating activation function and is several times faster than tanh and sigmoid units\nin computation.\n\n4\n\n\f2. A new regularization technique called drop-out was introduced to AlexNet to avoid over-fitting\nwith much less computation. The drop-out technique randomly drops some neurons with a particular probability, and the dropped neurons are not involved in forward and backward computation.\n3. Owing to the technological progress in recent years, AlexNet was supported by a more efficient\nGPU than LeNet (1980). This means that a larger dataset and more epochs can be trailed in the\ntraining process.\nWith the success of AlexNet, many researchers have been motivated to participate in this kind of research,\ninventing architectures with deeper structures and modified convolution such as VGG and GoogleNet.\nThese developments continually improve CNNs in the field of computer vision.\nThe two most important components of CNNs are the convolution layer and pooling layer. The\nconvolution layer implements the convolution operation, which extracts image features by computing\nthe inner product of an input image matrix and a kernel matrix; the number of channels of the input\nimage and kernel matrix must be the same. For example, if the input image is an RGB color space, then\nthe depth of the kernel matrix must be three; otherwise, the kernel matrix cannot capture the information\nbetween different color spaces.\n\nFigure 1: The convolution operation.\n\nAnother important component is the pooling layer, also called the sub-sampling layer, which is\nmainly in charge of simpler tasks. The pooling layer will only retain part of the data after the convolution layer, which reduces the number of large features extracted by the convolution layer and makes\nthe retained features more refined.\nOnly with these two components can the convolution model be used to imitate human vision. In\npractical applications, the CNN model usually combines the convolution layer and pooling layer together.\nThis is because the convolution layer often extracts a great number of features, and most of the features\nmay be noise, which could lead to model learning in the wrong direction. This is the so-called model\nover-fitting problem.\n\n5\n\n\fFigure 2: The pooling operation.\n\nFurthermore, the fully connected layers are usually connected at the end of the sequence. The function of the fully connected layer is to organize the extracted features, which were processed by the\nconvolution and pooling layer. The correlation between the extracted features is learned in this layer.\nAlthough the pooling layer can reduce the occurrence of over-fitting after convolution, it is inappropriate to use after the fully connected layer. Another widely known regularization technique called\ndrop-out is designed to solve this issue. The drop-out technique randomly drops some neurons with a\nspecific probability, and the dropped neurons are not involved in forward and backward computation.\nThis idea directly limits the model’s learning; the model can only update its parameters subject to the\nremaining neurons in each epoch.\n\nFigure 3: The typical architecture of the convolution neural network, which is also the classic LeNet\nmodel.\nNext, we introduce how to generate the data, and how to design the architecture in the first workflow.\nThe input data that we provide the computer with is a pixel image drawing from time I to I + N ,\nwhere index I represents the beginning of each image and index N represents the total length of the\nhistorical data we want the computer to see. After the first image is generated, the beginning of the time\nsequence will advance and keep generating the new images until a specific number of images has been\ncreated, meaning that the time will move from I and I + N to time I + 1 and I + N + 1 and proceed as\nthus until M images have been collected. Then, because we assume increasing and decreasing patterns\n6\n\n\fexist in the foreign exchange, we label the images through time I + N + 1, which is out of the time\nregion of each generated image. Figure 4 depicts the process of generating and labelling data in detail.\n\nFigure 4: The process of creating and labelling data in the workflow 1.\n\nAfter the data are collected, we supervise the model as it learns how to classify the images into three\ncategories: buy, sell, and not taking any action. We expect the model to predict what kind of images\nwill rise or fall in the future; in other words, learning the data from time I to I + N , and predicting the\noutcomes at time I +N +1. Different from the typical image recognition problems with the CNN model,\napplications in finance need to make some modifications. Financial data have time-series characteristics,\nwhich cannot be captured by the convolution model. For this reason, our first workflow combines the\nconcept of moving windows into the CNN model.\nTo consider the time-series properties of the financial data, the single CNN model needs to be modified. It is intuitive to think of training the new CNN model in different time regions; in more detail, we\nuse day I to I + N + 20 to generate data and train a convolution model. After the first run, we move\nto the next time window and train a new convolution model. This process continues to run until all the\npredictions have been made. There are two main advantages of this process: the different CNN models\ncan capture different features in the particular time interval, and this also prevent the CNN models from\nusing noisy features from a long time ago.\nFor example, we may use day 1 to day 20 to make the data and labels, and then train a CNN model\nto predict the outcome on day 21. In the second run, we use day 2 to day 21 to generate the new images\nand labels, and train a new convolution model again to predict the outcome on day 22, and so forth.\nIn terms of the architecture of the convolution model, we first intend to try some simpler models,\nwhich only consist of two or three pairs of convolution and pooling layers before using the famous\nAlexNet model. This is because the images we want the computer to learn are simple sets of one to\nfour closed price line plots including high, low, and the moving average. They are not as complex as the\nImageNet Challenge.\nAll the architectures we used are shown in Figure 6, where Conv, Pool, and FC are the convolutional\n7\n\n\fFigure 5: The process that combines the moving windows into convolution model in the workflow 1.\n\nlayer, pooling layer, and fully connected layer, respectively.\n\nFigure 6: The three architectures in the workflow 1.\n\n1. Architecture 1: Conv + Conv + Pool + FC\n2. Architecture 2: (Conv + Pool) * 2 + FC\n3. Architecture 3: Conv * 4 + Pool + Conv + Pool + FC\n\n8\n\n\fIn the first architecture, we used two convolution layers, further connected to a pooling layer and a\nfully connected layer. In the second architecture, we used two pairs of convolution and pooling layers\nand a fully connected layer, which is similar to the architecture of LeNet. We expect that these two\nsimple architectures can enable the computer to learn the simple structure from the input images.\nIn the third architecture, we designed a deeper architecture consisting of more convolution layers.\nWe used this architecture because we tried to solve the under-fitting problem from the model; simple\narchitecture was not sufficient to learn features from input images.\nThe results of these experiments do not fit the expectation; whether simple or complex, the architectures do not fit the convolution model well. The experimental procedures are illustrated in detail in the\nExperimental Results section.\nAnother architecture which is widely used in our second workflow is the well-known AlexNet model.\nThe AlexNet model appeared in 2012, beat the previous champion, and became the state-of-the-art model\nin the ImageNet Challenge, which has over 15M labelled high resolution images and roughly 22,000\ncategories. The AlexNet model has a deeper structure than LeNet, containing five convolutional layers,\nthree fully connected layers, and a softmax layer. To prevent the model from over-fitting, the AlexNet\nmodel also uses a new regularization method called drop-out and data augmentation, which horizontally\nflips the image or performs random cropping. The AlexNet model also uses the ReLU as the activation\nfunction, which is a non-saturating activation function and is several times faster than tanh and sigmoid\nunits. With these improvements and excellent GPU support, the AlexNet model has become one of the\nmost powerful models today.\n\n3\n\nMethodology\n\nIn this section, we introduce the architectures we used in our experiments and justify our decision\nfor using these workflows. We also illustrate some data preprocessing techniques used to generate our\ninputs. The deep learning frameworks used in each workflow are the Python Keras module and NVIDIA\nDigits with the Caffe back-end. All the convolution models in both workflows were trained for 30 epochs\nand were speeded up by the GTX TITAN GPU. We also tried to observe the result of different epochs,\neven up to 4000 epochs, but the over-fitting almost significantly occurs at about 50 − −100 epochs. The\nworkflows are as follows:\n\n3.1\n\nWorkflow 1\n\nIn the first workflow, we used the real-world exchange rates of Japanese Yen from 2010 to 2011. We\ndesigned three kinds of convolution architectures and expected one of these architectures to fit the realworld data well. The overview of the raw data is shown in Figure 7 and the first workflow is enumerated\nin detail as follows:\n1. Transform the quantitative price data to image data using the Python Matplotlib module, and create\nclassification-based labels which consist of buy, sell, and not taking any action.\n\n9\n\n\f2. Create the three architectures of the CNN model by using the Python Keras deep learning module.\nEach of the architectures will be experimented independently.\n3. Train the CNN model and tweak the parameters to maximize accuracy. The number of epochs\nused for training ranges from 30 to 100.\n4. Evaluate the model with a confusion matrix for currency performance.\n5. Repeat the above steps until the best model is found.\n\nFigure 7: The exchange rates of Japanese Yen from November 9, 2010 to January 13, 2011.\n\n3.2\n\nWorkflow 2\n\nBecause the performance of workflow 1 was not as good as expected, we switched to using simulation\ndata from the GBM. We simulated 90 days foreign exchange rate data repeatedly, for 100 times, with\na 1% yearly return and 25% yearly standard errors. We believed these prices approximately followed a\nsubset of the real-world data, and therefore, we expected the new architecture to fit well in the subset of\nthe real world. One of the simulated data is shown in Figure 8 and the second workflow is enumerated\nin detail as follows:\n1. Transform the quantitative price data to image data using the Python Matplotlib module, and create\nclassification-based labels which consist of buy, sell, and not taking any action.\n10\n\n\f2. Create the AlexNet architecture of the CNN model by using NVIDIA DIGITS with the Caffe backend. NVIDIA DIGITS is a lightweight tool, especially good at presenting the training process in\nreal time.\n3. Train the AlexNet model and tweak the parameters to maximize accuracy. The number of epochs\nused for training is 50.\n4. Evaluate the model with a confusion matrix for currency performance.\n5. Repeat the above steps until the best model is found.\n\nFigure 8: One of the simulation data generated by the Geometric Brownian Motion process (GBM).\n\nThe structure of workflow 2 is almost the same as that of workflow 1. The main difference in\nworkflow 2 is the way data is labelled; in workflow 1, the same strategy is used to label all inputs, but\nmany kinds of strategies are used in workflow 2. In workflow 2, we used the AlexNet model with its\ndefault parameters, and tweaked only the epochs and the different kinds of input images. The strategies\nwe used in the workflow 2 are listed below:\n1. Use every 20 days period as an image and the following 5 days as holding days; that is, we use\nday 1 to day 20 as the input image, and use day 25 to label day 20. If the price on day 25 is greater\nthan day 20 by at least 1%, then we will buy on day 20 and sell on day 25. If the price on day 25\nis less than day 20 by at least 1%, then we will sell on day 20 and buy on day 25. Otherwise, no\naction will be taken.\n2. In this case, we tried to use the moving average as our strategy. Because we wanted the inputs to\nbe more distinguishable by the model, the rule we used was that if MA5 is greater than MA7 by at\n11\n\n\fleast 1% and MA7 is greater than MA10 by at least 1% on day 9, then we will buy on day 6 and\nsell on day 9. If MA5 is less than MA7 by at least 1% and MA7 is less than MA10 by at least 1%\non day 9, then we will sell on day 6 and buy on day 9. Otherwise, no action will be taken.\n3. Furthermore, we also simulated both open and closed price, and plotted it with the MA5, MA10,\nand MA20 lines. We used every 15 days period as the image and the following 5 days as the\nholding period. The strategy used here is that if the opening price on day 20 is greater than the\nclosing price on day 15 by at least 2%, then we will buy on day 15 and sell on day 20. If the\nopening price on day 20 is less than the closing price on day 15 by at least 1%, then we will sell\non day 15 and buy on day 20.\n\n4\n\nExperimental Results\n\nFirst, we introduce three ways to pre-process the image data; second, we discuss problems we encountered in the experimental procedure and illustrate how to solve them. The pre-process frameworks\nwe used are the Python Matplotlib module and Python Pillow module. The following are the three ways\nin which we pre-process our images:\n1. Crop the images without the information of the x-axis and y-axis. This is because we want our\ninput data to be as clean as possible.\n2. Use the RGB color space to capture the information of moving average lines. Different colors\nwill be given to each moving average line, so the moving average lines will be represented in the\ndifferent channels.\n3. Invert the color space to highlight only the lines in the image. The background will become black,\nwhich means the value of each background pixel is zero.\nThe moving average lines we used are MA5, MA7, MA10, and MA20. We used moving average\nlines to simulate our inputs and increase similarity to the trading charts. We also rescaled the images\nto different sizes, for example, 100 × 150 or 300 × 400. We also tried to set the different y-axes in the\nsame scale. The image of the moving average lines is shown in Figure 9 and the inverted one is shown\nin Figure 10.\n\n4.1\n\nWorkflow 1\n\nIn workflow 1, we tried three architectures. The default time region is 20; in each region, we used\nevery 5-day period to create the image data and used the next day to label the input images. Each\narchitecture used the framework of the moving windows and predicted a hundred times. The three\narchitectures we tried are as follows:\n1. Architecture 1: Conv + Conv + Pool + FC\n12\n\n\fFigure 9: The image data with price and moving average lines without resizing. Black is the price line,\nred is the MA5 line, blue is the MA10 line, and green is the MA20 line. There are still many different\npermutations and combinations of the price and moving average lines.\n\nFigure 10: The image data only with price line preprocessed with inversion.\n\n13\n\n\f2. Architecture 2: (Conv + Pool) * 2 + FC\n3. Architecture 3: Conv * 4 + Pool + Conv + Pool + FC\nWe used the first two architectures (architecture 1 and architecture 2) because we expected a simple\nmodel could solve our problem; however, the results were not good. Therefore, we next used a deeper\nstructure with architecture 3; we added more convolution layers and filters in the first two layers to help\nthe model extract more detailed information. We hoped a more complex architecture would help solve\nthis problem. Unfortunately, neither the simple nor the complex architectures worked well. The complex\none did not improve the performance of classification. The experimental results of each architecture are\nshown below.\n\n4.2\n\nArchitecture 1\n\nFor architecture 1, we carried out three experiments. We inverted all input images and resized them\nto 100 × 150. We used different parameters in each experiment, as follows:\n1. In the first experiment, we used a kernel size of 30 × 40, with 5 kernels and 128 fully connected\nunits. The pooling layer we used was MaxPooling 2 × 2 and the time region used was 20, which\nmeans using 20 days historical information to predict the action for the next day.\n2. In the second experiment, we used a kernel size of 30×40, with 10 kernels and 128 fully connected\nunits. The pooling layer we used was MaxPooling 2 × 2 and the time region used was 20.\n3. In the third experiment, we used a kernel size of 30 × 40, with 5 kernels and 128 fully connected\nunits. The pooling layer we used was MaxPooling 2 × 2, but this time we used 30 days as our time\nregion, which means using 30 days historical information to predict the action for the next day.\nThe results of the three experiments are described in Figures 11—13, respectively. There is no\nsignificant improvement between parameters; the model often predicts the action to be doing nothing.\n\nFigure 11: The confusion matrix of the experiment 1 in architecture 1.\n\n14\n\n\fFigure 12: The confusion matrix of the experiment 2 in architecture 1.\n\nFigure 13: The confusion matrix of the experiment 3 in architecture 1.\n\n4.3\n\nArchitecture 2\n\nThe parameters used in the second experiment are the same as those used in the first experiment; only\nthe architecture of the model is different. The performance of the second architecture is also poor, with\nthe model once again giving the prediction of taking no action often. One result is shown in Figure 14.\n\nFigure 14: The confusion matrix of the experiment 1 in architecture 2.\n\nWe made some changes to the architectures because we obtained poor performance with the architectures and experiments above: we added two more convolution layers and an additional pooling layer\nto make the model deeper and more complex.\n15\n\n\f4.4\n\nArchitecture 3\n\nWith the new, more complex architecture, we designed three kinds of experiments. The parameters\nof each experiment are almost the same; the only difference between the three experiments is the number\nof kernels. This is because we expected more filters would capture more features of the image. In\nexperiments 1–3, the number of kernels is designated as 5, 10 and 20. The results of each experiment are\ndescribed in Figures 15–17.\n\nFigure 15: The confusion matrix of the experiment 1 in architecture 3.\n\nFigure 16: The confusion matrix of the experiment 2 in architecture 3.\n\nFigure 17: The confusion matrix of the experiment 3 in architecture 3.\n\n16\n\n\fFrom the results of the three architectures, we can clearly see that none of the experiments yielded\ngood performance. Additionally, each model is unstable due to over-fitting. This is because the number\nof input images is too small to train the convolution model; if the time region is 20 and if we use every 5\ndays period to create an image, we only have 16 images of training data.\nThe convolution model can fit the given 16 images training data well, but cannot recognize images\nwith many differences to the training data. The only way to obtain more real-world training data is\nto extend the time region; in finance, however, older information does not help predict future data.\nAdditional data would only increase the occurrence of noise, meaning we cannot simply extend the time\nregion to collect more training data; an alternative approach is required.\n\n4.5\n\nWorkflow 2\n\nBefore addressing the real-world data, we wanted to fit the model with the simulated data. This\nis because the simulated data can give us sufficient data, with little noise. In addition, simulated data\naccurately represents a subset of the real-world data, and therefore may be easier to fit. If we can fit\nthe small world well, the convolution model can learn strategies from it. We used a mean of 1% and a\nstandard error of 25% to simulate 90 days data; we simulated it many times to generate enough data for\nthe convolution model. The three experiments, trained with the simulated data, are introduced in detail\nas follows:\n\n4.6\n\nExperiments 1\n\nIn experiment 1, we used every 20 days period to create an image and the following 5 days as the\nholding days; that is, we may use day 1 to day 20 as the input image, and day 25 to label day 20. If\nthe price on day 25 is larger than day 20 by at least 1%, then we will buy on day 20 and sell on day 25.\nIf the price on the day 25 is smaller than day 20 by at least 1%, then we will sell on day 20 and buy\non day 25. Otherwise, no action will be taken. The images of the three different classes are shown in\nFigures 18—20.\n\nFigure 18: The experiment 1 with label 1.\n\nWe can clearly see that each class cannot be easily distinguished by humans; this also makes it\ndifficult for the convolution model to recognize the pattern of each class. In the training process of\n17\n\n\fFigure 19: The experiment 1 with label −1.\n\nFigure 20: The experiment 1 with label 0.\n\nthis case, which is shown in Figure 21, the loss of the training data and the validation data were not\ndecreasing. The over-fitting problem also occurred after the 100th epoch.\nThis time, the accuracy of the simple convolution model is better than the moving average one. The\nmodel predicts better in label 1 and −1, but there are still many regions in which it could be improved.\nFigures 22 and 23 show the confusion matrix of the training and testing data.\n\n4.7\n\nExperiments 2\n\nInspired by experiment 1, we tried to use the moving average as our strategy. Because we wanted the\ninputs to be more distinguishable by the model, the rule we used was that if MA5 is greater than MA7\nby at least 1% and MA7 is greater than MA10 by at least 1% on day 9, then we will buy on day 6 and\nsell on day 9. If MA5 is less than MA7 by at least 1% and MA7 is less than MA10 by at least 1% on day\n9, then we will sell on day 6 and buy on day 9. Otherwise, no action will be taken.\nThe three kinds of labelled images are shown in Figures 24–26, and we can see that the pattern is\nmore significant in the buying (1) and selling (−1) labels now. This makes it easier for the convolution\nmodel to detect the difference between the strategies. After the trials of experiment 3, we achieved an\naccuracy rate of 82%, which is a significant improvement over experiments 1 and 2. We also scaled\nthe images to the maximum and minimum of the prices and the moving average; this yielded an 80%\naccuracy rate.\nThe training process is shown in Figure 27. In the experiment, we used 25% of the data for validation\n18\n\n\fFigure 21: The training process of the experiment 1.\n\nFigure 22: The training process of the experiment 1.\n\n19\n\n\fFigure 23: The training process of the experiment 1.\n\nFigure 24: The experiment 2 with label 1.\n\n20\n\n\fFigure 25: The experiment 2 with label −1.\n\nFigure 26: The experiment 2 with label 0.\n\n21\n\n\fand 25% for testing. The accuracy rate increased to 82% in the 70th epoch. The problem of over-fitting\ndoes not occur, which can be explained by the loss of the training data and the validation data.\n\nFigure 27: The training process of the experiment 3.\n\nThe confusion matrix of the training data and the testing data are shown in Figures 28 and 29. From\nthe result, we can see that the accuracy of each class is not significantly impacted by the over-fitting\nproblem. The accuracy of the testing data is only slightly lower than the training data.\n\nFigure 28: The confusion matrix of the training data.\n\nThe experimental results of the images scaled to the maximum and minimum of the prices and\nmoving average are as follows. Figures 30–32 show the images classified by the MA strategy. Figure 33\n22\n\n\fFigure 29: The confusion matrix of the testing data.\n\ndescribes the training process. Figures 34 and 35 show the confusion matrix of the training and the\ntesting data. The results of this case achieved an accuracy rate of 82%, which is better compared to the\nearlier rate.\n\nFigure 30: The experiment 2 with scaling with label 1.\n\n4.8\n\nExperiments 3\n\nWe also simulated both open and closed price, and plotted them with the MA5, MA10, and MA20\nlines. We used every 15 days period to create an image and the following 5 days as the holding days. If\nthe opening price on day 20 is greater than the closing price on day 15 by at least 2%, then we will buy\non day 15 and sell on day 20. If the opening price on day 20 is less than the closing price on day 15 by at\nleast 1%, then we will sell on day 15 and buy on day 20. The three kinds of labelled images are shown\n23\n\n\fFigure 31: The experiment 2 with scaling with label −1.\n\nFigure 32: The experiment 2 with scaling with label 0.\n\n24\n\n\fFigure 33: The training process of the experiment 2 with scaling.\n\nFigure 34: The confusion matrix of the training data with scaling.\n\nFigure 35: The confusion matrix of the testing data with scaling.\n\n25\n\n\fin Figures 36–38.\n\nFigure 36: The experiment 3 using open, close, MA5, MA10, and MA20 with label 1.\n\nIn this case, the images are also distinguished by our strategy. We expected the accuracy of the\nclassification will be good; the results proved this. In Figure 39, the model obtained an accuracy rate of\n87% in the 30th epoch, and the accuracy rate for each class was also better than that of experiment 2.\nWe also examine the visualization after the convolution layer. The outputs after the first two convolution layers with the demo image are shown in Figures 40 and 41; we can clearly see that the kernels in the\nfirst two layers can capture the shape of the lines. In this image, which is the buy action, the convolution\nmodel can clearly capture the pattern of the increasing trend.\n\n5\n\nConclusions\n\nIn workflow 1, neither the simple nor complex CNN architectures produced the expected performance. The main cause of this is the lack of data for each convolution model. Attempting to use\nadditional, older historical data would only introduce additional noise and further mislead the convolution model. Therefore, we narrowed the scope of our research to fit in the simulated world, which is\ngenerated by applying the GBM calibrated from the real-world data.\nIn workflow 2, the main difference between the first two experiments (experiment 1 and experiment\n2) and the last two experiments (experiment 3 and experiment 4) is the strategies employed. In the first\n26\n\n\fFigure 37: The experiment 3 using open, close, MA5, MA10, and MA20 with label −1.\n\ntwo experiments, the trend of the different labels was not obvious, whereas in the last two experiments,\nthe trend was clearly seen by the human eye. Therefore, the convolution model showcases better performance for the last two strategies, especially for the buy and sell actions.\nWe conclude that if the strategy is clear enough to make the images obviously distinguishable, then\nthe CNN model can predict the prices of a financial asset; the default AlexNet model is also considered\ngood enough for prediction.\nThere are additional factors we intend to research in future; for example, combining the convolution\nmodel with the other architectures, like the LSTM. The architecture of the time-series model may help\nthe convolution model to capture more information from the pixel images.\n\n27\n\n\fFigure 38: The experiment 3 using open, close, MA5, MA10, and MA20 with label 0.\n\nFigure 39: The training process of the experiment 3.\n\n28\n\n\fFigure 40: The visualization of the first convolution layer with the demo image.\n\nFigure 41: The visualization of the second convolution layer with the demo image.\n\n29\n\n\fReferences\n1. S. Browne. Optimal investment policies for a firm with a random risk process: Exponential utility\nand minimizing the probability of ruin. Mathematics of Operations Research, 20(4):937–958, 1995.\n2. L. Di Persio and O. Honchar. Artificial neural networks approach to the forecast of stock market price\nmovements. International Journal of Economics and Management Systems, 1:158–162, 2016.\n3. K. Fukushima and S. Miyake. Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition. In Competition and cooperation in neural nets, pages 267–285.\nSpringer, 1982.\n4. S. E. Shreve. Stochastic Calculus for Finance II: Continuous-Time Models. Springer, New York,\n2004.\n5. H. Wang, B. Raj, and E. P. Xing. On the origin of deep learning. arXiv preprint arXiv:1702.07800,\n2017.\n\n30\n\n\f"
        ],
        [
         "4",
         "4",
         "cs.CE",
         "Computational Engineering",
         "0901.2665v1.pdf",
         "A Density Matrix-based Algorithm for Solving Eigenvalue Problems\nEric Polizzi∗\nDepartment of Electrical and Computer Engineering, University of Massachusetts, Amherst\n(Dated: January 17, 2009)\n\narXiv:0901.2665v1 [cs.CE] 17 Jan 2009\n\nA new numerical algorithm for solving the symmetric eigenvalue problem is presented. The\ntechnique deviates fundamentally from the traditional Krylov subspace iteration based techniques\n(Arnoldi and Lanczos algorithms) or other Davidson-Jacobi techniques, and takes its inspiration\nfrom the contour integration and density matrix representation in quantum mechanics. It will be\nshown that this new algorithm - named FEAST - exhibits high efficiency, robustness, accuracy\nand scalability on parallel architectures. Examples from electronic structure calculations of Carbon\nnanotubes (CNT) are presented, and numerical performances and capabilities are discussed.\nPACS numbers: 02.60.-x,02.70.Hm,02.70.-c,02.10.Ud,31.15.-p,71.15.Dx\nKeywords: diagonalization technique, contour integration, electronic structure calculations\n\nI.\n\nINTRODUCTION\n\nThe generalized eigenvalue problem, that is the determination of nontrivial solutions (λ, x) of Ax = λBx\nwith A and B square matrices, is a central topic in numerical linear algebra and arises from a wide range of\napplications in sciences and engineering. In electronic\nstructure calculations, in particular, the eigenvalue problem is one of the most challenging applied numerical\nprocess - also called diagonalization procedure or spectral decomposition. In these calculations, the electron\ndensity can be formally calculated by summation of the\namplitude square of the wave functions Ψm solution of\nthe Schrödinger-like eigenvalue problem HΨm = Em SΨm\nwith different discrete energies Em (where H represents\nthe Hamiltonian Hermitian matrix and S is a symmetric positive matrix obtained using non-orthogonal basis\nfunctions). This procedure can be quite computationally\nchallenging for large-scale simulations of systems containing more than a hundred of atoms and/or where a large\nnumber of eigenpairs (Em , Ψm ) are needed. Progress in\nelectronic structure calculations as for other large-scale\nmodern applications, are then much likely dependent on\nadvances in diagonalization methods.\nIn the past decades, the eigenvalue problem has led\nto many challenging numerical questions and a central\nproblem [1]: how can we compute eigenvalues and eigenvectors in an efficient manner and how accurate are they?\nPowerful tools have then been developed from Jacobi\nmethod and power iterations, to iterative Krylov subspace techniques (including Arnoldi, and Lanczos methods), or other Davidson-Jacobi techniques [2]. Traditional numerical algorithms and library packages are yet\nfacing new challenges for addressing the current largescale simulations needs for ever higher level of efficiency,\naccuracy and scalability in modern parallel architectures.\nThis article presents a new robust and scalable algorithm design for solving the eigenvalue problem- named\n\n∗ URL:\n\nhttp://www.ecs.umass.edu/ece/polizzi\n\nFEAST- which deviates fundamentally from the traditional techniques above, and takes its inspiration from\nthe density matrix representation and contour integration in quantum mechanics. Section II summarizes the\nelectronic structure and contour integration problems\nwhich have motivated the development of the new algorithm. The FEAST algorithm is then described in detail in Section III, and numerical examples and performance results are presented in Section IV. Finally, Section V presents some discussions regarding the efficiency,\nrobustness and scalability of the algorithm.\nII. THE CONTOUR INTEGRATION\nTECHNIQUE IN ELECTRONIC STRUCTURE\nCALCULATIONS\n\nAlthough new fast sparse solvers have allowed considerable time saving for obtaining the eigenpairs (Em , Ψm )\nin electronic structure calculations, such as the Rayleighquotient multigrid [3] developed for the MIKA package,\nor the parallel Chebyshev subspace iteration technique\ndeveloped for the PARSEC package [4, 5], these calculations are still considered computationally extremely challenging and linear scalability is not easily achievable.\nAn alternative approach to the Schrödinger picture\nfor obtaining the electron density consists in performing a contour integration of the diagonal elements of\nthe Green’s function matrix G(Z) = (ZS − H)−1 over the\ncomplex energy space [6]. At zero temperature, the resulting expression for the electron density in real-space\nis:\nZ\nX\n1\nn(r) = −\ndZ G(r, r, Z) =\n|Ψm (r)|2 , (1)\n2πı C\nm\nwhere the complex contour C includes all the eigenvalues Em below the Fermi level EF , and where the spin\nfactor is not considered. It should be noted that at non\nzero temperatures, this expression would also include the\ncontribution of the residues of all poles of the FermiDirac distribution function on the imaginary axis at the\nposition of the Fermi level [7]. For transport problems\n\n\f2\nand open systems, in turn, the contour integration is often used to compute the equilibrium part of the electron\ndensity [8] where self-energy boundary conditions need to\nbe included in the Hamiltonian matrix H. The contour\nintegration technique represents a priori an attractive alternative approach to the traditional eigenvalue problem\nfor computing the electron density since the number of\nGreen’s function to be calculated -typically ∼ O(10) using a Gauss quadrature procedure- is independent of the\nsize of the system. In particular, an efficient linear scaling strategy CMB (CMB stands for Contour integration\n- Mode approach - Banded system solver), has been proposed in [9, 10] for simulating nanowire-type structures\nwithin a real-space mesh framework while overcoming the\nimpossible numerical task of inverting a large size matrix\nat each point of the contour. For arbitrary systems (i.e.\nbeyond nanowire structures), however, there are no numerical advantages of abandoning the traditional eigenvalue problem in favor of the contour integration technique for computing the electron density. In addition, it\nis clear from equation (1) that the contour integration\ntechnique does not provide a natural route for obtaining\nthe individual eigenvectors but rather the summation of\ntheir amplitudes square. In the following section, a new\nnumerical algorithm design FEAST is proposed for obtaining directly the eigenpairs solutions using the density\nmatrix representation and a numerically efficient contour\nintegration technique.\nIII.\nA.\n\nFEAST\n\nIntroduction\n\nIn this section, a new algorithm is presented for solving\ngeneralized eigenvalue problems of this form\nAx = λBx,\n\nin quantum mechanics. One can show that this factorization can be expressed in terms of the eigenvectors present\ninside the contour as follows:\n\nρ=−\n\nZ\n\ndZ G(Z) ==\n\nC\n\nM\nX\n\n|xm ihxm |.\n\n(3)\n\nm=1\n\nIn matrix notations the second term of the equation reads\nXXT where XN×M = {x1 , x2 , ..xM } (M being the number of eigenvalue inside the contour and N the size of\nG). It should be noted that the diagonal elements of ρ\nrepresent the electron density in quantum mechanics (1)\ndiscussed in Section II.\nPostmultiplying ρ by a set of M linearly independent random vectors YN×M = {y1 , y2 , ..yM }, the first expression in (3) leads to a new set of M independent vectors\nQN×M = {q1 , q2 , ..qM } obtained by solving linear systems along the contour\nZ\n1\nQN×M = −\ndZ G(Z)YN×M ,\n(4)\n2πı C\nwhile the second expression in (3), implies these vectors\nQ can be formally generated by the eigenfunctions X\ninside the contour\nQN×M = XN×M WM×M\n\nwith\n\nWi,j = xT\ni yj .\n\n(5)\n\nIn other words, each Q column vector obtained in (4)\nrepresents a different linear combination of unknown basis functions X in (5). Using a Rayleigh-Ritz procedure, the problem (2) is now equivalent to computing the\neigenpairs (ǫm , Φm ) of the following reduced generalized\neigenvalue problem of size M:\nAQ Φ = ǫBQ Φ\n\n(2)\n\nwithin a given interval [λmin , λmax ], where A is real symmetric or Hermitian and B is a symmetric positive definite (s.p.d). One common way to accelerate the convergence rate of traditional iterative techniques consists\nin performing a factorization of the Green’s function\nG(σ) = (σB − A)−1 for some reasonable shift σ close\nto the eigenvalues in the search interval and which leads\nto solving linear systems (i.e. shifting strategy). More\nrecently, Sakurai et al. [11, 12] have proposed a root\nfinding technique which consists of a contour integration\nof a projected Laurent series-type decomposition of the\nGreen’s function. In principle, a set of complex moments\ncan be obtained by solving few linear systems along the\ncontour, which can generate an identical subspace to the\none spanned by the eigenvectors present inside the contour. In practice, however, robustness and accuracy are\nnot easily achievable. In our approach, we avoid decomposing directly the Green’s function and perform instead\nan exact mathematical factorization of its contour integration - which represents the reduced density matrix ρ\n\n1\n2πı\n\nT\n\nwith AQ = Q AQ\n\nT\n\nand BQ = Q BQ.\n\n(6)\n(7)\n\nThe Ritz values and vectors are then given by:\nλm = ǫm , m = 1, . . . , M\nXN×M = QN×M ΦM×M\n\n(8)\n(9)\n\nwhere ΦM×M = {Φ1 , Φ2 , ..ΦM }. One can show that the\nobtained eigenvectors X are naturally B-orthonormal i.e.\nxT\ni Bxj = δi,j , if the eigenvectors of the reduced problem\n(6) are BQ -orthonormal i.e. ΦT\ni BQ Φj = δi,j .\n\nB.\n\nPractical Considerations and Pseudocode\n\nIn practice, the vectors Q are computed by performing\na numerical integration of each vectors G(Z)Y (4) along\nthe complex contour C. Let us consider a circle centered\nin the middle of the search interval [λmin , λmax ], it should\nbe noted that the expression of the contour integration\n\n\f3\n(ii) Postmultiplying the density matrix (3) by M0 random vectors (rather than M) where M0 is greater than\nM. The reduced dense generalized eigenvalue problem\n(6) of size M0 can be solved using standard eigenvalue\nLAPACK routines [13]. Since we do not perform the orthogonalization of the vectors Q, one has to make sure\nthat BQM0 ,M0 is symmetric positive definite i.e. M0 does\nnot exceed an upper limit which can easily be obtained\na posteriori.\n\nC+\n\nλmin\n\nλ\n\nλmax\n\nFIG. 1: Schematic representation of the complex contour integral defined by the positive half circle C + . In practice, the vectors Q are computed via a numerical integration (e.g. GaussLegendre quadrature) where only few linear systems G(Z)Y\nneeds to be solved at specific points Ze along the contour.\n\n1- Select M0 > M random vectors YN×M0 ∈ RN×M0\n2- Set Q = 0 with Q ∈ RN×M0 ; r = (λmax − λmin )/2;\nFor e = 1, . . . Ne\n\ncan be further simplified since G(Z̄) = G† (Z). Denoting C + the positive half circle of the complex contour, it\ncomes if A is Hermitian:\nZ\n1\ndZ {G(Z) − G† (Z)},\n(10)\nρ=−\n2πı C +\nand if A is real symmetric:\nZ\n1\nρ=−\ndZ ℑ{G(Z)},\nπ C+\n\ncompute θe = −(π/2)(xe − 1),\ncompute Ze = (λmax + λmin )/2 + r exp(ıθe ),\nsolve (Ze B − A)Qe = Y to obtain Qe ∈ CN×M0\ncompute Q = Q − (ωe /2)ℜ{r exp(ıθe ) Qe }\nEnd\n3- Form AQ M0 ×M0 = QT AQ and BQ M0 ×M0 = QT BQ\n4- Solve AQ Φ = ǫBQ Φ to obtain the M0 eigenvalue ǫm ,\n\n(11)\n\nwhere ℑ{} stands for the imaginary part. Using a Ne point Gauss-Legendre quadrature on the positive half circle C + (see Figure 1), with xe the eth Gauss node associated with the weight ωe , one obtains if A is Hermitian\nand Y, Q ∈ CN×M :\n\nand eigenvectors ΦM0 ×M0 ∈ RM0 ×M0\n5- Set λm = ǫm and compute XN×M0 = QN×M0 ΦM0 ×M0\nIf λm ∈ [λmin , λmax ], λm is an eigenvalue solution\nand its eigenvector is Xm (the mth column of X).\n6- Check convergence for the trace of the eigenvalues λm\nIf iterative refinement is needed, compute Y = BX\n\nQ=−\n\nNe\nX\n1\ne=1\n\n4\n\n\u0001\nωe r exp(ıθe ) G(Ze ) + exp(−ıθe ) G† (Ze ) Y,\n\n(12)\n\nand go back to step 2\n\n(15)\n\nFIG. 2: FEAST pseudocode (sequential version) for solving\nthe generalized eigenvalue problem Ax = λBx, where A is\nreal symmetric and B is s.p.d., and obtaining all the M eigenpairs within a given interval [λmin , λmax ]. The numerical integration is performed using Ne -point Gauss-Legendre quadrature with xe the eth Gauss node associated with the weight\nωe . For the case Ne = 8, one can use:\n(x1 , ω1 ) = (0.183434642495649, 0.362683783378361),\n(x3 , ω3 ) = (0.525532409916328, 0.313706645877887),\n(x5 , ω5 ) = (0.796666477413626, 0.222381034453374),\n(x7 , ω7 ) = (0.960289856497536, 0.101228536290376),\nand (x2i , ω2i )i=1,...,4 = (−x2i−1 , ω2i−1 )\n\nwhere ℜ{} stands for the real part.\nIn order to reduce the numerical quadrature error of\nthe contour integral, one may consider the two following\nimprovements:\n(i) Performing outer-iterative refinement steps. Once the\neigenvectors X are obtained (9), a new set of initial guess\nvectors Y = BX can be used. Postmultiplying the density matrix (3) by Y, one now obtains from (5) that Q\nconverges to X since XT BX = I (i.e. Wi,j = δi,j and\nthen ρBX = X). A fast test for convergence can be\nobtained by checking the trace of the eigenvalues (8).\n\nThe performances of the basic FEAST algorithm will\nthen depend on a trade off between the choices of the\nnumber of Gauss quadrature points Ne , the size of the\nsubspace M0 , and the number of outer refinement loops.\nSo far, using M0 ≥ 1.5M, Ne = 8, and with at most 2\nrefinement loops, we have consistently obtained a relative residual equal or smaller than 10−10 seeking up to\n1000 eigenpairs for a variety of problems. The basic\npseudocode for the FEAST algorithm is given in Figure 2 in the case of A real symmetric. In the case of\nA complex Hermitian, we note the following changes:\n\nwith\nr=\n\nλmax − λmin\n, θe = −(π/2)(xe − 1),\n2\nλmax + λmin\n+ r exp(ıθe ).\nZe =\n2\n\n(13)\n(14)\n\nIf A is real symmetric, Y, Q ∈ RN×M and one can use:\nQ=−\n\nNe\nX\n1\ne=1\n\n2\n\nωe ℜ{r exp(ıθe ) G(Ze )Y},\n\n\f4\nY, Q ∈ CN×M0 , Φ ∈ CM0 ×M0 , and the construction of\nthe vectors Q in step-2 of the pseudocode must be modified to satisfy (12).\n\nIV.\n\nNUMERICAL EXPERIMENTS\n\nIn this section, we propose to demonstrate the numerical stability, robustness and scalability of the FEAST\nalgorithm using three examples derived from electronic\nstructure calculations of Carbon nanotube (CNT).\n\nA.\n\nExample I\n\nLet us first consider a family of eigenvalue problems,\nTest-CNT, obtained using a 2D FEM discretization of the\nDFT/Kohn-Sham equations at a given cross-section of a\n(13,0) CNT – the 2D atomistic potential is derived from\nthe mode approach used in the CMB strategy for solving\nthe full 3D problem presented in [9]. In Test-CNT, A is\nreal symmetric, and B is s.p.d., the size of both matrices\nis N = 12, 450 and their sparsity pattern is identical with\na number of non-zero elements nnz = 86, 808.\nIn Table I, we report the times and relative residual\nobtained by the public domain eigenvalue solver package ARPACK [14] (using the shift-invert strategy) and\nthe FEAST algorithm presented in Figure 2 for solving\nthe Test-CNT example seeking up to M = 800 (lowest) eigenpairs. The inner linear systems in ARPACK\nand FEAST are solved using the shared-memory parallel\ndirect solver PARDISO [15]. It should be noted, however, that FEAST benefits more than ARPACK from the\nPARDISO solver, as the inner linear systems have multiple right-hand sides. Although both algorithms could\nbenefit from a parallel distributed implementation (e.g.\nusing the P ARPACK package), the simulation runs are\nhere restricted to a given node of a 8-cores Intel Clovertown system (16Gb,2.66GHz) where the linear systems\nin FEAST are factorized and solved one after another.\nThe performances of ARPACK and FEAST can also depend on fine tunings parameters such as the choices of the\nsize of the subspace M0 (M0 = 1.5M here for both algorithms), the inner systems solvers, the number of contour\npoints Ne for FEAST, or the stopping criteria for obtaining the residual. The simulation results in this section\nare then not intended to compare quantitatively the two\nsolvers but rather to point out the potentialities of the\nFEAST algorithm.\nIn our experiments, the convergence criteria on the relative residual for FEAST is obtained\nP when the relative\nerror on the trace of the eigenvalues m λm in the search\ninterval is smaller or equal to 10−13 . Table II shows the\nvariation of the relative error on the trace with the number of outer-iterative refinement for FEAST. These results demonstrate that only 2 to 3 refinement loops are\nnecessary to obtain the small relative residuals for the\ndifferent cases reported in Table I. It should be noted\n\nTEST-CNT\nARPACK\nN = 12, 450 Time(s)\nResid.\nM = 100\n12.2\n2.0 ∗ 10−11\nM = 200\n31\n2.0 ∗ 10−11\nM = 400\n86\n1.4 ∗ 10−11\nM = 800\n213\n4.5 ∗ 10−9\n\nFEAST\nTime(s)\nResid.\n7.8\n4.5 ∗ 10−10\n14\n5.5 ∗ 10−10\n21\n1.8 ∗ 10−10\n58\n3.4 ∗ 10−11\n\nTABLE I: Simulations times and relative residual\nmaxi (||Axi − λi Bxi ||1 /||Axi ||1 ), obtained by the solver\nARPACK and FEAST on the TEST-CNT system seeking\nM (lowest) eigenpairs for different search intervals. The\nsimulations are performed on a Intel Clovertown (8cores, 1\nnode, 2.66GHz, 16Gb). The shift-strategy has been used in\nARPACK to accelerate the convergence (the regular mode\nwould give ∼ 300s for M = 100). The inner linear systems\nin ARPACK and FEAST are both solved using the direct\nparallel solver PARDISO [15] on 8 cores. Finally, the size\nof the subspace has been chosen to be M0 = 1.5M for both\nalgorithms, and the number of contour points for FEAST is\nfixed at Ne = 8.\n\nthat only one loop is necessary to obtain the eigenvalues\nwith an accuracy of ∼ 10−5 or below.\nTEST-CNT\nN = 12, 450\nM = 100\nM = 200\nM = 400\nM = 800\n\nRelative error on the Trace\n1st loop\n2nd loop\n3rd loop\n3.0 ∗ 10−6 2.9 ∗ 10−12 1.0 ∗ 10−15\n1.8 ∗ 10−5 4.8 ∗ 10−12 2.1 ∗ 10−14\n2.4 ∗ 10−8 3.2 ∗ 10−16\n1.8 ∗ 10−9 4.3 ∗ 10−16\n\nTABLE II: Variation\nof the relative error on the trace of\nP\nthe eigenvalues m λm for different search intervals with the\nnumber of iterative refinement loops. The convergence criteria is set to 10−13 where the final relative residual on the\neigenpairs is reported in Table I.\n\nThe simulation results in Table I demonstrate very\ngood scalability for FEAST while the search interval\nkeeps increasing but the number of contour points Ne\nstays identical (i.e. the number of numerical operations\nstays the same for a given loop of FEAST with a fixed\nNe = 8 linear systems to solve). In addition, from Table III, one can see how the robustness of the FEAST\nalgorithm is affected while the number of contour points\nNe changes. In particular, Ne = 4 points along the contour did suffice to capture M = 100 eigenpairs with a\nrelatively small residual (decreasing the simulation time\nreported in Table I for this case), while the case Ne = 16\npoints generated a residual smaller than the one obtained\nby ARPACK (using M0 = 1.5M).\nB.\n\nExample II\n\nIn another set of numerical experiments, we intend to\ndemonstrate the robustness of FEAST in capturing the\n\n\f5\n\nTEST-CNT\nARPACK\nN = 12, 450\nM = 100\nTime(s)\nResid.\n(N, M)\n12.2\n2.0 ∗ 10−11\n2(N, M)\n85\n3.5 ∗ 10−11\n4(N, M)\n668\n4.6 ∗ 10−11\n8(N, M)\n5492 6.2 ∗ 10−11\n\nFEAST\nTime(s)\n7.8\n27\n109\n523\n\nResid.\n4.5 ∗ 10−10\n7.7 ∗ 10−10\n8.8 ∗ 10−10\n6.5 ∗ 10−10\n\nTABLE IV: Simulations times and relative residual\nmaxi (||Axi − λi Bxi ||1 /||Axi ||1 ), obtained by the solver\nARPACK and FEAST on the k(N, M) TEST-CNT systems\nwhich artificially reproduce k times the original TEST-CNT\nsystem. The kM (lowest) eigenpairs are found where each\neigenvalue has a multiplicity of k.\n\n0\n-2\n\nE (eV)\n\nmultiplicity of the eigenvalues. We propose to create artificially new TEST-CNT systems called k(N, M) where\nthe matrices A and B are repeated k times along the\nmain diagonal (the new system matrix is block diagonal\nwith k blocks). Physically, these systems can describe\nthe cross section of a bundle composed by k CNTs, where\nwe do not consider the interactions between the different\ntubes such that each eigenvalue is now k times degenerate. If we keep the same search interval used to obtain\nM = 100 eigenpairs for k = 1 (where the size of the matrices A and B is N), 100k eigenpairs must now be found\nfor k ≥ 1, where each one of them have the multiplicity k.\nIn Table IV, we report the simulation times and relative\nresiduals obtained using ARPACK and FEAST on these\nk(N, M) TEST-CNT systems. For the case 8(N, M), for\nexample, the size of the new system matrix is 99, 600\nand the first 100 eigenvalues have all the multiplicity 8\n(so 800 eigenpairs are found in total). The simulation results show linear scalability performances with the size of\nthe system and the number of eigenpairs. In contrast to\nARPACK where the number of matrix-vector multiplications and linear system solves would keep increasing with\nk, the number of operations in FEAST stays the same for\nall these cases. The scalability of the algorithm depends\nthen mainly on the scalability of the linear system solver.\n\nExample III\n\nWe have shown that FEAST can re-use the computed\nsubspace as suitable initial guess for performing iterative refinements. This capability can also be of benefit\nto modern applications in science and engineering where\nit is often necessary to solve a series of eigenvalue problems that are close one another. In bandstructure calculations, in particular, many eigenvalue problems of the\nform (A + Sk )xk = λk Bxk need to be solved at different\nlocations in the k-space (i.e. for different values of k and\nwhere S is Hermitian with S0 = I). Let us consider the\neigenvalue sparse system of size N = 492, 982 obtained for\na (5,5) metallic CNT using our in-house DFT/real-space\nmesh technique framework for bandstructure calculations\nof nanowires-type structure [16]. In Figure 3, we propose\nto solve this eigenvalue problem using the same search\ninterval for the eigenvalues λ for different locations of k\nwhere the subspace computed by FEAST at the point\nk − 1 is successively used as initial guess for the neighboring point k. In addition, the inner linear systems in\n\n-4\n-6\n-8\n\n-10\n\n#Eigenvalues\n\nTABLE III: Performance results obtained by FEAST seeking\nM = 100 eigenpairs for different values of Ne . The convergence is obtained when the error on the trace is equal or\nsmaller to 10−13 .\n\nC.\n\n#FEAST loop\n\nTEST-CNT\nFEAST\nM = 100 Time(s)\nResid.\n# loops\nNe = 4\n7.0\n8.3 ∗ 10−8\n6\nNe = 8\n7.8\n4.5 ∗ 10−10\n4\nNe = 16\n10.2\n3.4 ∗ 10−12\n3\n\n-12\n20\n\n20\n\n18\n\n18\n\n16\n\n16\n\n14\n\n14\n\n12\n4\n\n12\n4\n\n3\n\n3\n\n2\n\n2\n\n1\n\n1\n\n0\n\nΓ\n\nk\n\n0\n\nX\n\nFIG. 3: Bandstructure calculations of a (5,5) metallic CNT.\nThe eigenvalue problems are solved successively for all the k\npoints (from Γ to X), while the computed subspace of size\nM0 = 25 at the point k is used as initial guess for the point\nk + 1. The number of eigenvalues found ranges from 13 to 20,\nand by the third k point, the FEAST convergence is obtained\nusing only one refinement loop. The convergence is obtained\nwith the relative error on trace of the eigenvalues smaller or\nequal to 10−8 , while the inner linear systems are solved using an iterative method with an accuracy of 10−3 . The final\nrelative residuals on the eigenpairs range from 10−3 to 10−5 .\n\nFEAST are solved using an iterative method with preconditioner where a modest relative residual of 10−3 is used\n(e.g. a suitable banded preconditioner can be obtained\nusing a mode approach [9]). It should be noted that the\nconvergence criteria for the relative error on the trace\nof the eigenvalues is chosen much smaller at 10−8 , while\nthe eigenvectors are expected to be obtained within the\nsame (or a smaller) order of accuracy that the one used\n\n\f6\nfor the solutions of the inner systems. Figure 3 shows\nthat 13 to 20 eigenvalues (i.e. energies) are found within\nthe selected search interval along the different k points\n(from the Γ to the X point in the graph). Although the\nsize of the subspace stays identical at M0 = 25, after the\nfirst initial point at k = 0 (Γ point in the graph) FEAST\nconverges within only one refinement loop for almost all\nthe other k points.\nV.\n\nDISCUSSIONS\n\nIn comparison to iterative Krylov subspace techniques,\nFEAST can be cast as a ”direct” technique which is\nbased on an exact mathematical derivation (3). FEAST\ndoes naturally then capture all the multiplicity and noorthogonalization procedure is necessary (such as GramSchmidt orthogonalization process). As described above,\nthe main computational tasks in FEAST consist of solving Ne independent linear systems along the contour\nwith M0 right-hand-sides and a reduced dense generalized eigenvalue problem of size M0 . Since FEAST has\nthe ability to re-use the basis from the previously computed subspace, an outer-iterative refinement procedure\nis proposed to improve the accuracy of the solutions. The\ncapability to take advantage of suitable initial guess can\nalso be of benefit to modern applications in sciences and\nengineering where it is often necessary to solve a series\nof eigenvalue problems that are close one another (e.g.\nBandstructure calculations in Example III Section IV).\nIn one sense, the difficulty of solving an eigenvalue\nproblem has been replaced by the difficulty of solving\na linear system with multiple right-hand sides. For large\nsparse systems, this latter can be solved using either a\ndirect system solver such as PARDISO [15] (as proposed\nin Section IV), or an iterative system solver with preconditioner. In turn, for banded systems or banded preconditioner, FEAST can be seen as an outer-layer for the\nauthor’s SPIKE parallel system solver [17]. It should be\nnoted that the inner linear systems arising from standard eigenvalue solvers (using the shift-strategy), need\noften to be solved highly accurately via direct methods.\nDirect system solvers, however, are not always suited\nfor addressing large-scale modern applications because of\nmemory requirements. In Example III of Section IV we\nhave shown that FEAST can take advantage of iterative\nsolvers for solving the inner linear systems with modest\n\n[1] G. Golub and H. A. van der Vorst, J. of Comput. and\nAppl. Math. 123, 35 (2000).\n[2] Z. Bai, J. Demmel, J. Dongarra, A. Ruhe, and H. van der\nVorst, Templates for the solution of Algebraic Eigenvalue\nProblems: A Practical Guide (Society for Industrial and\nApplied Mathematics, Philadelphia, PA, 2000).\n[3] T. Torsti, M. Heiskanen, M. J. Puska, and R. M. Nieminen, Int. J. Quantum Chem 91, 171 (2003).\n\nrelative residual and obtaining the eigenvectors solution\nwithin the same order of accuracy. The resulting subspace could also be used as a very good initial guess for a\none step more accurate refinement procedure (i.e. using\nmore accurate relative residual for the inner systems).\nFEAST exhibits important potentialities for parallelism at three different levels: (i) many search interval\n[λmin , λmax ] can be run independently, (ii) each linear\nsystems can be solved simultaneously (e.g. on each node\nof parallel architecture where the factorization of the linear system can be done only once for all the refinement\nloops), (iii) the linear system solver can be parallel (e.g.\nwithin a given node as in Section IV). Depending on\nthe parallel architecture at hand, the local memory of\na given node and the properties of the matrices of the\neigenvalue problems, one may preferably select one parallel option among the others, or just take advantage of\na combination of those. In particular, there will be a\ntrade off between how many search intervals to consider\nand how many eigenpairs FEAST can handle by intervals. For example if M0 is more than few thousands, one\ncould either (i) solve the obtained reduced system of size\nM0 using efficient dense parallel symmetric eigenvalue\nsolvers [18], or (ii) propose to divide the initial search\ninterval into two or more to be processed in parallel. In\naddition, it should be noted that the orthonormalization\nstep is absent from FEAST which will drastically reduce\nthe communication overhead for performing scalar products on high-end parallel architectures (the scalar product in step-3 in Fig. 2 has to be done only once per\niterative refinement). Given the recent advances in parallel architectures and parallel linear system solvers, it\nis reasonable to envision using FEAST in a near future\nfor obtaining up to millions of eigenpairs of large sparse\nsymmetric eigenvalue problems. Finally the capabilities\nof FEAST could potentially be enhanced for addressing\nnon-symmetric eigenvalue problems where the contour\nintegration would then be performed in a given region of\nthe complex space.\nAcknowledgments\n\nThe author wishes to acknowledge helpful discussions\nwith Dr. Ahmed Sameh and Dr. Massimo Fischetti.\nThis material is supported by NSF under Grant #CCF0635196.\n\n[4] Y. Saad, Y. Zhou, C. Bekas, M. L. Tiago, and J. Chelikowsky, physica status solidi (b) 243, 2188 (2006).\n[5] Y. Zhou, Y. Saad, M. L. Tiago, and J. Chelikowsky, Phys.\nRev. E. 74, 066704 (2006).\n[6] R.Zeller, J. Deutz, and P. Dederichs, Solid state communications 44, 993 (1982).\n[7] J. Taylor, H. Guo, and J. Wang, Phys. Rev. B. 63, 245407\n(2001).\n\n\f7\n[8] M. Brandbyge, J.-L. Mozos, P. Ordejon, J. Taylor, and\nK. Stokbro, Phys. Rev. B. 65, 165401 (2002).\n[9] D. Zhang and E. Polizzi, J. Comput. Elec. 7, 427 (2008).\n[10] D. Zhang and E. Polizzi, 2008 NSTI Nanotechnology\nConference and Trade Show. Technical Proceedings. 1,\n12 (2008).\n[11] T. Sakurai and H. Sugiura, J. of Comput. and Appl.\nMath. 159, 119 (2003).\n[12] T. Sakurai, Y. Kodaki, H. Tadano, D. Takahashi,\nM. Sato, and U. Nagashima, Future Generation Computer Systems 24, 613 (2008).\n[13] E. Anderson, Z. Bai, C. Bischof, S. Blackford, J. Demmel, J. D. C. J. Dongarra, A. Greenbaum, S. Hammarling, A. McKenney, and D. Sorensen, LAPACK Users\nGuide (Society for Industrial and Applied Mathematics,\nPhiladelphia, PA, 1999), third ed. ed.\n\n[14] R. B. Lehoucq, D. C. Sorensen, and C. Yang., ARPACK\nUsers Guide: Solution of Large Scale Eigenvalue Problems with Implicitly Restarted Arnoldi Methods. (Society\nfor Industrial and Applied Mathematics, Philadelphia,\nPA, 1998).\n[15] O. Schenk and K. Gärtner, Journal of Future Generation\nComputer Systems 20, 475 (2004).\n[16] D. Zhang and E. Polizzi, in preparation (2009).\n[17] E. Polizzi and A. Sameh, Parallel Computing 32, 177\n(2006).\n[18] L. Blackford, J. Choi, A. Cleary, E. D’Azevedo, J. Demmel, I. Dhillon, J. Dongarra, S. Hammarling, G. Henry,\nA. Petitet, et al., ScaLAPACK Users Guide (Society for\nIndustrial and Applied Mathematics, Philadelphia, PA,\n1997).\n\n\f"
        ],
        [
         "5",
         "5",
         "cs.CE",
         "Computational Engineering",
         "1607.02573v1.pdf",
         "Microwave Tomographic Imaging of Cerebrovascular\nAccidents by Using High-Performance Computing\n\narXiv:1607.02573v1 [cs.CE] 9 Jul 2016\n\nP.-H. Tourniera,b , I. Aliferisc , M. Bonazzolid , M. de Buhane , M. Darbasf ,\nV. Doleand,g , F. Hechta,b , P. Joliveth , I. El Kanfoudc , C. Migliaccioc ,\nF. Natafa,b , C. Pichotc , S. Semenovi\na\n\nLaboratoire Jacques-Louis Lions, UMR CNRS 7598, Sorbonne Universités, UPMC,\nParis, France\nb\nINRIA-Paris, EPC Alpines, Paris, France\nc\nLaboratoire LEAT, UMR CNRS 7248, Université Nice Sophia Antipolis, Sophia\nAntipolis, France\nd\nLaboratoire J.A. Dieudonné, UMR CNRS 7351, Université Nice Sophia Antipolis, Nice,\nFrance\ne\nMAP5, UMR CNRS 8145, Université Paris-Descartes, Sorbonne Paris Cité, France\nf\nLAMFA, UMR CNRS 7352, Université de Picardie Jules Verne, Amiens, France\ng\nDept of Maths and Stats, University of Strathclyde, Glasgow, UK\nh\nIRIT, UMR CNRS 5505, Toulouse, France\ni\nEMTensor GmbH, TechGate, 1220 Vienna, Austria\n\nAbstract\nThe motivation of this work is the detection of cerebrovascular accidents by\nmicrowave tomographic imaging. This requires the solution of an inverse\nproblem relying on a minimization algorithm (for example, gradient-based),\nwhere successive iterations consist in repeated solutions of a direct problem. The reconstruction algorithm is extremely computationally intensive\nand makes use of efficient parallel algorithms and high-performance computing. The feasibility of this type of imaging is conditioned on one hand\nby an accurate reconstruction of the material properties of the propagation\nmedium and on the other hand by a considerable reduction in simulation\ntime. Fulfilling these two requirements will enable a very rapid and accurate\ndiagnosis. From the mathematical and numerical point of view, this means\nsolving Maxwell’s equations in time-harmonic regime by appropriate domain\ndecomposition methods, which are naturally adapted to parallel architectures.\nKeywords: inverse problem, scalable preconditioners, Maxwell’s equations,\nmicrowave imaging\nPreprint submitted to Parallel Computing\n\nJuly 12, 2016\n\n\f1. Introduction\nA stroke, also known as cerebrovascular accident, is a disturbance in the\nblood supply to the brain caused by a blocked or burst blood vessel. As a\nconsequence, cerebral tissues are deprived of oxygen and nutrients. This results in a rapid loss of brain functions and often death. Strokes are classified\ninto two major categories: ischemic (85% of strokes) and hemorrhagic (15%\nof strokes). During an acute ischemic stroke, the blood supply to a part of\nthe brain is interrupted by thrombosis - the formation of a blood clot in a\nblood vessel - or by an embolism elsewhere in the body. A hemorrhagic stroke\noccurs when a blood vessel bursts inside the brain, increasing pressure in the\nbrain and injuring brain cells. The two types of strokes result in opposite\nvariations of the dielectric properties of the affected tissues. How quickly\none can detect and characterize the stroke is of fundamental importance for\nthe survival of the patient. The quicker the treatment is, the more reversible\nthe damage and the better the chances of recovery are. Moreover, the treatment of ischemic stroke consists in thinning the blood (anticoagulants) and\ncan be fatal if the stroke is hemorrhagic. Therefore, it is vital to make a\nclear distinction between the two types of strokes before treating the patient.\nMoreover, ideally one would want to monitor continuously the effect of the\ntreatment on the evolution of the stroke during the hospitalization. The two\nmost used imaging techniques for strokes diagnosis are MRI (magnetic resonance imaging) and CT scan (computerized tomography scan). One of their\ndownsides is that the travel time from the patient’s home to the hospital is\nlost. Moreover, the cost and the lack of portability of MRI and the harmful\ncharacter of CT scan make them unsuitable for a continuous monitoring at\nthe hospital during treatment.\nThis has motivated the study of an additional technique: microwave tomography. The measurement system is lightweight and thus transportable.\nThe acquisition of the data is harmless and faster than CT or MRI. Hence,\nthis imaging modality could be used by an emergency unit and for monitoring at the hospital. At frequencies of the order of 1 GHz, the tissues are\nwell differentiated and can be imaged on the basis of their dielectric properties. After the first works on microwave imaging in 1982 by Lin and Clarke\n[1], other works followed, but almost always on synthetic simplified models\n[2]. New devices are currently designed and studied by EMTensor GmbH\n2\n\n\fFigure 1: Left: Operating principle of the diagnosis apparatus. Middle:\nimaging chamber prototype of EMTensor, by courtesy of EMTensor company.\nRight: the corresponding simulation domain.\n(Vienna, Austria) [3].\nThe purpose of this work is to solve in parallel the inverse problem associated with the time-harmonic Maxwell’s equations which model electromagnetic waves propagation. The dielectric properties of the brain tissues of\na patient yield the image that could be used for a rapid diagnosis of brain\nstrokes. Simulation results presented in this work have been obtained on the\nimaging system prototype developed by EMTensor GmbH [3] (see Figure 1).\nIt is composed of 5 rings of 32 ceramic-loaded rectangular waveguides around\na metallic cylindrical chamber of diameter 28.5 cm and total height 28 cm.\nThe head of the patient is inserted into the chamber as shown in Figure 1\n(left). The imaging chamber is filled with a matching solution and a membrane is used to isolate the head. Each antenna successively transmits a\nsignal at a fixed frequency, typically 1 GHz. The electromagnetic wave propagates inside the chamber and in the object to be imaged according to its\nelectromagnetic properties. The retrieved data then consist in the scattering\nparameters measured by the 160 receiving antennas, which are used as input\nfor the inverse problem. These raw data can be wirelessly transferred to a remote computing center. The HPC machine will then compute the 3D images\nof the patient’s brain. Once formed, these images can be quickly transmitted\nfrom the computing center to the hospital, see Figure 2.\nThe paper is organized as follows. In Section 2 the direct problem and the\ntime-harmonic Maxwell’s equations in curl-curl form with suitable boundary\nconditions are introduced. In Section 3, we briefly describe the discretization\nmethod with edge finite elements. Section 4 is devoted to the introduction\nof the domain decomposition preconditioner. In Section 5 we explain how\nto compute the scattering coefficients. We also compare measurement data\n3\n\n\fFigure 2: Design concept of the diagnosis technology, by courtesy of EMTensor company.\nobtained by EMTensor with the coefficients computed by the simulation.\nWe introduce the inverse problem in Section 6. Section 7 is dedicated to\nnumerical results. We first perform a strong scaling analysis to show the\neffectiveness of the domain decomposition method. Then, we present results\nobtained by solving the inverse problem in a realistic configuration, with noisy\nsynthetic data generated using a numerical brain model with a simulated\nhemorrhagic stroke. Finally, we conclude this paper in Section 8 and give\ndirections for future research.\n2. The direct problem\nLet the domain Ω ⊂ R3 represent the imaging chamber (see Figure 1,\nright). We consider in Ω a heterogeneous non-magnetic dissipative linear\nisotropic dielectric medium, of dielectric permittivity ε(x) > 0 and electrical\nconductivity σ(x) ≥ 0. For each transmitting antenna j = 1, . . . , N emitting\na time periodic signal at angular frequency ω, the complex amplitude Ej (x) of\nthe associated electric field Ej (x, t) = <(Ej (x)eiωt ) is solution to the following\nsecond order time-harmonic Maxwell’s equation:\n∇ × (∇ × Ej ) − µ0 (ω 2 ε − iωσ)Ej = 0\n\nin Ω,\n\n(1)\n\nwhere µ0 is the permeability of free space. Note that the coefficient\nκ =\n\u0001\nµ0 (ω 2 ε − iωσ) in the equation can be written as κ = ω 2 µ0 ε − i ωσ , and in the\nnext sections we will consider the relative complex permittivity εr given by\nthe relation εr ε0 = ε − i ωσ , where ε0 is the permittivity of free space. Let n\nbe the unit outward normal to ∂Ω. Equation (1) is equipped with perfectly\nconducting boundary conditions on the metallic walls Γm :\nEj × n = 0\n4\n\non Γm ,\n\n\fand with impedance boundary conditions on the outer section of the transmitting waveguide j and of the receiving waveguides i = 1, . . . , N , i 6= j\n(see [4]):\n(∇ × Ej ) × n + iβn × (Ej × n) = gj\n(∇ × Ej ) × n + iβn × (Ej × n) = 0\n\non Γj ,\non Γi , i 6= j.\n\n(2)\n(3)\n\nHere β is the propagation wavenumber along the waveguide, corresponding\nto the propagation of the TE10 fundamental mode. Equation (2) imposes an\nincident wave which corresponds to the excitation of the fundamental mode\nE0j of the j-th waveguide, with gj = (∇ × E0j ) × n + iβn × (E0j × n). On\nthe other hand equation (3) corresponds to a first order absorbing boundary\ncondition of Silver–Müller approximating a transparent boundary condition\non the outer section of the receiving waveguides i = 1, . . . , N , i 6= j. The\nbottom of the chamber is metallic, and we impose an impedance boundary\ncondition on the top of the chamber. We end up with the following boundary\nvalue problem for each transmitting antenna j = 1, . . . , N : find Ej such that\n\n∇ × (∇ × Ej ) − µ0 (ω 2 ε − iωσ)Ej = 0\nin Ω,\n\n\n\n\nEj × n = 0\non Γm ,\n(4)\n\n(∇ × Ej ) × n + iβn × (Ej × n) = gj on Γj ,\n\n\n\n(∇ × Ej ) × n + iβn × (Ej × n) = 0\non Γi , i 6= j.\nNow, let V = {v ∈ H(curl, Ω), v × n = 0 on Γm }, where H(curl, Ω) = {v ∈\nL2 (Ω)3 , ∇ × v ∈ L2 (Ω)3 } is the space of square integrable functions whose\ncurl is also square integrable. For each transmitting antenna j = 1, . . . , N ,\nthe variational form of problem (4) reads: find Ej ∈ V such that\nZ\n\u0002\n\u0003\n(∇ × Ej ) · (∇ × v) − µ0 (ω 2 ε − iωσ)Ej · v\nZ Ω\nZ\n(5)\niβ(Ej × n) · (v × n) =\ngj · v ∀v ∈ V.\n+ S\nN\ni=1\n\nΓi\n\nΓj\n\n3. Edge finite elements\nNédélec edge elements [5] are finite elements particularly suited for the\napproximation of electromagnetic fields. Indeed, given a tetrahedral mesh T\nof the computational domain Ω, the finite dimensional subspace Vh generated\n5\n\n\fby Nédélec basis functions is included in H(curl, Ω), since their tangential\ncomponent across faces shared by adjacent tetrahedra of T is continuous.\nThey thus match the continuity properties of the electric field. Nédélec elements are called edge elements because the basis functions are associated\nwith the edges of the mesh T . More precisely, for a tetrahedron T ∈ T , the\nlocal basis functions are associated with the oriented edges e = {ni , nj } of T\nas follows\nwe = λi ∇λj − λj ∇λi ,\nwhere the λ` are the barycentric coordinates of a point with respect to the\nnode n` of T . Note that the polynomial degree of we is 1 since the barycentric\ncoordinates λ` are polynomials of degree 1 and their gradients are constant.\nAs these basis functions are vector functions, we only need one set of unknowns to approximate all the components of the field, and not three sets\nof unknowns, one for each component of the field as is instead required for\nusual nodal (scalar) finite elements.\nThe finite element discretization of the variational problem is obtained\nby taking test functions v ∈ Vh , the edge finite element space on the mesh\nT , and by looking for a solution Eh ∈ Vh in the same space: find Eh ∈ Vh\nsuch that\nZ\n\u0002\n\u0003\n(∇ × Eh ) · (∇ × v) − µ0 (ω 2 ε − iωσ)Eh · v\nΩ\nZ\nZ\n(6)\n+ S\niβ(Eh × n) · (v × n) =\ngj · v ∀v ∈ Vh .\nN\ni=1\n\nΓi\n\nΓj\n\nLocally,\nover each tetrahedron T , we write the discretized field as Eh =\nP\ne\ne∈T ce w , a linear combination with coefficients ce of the basis functions\nassociated with the edges e of T , and the coefficients ce will be the unknowns\nof the resulting linear system. For edge finite elements (of degree 1) these\ncoefficients can be interpreted as the circulations of Eh along the edges of\nthe tetrahedra:\nZ\n1\nEh · te ,\nce =\n|e| e\nwhere te is the tangent vector to the edge e of length |e|, the length of e.\nThis is a consequence of the fact that the basis functions are in duality with\nthe degrees of freedom given by the circulations along the edges, that is:\n(\nZ\n1 if e = e0 ,\n1\n0\nwe · te =\n|e| e\n0 if e 6= e0 .\n6\n\n\f4. Domain decomposition preconditioning\nThe finite element discretization (6) of the variational problem (5) produces linear systems\nAuj = bj\nfor each transmitting antenna j. However, the matrix A can be ill-conditioned.\nThis, combined with the fact that the underlying PDE is indefinite, highlights\nthe need for a robust and efficient preconditioner. Here we employ domain\ndecomposition preconditioners, which are extensively described in [6], as they\nare naturally suited to parallel computing. Our domain decomposition preconditioner is presented in the following.\nLet T be the mesh of the computational domain Ω. First, T is partitioned\ninto NS non-overlapping meshes {Ti }16i6NS using standard graph partitioners\nsuch as SCOTCH [7] or METIS [8]. If δ is a positive integer, the overlapping\ndecomposition {Tiδ }16i6NS is defined recursively as follows: Tiδ is obtained\nby including all tetrahedra of Tiδ−1 plus all adjacent tetrahedra of Tiδ−1 ; for\nδ = 0, Tiδ = Ti . Note that the number of layers in the overlap is then 2δ. Let\nVh be the edge finite element space defined on T , and {Viδ }16i6NS the local\nedge finite element spaces defined on {Tiδ }16i6NS , δ > 0. Now consider the\nrestrictions {Ri }16i6NS from Vh to {Viδ }16i6NS , and a local partition of unity\n{Di }16i6NS such that\nNS\nX\nRiT Di Ri = In×n .\n(7)\ni=1\n\nAlgebraically speaking, if n is the global number of unknowns and {ni }16i6NS\nare the numbers of unknowns for each local finite element space, then Ri is\na Boolean matrix of size ni × n, and Di is a diagonal matrix of size ni × ni ,\nfor all 1 6 i 6 NS . Note that RiT , the transpose of Ri , is a n × ni matrix\nthat gives the extension by 0 from Viδ to Vh .\nUsing these matrices, one can define the following one-level preconditioner, called Optimized Restricted Additive Schwarz preconditioner (ORAS)\n[9]:\nNS\nX\n−1\nMORAS\n=\nRiT Di Bi−1 Ri ,\n(8)\ni=1\n\nwhere {Bi }16i6NS are local operators corresponding to the subproblems with\nimpedance boundary conditions (∇ × E) × n + ikn × (E × n), where k =\n√\nω µ0 εr ε0 is the wavenumber. These boundary conditions were first used as\n7\n\n\ftransmission conditions at the interfaces between subdomains in [10]. The\nlocal matrices {Bi }16i6NS of the ORAS preconditioner make use of more efficient transmission boundary conditions than the submatrices {Ri ARiT }16i6NS\nof the original Restricted Additive Schwarz (RAS) preconditioner [11]. It is\nimportant to note that when a direct solver is used to compute the action\nof Bi−1 on multiple vectors, it can be done in a single forward elimination\nand backward substitution. More details on the solution of linear systems\nwith multiple right-hand sides are given in Section 7. The preconditioner\n−1\nMORAS\n(8) is naturally parallel since its assembly requires the concurrent\nfactorization of each {Bi }16i6NS , which are typically stored locally on different processes in a distributed computing context. Likewise, applying (8)\nto a distributed vector only requires peer-to-peer communications between\nneighboring subdomains, and a local forward elimination and backward substitution. See chapter 8 of [6] for a more detailed analysis.\n4.1. Partition of unity\nThe construction of the partition of unity is intricate, especially for Nédélec\nedge finite elements.\nThe starting point is the construction of partition of unity functions\n{χi }16i6NS for the classical P1 linear nodal finite element, whose degrees\nof freedom are the values at the nodes of the mesh. First of all, we define for\ni = 1, . . . , NS the function χ\nei as the continuous piecewise linear function on\nT , with support contained in Tiδ , such that\n(\n1 at all nodes of Ti0 ,\nχ\nei =\n0 at all nodes of Tiδ \\ Ti0 .\nThe function χi can then be defined as the continuous piecewise linear function on T , with support contained in Tiδ , such that its (discrete) value for\neach degree of freedom is evaluated by:\nχi =\n\nχ\nei\n.\nNS\nX\nχ\nej\n\n(9)\n\nj=1\n\nP S\nThus, we have N\ni=1 χi = 1 both at the discrete and continuous level. Remark\nthat if δ > 1, not only the function χi but also its derivative is equal to zero on\nthe border of Tiδ . This is essential for a good convergence if Robin boundary\n8\n\n\fconditions are chosen as transmission conditions at the interfaces between\nsubdomains. Indeed, if this property is satisfied, the continuous version of\nthe ORAS algorithm is equivalent to P. L. Lions’ algorithm (see [9] and\n[6] §2.3.2). Note that in the practical implementation, the functions χ\nei and\nej in (9)\nχi are constructed locally on Tiδ , the relevant contribution of the χ\nbeing on Tjδ ∩ Tiδ . This removes all dependency on the global mesh T , which\ncould be otherwise problematic at large scales.\nNow, the degrees of freedom of Nédélec finite elements are associated with\nthe edges of the mesh. For these finite elements, we can build a geometric\npartition of unity based on the support of the degrees of freedom (the edges of\nthe mesh): the entries of the diagonal matrices Di , i = 1, . . . , NS are obtained\nfor each degree of freedom by interpolating the piecewise linear function χi at\nthe midpoint of the corresponding\nedge. The partition of unity property (7)\nPNS\nis then satisfied since i=1 χi = 1.\n4.2. Software stack\nAll operators related to the domain decomposition method can be easily generated using finite element Domain-Specific Languages (DSL). Here\nwe use FreeFem++ [12] (http://www.freefem.org/ff++/) since it has already been proven that it can enable large-scale simulations using overlapping\nSchwarz methods [13] when used in combination with the library HPDDM\n[14] (High-Performance unified framework for Domain Decomposition Methods, https://github.com/hpddm/hpddm). HPDDM implements several domain decomposition methods such as RAS, ORAS, FETI, and BNN. It uses\nmultiple levels of parallelism: communication between subdomains is based\non the Message Passing Interface (MPI), and computations in the subdomains can be executed on several threads by calling optimized BLAS libraries\n(such as Intel MKL), or shared-memory direct solvers like PARDISO. Domain\ndecomposition methods naturally offer good parallel properties on distributed\narchitectures. The computational domain is decomposed into subdomains in\nwhich concurrent computations are performed. The coupling between subdomains requires communications between computing nodes via messages. The\nstrong scalability of the ORAS preconditioner as implemented in HPDDM\nfor the direct problem presented in Section 2 will be assessed in Section 7.\n\n9\n\n\f5. Computing the scattering parameters\nIn order to compute the numerical counterparts of the reflection and\ntransmission coefficients obtained by the measurement apparatus of the imaging chamber shown in Figure 1, we use the following formula, which is appropriate in the case of open-ended waveguides:\nZ\nEj · E0i\n, i, j = 1, . . . , N,\n(10)\nSij = ZΓi\n0 2\n|Ei |\nΓi\n\nwhere E0i is the TE10 fundamental mode of the i-th receiving waveguide and\nEj is the solution of the problem where the j-th waveguide transmits the\nsignal (Ej denotes the complex conjugate of Ej ). The Sij with i 6= j are the\ntransmission coefficients, and the Sjj are the reflection coefficients. They are\ngathered in the scattering matrix, also called S-matrix.\nHere we compare the coefficients computed from the simulation with a\nset of measurements obtained by EMTensor. For this test case, the imaging chamber was filled with a homogeneous matching solution. The electric\npermittivity ε of the matching solution is chosen by EMTensor in order to\nminimize contrasts with the ceramic-loaded waveguides and with the different brain tissues. The choice of the conductivity σ of the matching solution is a compromise between the minimization of reflection artifacts from\nmetallic boundaries and the desire to have best possible signal-to-noise ratio.\nHere the relative complex permittivity of the matching solution at frequency\nf = 1 GHz is εgel\nr = 44 − 20i. The relative complex permittivity inside the\nceramic-loaded waveguides is εcer\nr = 59.\nThe set of experimental data at hand given by EMTensor consists in\ntransmission coefficients for transmitting antennas in the second ring from\nthe top. Figure 3 shows the normalized magnitude (dB) and phase (degree)\nof the complex coefficients Sij corresponding to a transmitting antenna in\nthe second ring from the top and to the 31 receiving antennas in the middle ring (note that measured coefficients are available only for 17 receiving\nantennas). The magnitude in dB is calculated as 20 log10 (|Sij |). The normalization is done by dividing every transmission coefficient by the transmission\ncoefficient corresponding to the receiving antenna directly opposite to the\ntransmitting antenna, which is thus set to 1. Since we normalize with respect to the coefficient having the lowest expected magnitude, the magnitude\n10\n\n\f70\n\nsimulation\nmeasurements\n\n60\n\nmagnitude (dB)\n\n50\n40\n30\n20\n10\n0\n5\n\n10\n\n15\n20\nreceiver number\n\n25\n\n30\n\nsimulation\nmeasurements\n\n0\n\nphase (degree)\n\n-500\n\n-1000\n\n-1500\n\n-2000\n5\n\n10\n\n15\n20\nreceiver number\n\n25\n\n30\n\nFigure 3: The normalized magnitude (top) and phase (bottom) of the transmission coefficients computed with the simulation and measured experimentally.\nof the transmission coefficients displayed in Figure 3 is larger than 0 dB. We\ncan see that the transmission coefficients computed from the simulation are\nin very good agreement with the measurements.\n6. The inverse problem\nThe inverse problem that we consider consists in finding the unknown dielectric permittivity ε(x) and conductivity σ(x) in Ω, such that the solutions\nEj , j = 1, . . . , N of problem (4) lead to corresponding scattering parameters Sij (10) that coincide with the measured scattering parameters Sijmes ,\nfor i, j = 1, . . . , N . In the following, we present the inverse problem in the\ncontinuous setting for clarity.\nLet κ = µ0 (ω 2 ε − iωσ) be the unknown complex parameter of our inverse problem, and let us denote by Ej (κ) the solution of the direct problem\n11\n\n\f(4) with dielectric permittivity ε and conductivity σ. The corresponding\nscattering parameters will be denoted by Sij (κ) for i, j = 1, . . . , N :\nZ\nEj (κ) · E0i\n, i, j = 1, . . . , N.\nSij (κ) = ΓiZ\n0 2\n|Ei |\nΓi\n\nThe misfit of the parameter κ to the data can be defined through the following\nfunctional:\nZ\n2\n0\nE\n(κ)\n·\nE\nN\nN\nN\nN\nj\ni\n1 XX\n1 X X Γi\n2\nZ\nJ(κ) =\nSij (κ) − Sijmes =\n− Sijmes .\n2 j=1 i=1\n2 j=1 i=1\n|E0i |2\nΓi\n\n(11)\nIn a classical way, solving the inverse problem then consists in minimizing\nthe functional J with respect to the parameter κ. Computing the differential\nof J in a given arbitrary direction δκ yields\nZ\n\n\n0\nδE\n(κ)\n·\nE\nN X\nN\ni\nX\n\n\u0001 Γi j\nmes\n , δκ ∈ C,\n\nZ\nDJ(κ, δκ) =\n<  Sij (κ) − Sij\n\n0\n2\nj=1 i=1\n|Ei |\nΓi\n\nwhere δEj (κ) is the solution of the following linearized problem:\n\n∇ × (∇ × δEj ) − κδEj = δκEj in Ω,\n\n\nδEj × n = 0\non Γm ,\n\n (∇ × δE ) × n + iβn × (δE × n) = 0\non Γi , i = 1, . . . , N.\nj\nj\n(12)\nWe now use the adjoint approach in order to simplify the expression of\nDJ. This will allow us to compute the gradient efficiently after discretization,\nwith a number of computations independent of the size of the parameter\nspace. Considering the variational formulation of problem (12) with a test\n\n12\n\n\ffunction F and integrating by parts, we get\nZ\nZ\nδκEj · F =\n(∇ × (∇ × δEj ) − κδEj ) · F\nΩ\nZ\nZΩ\n(∇ × (∇ × F) − κF) · δEj −\n((∇ × δEj ) × n) · F\n=\nΩ\n∂Ω\nZ\n+\n((∇ × F) × n) · δEj\n∂Ω\n\nZ\n(∇ × (∇ × F) − κF) · δEj +\n\n=\nΩ\n\nN Z\nX\ni=1\n\nZ\n(∇ × δEj ) · (F × n) +\n\n+\nΓm\n\nN Z\nX\n\niβ(n × (F × n)) · δEj\n\nΓi\n\n((∇ × F) × n) · δEj .\n\nΓi\n\ni=1\n\nIntroducing the solution Fj (κ) of the following adjoint problem\n\n∇ × (∇ × Fj ) − κFj = 0\nin Ω,\n\n\n\n\n\nFj × n = 0\non Γm ,\n\nmes\n(Sij (κ) − Sij ) 0\n\nZ\n(∇\n×\nF\n)\n×\nn\n+\niβn\n×\n(F\n×\nn)\n=\nEi on Γi , i = 1, . . . , N,\n\nj\nj\n\n\n\n|E0i |2\n\nΓi\n\n(13)\nwe get\nZ\nZ\nδκEj · Fj =\nΩ\n\nN\nX\n\nE0i · δEj\n\n(Sij (κ) − Sijmes ) ΓZi\n\ni=1\n\n.\n|E0i |2\n\nΓi\n\nFinally, the differential of J can be computed as\nDJ(κ, δκ) =\n\nN\nX\n\n\u0014Z\n<\n\n\u0015\nδκ Ej · Fj .\n\nΩ\n\nj=1\n\nWe can then compute the gradient to use in a gradient-based local optimization algorithm. The numerical results presented in Section 7 are obtained using a limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS)\nalgorithm. Note that every evaluation of J requires the solution of the state\nproblem (4) while the computation of the gradient requires the solution of (4)\n13\n\n\fas well as the solution of the adjoint problem (13). Moreover, the state and\nadjoint problems use the same operator. Therefore, the computation of the\ngradient only needs the assembly of one matrix and its associated domain\ndecomposition preconditioner.\nNumerical results for the reconstruction of a hemorrhagic stroke from synthetic data are presented in the next section. The functional J considered in\nthe numerical results is slightly different from (11), as we add a normalization\nterm for each pair (i, j) as well as a Tikhonov regularizing term:\nN\nN\n1 X X Sij (κ) − Sijmes\nJ(κ) =\n2\n2 j=1 i=1\nSijempty\n\n2\n\nα\n+\n2\n\nZ\n\n|∇κ|2 ,\n\n(14)\n\nΩ\n\nwhere Sijempty refers to the coefficients computed from the simulation with\nthe empty chamber, that is the chamber filled only with the homogeneous\nmatching solution as described in the previous section, with no object inside.\nIn this way, the contribution of each pair (i, j) in the misfit functional is\nnormalized and does not depend on the amplitude of the coefficient, which\ncan vary greatly between pairs (i, j) as displayed in Figure 3. The Tikhonov\nregularizing term aims at reducing the effects of noise in the data. For\nnow, the regularization parameter α is chosen empirically so as to obtain a\nvisually good compromise between reducing the effects of noise and keeping\nthe reconstructed image pertinent. All calculations carried out in this section\ncan be accommodated in a straightforward manner to definition (14) of the\nfunctional.\nAs is usually the case with most medical imaging techniques, the reconstruction is done layer by layer. For the imaging chamber of EMTensor that\nwe study in this paper, one layer corresponds to one of the five rings of 32\nantennas. This allows us to exhibit another level of parallelism, by solving\nan inverse problem independently for each of the five rings in parallel. More\nprecisely, each of these inverse problems is solved in a domain truncated\naround the corresponding ring of antennas, containing at most two other\nrings (one ring above and one ring below). We impose absorbing boundary\nconditions on the artificial boundaries of the truncated computational domain. For each inverse problem, only the coefficients Sij with transmitting\nantennas j in the corresponding ring are taken into account: we consider 32\nantennas as transmitters and at most 96 antennas as receivers.\n\n14\n\n\fTime to solution (in seconds)\n\n500\n\n(43)\n\nSetup\n\nSolve\n\nLinear speedup\n\n(53)\n\n200\n\n(64)\n\n50\n\n(81)\n\n10\n\n256\n\n512\n1 024\n# of subdomains\n\n2 048\n\nFigure 4: Strong scaling experiment. Colors indicate the fraction of the\ntotal time spent in the setup and solution phases. The number of GMRES\niterations is reported in parentheses.\n7. Numerical results\nResults in this paper were obtained on Curie, a system composed of\n5,040 nodes made of two eight-core Intel Sandy Bridge processors clocked\nat 2.7 GHz. The interconnect is an InfiniBand QDR full fat tree and the\nMPI implementation used was BullxMPI version 1.2.8.4. Intel compilers and\nMath Kernel Library in their version 16.0.2.181 were used for all binaries and\nshared libraries, and as the linear algebra backend for dense computations.\nOne-level preconditioners such as (8) assembled by HPDDM require the use\nof a sparse direct solver. In the following experiments, we have been using\neither PARDISO [15] from Intel MKL or MUMPS [16]. All linear systems\nresulting from the edge finite elements discretization are solved by GMRES\nright-preconditioned with ORAS (8) as implemented in HPDDM. The GMRES algorithm is stopped once the unpreconditioned relative residual is lower\nthan 10−8 . First, we perform a strong scaling analysis in order to assess the\nefficiency of our preconditioner. We then solve the inverse problem in a realistic configuration, with noisy synthetic data generated using a numerical\nbrain model with a simulated hemorrhagic stroke.\n7.1. Scaling analysis\nUsing the domain decomposition preconditioner (8), we solve the direct\nproblem corresponding to the setting of Section 5 where the chamber is filled\n15\n\n\fNS\n256\n512\n1,024\n2,048\n\nSetup\n293.36\n95.11\n35.13\n25.89\n\nSolve\n73.06\n36.92\n20.55\n12.77\n\n# of iterations\n43\n53\n64\n81\n\nSpeedup\n1\n2.8\n6.6\n9.5\n\nFigure 5: Strong scaling experiment. Timings (in seconds) of the setup and\nsolution phases.\nwith a homogeneous matching solution. We consider a right-hand side corresponding to a transmitting antenna in the second ring from the top. Given\na fine mesh of the domain composed of 82 million tetrahedra, we increase\nthe number of MPI processes to solve the linear system of 96 million doubleprecision complex unknowns yielded by the discretization of Maxwell’s equation using edge elements. The global unstructured mesh is partitioned using\nSCOTCH [7] and the local solver is PARDISO from Intel MKL. We use one\nsubdomain and two OpenMP threads per MPI process. Results are reported\nin Figure 5 and illustrated in Figure 4 with a plot of the time to solution\nincluding both the setup and solution phases on 256 up to 2048 subdomains.\nThe setup time corresponds to the maximum time spent for the factorization of the local subproblem matrix Bi in (8) over all subdomains, while the\nsolution time corresponds to the time needed to solve the linear system with\nGMRES. We are able to obtain very good speedups up to 4096 cores (2048\nsubdomains) on Curie, with a superlinear speedup of 9.5 between 256 and\n2048 subdomains.\n7.2. Reconstruction of a hemorrhagic stroke from synthetic data\nIn this subsection, we assess the feasibility of the microwave imaging technique presented in this paper for stroke detection and monitoring through a\nnumerical example in a realistic configuration. We use synthetic data corresponding to a numerical model of a virtual human head with a simulated\nhemorrhagic stroke as input for the inverse problem. The numerical model of\nthe virtual head comes from CT and MRI tomographic images and consists\nof a complex permittivity map of 362 × 434 × 362 data points. Figure 6 (left)\nshows a sagittal section of the head. In the simulation, the head is immersed\nin the imaging chamber as shown in Figure 6 (right). In order to simulate a\nhemorrhagic stroke, a synthetic stroke is added in the form of an ellipsoid in\nwhich the value of the complex permittivity εr has been increased. For this\n16\n\n\fFigure 6: Left: sagittal section of the brain. Right: numerical head immersed in the imaging chamber, with a simulated ellipsoid-shaped hemorrhagic stroke.\ntest case, the value of the permittivity in the ellipsoid is taken as the mean\nvalue between the relative permittivity of the original healthy brain and the\nrelative permittivity of blood at frequency f = 1 GHz, εblood\n= 68 − 44i.\nr\nThe imaging chamber is filled with a matching solution. The relative permittivity of the matching solution is chosen by EMTensor as explained in\nSection 5 and is equal to εgel\nr = 44 − 20i at frequency f = 1 GHz. In the real\nsetting, a special membrane fitting the shape of the head is used in order to\nisolate the head from the matching medium. We do not take this membrane\ninto account in this synthetic test case. The synthetic data are obtained by\nsolving the direct problem on a mesh composed of 17.6 million tetrahedra\n(corresponding to approximately 20 points per wavelength) and consist in\nthe transmission and reflection coefficients Sij calculated from the simulated\nelectric field as in (10). We subsequently add noise to the real and imaginary\nparts of the coefficients Sij (10% additive Gaussian white noise, with different\nvalues for real and imaginary parts). The noisy data are then used as input\nfor the inverse problem. Furthermore, we assume no a priori knowledge on\nthe input data, and we set the initial guess for the inverse problem as the\nhomogeneous matching solution everywhere inside the chamber. We use a\npiecewise linear approximation of the unknown parameter κ, defined on the\nsame mesh used to solve the state and adjoint problems. For the purpose of\nparallel computations, the partitioning introduced by the domain decomposition method is also used to compute and store locally in each subdomain\nevery entity involved in the inverse problem, such as the parameter κ and\nthe gradient.\n\n17\n\n\fFigure 7: Top row: imaginary part of the exact permittivity used to produce\nnoisy data as input for the inverse problem during time evolution of a simulated hemorrhagic stroke (indicated by the black arrow). The size of the\nellipsoid is 3.9 cm×2.3 cm×2.3 cm and 7.7 cm×4.6 cm×4.6 cm in the middle\nand right column respectively. Bottom row: corresponding reconstructions\nobtained by taking into account only the first ring of transmitting antennas.\nFigure 7 shows the imaginary part of the exact and reconstructed permittivity for three steps of the evolution of the hemorrhagic stroke, from the\nhealthy brain (left column) to the large stroke (right column). Increasing the\nsize of the ellipsoid in which the value of the permittivity is raised simulates\nthe evolution of the stroke. Each of the three reconstructions in Figure 7\ncorresponds to the solution of an inverse problem in the truncated domain\ncontaining only the first two rings of antennas from the top, and where only\nthe coefficients Sij corresponding to transmitting antennas j in the first ring\nare taken into account. Each reconstruction starts from an initial guess consisting of the homogeneous matching solution and is obtained after reaching\na convergence criterion of 10−2 for the value of the cost functional, which\ntakes around 30 iterations of the L-BFGS algorithm.\nFigure 8 gathers the results of a strong scaling experiment which consists\n18\n\n\fTime in minutes\n\n16\n\nLinear speedup\n\n8\n4\n2\n1\n0.5\n\n64\n\n128 256 512 1 0 2 0 4 0\n24 48 96\n# of MPI processes\n\nFigure 8: Strong scaling experiment: total time needed to obtain the third\nreconstructed image shown in Figure 7.\nin solving the same inverse problem corresponding to the third reconstructed\nimage of Figure 7 for an increasing number of MPI processes. We report\nthe total computing time needed to obtain the reconstructed image. For this\nexperiment we use one subdomain and one OpenMP thread per MPI process.\nThe mesh of the computational domain is composed of 674 580 tetrahedra,\ncorresponding to approximately 10 points per wavelength.\nNote that evaluating the functional or its gradient requires the solution of\na linear system with 32 right-hand sides, one right-hand side per transmitter.\nThis introduces a trivial level of parallelism since the solution corresponding\nto each right-hand side can be computed independently. However when considering a finite number of available processors, there is a tradeoff between\nthe parallelism induced by the multiple right-hand sides and the parallelism\ninduced by the domain decomposition method. Additionally, we solve for\nmultiple right-hand sides simultaneously using a pseudo-block method implemented inside GMRES which consists in fusing the multiple arithmetic\noperations corresponding to each right-hand side (matrix-vector products,\ndot products), resulting in higher arithmetic intensity. The scaling behavior\nof this pseudo-block algorithm with respect to the number of right-hand sides\nis nonlinear, as is the scaling behavior of the domain decomposition method\nwith respect to the number of subdomains. Thus, for a given number of\nprocessors, we find the optimal tradeoff between parallelizing with respect\nto the number of subdomains or right-hand sides through trial and error.\n\n19\n\n\fFor example, the best computing time for 2048 MPI processes is achieved\nby using 8 domain decomposition communicators (i.e. 8 concurrent direct\nsolves) with 256 subdomains treating 4 right-hand sides each.\nFigure 8 shows that we can generate an image with a total computing\ntime of less than 2 minutes (94 seconds) using 4096 cores of Curie. These\npreliminary results are very encouraging as we are already able to achieve a\nsatisfactory reconstruction time in the perspective of using such an imaging\ntechnique for monitoring. This allows clinicians to obtain almost instantaneous images 24/7 or on demand. Although the reconstructed images do not\nfeature the complex heterogeneities of the brain, which is in accordance with\nwhat we expect from microwave imaging methods, they allow the characterization of the stroke and its monitoring.\n8. Conclusion\nWe have developed a tool that reconstructs a microwave tomographic image of the brain in less than 2 minutes using 4096 cores. This computational\ntime corresponds to clinician acceptance for rapid diagnosis or medical monitoring at the hospital. These images were obtained from noisy synthetic\ndata from a very accurate model of the brain. To our knowledge, this is the\nfirst time that such a realistic study (operational acquisition device, highly\naccurate three-dimensional synthetic data, 10% noise) shows the feasibility\nof microwave imaging. This study was made possible by the use of massively\nparallel computers and facilitated by the HPDDM and FreeFem++ tools\nthat we have developed. The next step is the validation of these results on\nclinical data.\nRegarding the numerical aspects of this work, we will accelerate the solution of the series of direct problems, which accounts for more than 80% of\nour elapsed time. We explain here the three main avenues of research:\n• The present ORAS solver for Maxwell’s equations is a one level algorithm, which cannot scale well over thousands of subdomains. The introduction of a two-level preconditioner with an adequate coarse space\nwould allow for very good speedups even for decompositions into a large\nnumber of subdomains.\n• Recycling information obtained during the convergence of the optimization algorithm will also enable us to improve the performance of the\nmethod, see [17].\n20\n\n\f• Iterative block methods that allow for simultaneous solutions of linear\nsystems have not been fully investigated. Arithmetic intensity would\nbe increased since block methods may converge in a smaller number of\niterations while exploiting modern computer architectures effectively.\nAcknowledgments\nThis work was granted access to the HPC resources of TGCC@CEA under\nthe allocations 2016-067519 and 2016-067730 made by GENCI. This work\nhas been supported in part by ANR through project MEDIMAX, ANR-13MONU-0012.\n[1] J. C. Lin, M. J. Clarke, Microwave imaging of cerebral edema, Proceedings of the IEEE 70 (5) (1982) 523–524.\n[2] S. Y. Semenov, D. R. Corfield, Microwave tomography for brain imaging: feasibility assessment for stroke detection, International Journal of\nAntennas and Propagation.\n[3] S. Semenov, B. Seiser, E. Stoegmann, E. Auff, Electromagnetic tomography for brain imaging: from virtual to human brain, in: 2014 IEEE\nConference on Antenna Measurements & Applications (CAMA), 2014.\n[4] R. Beck, R. Hiptmair, Multilevel solution of the time-harmonic\nMaxwell’s equations based on edge elements, International Journal for\nNumerical Methods in Engineering 45 (7) (1999) 901–920.\n[5] J.-C. Nédélec, Mixed finite elements in R3 , Numerische Mathematik\n35 (3) (1980) 315–341.\n[6] V. Dolean, P. Jolivet, F. Nataf, An Introduction to Domain Decomposition Methods: algorithms, theory and parallel implementation, SIAM,\n2015.\n[7] F. Pellegrini, J. Roman, SCOTCH: A Software Package for Static Mapping by Dual Recursive Bipartitioning of Process and Architecture\nGraphs, in: High-Performance Computing and Networking, Springer,\n1996, pp. 493–498.\n\n21\n\n\f[8] G. Karypis, V. Kumar, A fast and high quality multilevel scheme for\npartitioning irregular graphs, SIAM Journal on Scientific Computing\n20 (1) (1998) 359–392.\n[9] A. St-Cyr, M. J. Gander, S. J. Thomas, Optimized multiplicative, additive, and restricted additive Schwarz preconditioning, SIAM Journal\non Scientific Computing 29 (6) (2007) 2402–2425).\n[10] B. Després, P. Joly, J. E. Roberts, A domain decomposition method for\nthe harmonic Maxwell equations, in: Iterative methods in linear algebra\n(Brussels, 1991), North-Holland, Amsterdam, 1992, pp. 475–484.\n[11] X.-C. Cai, M. Sarkis, A restricted additive Schwarz preconditioner for\ngeneral sparse linear systems, SIAM Journal on Scientific Computing\n21 (2) (1999) 792–797.\n[12] F. Hecht, New development in FreeFem++, Journal of Numerical Mathematics 20 (3-4) (2012) 251–265.\n[13] P. Jolivet, V. Dolean, F. Hecht, F. Nataf, C. Prud’homme, N. Spillane,\nHigh-performance domain decomposition methods on massively parallel architectures with FreeFem++, Journal of Numerical Mathematics\n20 (3-4) (2012) 287–302.\n[14] P. Jolivet, F. Hecht, F. Nataf, C. Prud’homme, Scalable domain decomposition preconditioners for heterogeneous elliptic problems, in: Proc.\nof the Int. Conference on High Performance Computing, Networking,\nStorage and Analysis, IEEE, 2013, pp. 1–11.\n[15] O. Schenk, K. Gärtner, Solving unsymmetric sparse systems of linear\nequations with PARDISO, Future Generation Computer Systems 20 (3)\n(2004) 475–487.\n[16] P. Amestoy, I. Duff, J.-Y. L’Excellent, J. Koster, A fully asynchronous\nmultifrontal solver using distributed dynamic scheduling, SIAM Journal\non Matrix Analysis and Applications 23 (1) (2001) 15–41.\n[17] M. L. Parks, E. De Sturler, G. Mackey, D. D. Johnson, S. Maiti, Recycling Krylov Subspaces for Sequences of Linear Systems, SIAM Journal\non Scientific Computing 28 (5) (2006) 1651–1674.\n\n22\n\n\f"
        ],
        [
         "6",
         "6",
         "cs.CE",
         "Computational Engineering",
         "1709.00402v2.pdf",
         "Isogeometric analysis of thin Reissner-Mindlin plates and shells: locking\nphenomena and B-bar method\nQingyuan Hua,b , Yang Xiac , Sundararajan Natarajand , Andreas Zilianb , Ping Huc , Stéphane P.A.\nBordase,b,∗\na\n\nDepartment of Engineering Mechanics, Dalian University of Technology, Dalian 116024, P.R. China\nDepartment of Computational Engineering Sciences, Faculty of Sciences, Technology and Communication,\nUniversity of Luxembourg, Luxembourg\nc\nSchool of Automotive Engineering, Dalian University of Technology, Dalian 116024, P.R. China\nd\nIntegrated Modelling and Simulation Lab, Department of Mechanical Engineering, Indian Institute of Technology,\nMadras, Chennai-600036, India\ne\nVisiting Professor, Institute of Research and Development, Duy Tan University, K7/25 Quang Trung, Danang,\nVietnam\n\narXiv:1709.00402v2 [cs.CE] 22 Feb 2018\n\nb\n\nAbstract\nWe propose a local type of B-bar formulation, addressing locking in degenerated Reissner-Mindlin\nplate and shell formulations in the context of isogeometric analysis. Parasitic strain components are\nprojected onto the physical space locally, i.e. at the element level, using a least-squares approach.\nThe formulation allows the flexible utilization of basis functions of different order as the projection\nbases. The present formulation is much cheaper computationally than the classical B̄ method. We\nshow the numerical consistency of the scheme through numerical examples, moreover they show\nthat the proposed formulation alleviates locking and yields good accuracy even for slenderness\nratios of 1 × 105 , and has the ability to capture deformations of thin shells using relatively coarse\nmeshes. In addition it can be opined that the proposed method is less sensitive to locking and\nmesh distortion.\nKeywords: Isogeometric, Reissner-Mindlin shell, Locking, B-bar method, Least square\n\nHighlights\n• A local type of B-bar formulation is proposed to address the locking phenomenon in ReissnerMindlin plates and shells.\n∗\n\nCorresponding author\nEmail addresses: qingyuanhucn@gmail.com (Qingyuan Hu), stephane.bordas@gmail.com (Stéphane P.A.\nBordas)\nPreprint submitted to Elsevier\n\nFebruary 23, 2018\n\n\f• The parasitic strains are projected onto the physical space locally thus proposed formulation\nis computationally less expensive than classical B-bar method.\n• Different sets of basis functions are used as the projection bases to achieve better performance.\n• The formulation is less sensitive to mesh distortion when locking happens than pure IGA.\n\n1. Introduction\nThe conventional Lagrange-based finite element method (FEM) employs polynomial basis functions to represent the geometry and the unknown fields. The commonly employed approximation\nfunctions are Lagrangian polynomials. However, these Lagrange polynomials are usually built\nupon a mesh structure which needs to be generated, from the CAD or Image file provided for the\ndomain of interest. This mesh generation leads to the loss of certain geometrical features: e.g. a\ncircle becomes a polyhedral domain. Moreover, Lagrange polynomials lead to low order continuity\nat the interface between elements, which is disadvantageous in applications requiring high order\npartial differential equations.\nThe introduction of isogeometric analysis (IGA) [1] provides a general theoretical framework for\nthe concept of “design-through-analysis” which has attracted considerable attention. The key idea\nof IGA is to provide a direct link between the computer aided design (CAD) and the simulation, by\nutilizing the same functions to approximate the unknown field variables as those used to describe\nthe geometry of the domain under consideration, similar to the idea proposed in [2]. Moreover, it\nalso provides a systematic construction of high-order basis functions [3]. Note that, more recently,\na generalisation of the isogeometric concept was proposed, whereby the geometry continues to be\ndescribed by NURBS functions, as in the CAD, but the unknown field variables are allowed to\nlive in different (spline) spaces. This lead to the concept of sub and super-geometric analysis,\nalso known as Geometry Independent Field approximaTion (GIFT), described within a boundary\nelement framework in [4] and proposed in [5, 6] and later refined in [7]. Related ideas, aiming at\nthe construction of tailored spline spaces for local refinement were proposed recently in [8].\nIn the literature, the IGA has been applied to study the response of plate and shell structures, involving two main theories, viz., the Kirchhoff-Love theory and the Reissner-Mindlin theory. Thanks\n2\n\n\fto the C 1 -continuity of the NURBS basis functions adopted in IGA, Kiendl et al. [9] developed an\nisogeometric shell element based on Kirchhoff-Love shell theory. The isogeometric Kirchhoff-Love\nshell element for large deformations was presented in [10]. The isogeometric Reissner-Mindlin shell\nelement was implemented in [11], including linear elastic and nonlinear elasto-plastic constitutive\nbehavior. The blended shell formulation was proposed to glue the Kirchhoff-Love structures with\nReissner-Mindlin structures in [12]. In addition, the isogeometric Reissner-Mindlin shell formulation that is derived from the continuum theory was presented in [13], in which the exact director\nvectors were used to improve accuracy. The solid shell was developed in [14], in this formulation\nthe NURBS basis functions were used to construct the mid-surface and a linear Lagrange basis\nfunction was used to interpolate the thickness field.\nThe Kirchhoff-Love type elements are rotation-free and are only valid for thin structures. Due\nto the absence of rotational degrees of freedom (DoFs), special techniques are required to deal with\nthe rotational boundary conditions [9, 15, 16] and multi-patch connection [17]. Theoretically, the\nReissner-Mindlin theory is valid for both thick and thin structures, however it is observed from the\nliterature [11, 18] that both the FEM and the IGA approaches suffer from locking for thin structures\nwhen the kinematics is represented by Reissner-Mindlin theory, especially for lower order elements\nand coarse meshes. This has attracted engineers and mathematicians to develop robust elements\nthat alleviates this pathology. Adam et.al. proposed a family of concise and effective selective\nand reduced integration (SRI) [19] rules for beams [20], plates and shells [21], and non-linear\nshells using T-splines [22] within the IGA framework. Elguedj et. al. [23] presented B̄ method\nand F̄ method to handle nearly incompressible linear and non-linear problems. The B̄ method\nhas been successfully applied to shear locking problems in curved beams [24], two dimensional\nsolid shells [25], three-dimensional solid shells [26] and in nonlinear solid shell formulation [27].\nEchter and Bischoff [18, 28] employed the e discrete shear gap (DSG) method [29] within the\nIGA framework to alleviate shear locking syndrome effectively. Other approaches include twist\nKirchhoff theory [30], virtual element method [31], collocation method [32, 33], simple first order\nshear deformation theory [34], and single variable method [35, 36]. The above approaches have\nbeen employed with Lagrangian elements and IGA framework with varying order of success.\n3\n\n\fThe works of Robin Bouclier, Thomas Elguedj and Alian Combescure [25–27] focused on solidshells in IGA and achieve good un-locking performance, thus it is worthy to further test the\nperformance of the B̄ method for degenerated Reissner-Mindlin plates/shells within IGA framework. This paper builds on [37] for beam and rod structures using Timoshenko theory, in order to\nalleviate the locking phenomena, the locking strains are projected onto lower order physical space\nby the least square method. The novel idea behind the formulation is to use multiple sets of basis\nfunctions to project the locking strains locally i.e. element-wise, instead of projecting globally i.e.\nall over the patch. The local projecting algorithm is inspired by the local B̄ method [26] and also\nby the work of local least square method [38, 39]. These kind of formulations allow one to perform\nleast-square projections locally, thereby reducing the computational effort significantly.\nThe outline of this paper is as follows: Section 2 gives an overview of Reissner-Mindlin theory\nfor plates and shells. In Section 3, we present the novel approach, the local B̄ method to alleviate\nthe locking (both shear and membrane) problems encountered in thin structures whilst employing Reissner-Mindlin formulation. The robustness, accuracy and the convergence properties are\ndemonstrated with some benchmark examples in Section 4, followed by concluding remarks in the\nlast section.\n\n2. Isogeometric formulation of Reissner-Mindlin plates and shells\n2.1. Reissner-Mindlin shell model\nFigure (1) represents the mid-surface of the shell in the parametric and physical spaces. In IGA,\nthe parametric space is typically Cartesian, while the physical domain of the undeformed shell can\nbe of complex shape, not necessarily rectangular. For simplicity, we consider a rectangular shell\nof constant thickness h, and assume the linear elastic material to be homogeneous and isotropic,\nwhich is described by Young’s modulus E and Poisson’s ratio ν.\nThe main difference between the Reissner-Mindlin and the Kirchhoff-Love shell theory is in\nthe assumptions on the deformation behavior of the section and in the resulting independent\nkinematic quantities attached to the mid-surface in order to describe the deformation. According\nto the Reissner-Mindlin theory, a first order kinematic description is used in the thickness direction\n4\n\n\fη\nn\nη\nξ\nξ\n\nh\n\nη\n\nz\nξ\n\ny\nx\n\nFigure 1: Mid-surface in the parameter space (left) and physical space (right) for a degenerated shell mid-surface.\nThe real model is recovered by Eq.(1).\n\nto account for the transverse shear deformations. Assuming a Cartesian coordinate system, any\narbitrary point P in the shell structure is described by:\n\nxP = x + ζn,\n\n(1)\n\nand its displacement is calculated assuming small deformation as\n\nuP = u + ζθ × n,\n\n(2)\n\nwhere x is the geometry of the mid-surface as shown in Fig.1, ζ ∈ [− h2 , h2 ] denotes the thickness. u,\nθ and n are the displacement vector, the rotation vector and the normal vector on the mid-surface\npoint projected by point P . The linearized strain tensor valid for small deformations is adopted\nhere\n1\nε = (uP,x + uT\nP,x ).\n2\n\n(3)\n\n2.2. Isogeometric approach\nIn the context of shells, bi-variate NURBS basis functions are employed. Let Ξ = {ξ1 , . . . , ξn+p+1 }\nand H = {η1 , . . . , ηm+q+1 } be open knot vectors, and wA be given weights, A = {1, . . . , nm}. Then,\nthe NURBS basis functions RA (ξ, η) are constructed, where p and q are the orders along the directions ξ and η respectively. For more details about IGA, interested readers are referred to [40]\n5\n\n\fand references therein.\nFollowing the degenerated type formulation, the geometry of the undeformed mid-surface is\ndescribed by\nx=\n\nnm\nX\n\nRA xA ,\n\n(4)\n\nA=1\n\nand the mid-surface discrete displacement field is interpolated as\n\nh\n\nU =\n\nnm\nX\n\nRA qA ,\n\n(5)\n\nA=1\n\nwhere xA defines the location of the control points, U h = (u, θ)T , qA = (uA , θA )T is the vector\nof control variables corresponding to each control point, specifically uA = (u, v, w)T and θA =\n(θx , θy , θz )T . The approximation space for displacement field is denoted as Qp,q in order to highlight\nthe orders of the basis functions.\nOnce the mid-surface is described using Eq. (4) and Eq. (5), any arbitrary point P in the shell\nbody can be traced by the following discrete forms\n\nxP =\n\nnm\nX\n\nRA (xA + ζnA ),\n\n(6)\n\nRA (uA + ζθA × nA ),\n\n(7)\n\nA=1\n\nuP =\n\nnm\nX\nA=1\n\nwhere\nn=\n\nx,ξ × x,η\n||x,ξ × x,η ||2\n\n(8)\n\nis the normal vector. The normal vectors at the Greville abscissae nA are adopted here because\nit can achieve a good balance between the accuracy and the efficiency [21]. It should be noted\nthat the above equation includes the plate formulation, which can be considered as a special case.\nFor plates, one always has n(x, y, z) = (0, 0, 1)T , which means that there are only two rotational\ndegrees of freedom, θx and θy .\n\n6\n\n\fUsing Voigt notation, the relation between the strains and the stresses is expressed as\n\nσ = Dg ε ,\n\n(9)\n\nwhere Dg is the global constitutive matrix, and\nDg = T T Dl T ,\n\n(10)\n\nhere Dl is the given local constitutive matrix. To make Dl suitable for the physical geometry, the\ntransformation matrix T is employed, which is composed of x,ξ and x,η . In addition, to fulfill the\nplane stress state σ33 = 0, the local constitutive matrix is given by\n\n\n\n1\n\nν\n\n\n\n0\nE \n\nDl =\n\n2\n1 − ν 0\n\n\n0\n\n\n0\nin which κ =\n\n5\n6\n\nν 0\n\n0\n\n0\n\n1 0\n\n0\n\n0\n\n0 0\n\n0\n\n0\n\n0 0\n\n1−ν\n2\n\n0\n\n0 0\n\n0\n\nκ 1−ν\n2\n\n0 0\n\n0\n\n0\n\n0 \n\n0 \n\n\n\n0 \n,\n\n0 \n\n\n0 \n\n\n\n(11)\n\nκ 1−ν\n2\n\nis the shear correction factor.\n\nUpon employing the Galerkin framework and using the following discrete spaces for the displacement field,\nn\no\n\u0002\n\u0003d\nS = U ∈ H 1 (Ω) , U |Γu = U d ,\nn\no\n\u0002\n\u0003d\nV = V ∈ H 1 (Ω) , V |Γu = 0 ,\n\n(12)\n(13)\n\nthe variational function reads: find U ∈ S such that\nb(U , U ∗ ) = l(U ∗ )\n\n7\n\n∀U ∗ ∈ V,\n\n(14)\n\n\fin which the bilinear term is\n∗\n\nZ\n\nb(U , U ) =\n\nε (U ∗ )T Dgε (U )dΩ.\n\n(15)\n\nΩ\n\nWhen the displacements and the rotations are approximated with polynomials from the same\nspace, the discretized framework experiences locking (shear and membrane) when the thickness\nbecomes very small. The numerical procedure fails to satisfy the Kirchhoff limit as the shear strain\ndoes not vanish with the thickness of the shell approaching zero. One explanation of shear locking is\nthat different variables involved are not compatible [35], which is also known as field inconsistency.\nAnother explanation is that in curved elements shearless bending [41] and inextensible bending\ndeformations cannot be represented exactly, because of the appearance of spurious membrane\nand shear terms that absorb the major part of the strain energy [24], and this results in an\noverestimation of the stiffness. In the next section, we introduce the B-bar method to deal with\nlocking in Reissner-Mindlin plates and shells within the framework of IGA.\n\n3. B-bar method for Reissner-Mindlin plates and shells\nIn this section, we firstly illustrate the main idea behind the B̄ method, then after introducing\nthe classical B̄ method, in order to further improve the efficiency of the formulation, a local form\nof B̄ formulation is proposed.\n3.1. The idea behind the B-bar method\nImagining we have a unit length 2-node Timoshenko beam element at hand, the element length\nparameter x ∈ [0, 1]. By the help of the shape functions\n\nN1 = 1 − x,\n\nN2 = x,\n\n(16)\n\nthe deflection field w and the rotation field θ are built as\n\nw = N1 w1 + N2 w2 ,\n8\n\nθ = N1 θ1 + N2 θ2 ,\n\n(17)\n\n\fin which the wi and θi are nodal variables, i = 1, 2.\nIn case of thin beams, the Timoshenko beam theory tends to degenerate into the Euler beam\ntheory, and the shear strain (in Timoshenko beam theory) tends to vanish, thus we have the\nfollowing equation\nγ=\n\ndw\n− θ = 0,\ndx\n\n(18)\n\nand in discrete form\n\u0002\n\u0003\nγ h = (−w1 + w2 ) − (1 − x)θ1 + xθ2 = 0.\n\n(19)\n\nThe field inconsistency phenomenon occurs, that is the order of θi is higher than the order of wi ,\nwhich makes the shear strain difficult to vanish.\nThe idea of B̄ method is to make the field to be order consistency. We build a pseudo shear\nstrain by a zero order basis functions as\n\nγ̄ h =\n\nX\n\nh\n\nN̄Ā γ̄ Ā .\n\n(20)\n\nĀ\n\nSince the order of shape functions Ni is one, we define the one order lower set of shape functions\nh\n\nas N̄1 = 1, Ā = 1, and γ̄ 1 is the corresponding pseudo DoF. Next we perform a least square\nh\n\nprojection to solve for γ̄ 1\n\nZ\n\n1\n\nN̄1 (γ̄ h − γ h ) dx = 0,\n\n(21)\n\n0\n\nwe have\n\nSolve the above equation for γ̄ 1\n\n1\n1\nh\nγ̄ 1 + w1 − w2 + θ1 + θ2 = 0.\n2\n2\n\n(22)\n\n1\n1\nh\nγ̄ 1 = −w1 + w2 − θ1 − θ2 ,\n2\n2\n\n(23)\n\nh\n\nand the pseudo shear strain we previously built becomes\n1\n1\nγ̄ h = (−w1 + w2 ) − ( θ1 + θ2 ).\n2\n2\n9\n\n(24)\n\n\fCompare the pseudo shear strain in Equation (24) with the original one in Equation (19), it is\nfound that the shape functions for wi remain unchanged, but the order of shape functions for θi is\nreduced to be the same as for wi . Thus it is believed that the pseudo shear strain could achieve a\ngood un-locking performance.\n3.2. Classical B-bar method in IGA\nAs stated before, the novel idea behind the B̄ method is using a modified strain instead of the\noriginal one. In classical B̄ method [23, 24], a common way is to use the projection of the original\nstrain to formulate the bilinear term\n∗\n\nZ\n\nε̄ε∗T Dε̄εdΩ.\n\nb̄(U , U ) =\n\n(25)\n\nΩ\n\nThe projected strain ε̄ε and the original strain ε are equal in the sense of the least square projection.\nThe projection space is chosen to be one order lower, i.e. Qp̄,q̄ = Qp−1,q−1 (see Figure (2)(a)). Built\nfrom one order lower knot vectors, with all the weights given as WĀ = 1, Ā = {1, . . . , n̄m̄}, one\norder lower B-spline basis functions N̄Ā are obtained. The L2 projection process is performed on\nthe physical domain as\nZ\nΩ\n\n\u0010\n\u0011\nN̄B̄ ε h − ε̄εh dΩ = 0,\n(26)\n\nB̄ = 1, . . . , n̄m̄,\nin which the discretized form of the projected strain is\n\nε̄εh =\n\nn̄m̄\nX\n\nh\n\nN̄Āε̄εĀ ,\n\n(27)\n\nĀ=1\nh\n\nwhere ε̄εĀ means the projection of ε h onto N̄Ā . Finally, we have\nh\n\nε̄ε =\n\nn̄m̄\nX\n\nN̄Ā MĀ−1\nB̄\n\nĀ,B̄=1\n\n10\n\nZ\nΩ\n\nε h N̄B̄ dΩ,\n\n(28)\n\n\fwhere MĀB̄ is is the inner product matrix\nZ\nMĀB̄ = (N̄Ā , N̄B̄ )Ω =\n\nΩ\n\nN̄Ā N̄B̄ dΩ.\n\n(29)\n\nThe Hu-Washizu principle could be utilized to prove the variational consistency of the B̄ method\n[23–25, 42]. In addition, [24, 26] proved that the B̄ method is equivalent to the mixed formulation.\n3.3. Local B-bar formulation for Reissner-Mindlin plates and shells\nFor degenerated plates and shells, although the geometries are represented in two dimensions,\nand there is the thickness parameter ζ involved in the formulation. In this case, if the whole part of\nstrain is projected, rank deficiency appears and the formulation yields inaccurate results as shown\nin [25] and also to our computational experience. Following the similar approach as outlined in\n[25, 26], in this work, only the average strain through the thickness is projected as\n(p̄+1)(q̄+1)\n\nMID(εεhe )\n\nX\n\n=\n\nZ\n\nN̄Ā MĀ−1\nB̄\n\nΩe\n\nĀ,B̄=1\n\nMID(εεhe )N̄B̄ dΩ,\n\n(30)\n\nwhere MID(εεhe ) is defined to be the average strain through the thickness within a single element\nMID(εεhe )\n\n1\n=\nh\n\nh\n2\n\nZ\n\n−h\n2\n\nε he dz.\n\n(31)\n\nThe modified bi-linear form is defined as\n\nb̄(U , U ∗ ) =\n\nZ\nΩ\n\n\n\nT\n ∗T\n\n∗ T\nε| {zDεε} − MID(εε ) DMID(εε) + MID(εε) DMID(εε) dΩ.\n|\n{z\n} |\n{z\n}\noriginal\n\noriginal average strain\n\n(32)\n\nprojected average strain\n\nWithin local B̄, if the shape functions over the elements possess C 0 continuity (which is obviously\nfulfilled), the Hu-Washizu principle can be rewritten as [43]\n\nδΠHW (U , ε̃ε, σ̃) =\n\nX Z\nm\n\nZ\n\nT\n\nΩm\n\n!\n\nZ\n\nσ̃ (εε − ε̃ε)dΩ −\n\nδε̃ε DεεdΩ + δ\n\nΩm\n\nT\n\ntδU dΓf\n\n(33)\n\nΓf m\n\nin which m denotes the number of elements. In this context the assumed displacements should\n11\n\n\fsatisfy C 0 continuous between elements, but discontinuous assumed strains are allowed, this explains\nwhy different sets of basis functions can be used. However, when the local B̄ method is applied for\ndegenerated plates, i.e. the average strain is projected, only the numerical consistency condition\nis satisfied according to the appendix of [25]. This conclusion also stands for the proposed method\nin this paper.\nReviewing the literature of the B̄ method since its appearance [44], one valuable contribution\nis the introduction of local B̄ concept [26, 37], thanks to which lots of computational effort has\nbeen saved without too much accuracy loss. The motivation of the local B̄ concept comes from\nthe observation that one needs to calculate the inverse of matrix M in Equation (28), which could\nbe computationally expensive if the projection is applied globally. Thus, from a practical point of\nview, it is highly recommended to project the strains locally [26, 37], i.e. element-wise\n(p̄+1)(q̄+1)\n\nε̄εhe\n\n=\n\nX\n\nN̄Ā MĀ−1\nB̄\n\nZ\n\nε he N̄B̄ dΩ,\n\n(34)\n\nN̄Ā N̄B̄ dΩ.\n\n(35)\n\nΩe\n\nĀ,B̄=1\n\nwith\nZ\nMĀB̄ = (N̄Ā , N̄B̄ )Ωe =\n\nΩe\n\nMoreover, with the opinion of projecting the original strains into the lower order space could\nrelease the locking constrains, we treat the corner, boundary and inner elements separately by\nusing the lowest possible order of each element, in order to release the locking constrains as much\nas possible. Thus instead of projecting the strains onto Qp−1,q−1 , different sets of projection spaces\nare adopted in this work, which could bring more flexibility into the formulation. This is called\nthe generalized strategy. The projection spaces need to be chosen carefully to avoid ill-condition\nor rand deficiency, readers interested in the corresponding mathematical theory is recommended\nto see [45], which is in the context of volume locking (nearly-incompressible) problems. Here,\nthe strategy in our previous work [37] is extended to bi-dimensional cases. Specifically, for space\nQ2,2 , we adopt Q1,1 for corner elements, Q0,1 and Q1,0 for boundary elements, and Q0,0 for inner\nelements, as shown in Figure (2)(b).\n\n12\n\n\fQ1,1\n\nQ1,1\n\nQ1,1\n\nQ0,1\n\nQ0,1\n\nQ1,1\n\nQ1,1\n\nQ1,1\n\nQ1,1\n\nQ1,1\n\nQ1,0\n\nQ0,0\n\nQ0,0\n\nQ1,0\n\nQ1,1\n\nQ1,1\n\nQ1,1\n\nQ1,1\n\nQ1,0\n\nQ0,0\n\nQ0,0\n\nQ1,0\n\nQ1,1\n\nQ1,1\n\nQ1,1\n\nQ1,1\n\nQ1,1\n\nQ0,1\n\nQ0,1\n\nQ1,1\n\nξ, p = 2\n(a) Local B̄\n\nξ, p = 2\n(b) Generalized local B̄\n\nη, q = 2, C 1\n\nQ1,1\n\nη, q = 2\n\nη, q = 2\n\nQ1,1\n\nξ, p = 2, C 1\n(c) SRI [21]\n\nFigure 2: Unlock strategies in bi-dimensional parameter space. Projecting spaces for locking strains are denoted as\nQp,q . For local B̄ method (a), one order lower B-spline basis functions are employed for all elements. The presented\ngeneralized local B̄ method (b) uses basis functions of different orders, inspired by the SRI method (c), while C 1\ncontinuities of elements are not restricted as in (c).\n\n3.4. Discussions\nThe approach of using multiple sets of lower order basis functions was firstly presented for\none-dimensional cases in [37]. This generalized strategy is similar with the SRI strategy in [21]. In\nthe case of only one quadrature point being used for an element, the functions are detected only at\nthis quadrature point but nowhere else, which is analogous to its projection onto a Q0,0 space. To\nachieve a better understanding of the proposed projection strategy, the basis functions are shown\nin Fig.3. There are four elements per side. In the local B̄ method, only one single set of basis\nfunctions (i.e. Q1,1 ) is used to form the projection space. While in the generalized local B̄ method,\nfour set of basis functions (i.e. Qp̄,q̄ ) are used. As shown in Fig.3 (b), this strategy means that\nthe modified strains are assumed to be constant for the four internal elements and bi-linear for the\nfour corner elements. In particular, for the side elements, the modified strains are assumed to be\nconstant along the patch boundary, and linear facing inside.\nThe advantage of SRI method is that its implementation is simple and the computations involve less calculations because fewer quadrature points are employed. But one must consider the\ncontinuity between neighboring elements, which requires additional efforts. However, for the local\nB̄ method, there is no problem in continuity because the projection procedure is applied elementwise. This conclusion can be found in [37]. Thus, one could always use knot vectors of order\n13\n\n\f0.5\n\n0.75\n\n1\n\n0.75\n\n0.5\n\n0.5\n\n0.25\n\n1\n\n0.75\n0.5\n\n0.25\n\n0.25\n\n0\n\n0.75\n\n0\n\n0.25\n\n0\n\n0\n\nvalue\n1\n0.75\n1\n\n1\n1\n\n0.75\n0.75\n\n0.5\n\n0.75\n\n0.5\n\n0.5\n0.25\n\n1\n\n0.75\n0.5\n0.25\n\n0.25\n\n0.25\n\n0.5\n0.25\n0\n\n0 0\n\n0 0\n\n(a) Local B̄\n\n(b) Generalized local B̄\n\nFigure 3: Bi-dimensional basis functions for projecting locking strains of each element, let Qp,q denote the projecting\nspace. There are 4 × 4 elements. (a) Basis functions of Q1,1 in the local B̄ method. (b) Basis functions in Qp̄,q̄ (see\nFig.2) in the generalized local B̄ method, the strains at the corners are assumed to be linear along both sides, the\nstrains at the patch sides are assumed to be linear toward inside and constant along the boundary, the strains within\nthe patch are assumed to be constant.\n\np̄ and continuity C p̄−1 (i.e. without inner repeated knots) as the projection knot vector. Apart\nfrom being accurate, since the lower order basis functions are used, for example linear functions\nfor corner elements and constants for inner elements, better efficiency can be achieved with fewer\nquadrature points than usual.\nCompared with the classical B̄ method, the local B̄ method shows promising advantages in\nterms of computational efficiency. In the classical B̄ method, the calculation of the inverse of\nmatrix M requires large amounts of memory. Thus, the adoption of local projection rather than\nglobal one saves lots of computational efforts. Similar conclusions had been carried out in the\ncontext of LLSQ fitting for boundary conditions [38] and further LLSQ algorithm [39]. In these\ncontributions, assuming A is the global Boolean assembly operator, the matrix\n\nAAT\n\n\u0001−1\n\nA\n\n(36)\n\nis used to calculate the uniformly weighted average of shared nodes. For instance, if a node\n14\n\n\f(control point) is shared by n elements, then the values corresponding to this node is divided by\nn. Furthermore, for the local B̄ method in [26], the same procedure named by strain smoothing\nis employed to ensure the continuity of the projected strains and thus to obtain results of better\naccuracy. The price to pay is that one need to calculate the average operator and the bandwidth is\nlarger than classical IGA. In this research to obtain the strain field of higher order continuity, we\nuse the original strain of discretized form εh instead of ε̄h , i.e. recover the displacement field firstly\nby Eq.(5) and then get the strain field as usual, in this way the continuity property of NURBS\nbasis functions is utilized.\n\n4. Numerical examples\nIn this section, we demonstrate the performance of the proposed generalized local B̄ formulation\nfor Reissner-Mindlin plates/shells by solving a few standard benchmark problems. The proposed\nformulation is implemented within the open source C++ IGA framework Gismo\n\n1\n\n[46]. The nu-\n\nmerical examples include: (a) Square plate; (b) Scordelis-Lo roof; (c) Pinched cylinder and (d)\nPinched hemisphere with a hole. Unless otherwise mentioned consistent units are employed in this\nstudy. In all the numerical examples, the following knot vectors are chosen as the initial ones:\nΞ = {0, 0, 0, 1, 1, 1},\n(37)\nH = {0, 0, 0, 1, 1, 1}.\nIn the following examples, (p + 1) × (q + 1) Gauss points are used for numerical integration, p × q\nGauss points are used for the family of B̄ methods, the reduced quadrature scheme in [21] is\nadopted for comparison. The following conventions are employed whilst discussing the results:\n• LB: Local B̄ method [26] without strain smoothing in Equation (36). The projection is\napplied element by element as shown in Eq.(34) and Figure (2)(a),\n• GLB: Generalized local B̄ method, which means that based on LB, the strategy of using\nmulti-sets of basis functions as shown in Figure (2)(b) is adopted,\n1\n\nhttps://ricamsvn.ricam.oeaw.ac.at/trac/gismo/wiki/WikiStart\n\n15\n\n\f• SRI: Selective reduced integration [21].\n4.1. Simply supported square plate\nConsider a square plate simply supported on its edges with thickness h and the length of the\nside L = 1. Owing to symmetry, only one quarter of the plate, i.e, a = L/2 is modeled as shown in\nFigure (4). The plate is assumed to be made up of homogeneous isotropic material with Young’s\nmodulus E = 200 GPa and Poisson’s ratio ν = 0.3, and subjected to uniform pressure p. The\ncontrol points and the corresponding weights are given in Table 1.\netry\nsymm\n\nsim\n\nA\n\nply\n\nL\n\nsu2\npp\n\nE = 200 × 109\nν = 0.3\n\nor\nte\n\nd\n\nm\nsy\n\nL=1\n\net\nm\n\nh = 10−3 , p = 10−2\n\nry\n\nz\ny\n\nor\nsupp\ny\nl\np\nL\nsim\n\nx\n\nor\n\nted\n\nh = 10−5 , p = 10−8\n\n2\n\nFigure 4: Mid-surface of the rectangular plate in Section 4.1: geometry and boundary conditions. Only one quarter\nof the plate is shown here. The red filled squares are the corresponding control points.\n\nTable 1: Control points and weights for the rectangular plate in Figure (4).\n\nx\ny\nz\nw\n\n1\n0.5\n0\n0\n1\n\n2\n0.75\n0\n0\n1\n\n3\n1\n0\n0\n1\n\n4\n0.5\n0.25\n0\n1\n\n5\n0.75\n0.25\n0\n1\n\n6\n1\n0.25\n0\n1\n\n7\n0.5\n0.5\n0\n1\n\n8\n0.75\n0.5\n0\n1\n\n9\n1\n0.5\n0\n1\n\nThe analytical out-of-plane displacements for a thin plate with simply supported edges are\ngiven by\n16p\nw(x, y) = 6\nπ D\nwhere D =\n\nEh3\n.\n12(1−ν 2 )\n\n∞\nX\n\n∞\nX\n\nm=1,3,5,··· n=1,3,5,···\n\nmπx\nL\n\nsin\nmn\n\nh\n\n\u0001\n\n\u0001\nm 2\nL\n\nsin\n+\n\nnπy \u0001\nL\n\u0001 i2\nn 2\nL\n\n(38)\n\nThe transverse displacement is constant when the applied load is proportional\n\nto h3 , thus the numerical solution does not depend on the thickness of the plate. Due to the fact\n\n16\n\n\fthat the error of wA can express the field error to some extent, only the errors of wA are studied\ninstead of the field error, the reference value is wA = -2.21804 × 10−6 .\nFigure (5) shows the normalized displacement as a function of mesh refinement for two different\nplate thickness. For thickness h = 10−3 , although the conventional IGA yields inaccurate results\nfor coarse meshes, the results tend to improve upon refinement. However, it suffers from severe\nshear locking syndrome when h = 10−5 . In this case the results seem remain horizontal with respect\nto number of control points, indicating that the elements are fully locked. For LB and GLB slight\nrank deficiency occur for coarse meshes. GLB get quite good results when h = 10\n\n−5 .\n\nFrom the\n\nnumerical study it is inferred that the proposed formulation alleviates shear locking phenomenon\nand yields accurate results even for extremely thin plates of slenderness ratio 105 .\n1.02\n1\nIGA, quad., h = 10−3\nIGA, quad., h = 10−5\nLB, h = 10−3\nLB, h = 10−5\nGLB, h = 10−3\nGLB, h = 10−5\n\nref\nwA /wA\n\n0.98\n0.96\n0.94\n0.92\n0.9\n0.88\n0.86\n\n36324\n\n1,156\nNumber of control points\n\n4,356\n\nFigure 5: Normalized central displacement wA with mesh refinement for the rectangular plate 4. h stands for\nthickness. IGA of order 2 suffer from locking. GLB gets good accuracy.\n\nFigure (6)(b) compares the convergence behavior by a severe locking case in which the plate\nthickness is h = 10−5 . In this study, elements by IGA of order 2 are locked even by more than\n1000 elements, the convergence rate is nearly zero. Elements by IGA of order 3 start with smaller\nerror, but suffer from locking until the elements are refined to a certain number. It is inferred that\nclassical IGA elements are locked until the elements are refined to a certain number, and higher\norder elements could reach this number earlier, this conclusion is already known in literatures. LB\nbehaves the same as GLB for mesh 2 × 2. GLB achieves a good convergence rate at beginning,\nbut in general the convergence rate is smaller as the meshes are refined, at the last two steps the\n17\n\n\ferrors become even larger, which is also observed for IGA of order 3. This could be explained as\nthe results slightly mismatch the reference value, as pointed out in Figure (6)(b).\n−2.28\n\n10−1\n\n−2.26\n\nref |/|w ref |\n|wA − wA\nA\n\n100\n\n10−2\n\nwA\n\n−2.24\n\n10−3\n10−4\n10−5\n10−6 0\n10\n\n·10−6\n\n−2.22\n−2.2\n\nIGA, quad.\nLB\nGLB\nIGA, cubic\n101\nNumber of elements per side\n\n−2.18\n−2.16\n100\n\n102\n\n(a) Convergence\n\nLB\nGLB\nIGA, cubic\nref\nwA\n101\nNumber of elements per side\n\n102\n\n(b) Partical enlarge\n\nFigure 6: Convergence of deflection wA for the rectangular plate 4 subjected to concentrated load, thickness h =\n10−5 . IGA elements are locked until the elements are refined to a certain number.\n\nFrom the above, it is opined that for plates, when the slenderness ratio reaches a large number,\ne.g. 103 or 105 , the classical IGA elements suffer from locking. However, the studies above are\ndone in an ideal condition that the plates are discreted by structured meshes. The influence of the\nmesh distortion on the performance of the proposed formulation is investigated by considering two\ndifferent kinds of mesh distortions as shown in Figure (7). Thickness h = 10−5 is considered here\nas the case when both locking and mesh distortion appear, and h = 10−1 is chosen as the control\ncase when only mesh distortion appears. The influence of mesh distortion on the normalized center\ndisplacement is depicted in Figure (8). When locking and mesh distortion occur at the same time,\nerrors of IGA drop down quickly, while GLB keeps good accuracy even for severe distortions. It\ncan be inferred that the results with classical IGA deteriorates with mesh distortion, while the\nproposed formulation is less sensitive to the mesh distortion.\n4.2. Scordelis-Lo roof\nNext to demonstrate the performance of the proposed formulation when a structure experiences\nmembrane locking, Scordelis-Lo roof problem is considered. It features a cylindrical panel with\n18\n\n\f543 21\n\n3\n2\n1\ne=0\n-1\n-2\n\n(a) Expansion\n\nr=0\n\n(b) Rotation\n\n1\n\n1\n\n0.8\n\n0.8\n\n0.6\n\n0.6\n\n0.4\n\nIGA, quad., h = 10−1\nIGA, quad., h = 10−5\nGLBM, h = 10−1\nGLBM, h = 10−5\n\n0.2\n0\n−2\n\nref\nwA /wA\n\nref\nwA /wA\n\nFigure 7: Illustration of control mesh distortions. In (a), four selected control points are moved along the diagonal.\nIn (b), four selected control points are moved around the center of the patch. Indexes e and r are employed to\nindicate the stages of distortions.\n\n−1\n\n0\n\n1\n\n2\n\n0.4\n\nIGA, quad., h = 10−1\nIGA, quad., h = 10−5\nGLBM, h = 10−1\nGLBM, h = 10−5\n\n0.2\n\n3\n\ne\n\n(a) Expansion e\n\n0\n\n0\n\n1\n\n2\n\nr\n\n3\n\n4\n\n5\n\n(b) Rotation r\n\nFigure 8: Normalized results of wA of the rectangular plate 4 with control mesh distortions 7. GLB means the\ngeneralized local B̄. h stands for thickness. When locking and mesh distortion occur at the same time, errors of IGA\nof order 2 drop down quickly, while GLB keeps good accuracy even for severe distortions.\n\n19\n\n\fends supported by rigid diaphragm. The roof is subjected to uniform pressure, pz = 6250 N/m2 and\nthe vertical displacement of the mid-point of the side edge is monitored to study the convergence\nbehavior. The material properties are Young’s modulus: E = 30 GPa and Poisson’s ratio, ν =\n0. Owing to symmetry only one quarter of the roof is modeled as shown in Figure (9). The roof\nis modeled with the control points (x, y, z) and the weights, w given in Table 2. The analytical\nref = -0.0361 m [47], however the results obtained by\nsolution based on the deep shell theory is wB\nref = −0.0361776, is taken as reference solution.\nmesh 100 × 100 of pure IGA of p = q = 4, wB\n\nrig\n\npz\n\nid\n\ny\netr\nm\nsym pz\n\npz\n\ndi\nap\nh\n\nE = 3 × 1010\n\nra\ng\n\nm\n\nν=0\nL=6\n\npz\n\nh = 0.03\nm\nm\nsy\n\nz\n\nR\n\nR=3\n\nry\net\n\ny\nx\n\npz = 6250\n\ne\nfrLe\n2\n\nB\n\nFigure 9: Scordelis-Lo roof problem in Section 4.2: geometry and boundary conditions. The red filled squares are\nthe corresponding control points. The mid-surface of the cylindrical panel is modeled and the roof is subjected to a\nuniform pressure.\n\nTable 2: Control points and weights for the Scordelis-Lo roof problem 9.\nx\ny\nz\nw\n\n1\n0\n0\n3\n1\n\n2\n1.091910703\n0\n3\n0.9396926208\n\n3\n1.928362829\n0\n2.298133329\n1\n\n4\n0\n1.5\n3\n1\n\n5\n1.091910703\n1.5\n3\n0.9396926208\n\n6\n1.928362829\n1.5\n2.298133329\n1\n\n7\n0\n3\n3\n1\n\n8\n1.091910703\n3\n3\n0.9396926208\n\n9\n1.928362829\n3\n2.298133329\n1\n\nFor the geometry considered here, R/h = 100 and L/h = 200, the structure experiences membrane locking as the transverse shear strain is negligible. The roof is dominated by membrane\nand bending deformations. The convergence of the normalized vertical displacement with mesh\nrefinement is shown in Figure (10). The results from the proposed formulation is compared with\nselective reduced integration technique [21]. It can be seen that except IGA of order 2, the convergence lines by others stall in the middle and then converge as before. It is noticed that for coarse\n20\n\n\fmeshes the present method leads to rank sufficient matrices, however for other problems slightly\nrank deficiency is possible as in the strain smoothing method by a single subcell [48]. In addition,\nthe contour plot of the deflection wB by IGA and GLB are given in Figure (11) as a function of\nmesh refinement. It is obvious that IGA is locked in the case of coarse mesh, while GLB captures\nthe deformation quite very well even for coarse meshes.\n100\nIGA, quad.\nSRI\nLB\nGLB\nIGA, cubic\n\n10−2\n\n2\nrank(KB̄ ) − rank(KIGA )\n\nref |/|w ref |\n|wB − wB\nB\n\n10−1\n\n10−3\n10−4\n\nLB\nGLB\n\n1\n0\n−1\n−2\n\n10−5 0\n10\n\n101\nNumber of elements per side\n\n102\n\n1 2\n\n(a) Convergence of of wB\n\n4\n8\n16\nNumber of elements per side\n\n(b) Rank of stiffness matrix\n\nFigure 10: Results of the Scordelis-Lo roof 9. IGA suffers from locking. LB and GLB finally converge.\n\n4.3. Pinched cylinder\nFrom the above two examples, it is clear that the proposed formulation yields accurate results\nwhen the structure experience either shear locking or membrane locking. To demonstrate the\nrobustness of the proposed formulation when the structure experiences both shear and membrane\nlocking, we consider the pinched cylinder problem. Again, due to symmetry only one quarter of\nthe cylinder is modeled as shown in Figure (12). The corresponding control points and weights\nare given in Table 3. The cylinder is made up of homogeneous isotropic material with Young’s\nmodulus, E = 30 GPa and Poisson’s ratio, ν = 0.3. The concentrated load acting on the cylinder\nref = -1.85942−7 m.,\nis P = 0.25 N. The reference value of the vertical displacement is take as wC\n\nwhich is obtained by mesh 100 × 100 of pure IGA of order 4. This example serves a test case to\nevaluate the performance when the structure is dominated by bending behaviour. The convergence\nof the vertical displacement with mesh refinement is shown in Figure (13) and it is evident that\n21\n\n\f(a) 4 × 4 IGA\n\n(b) 8 × 8 IGA\n\n(c) 4 × 4 GLB\n\n(d) 8 × 8 GLB\n\nFigure 11: Deflection field w × 102 of the Scordelis-Lo roof 9 by IGA of order 2 (a,b), and by the generalized local\nB̄ method (c,d). For coarse meshes pure IGA is locked, but deformation is captured very well by GLB.\n\n22\n\n\fthe proposed formulation yields more accurate results than the conventional IGA of order 2, in\naddition the B̄ formulation does not change the rank of the stiffness matrix as in Figure (13)(b).\nThe contour plot of wC is shown in Figure (14), the elements by GLB seem more flexible than IGA\nto be deformed.\nF\ne\nmm\n\ntry\n\nC\n\nm\n\nsy\n\nsy\n\nry\n\net\n\nm\n\nE = 3 × 1010\nν = 0.3\n\nrig\n\nL=6\n\nid\n\nR=3\n\nap\n\ndi\n\nR\n\nm\n\nag\n\nhr\n\nh = 0.03\nF =1×\ntry\n\nz\n\n1\n4\n\nsym\n\nme\n\ny\nx\n\nL\n2\n\nFigure 12: The mid-surface of a fourth of the pinched cylinder in Section 4.3. The red filled squares are the\ncorresponding control points.\n\nTable 3: Control points and weights for the pinched cylinder problem 12.\n\nx\ny\nz\nw\n\n1\n0\n0\n3\n1\n\n2\n3\n0\n3\n0.7071067812\n\n3\n3\n0\n0\n1\n\n4\n0\n1.5\n3\n1\n\n5\n3\n1.5\n3\n0.7071067812\n\n6\n3\n1.5\n0\n1\n\n7\n0\n3\n3\n1\n\n8\n3\n3\n3\n0.7071067812\n\n9\n3\n3\n0\n1\n\n4.4. Pinched hemisphere with hole\nAs the last example, consider a pinched hemisphere with 18◦ hole subjected to equal and\nopposite concentrated forces applied at the four cardinal points. The hemisphere is modeled with\nYoung’s modulus, E = 68.25 MPa, Poisson’s ratio ν = 0.3 and concentrated force, P = 1 N. As\nbefore, owing to symmetry, only one quadrant of the hemisphere is modeled as shown in Figure\n23\n\n\f100\n\n10−1\n\n2\nrank(KB̄ ) − rank(KIGA )\n\nref |/|w ref |\n|wC − wC\nC\n\nIGA, quad.\nSRI\nLB\nGLB\nIGA, cubic\n\n10−2\n\nLB\nGLB\n\n1\n0\n−1\n−2\n\n10−3 0\n10\n\n101\nNumber of elements per side\n\n102\n\n1 2\n\n(a) Convergence of of wC\n\n4\n8\n16\nNumber of elements per side\n\n(b) Rank of stiffness matrix\n\nFigure 13: Results of the pinched cylinder 12. GLB achieves a good accuracy and convergence.\n\n(15). The location of the control points is also shown. The control points and weights employed\nto model the hemisphere are given in Table 4. This example experiences severe membrane and\nshear locking, has right body rotations and the discretization experiences severe mesh distortion.\nThe mesh distortion further enhances the locking pathology, as shown in the plate example. To\nevaluate the convergence properties, the horizontal displacement uref\nD = 0.0940 m [21] is taken\nas the reference solution. The results form the proposed formulation are plotted in Figure (16).\nAs the elements are refined, GLB behaves similarly to SRI, but the results obtained by GLB is\nslightly accurate. Once again the rank of the stiffness is not changed by B̄ method. The reason\nthat prevents the error to go below is the mismatch between the convergence value and the adopted\nreference value, as shown in Figure (17). Moreover, the field of displacement ux is plotted in Figure\n(18), it seems that GLB has more abilities to capture the deformations.\nTable 4: Control points and the corresponding weights for the pinched hemisphere problem 15.\nx\ny\nz\nw\n\n1\n10\n0\n0\n1\n\n2\n10\n10\n0\n0.7071067810\n\n3\n0\n10\n0\n1\n\n4\n10\n0\n7.265425281\n0.8090169942\n\n5\n10\n10\n7.265425281\n0.5720614025\n\n24\n\n6\n0\n10\n7.265425281\n0.8090169942\n\n7\n3.090169944\n0\n9.510565163\n1\n\n8\n3.090169944\n3.090169944\n9.510565163\n0.7071067810\n\n9\n0\n3.090169944\n9.510565163\n1\n\n\f(a) 4 × 4 IGA\n\n(b) 8 × 8 IGA\n\n(c) 4 × 4 GLB\n\n(d) 8 × 8 GLB\n\nFigure 14: Field of w × 107 of the pinched cylinder 12 by IGA of order 2 (a,b), and by the generalized local B̄ method\n(c,d). IGA is locked for coarse meshes and refined meshes. For GLB the elements seem more flexible than IGA to\nbe deformed.\n\n25\n\n\ffree\n\nm\nsy\n\nsymme\n\ntry\n\ntry\nme\n\nE = 6.825 × 107\nν = 0.3\nR = 10\n\nz\n\nh = 0.04\nF =2×\ny\n\n1\n2\n\nF\n\nx\nR\nfree\n\nD\nF\n\nFigure 15: The mid-surface of a fourth of the pinched hemisphere in Section 4.4. The red filled squares are the\ncorresponding control points.\n\nIGA, quad.\nSRI\nLBM\nGLBM\nIGA, cubic\n\nref\n|uD − uref\nD |/|uD |\n\n10−1\n\n10−2\n\n10−3\n\n2\nrank(KB̄ ) − rank(KIGA )\n\n100\n\nLB\nGLB\n\n1\n0\n−1\n−2\n\n10−4 0\n10\n\n101\nNumber of elements per side\n\n102\n\n1 2\n\n(a) Convergence of of uD\n\n4\n8\n16\nNumber of elements per side\n\n(b) Rank of stiffness matrix\n\nFigure 16: Results of uD of the pinched hemisphere 15.\n\n26\n\n\f9.45\n\n·10−2\n\n9.4\n\nuD\n\n9.35\n9.3\n9.25\n9.2 0\n10\n\nIGA, quad.\nSRI\nLB\nGLB\nIGA, cubic\nuref\nD\n101\nNumber of elements per side\n\n102\n\nFigure 17: Partial enlarged of results uD , converged results using various methods all mismatch the reference value.\n\n(a) 4 × 4 IGA\n\n(b) 8 × 8 IGA\n\n(c) 4 × 4 GLB\n\n(d) 8 × 8 GLB\n\nFigure 18: Field of u × 102 of the pinched hemisphere 15 by IGA of order 2 (a,b), and by the generalized local B̄\nmethod (c,d). Severe locking is noticed by pure IGA, while elements deform more easily for GLB.\n\n27\n\n\f5. Conclusion remarks\nThe local B̄ method is adopted to unlock the degenerated Reissner-Mindlin plate and shell\nelements within the framework of isogeometric analysis. The plate/shell mid-surface and the\nunknown field is described with non-uniform rational B-splines. The proposed method uses multiple\nsets of lower order B-spline basis functions as projection bases, by which the locking strains are\nmodified in the sense of L2 projection, in this way field-consistent strains are obtained.\nThe salient features of the proposed local B̄ method are: (a) has less computational effort than\nclassical B̄; (b) yields better accuracy than classical IGA especially in cases of coarse meshes and\nmesh distortions; (c) suppresses both shear and membrane locking commonly encountered when\nlower order elements are employed and suitable for both thick and thin models;\nFuture work includes extending the approach to large deformations, large deflections and large\nrotations as well as investigating the behaviour of the stabilization technique for enriched approximations such as those encountered in partition of unity methods [48–50].\n\nAcknowledgments\nQ. Hu is funded by China Scholarship Council and National Natural Science Foundation of\nChina (No. 11272075). Y. Xia is funded by National Natural Science Foundation of China\n(No.61572021, 11272075). Stéphane Bordas thanks partial funding for his time provided by the\nEuropean Research Council Starting Independent Research Grant (ERC Stg grant agreement No.\n279578) ”RealTCut Towards real time multiscale simulation of cutting in non-linear materials\nwith applications to surgical simulation and computer guided surgery”. We also thank the funding\nfrom the Luxembourg National Research Fund (INTER/MOBILITY/14/8813215/CBM/Bordas\nand INTER/FWO/15/10318764). Q. Hu is thankful for Prof. Gengdong Cheng for the valuable\nsuggestions of this research subject.\n\n28\n\n\fReferences\nReferences\n[1] T. J. Hughes, J. A. Cottrell, Y. Bazilevs, Isogeometric analysis: CAD, finite elements, NURBS, exact geometry\nand mesh refinement, Computer methods in applied mechanics and engineering 194 (39) (2005) 4135–4195.\n[2] P. Kagan, A. Fischer, P. Z. Bar-Yoseph, New B-spline finite element approach for geometrical design and\nmechanical analysis, International Journal for Numerical Methods in Engineering 41 (3) (1998) 435–458.\n[3] S. Lipton, J. A. Evans, Y. Bazilevs, T. Elguedj, T. J. Hughes, Robustness of isogeometric structural discretizations under severe mesh distortion, Computer Methods in Applied Mechanics and Engineering 199 (5) (2010)\n357–373.\n[4] B. Marussig, J. Zechner, G. Beer, T.-P. Fries, Fast isogeometric boundary element method based on independent\nfield approximation, Computer Methods in Applied Mechanics and Engineering 284 (2015) 458–488.\n[5] G. Xu, E. Atroshchenko, S. Bordas, Geometry-independent field approximation for spline-based finite element\nmethods, in: Proceedings of the 11th World Congress in Computational Mechanics, 2014.\n[6] G. Xu, E. Atroshchenko, W. Ma, S. Bordas, Geometry-Independent Field approximaTion: CAD-analysis integration, geometrical exactness and adaptivity, Computer Methods in Applied Mechanics & Engineering.\n[7] E. Atroshchenko, G. Xu, S. Tomar, S. Bordas, Weakening the tight coupling between geometry and simulation\nin isogeometric analysis: from sub-and super-geometric analysis to Geometry Independent Field approximaTion\n(GIFT), arXiv preprint arXiv:1706.06371.\n[8] D. Toshniwal, H. Speleers, T. J. Hughes, Smooth cubic spline spaces on unstructured quadrilateral meshes\nwith particular emphasis on extraordinary points: Geometric design and isogeometric analysis considerations,\nComputer Methods in Applied Mechanics and Engineering.\n[9] J. Kiendl, K.-U. Bletzinger, J. Linhard, R. Wüchner, Isogeometric shell analysis with Kirchhoff–Love elements,\nComputer Methods in Applied Mechanics and Engineering 198 (49) (2009) 3902–3914.\n[10] D. Benson, Y. Bazilevs, M.-C. Hsu, T. Hughes, A large deformation, rotation-free, isogeometric shell, Computer\nMethods in Applied Mechanics and Engineering 200 (13) (2011) 1367–1378.\n[11] D. Benson, Y. Bazilevs, M.-C. Hsu, T. Hughes, Isogeometric shell analysis: the Reissner–Mindlin shell, Computer\nMethods in Applied Mechanics and Engineering 199 (5) (2010) 276–289.\n[12] D. Benson, S. Hartmann, Y. Bazilevs, M.-C. Hsu, T. Hughes, Blended isogeometric shells, Computer Methods\nin Applied Mechanics and Engineering 255 (2013) 133–146.\n[13] W. Dornisch, S. Klinkel, B. Simeon, Isogeometric Reissner–Mindlin shell analysis with exactly calculated director\nvectors, Computer Methods in Applied Mechanics and Engineering 253 (2013) 491–504.\n[14] S. Hosseini, J. J. Remmers, C. V. Verhoosel, R. Borst, An isogeometric solid-like shell element for nonlinear\nanalysis, International Journal for Numerical Methods in Engineering 95 (3) (2013) 238–256.\n\n29\n\n\f[15] J. A. Cottrell, A. Reali, Y. Bazilevs, T. J. Hughes, Isogeometric analysis of structural vibrations, Computer\nmethods in applied mechanics and engineering 195 (41) (2006) 5257–5296.\n[16] Q. Hu, F. Chouly, G. Cheng, S. P. A. Bordas, Skew-symmetric nitsche’s formulation in isogeometric analysis:\nDirichlet and symmetry conditions, patch coupling and frictionless contact, arXiv preprint arXiv:1711.10253v2.\n[17] J. Kiendl, Y. Bazilevs, M.-C. Hsu, R. Wüchner, K.-U. Bletzinger, The bending strip method for isogeometric analysis of Kirchhoff–Love shell structures comprised of multiple patches, Computer Methods in Applied\nMechanics and Engineering 199 (37) (2010) 2403–2416.\n[18] R. Echter, M. Bischoff, Numerical efficiency, locking and unlocking of NURBS finite elements, Computer Methods in Applied Mechanics and Engineering 199 (5) (2010) 374–382.\n[19] T. J. Hughes, M. Cohen, M. Haroun, Reduced and selective integration techniques in the finite element analysis\nof plates, Nuclear Engineering and Design 46 (1) (1978) 203–222.\n[20] C. Adam, S. Bouabdallah, M. Zarroug, H. Maitournam, Improved numerical integration for locking treatment\nin isogeometric structural elements, Part I: Beams, Computer Methods in Applied Mechanics and Engineering\n279 (2014) 1 – 28.\n[21] C. Adam, S. Bouabdallah, M. Zarroug, H. Maitournam, Improved numerical integration for locking treatment\nin isogeometric structural elements. Part II: Plates and shells, Computer Methods in Applied Mechanics and\nEngineering 284 (2015) 106–137.\n[22] C. Adam, S. Bouabdallah, M. Zarroug, H. Maitournam, A Reduced Integration for Reissner-Mindlin Non-linear\nShell Analysis Using T-Splines, in: Isogeometric Analysis and Applications 2014, Springer, 2015, pp. 103–125.\n[23] T. Elguedj, Y. Bazilevs, V. M. Calo, T. J. R. Hughes, B over-bar and F over-bar projection methods for\nnearly incompressible linear and non-linear elasticity and plasticity using higher-order nurbs elements, Computer\nMethods in Applied Mechanics and Engineering 197 (33-40) (2008) 2732–2762.\n[24] R. Bouclier, T. Elguedj, A. Combescure, Locking free isogeometric formulations of curved thick beams, Computer\nMethods in Applied Mechanics and Engineering 245 (2012) 144–162.\n[25] R. Bouclier, T. Elguedj, A. Combescure, On the development of NURBS-based isogeometric solid shell elements:\n2D problems and preliminary extension to 3D, Computational Mechanics 52 (5) (2013) 1085–1112.\n[26] R. Bouclier, T. Elguedj, A. Combescure, Efficient isogeometric NURBS-based solid-shell elements: Mixed formulation and B-bar method, Computer Methods in Applied Mechanics and Engineering 267 (2013) 86–110.\n[27] R. Bouclier, T. Elguedj, A. Combescure, An isogeometric locking-free nurbs-based solid-shell element for geometrically nonlinear analysis, International Journal for Numerical Methods in Engineering 101 (10) (2015)\n774–808.\n[28] R. Echter, B. Oesterle, M. Bischoff, A hierarchic family of isogeometric shell finite elements, Computer Methods\nin Applied Mechanics and Engineering 254 (2013) 170–180.\n[29] K.-U. Bletzinger, M. Bischoff, E. Ramm, A unified approach for shear-locking-free triangular and rectangular\nshell finite elements, Computers & Structures 75 (3) (2000) 321–334.\n\n30\n\n\f[30] F. Brezzi, J. Evans, T. Hughes, L. Marini, New quadrilateral plate elements based on Twist-Kirchhoff theory,\nComput Methods Appl Mech Eng (submitted).\n[31] F. Brezzi, L. D. Marini, Virtual element methods for plate bending problems, Computer Methods in Applied\nMechanics and Engineering 253 (2013) 455–462.\n[32] L. B. da Veiga, C. Lovadina, A. Reali, Avoiding shear locking for the Timoshenko beam problem via isogeometric\ncollocation methods, Computer Methods in Applied Mechanics and Engineering 241 (2012) 38–51.\n[33] F. Auricchio, L. B. da Veiga, J. Kiendl, C. Lovadina, A. Reali, Locking-free isogeometric collocation methods\nfor spatial Timoshenko rods, Computer Methods in Applied Mechanics and Engineering 263 (2013) 113–126.\n[34] S. Yin, J. S. Hale, T. Yu, T. Q. Bui, S. P. Bordas, Isogeometric locking-free plate element: a simple first order\nshear deformation theory for functionally graded plates, Composite Structures 118 (2014) 121–138.\n[35] J. Kiendl, F. Auricchio, T. Hughes, A. Reali, Single-variable formulations and isogeometric discretizations for\nshear deformable beams, Computer Methods in Applied Mechanics and Engineering 284 (2015) 988–1004.\n[36] L. Beirão Da Veiga, T. Hughes, J. Kiendl, C. Lovadina, J. Niiranen, A. Reali, H. Speleers, A locking-free model\nfor Reissner–Mindlin plates: Analysis and isogeometric implementation via NURBS and triangular NURPS,\nMathematical Models and Methods in Applied Sciences 25 (08) (2015) 1519–1551.\n[37] P. Hu, Q. Hu, Y. Xia, Order reduction method for locking free isogeometric analysis of Timoshenko beams,\nComputer Methods in Applied Mechanics and Engineering 308 (2016) 1–22.\n[38] T. J. Mitchell, S. Govindjee, R. L. Taylor, A method for enforcement of Dirichlet boundary conditions in isogeometric analysis, in: Recent Developments and Innovative Applications in Computational Mechanics, Springer,\n2011, pp. 283–293.\n[39] S. Govindjee, J. Strain, T. J. Mitchell, R. L. Taylor, Convergence of an efficient local least-squares fitting method\nfor bases with compact support, Computer Methods in Applied Mechanics and Engineering 213 (2012) 84–92.\n[40] V. P. Nguyen, C. Anitescu, S. P. Bordas, T. Rabczuk, Isogeometric analysis: an overview and computer implementation aspects, Mathematics and Computers in Simulation 117 (2015) 89–116.\n[41] F. Koschnick, M. Bischoff, N. Camprubı́, K.-U. Bletzinger, The discrete strain gap method and membrane\nlocking, Computer Methods in Applied Mechanics and Engineering 194 (21) (2005) 2444–2463.\n[42] J. Simo, T. Hughes, On the variational foundations of assumed strain methods, Journal of Applied Mechanics\n53 (1) (1986) 51–54.\n[43] B.-Z. Huang, V. B. Shenoy, S. Atluri, A quasi-conforming triangular laminated composite shell element based\non a refined first-order theory, Computational mechanics 13 (4) (1994) 295–314.\n[44] T. J. R. Hughes, Generalization of selective integration procedures to anisotropic and nonlinear media, Internat.\nJ. Numer. Methods Engrg. 15 (1980) 1413–1418.\n[45] P. Antolin, A. Bressan, A. Buffa, G. Sangalli, An isogeometric method for linear nearly-incompressible elasticity\nwith local stress projection, Computer Methods in Applied Mechanics and Engineering.\n[46] B. Juettler, U. Langer, A. Mantzaflaris, S. Moore, W. Zulehner, Geometry + simulation modules: Implementing\n\n31\n\n\fisogeometric analysis, Proc. Appl. Math. Mech. 14 (1) (2014) 961–962.\n[47] R. H. Macneal, R. L. Harder, A proposed standard set of problems to test finite element accuracy, Finite\nelements in analysis and design 1 (1) (1985) 3–20.\n[48] S. P. Bordas, T. Rabczuk, N.-X. Hung, V. P. Nguyen, S. Natarajan, T. Bog, N. V. Hiep, et al., Strain smoothing\nin FEM and XFEM, Computers & structures 88 (23) (2010) 1419–1443.\n[49] L. Chen, T. Rabczuk, S. P. A. Bordas, G. Liu, K. Zeng, P. Kerfriden, Extended finite element method with edgebased strain smoothing (ESm-XFEM) for linear elastic crack growth, Computer Methods in Applied Mechanics\nand Engineering 209 (2012) 250–265.\n[50] M. Surendran, S. Natarajan, S. Bordas, G. Palani, Linear smoothed extended finite element method, arXiv\npreprint arXiv:1701.03997.\n\n32\n\n\f"
        ],
        [
         "7",
         "7",
         "cs.CE",
         "Computational Engineering",
         "1203.6728v1.pdf",
         "SYSTEM IDENTIFICATION FOR INDOOR CLIMATE CONTROL\nA.W.M. (Jos) van Schijndel, P.W.M.H. (Paul) Steskens,\nEindhoven University of Technology\nEindhoven\nABSTRACT\nThe study focuses on the applicability of system identification to identify building and system\ndynamics for climate control design. The main problem regarding the simulation of the dynamic\nresponse of a building using building simulation software is that (1) the simulation of a large\ncomplex building is time consuming, and (2) simulation results often lack information regarding\nfast dynamic behaviour (in the order of seconds), since most software uses a discrete time step,\nusually fixed to one hour. The first objective is to study the applicability of system identification\nto reduce computing time for the simulation of large complex buildings. The second objective is\nto research the applicability of system identification to identify building dynamics based on\ndiscrete time data (one hour) for climate control design. The study illustrates that system\nidentification is applicable for the identification of building dynamics with a frequency that is\nsmaller as the maximum sample frequency as used for identification. The research shows that\nsystem identification offers good perspectives for the modelling of heat, air and moisture\nprocesses in a building. The main advantages of system identification models compared to the\nmodelling of building dynamics using building simulation software are, that (1) the computing\ntime is reduced significantly, and (2) system identification models run in a MATLAB\nenvironment, in which many building simulation tools have been developed.\n\n1. INTRODUCTION\nIt is widely accepted that the modelling and simulation of the dynamic response of a\nbuilding contributes to improve user comfort, to reduce energy consumption and to improve\nHeating, Ventilation and Air Conditioning (HVAC) performance in buildings. For the past 50\nyears, a wide variety of building energy simulation programs have been developed, enhanced\nand are in use throughout the building energy community (Crawly et al.(2005)).\nCurrently, two broad but distinct approaches to modelling the dynamic response of a\nbuilding are common: a forward (classical) approach and a data-driven (inverse) approach. The\nobjective of the forward approach is to predict output variables of a specified model with known\nstructure and known parameters when subject to specified input variables. To ensure accuracy,\nmodels have tended to become increasingly complex. This approach presumes detailed\nknowledge not only of the various natural phenomena affecting system behaviour, but also of the\nmagnitude of various interactions. The main advantage of this approach is that the system need\n1|Page\n\n\fnot be physically built to predict its behaviour. This approach is ideal to preliminary design and\nanalysis stage (ASHRAE (2005)).\nAn alternative approach is a data-driven modelling approach. The objective of the datadriven approach is to determine a mathematical description of the system and to estimate system\nparameters in a situation, when input and output variables are known and measured. In contrast\nto the forward approach, the data-driven approach is relevant when the system has already been\nbuilt and actual performance data are available for model development and identification. Datadriven modelling often allows identification of system models that are not only simpler to use\nbut also are more accurate predictors of future system performance than forward models. In\nliterature, this data-driven modelling approach is also known as system identification (SI)\n(ASHRAE (2005)).\nSeveral studies show that system identification is a useful tool for building energy\nsimulation (BES) and the analysis of heat, air and moisture (HAM) processes in buildings.\nLowry and Lee (2004), for example, describes the application of the data-driven modelling\napproach to the thermal response of a conventional office space using data collected from an\nexisting building management system. A similar system identification technique for estimation\nof the heat dynamics of buildings, based on discrete time building performance data is described\nby Madsen and Holst (1995). Moreover, Cunningham (2001) used system identification\ntechniques to infer room or building ventilation and moisture release rates from psychometric\ndata. Mechaqrane and Zouak (2004) describe the use of system identification to predict the\nindoor air temperature of a residential building. As an alternative to the use of test facilities,\nsome researchers have compared their models with theoretical predictions from standard\ncomputer simulation software such as TRNSYS (Pape et. al (1991)). Pape et. al (1991) identify\nbuilding dynamics based on discrete time results from building simulation software with a time\nstep of one hour. Fast dynamic information regarding system dynamics within an hour is not\navailable.\nThe main problems regarding the simulation of the dynamic response of a building using\nbuilding simulation software are that:\no The simulation of a large complex building using standard building simulation\nsoftware is time consuming.\no The simulation results often lack information regarding fast dynamic behaviour\n(in the order of seconds) of the building, since most software uses a discrete time\nstep. This time step is usually fixed to one hour. However, information about fast\ndynamic behaviour and transient responses, which often lie within this time step,\nmay be essential for application of an appropriate control strategy such as on/off\ncontrol.\nThe paper has two objectives. The first objective is to study the applicability of system\nidentification to reduce computing time for the simulation of large complex buildings. The\nsecond objective is to research the applicability of system identification to identify building\ndynamics based on discrete time data (one hour) for climate control design. The study provides\nan answer to the central questions:\no Is system identification useful to reduce computing time of building energy simulations?\no Can system identification be applied for the identification of building dynamics and\nclimate control design?\n\n2|Page\n\n\fFurthermore, an evaluation of the advantages, disadvantages and limitations of system\nidentification considering climate control is discussed, focusing on accuracy and computing\ntime.\nThe research methodology consists of five stages. First, a model identification toolbox is\nselected for model identification of building dynamics. Second, the quality of the results\nproduced by this toolbox is evaluated based on the results described in the paper “Modelling the\npassive thermal response of a building using sparse BMS data” (Lowry and Lee (2004)). Third,\nthe applicability of system identification for internal temperature control is researched based on\nfive case studies. Table 1 presents the input and output data that have been used for the\nidentification and application of the SI models:\nI. Identification of a building model based on discrete time data, containing free-floating\nindoor air temperature and application in a situation with free-floating indoor air\ntemperature (continuous time).\nII. Identification of a building model based on discrete time data, containing free-floating\nindoor air temperature and application in a situation with indoor air temperature control\n(continuous and discrete time).\nIII. Identification of a building model based on discrete time data, containing on/off-controlled\nindoor air temperature and application in a situation with similar set points for heating and\ncooling (discrete time).\nIV. Identification of a building model based on discrete time data, containing on/off-controlled\nindoor air temperature and application in a situation with similar set points for heating and\ncooling (continuous time).\nV. Identification of a building model based on discrete time data, containing on/off-controlled\nindoor air temperature and application in a situation with different set points for heating\nand cooling (discrete time).\nMoreover, case study III, IV and V involve a parameter study considering the sensitivity of the\nset points for heating and cooling with respect to the quality of the developed SI models. Fourth,\nmodel identification based on similar simulations of Heat, Air and Moisture (HAM) processes in\nHAMBase (De Wit (2004)) has been evaluated. Lastly, external data, retrieved from a building\nperformance simulation in ESP-r (Data Model Summary ESP-r Version 9 Series (2001)), has\nbeen used to identify building dynamics.\n\n3|Page\n\n\fTable 1: Case Studies\nCase\nIdentification\nApplication\nStudy\nDiscrete free-floating indoor air Continuous free-floating indoor air\nI\ntemperature\ntemperature\nTi [oC]\n\nTi [oC]\n\nt [s]\n\nt [s]\nII\n\nDiscrete free-floating indoor air Continuous and discrete controlled\ntemperature\nindoor air temperature\nTi [oC]\nTi [oC]\n\nTc\nTh\nt [s]\n\nt [s]\nIII\n\nOn/off controlled discrete indoor On/off controlled discrete indoor air\nair temperature\ntemperature\no\nTi [ C]\nTi [oC]\nTc,ID\nTc=Tc,ID\nTh,ID\nt [s]\n\nTh=Th,ID\nt [s]\n\n4|Page\n\n\fIV\n\nOn/off controlled discrete indoor On/off controlled continuous indoor\nair temperature\nair temperature\no\nTi [oC]\nTi [ C]\nTc,ID\n\nTc\n\nTh,ID\n\nTh\nt [s]\n\nt [s]\n\nV\n\nOn/off controlled discrete indoor On/off controlled discrete indoor air\nair temperature\ntemperature\nTi [oC]\n\nTi [oC]\n\nTc,ID\n\nTc\nTh\n\nTh,ID\nt [s]\n\nt [s]\n\nThe paper is organized as follows: Section 2 describes the verification of the system\nidentification tool, which has been used for model identification. In section 3, the application of\nsystem identification based on a HAMBase (De Wit (2004)) building energy simulation is\nreported. Section 4 reports the application and perspectives of system identification regarding the\nmodelling of HAM transport in buildings. In Section 5 the application of system identification\nbased on an ESP-r building performance simulation is described. Section 6 gives the conclusions\nand limitations regarding system identification related to the identification of building dynamics.\n\n5|Page\n\n\f2. LITERATURE BASED VERIFICATION\nLowry and Lee (2004) describe the application of system identification to the thermal\nresponse of a conventional office using data collected from an existing building management\nsystem (BMS). The input and output data are used for estimation of the system identification\nmodel. Data were collected using BMS at 15-min intervals over a three week period, therefore\ngiving more than 2000 measurements. Lowry and Lee (2004) estimate the parameters of an\nOutput-Error model (OE211) represented by the Equation (1):\n\nTi =\n\na.z -1 - b.z -2\nd.z -1 − ez -2\nT\n−\np +ε\n0\nc.z -1\nf.z -1\n\n(1)\n\nThe results from the study described by Lowry and Lee (2004) have been reproduced in\nthis research focusing on the verification of the system identification tools. The MATLAB\nSystem Identification Toolbox (Ljung (1997)) has been used for model identification. Table 2\nshows a comparison of the model coefficients documented in Lowry and Lee (2004) and the\ncoefficients resulting from the present study. Table 1 shows that both models agree well with\neach other. Moreover, the models are nearly identical.\nIt is concluded that the MATLAB System Identification Toolbox is an appropriate tool\nfor model estimation. The research proceeds with the identification of building dynamics based\non a building energy simulation.\nTable 2: Comparison of model coefficients\nCoefficients\na\nb\nc\nLowry and\n0.2361\n-0.2334\n1-0.9961\nLee (2004)\nPresent study\n0.2346\n-0.2319\n1-0.9960\n\nd\n-0.03350\n\ne\n0.03370\n\nf\n1-0.9377\n\n-0.03353\n\n0.03364\n\n1-0.9374\n\n6|Page\n\n\f3. BUILDING ENERGY SIMULATION MODEL\nIn this section, the applicability of system identification for indoor air temperature control\nis researched based on five case studies (Table 1). System identification models have been\ndeveloped following the methodology that is presented in Figure 1. Three steps have been\nillustrated: Identification, Simulation and Comparison.\nI. Identification\n\nIII. Comparison\n\nResults from measurements or BS\n(HAMBase, ESP-r, etc.)\nDiscrete time data\nFirst half\n(1974)\n\nHAMBase\nReference model\n\nClimate data (2000)\n\nSecond half\n(1974)\nSimulation\nwith climate control\n\nInput signal examination\n\nModel Identification\nSecond half\n(2000)\n\nSimulation\n\nModel Validation\n(µε,Ι σε,Ι)\nII. Simulation\nClimate data (2000)\nSimulation (with climate control)\nSecond half\n(2000)\n\nComparison\n(µε,ΙΙ σε,ΙΙ)\n\nFigure 1: Modelling methodology\nThe first step is the Identification of building dynamics using discrete time data. Discrete\ntime data series are collected from measurements or from building simulation results. The first\n7|Page\n\n\fhalf of these data series is used for parameter estimation and the second half reserved for model\nvalidation. Before the data is used for identification of building dynamics, the input signal is\nexamined.\nFirst of all, the sampling and Nyquist frequency Brigham (2004) of the signal are\nanalyzed. The reader should notice that, when sampling a signal, the sampling frequency must be\ngreater than twice the bandwidth of the input signal in order to be able to reconstruct the original\nperfectly from the sampled version (Nyquist-Shannon theorem Brigham (2004)).\nMathematically, the theorem is formulated as a statement about the Fourier transformation. If a\nfunction s(x) has a Fourier transform F[s(x)] = S(f) = 0 for |f| ≥ W then it is completely\ndetermined by giving the value of the function at a series of points spaced 1/(2W) apart. The\nvalues sn = s(n/(2W)) are called the samples of s(x). The minimum sample frequency that allows\nreconstruction of the original signal, that is 2W samples per unit distance, is known as the\nNyquist frequency. With respect to the Nyquist-Shannon theorem, a system identification model\nis able to capture information regarding building dynamics perfectly for all frequencies with a\nsmaller frequency than the frequency of the data used for identification (Sample frequency). For\nhigher frequencies the identified model may cause deviations.\nSecond, the input signal should deliver as much input power into the system as possible.\nThe amount of input power, present in the input signal, is defined as the Crest factor Cf, Equation\n(2) (Girod et al. (2001)). The smaller the Crest factor, the better the signal excitation resulting in\nlarger total energy delivery and enhanced signal-to-noise ratio. The theoretical lower bound for\nthe Crest factor is 1.\nCf =\n\nmax(u (t ))\nu2\n\n(2)\n\nAfter examination of the input signal, the MATLAB System Identification Toolbox has\nbeen used to estimate a state-space model describing building dynamics. The model is validated\nby comparing the simulation results and the second half of the original discrete time data. The\nsecond step is a Simulation. The SI model is simulated in MATLAB based on climate data of the\nyear 2000. The third step is the Comparison of the results. A reference model of the building is\nsimulated in HAMBase. Both the results from the SI model simulation and the results from the\nHAMBase reference model are compared.\n\n8|Page\n\n\f3.1. Free-floating based model identification and free-floating application\nThis section describes the identification of building dynamics based on a discrete time\nsimulation results with free-floating indoor air temperature. The SI model is applied and\nsimulated in a situation with free-floating indoor air temperature (discrete time). First, an SI\nmodel has been identified in accordance with the Identification step (Figure 1). Second, the\nSimulation step has been applied (Figure 1). The SI model has been simulated in MATLAB with\nfree-floating indoor air temperature. Third, the results have been compared with the results from\nthe reference HAMBase/Matlab reference model in accordance with the Comparison step\n(Figure 1).\n(I.) The residential reference building consisting of four rooms has been simulated in\nHAMBase/Matlab using climate data of the years 1974 and 2000. A simulation time step of one\nhour is used, giving 8750 values. The temperature in the building is free-floating. The first half\nof the climate data of the year 1974 has been used for model identification. The second half has\nbeen used for model validation. Both model input variables, outside air temperature and solar\ngains, and output variables, inside air temperature, are presented in Figure 2. Figure 2 illustrates\nthe input and output variables of one room. (The reader should notice that the solar gains are\ndefined as the heat gain supplied to the room by the solar radiance). Before identification of\nbuilding dynamics, the input signal is examined. The sample frequency of the data is 2.8.10-4 Hz.\nBased on the Nyquist-Shannon theorem, the model is expected to represent building dynamics\nfor frequencies smaller than 2.8.10-4 Hz. The amount of input power into the system is\ndetermined by calculation of the Crest factors Cf,T0, Cf,Qsolar, and Cf,Ti (Equation (2)). Regarding\nthe outside air temperature and the solar gains the Crest factors are respectively 3.09 (Cf,T0) and\n5.47 (Cf,Qsolar). Considering the inside air temperature a Crest factor of 2.61 (Cf,Ti) has been\ncalculated. After examination of the input signal, the MATLAB System Identification Toolbox\nhas been used to develop a state-space model describing free-floating indoor air temperature.\nFigure 3 shows a representation of the state-space model, which describes the indoor air\ntemperature in the building.\n\n9|Page\n\n\fFigure 2: Input variables, outside air temperature and solar gains in one room, and output\nvariable, inside air temperature in one room, derived from a discrete time simulation in\nHAMBase/Matlab. (Cf,T0 = 3.09, Cf,Qsolar = 5.47, and Cf,Ti = 2.61 (Equation (2))).\n\nOutdoor Air\nTemperature\n(To)\nSolar Gains\n(Qsolar)\n\nIndoor Air\nTemperature\n(Ti)\nx’ = A.x+B.u\ny = C.x+D.u\n\nFigure 3: Continuous time state-space model of the building dynamics describing the indoor air\ntemperature in the building. Input variables are the outdoor air temperature and the solar heat\ngains.\nSeveral system identification models of different mathematical order have been\ndeveloped. A way of obtaining insight into the quality of a model is to simulate the model with\nthe input from a fresh data set and compare the simulated output with the measured one. This\ngives a feel for which properties of the system have been picked up by the model, and which\nhave not. Table 2 presents a comparison of the models based on the second half of the original\ndiscrete time results from HAMBase/Matlab and a simulation of the SI model (discrete time).\n10 | P a g e\n\n\fThe mean (µε,Ι) and the standard deviation (σε,Ι) of the error have been presented for several\nmodels of different mathematical order. The research showed that the mean error (µε,Ι) is\nrepresentative for the accuracy of the SI model. Considering the accuracy of the identified\nmodels, the standard deviation showed to be of minor importance. Furthermore, Table 3 shows\nthat the accuracy of the model increases with increasing mathematical model order.\nTable 3: Comparison of SI models\nModel order\nµε,Ι [oC]\n1\n0.9891\n2\n0.6215\n3\n0.7299\n4\n0.2023\n8\n0.0488\n18\n0.0073\n40\n0.0133\n\nσε,Ι [oC]\n1.9983\n1.1735\n0.6956\n0.3157\n0.2371\n0.2356\n0.2674\n\n(II, III.) The research proceeds with a simulation of the reference building in\nHAMBase/Simulink with free-floating indoor air temperature. A comparison between the results\nof the SI model simulation (continuous time) and the HAMBase/Simulink simulation with freefloating indoor air temperature (continuous time) is presented in Figure 4. Considering the error\nbetween the system identification model and the HAMBase/Simulink model, the figure shows an\naverage error (µε,ΙΙ) of 0.1868oC. Furthermore, a frequency analysis showed that dynamics with a\nmaximum frequency of approximately 2.10-3 Hz are present in the SI model. Information\nregarding the fast dynamic behaviour within one second is not captured by the model. The\nobservation that the frequency of the identified system dynamics is limited to the sample\nfrequency of the signal, used for identification of the model, is confirmed by the NyquistShannon theorem Brigham (2004). The simulation results show that model identification is\nuseful for identification and prediction of free-floating building dynamics for frequencies smaller\nthan the sample frequency.\nIn conclusion, the research shows that system identification is useful for the identification\nof free-floating building dynamics and application of the SI model in the same free-floating\nconfiguration. The computing time used for the simulation of the SI model in MATLAB is\nconsiderably small compared to the computing time needed for the simulation of the\nHAMBase/Simulink model. The computing time is reduced from several minutes to several\nseconds. The research proceeds with the identification free-floating building dynamics and the\napplication of the identified model with indoor air temperature control.\n\n11 | P a g e\n\n\fFigure 4: Comparison between the indoor air temperatures predicted by the SI model and the\nHAMBase/Simulink model.\n\n12 | P a g e\n\n\f3.2. Free-floating based model identification and climate control application\nThis section describes the identification of a building model based on discrete time data,\ncontaining free-floating indoor air temperature and application in a situation with indoor air\ntemperature control (continuous time). The SI model, which has been identified based on a\nsimulation with free-floating indoor air temperature (Section 3.1), has been simulated with\nindoor air temperature control in MATLAB/Simulink. (II.). The indoor temperature is controlled\nbased on a (continuous) time step, which lies within the time step that has been used for\nidentification of the SI model. Therefore, fast dynamic system behaviour must be present in the\nSI model to predict the indoor air temperature accurately.\nA control strategy based on the SI model combined with on/off-control of the indoor air\ntemperature has been established. Figure 5 illustrates the structure of the building model. Input\nvariables of the SI model are the outside air temperature, solar gains (Figure 3) and the heating\nand cooling power supplied to the room by the HVAC installation. The heat supplied by the\nHVAC installation has been added to the solar heat gain and the sum of both is used as a new\ninput for the model. Output variables are the inside air temperature in the building. The HVAC\ninstallation is controlled by feedback control of the indoor air temperature.\nIndoor Air\nTemperature (Ti)\n\nOutdoor Air\nTemperature (To)\nSolar Gains (Qsolar)\nHVAC\ninstallation\n\n+\n\nx’ = A.x+B.u\ny = C.x+D.u\n\nHeating/cooling\npower (Qh/Qc)\n\nFigure 5: Structure of the building model. Input variables are the outside air temperature, solar\ngains and the heating and cooling power. The heat supplied by the HVAC installation has been\nadded to the solar heat gain and the sum of both is used as a new input for the model. Output\nvariables are the indoor air temperatures in the building.\n(III.) Both the SI building model (Figure 5) and the reference HAMBase/Simulink\nreference model have been simulated with set points for heating and cooling of 18oC and 22oC.\nThe temperature in the building is controlled 24 hours a day. The heating and cooling capacities\nin the room are respectively 1500 W and 1000 W. The results from both simulations have been\ncompared. Figure 6 shows the indoor air temperature in a room of the building. Considering the\nerror between the results from the SI model simulation and the HAMBase/Simulink results, the\nfigure shows an average error (µε,ΙΙ) of 0.1 oC and a standard deviation (σε,ΙΙ) of 1.76 oC.\nMoreover, the SI model estimates the yearly energy consumption for heating and cooling for\nroom 1 to be 3.98*1010 J compared to 3.85*1010 J predicted by the HAMBase/Simulink model.\nIn conclusion, a relatively large deviation between the building dynamics represented by the SI\nmodel and the HAMBase/Simulink building model is present.\n\n13 | P a g e\n\n\fFigure 6: Indoor air temperature in two rooms of the building predicted by the SI model and the\nHAMBase/Simulink model. Both models have been simulated with set points for heating and\ncooling of 18oC and 22oC\nThe research shows that system identification is not applicable for the design of a climate\ncontrol strategy based on discrete data containing (free-floating) inside air temperature, outside\nair temperature and solar gains. A frequency analysis shows that system dynamics with a\nmaximum frequency of approximately 2.10-3 Hz are present in the SI model. Information\nregarding the fast dynamic behaviour within ten minutes is not captured by the model. The\nobservation that the frequency of the identified system dynamics is limited to the sample\nfrequency of the signal, used for identification of the model, is confirmed by the NyquistShannon theorem (Brigham (2004)). However the average error (µε,ΙΙ) is acceptable not all\nproperties of the building have been picked up by the SI model, resulting in a relatively large\nfluctuation of the error. This difference in system behaviour may be caused by the fact that the\ntransfer from heating/cooling power to indoor air temperature is not defined in the model.\nMoreover, it is assumed that the transfer from heating/cooling power to indoor air temperature is\nsimilar to the transfer from the solar gains to the indoor air temperature. After all, this\nassumption seems invalid.\nOften, indoor air temperature in a building as well as outdoor air temperature and solar\ngains are measured after a building has been built. These data are indirectly used to decide what\ntype of HVAC installation is installed and to develop a proper indoor air temperature control\nstrategy. A system identification model of building dynamics is obtained based on data obtained\nfrom measurements in a free-floating situation. However, the reader should notice that, when\n14 | P a g e\n\n\fdiscrete time data, for example measurements, are used to develop an SI model of building\ndynamics, this model can only be applied for simulation (1) with a maximum sample frequency\nas used for identification and (2) with free-floating climate conditions . The model is not suitable\nfor simulation with climate control and control strategy design, since the model lacks (1)\nvaluable information for smaller frequencies than the sample frequency of the input signal used\nfor identification and (2) information regarding the transfer from heating/cooling power to indoor\nair temperature.\nThe research proceeds with the identification of building dynamics based on on/offcontrol, in order to include the transfer from heating/cooling power to indoor air temperature in\nthe SI model.\n\n15 | P a g e\n\n\f3.3 On/off-control based model identification (discrete time) and on/off-control based\nmodel simulation (discrete time)\nThis section describes the identification of a building model based on discrete time data,\ncontaining on/off-controlled indoor air temperature and application in a situation with a similar\ntype of indoor air temperature control (discrete time). A model based on an on/off-controlled\nsimulation in HAMBase/Matlab is developed. Instead of a free-floating indoor air temperature\n(Section 3.1), the temperature in the building is controlled by on/off-control of the HVAC\ninstallation with set points for heating and cooling of respectively 18oC and 22oC. To these set\npoints, which have been used in the HAMBase/Matlab simulation for identification of the model,\nis referred to as the identification set points. The identification of an SI model based on an\non/off-controlled simulation enables the identification of the transfer from heating/cooling power\nto indoor air temperature. The modelling approach is similar to the approach that is described in\nSection 3.1.\n(I.) A simulation time step of one hour is used based on climate data of the years 1974.\nInput variables of the model are the outside air temperature (Figure 2), the solar gains (Figure 2)\nand heating/cooling power (Figure 7). The output variables are the inside air temperature in the\nbuilding. Before identification of building dynamics, the input signal is examined. The sample\nfrequency of the data is 2.8.10-4 Hz. Based on the Nyquist-Shannon theorem, the model is\nexpected to represent building dynamics for frequencies smaller than 2.8.10-4 Hz. The amount of\ninput power into the system is determined by calculation of the Crest factors Cf,T0, Cf,Qsolar, Cf,Ti,\nand Cf,Qh/Qc (Equation (2)). Regarding the outside air temperature, the solar gains and the\nheating/cooling power the Crest factors are respectively 3.09 (Cf,T0), 5.47 (Cf,Qsolar), and 2.23\n(Cf,Qh/Qc). Considering the inside air temperature a Crest factor of 1.76 (Cf,Ti) and has been\ncalculated. After examination of the input signal, the MATLAB System Identification Toolbox is\nused to develop a discrete time state-space model describing indoor air temperature. A fourth\norder state-space model has been selected. Based on the results presented in Table 3, a relatively\naccurate model is expected with an average error (µε,Ι) of approximately 0.20.\n\n16 | P a g e\n\n\fFigure 7: Indoor air temperature and heating/cooling power, derived from a discrete time\nbuilding simulation in HAMBase/Matlab with indoor air temperature control. For simplicity,\nheating/cooling power and indoor air temperature of only one room are depicted. (Cf,Ti = 1.76\nand Cf,Qh/Qc = 2.23)\n(II.) A control strategy based on the fourth order SI model combined with on/off-control\nof the indoor air temperature has been established. Climate data of the year 2000 has been used\nfor simulation. Input variables of the SI model are the outside air temperature, solar gains and the\nheating and cooling power supplied to the room by the HVAC installation. Output variables are\nthe indoor air temperatures in the building. The model, which is presented in Figure 8, is\nimplemented in MATLAB and simulated with time steps of one hour.\n\nOutdoor Air\nTemperature (To)\nSolar Gains (Qsolar)\nHVAC\ninstallation\n\n+\n\nx(n+1) = A.x(n)+B.u(n)\ny(n) = C.x(n)+D.u(n)\n\nIndoor Air\nTemperature (Ti)\n\nHeating/cooling\npower (Qh/Qc)\n\nFigure 8: Discrete time SI model combined with on/off-control.\n(III.) The reference model of the building has been simulated in HAMBase/Matlab.\nFigure 9 shows that HAMBase/Matlab predicts the indoor air temperature to be between 18oC\nand 22oC. Comparing the results from the reference model with the results from the SI model\nsimulation, an average error (µε,ΙΙ) of 0.0533 and a standard deviation (σε,ΙΙ) of 0.3971 are\nobserved. Figure 9 shows that the results of both simulations are nearly identical. Comparing the\nSI model and the reference model, an analysis of dynamic system behaviour (Figure 9) and a\nfrequency analysis show that the dynamic properties of the building within the sample frequency\nof the input signal have been picked up by the SI model. The SI model contains no information\nfor larger frequencies than the sample frequency.\n\n17 | P a g e\n\n\fFigure 9: Comparison between indoor air temperatures predicted by the SI model and the\nHAMBase/Matlab reference model. Both simulations have been performed with set points for\nheating and cooling of 18oC and 22oC and a time step of one hour.\nIn conclusion, the research shows that system identification is useful for the\nidentification of building dynamics based on a simulation with on/off-controlled indoor air\ntemperature and application of the SI model in the same on/off-controlled configuration.\nMoreover, the application of the model is limited to a situation using a similar (discrete) time\nstep, a similar configuration of the building, and similar set points as has been used for model\nidentification. The study proceeds with the research of the applicability of the SI model for\ncontinuous time steps as well as for different set points for heating and cooling. The research of\nthe applicability for continuous time steps is reported in Section 3.4. Section 3.5 reports the\napplicability with respect to different set points for heating and cooling.\n\n18 | P a g e\n\n\f3.4 On/off-control based model identification (discrete time) and on/off-control based\nmodel simulation (continuous time)\nThis section describes the identification of a building model based on discrete time data,\ncontaining on/off-controlled indoor air temperature and application in a situation with a similar\ntype of indoor air temperature control (continuous time). A SI model based on an on/offcontrolled simulation in HAMBase/Matlab has been developed. The indoor air temperature in\nthe building is controlled by on/off-control of the HVAC installation with identification set\npoints for heating and cooling of respectively 18oC and 22oC. The modelling approach is similar\nto the approach that is described in Section 3.1.\n(I.) A simulation time step of one hour is used based on climate data of the years 1974\nand 2000. Input variables of the model are the outside air temperature (Figure 2), the solar gains\n(Figure 2) and heating/cooling power (Figure 7). The output variables are the inside air\ntemperature in the building (Figure 7). Before identification of building dynamics, the input\nsignal is examined. The sample frequency of the data is 2.8.10-4 Hz. Based on the NyquistShannon theorem, the model is expected to represent building dynamics for frequencies smaller\nthan 2.8.10-4 Hz. The amount of input power into the system is determined by calculation of the\nCrest factors Cf,T0, Cf,Qsolar, Cf,Ti, and Cf,Qh/Qc (Equation (2)). Regarding the outside air\ntemperature, the solar gains and the heating/cooling power the Crest factors are respectively 3.09\n(Cf,T0), 5.47 (Cf,Qsolar), and 2.23 (Cf,Qh/Qc). Considering the inside air temperature a Crest factor of\n1.76 (Cf,Ti) and has been calculated. After examination of the input signal, the MATLAB System\nIdentification Toolbox is used to develop a continuous time state-space model describing indoor\nair temperature. A fourth order state-space model has been selected. Based on the results\npresented in Table 2, a relatively accurate model is expected with an average error (µε,Ι) of\napproximately 0.20.\n(II.) A control strategy based on the fourth order SI model combined with on/off-control\nof the indoor air temperature has been established. Input variables of the SI model are the outside\nair temperature, solar gains and the heating and cooling power supplied to the room by the\nHVAC installation. Output variables are the indoor air temperatures in the building. The model,\nwhich is presented in Figure 10, is implemented in MATLAB/Simulink.\n\n19 | P a g e\n\n\fOutdoor Air\nTemperature (To)\nSolar Gains (Qsolar)\nHVAC\ninstallation\n\n+\n\nx’ = A.x+B.u\ny = C.x+D.u\n\nIndoor Air\nTemperature (Ti)\n\nHeating/cooling\npower (Qh/Qc)\n\nFigure 10: Continuous time SI model combined with on/off-control.\n(III.) The reference model of the building has been simulated in HAMBase/Simulink. Figure 11\nshows that HAMBase/Simulink predicts the indoor air temperature to be between 18oC and\n22oC. Comparing the results from HAMBase/Simulink with the results from the SI model\nsimulation, an average error (µε,ΙΙ) of 0.25 and a standard deviation (σε,ΙΙ) of 1.99 have been\nobserved. Focusing on the cooling set point, the SI model estimates the indoor air temperature to\nbe considerably higher compared to the temperature predicted by the HAMBase/Simulink model,\nalthough both models have been simulated with the same capacities of heating and cooling\npower. Fast dynamic properties have not been picked up by the SI model.\n\nFigure 11: Comparison between indoor air temperatures predicted by the SI model and\nthe HAMBase/Simulink model. Both simulations have been performed with set points for\nheating and cooling of 18oC and 22oC.\n20 | P a g e\n\n\fConsidering overall system dynamics both the results of the SI model and the\nHAMBase/Simulink reference model agree well with each other. The mean error of the results\n(µε,Ι and (µε,ΙΙ) is relatively small. Regarding fast building dynamics within the identification\ntime step of 1 hour, system dynamics has not been captured by the SI model. Figure 11 shows\nthat fast dynamic behaviour of the building within one hour (sample time) has not been picked\nup by the SI model. Information regarding system dynamics within the identification time step\nof one hour is not present in the SI model.\nThe research shows that an SI model that has been identified based on a discrete time step\nof one hour is not applicable for simulation in continuous time. Moreover, it is concluded that\nsystem identification is not suitable for the identification of fast building dynamics within the\ntime step that has been used for identification of the model. This observation is confirmed\ntheoretically by the Nyquist-Shannon theorem.\nThe research proceeds with the study of system identification with respect to the\nsimulation of an SI model with different set points for heating and cooling than have been used\nfor identification (identification set points).\n\n21 | P a g e\n\n\f3.5 On/off-control based model identification (on/off-control A) and on/off-control based\nmodel simulation (on/off-control B)\nThis section describes the identification of a building model based on discrete time data,\ncontaining on/off-controlled indoor air temperature and application in a situation with a different\ntype of indoor air temperature control (discrete time). The sensitivity of the identification set\npoints with respect to the transfer information between SI model output variable (indoor air\ntemperature) and controlled input variable (heating/cooling power) has been researched. First,\nbuilding energy simulations in HAMBase/Matlab have been performed using different\nidentification set points for heating and cooling, resulting in several discrete time data sets (I.).\nSecond, for every data set, the input signal Ti has been examined focusing on the amount of input\npower present in the input signal (Ti). Therefore, the corresponding Crest factor Cf,Ti (Equation\n(2)) has been calculated. Third, system identification models have been identified based on these\ndata sets. Lastly, the SI models have been simulated in MATLAB (II.) (discrete time) and the\nresults have been compared (III.) with the simulation results of the HAMBase/Matlab reference\nmodel. Both simulations have been performed with on/off-control between 18oC and 22oC.\nA comparison of the results of the SI models with both the results of a reference\nsimulation, using the original data set and specific identification set points (µε,Ι), and the results\nobtained from a reference simulation, using a new data set and set points for heating and cooling\nof 18oC and 22oC (µε,ΙΙ) have been presented in Table 4 For every SI model, Table 4 resents the\nidentification set points that have been used for identification, the Crest factor with respect to the\nindoor air temperature Ti, and the mean error of the results (µε,Ι) and (µε,ΙΙ).\n\n22 | P a g e\n\n\fTable 4: Comparison between on/off-controlled simulations\nModel\nOrder Identificati\nCrest\nMean\nMean\non set\nFactor\nerror I\nerror II\npoints\nCf,Ti\n(µε,Ι) [oC] (µε,ΙΙ) [oC]\nTh\nTc\nSI\n4\n10\n24\n1.9152\n0.0063\n0.1978\nSI\n8\n10\n24\n1.9152\n0.0067\n0.2992\nSI\n24\n10\n24\n1.9152\n0.0385\n0.3198\nSI\n4\n12\n24\n2.0702\n0.3430\n0.269\nSI\n8\n12\n24\n2.0702\n0.0138\n0.2266\nSI\n4\n16\n24\n2.4962\n0.0334\n0.09058\nSI\n8\n16\n24\n2.4962\n0.0089\n0.1073\nSI\n4\n16\n22\n2.1682\n0.0591\n0.1341\nSI\n8\n16\n22\n2.1682\n0.0144\n0.03871\nSI\n4\n18\n22\n2.3863\n0.0656\n0.1429\nSI\n8\n18\n22\n2.3863\n0.0371\n0.08719\nSI\n4\n20\n22\n2.5912\n0.0432\n0.2115\nSI\n8\n20\n22\n2.5912\n0.0102\n0.08155\nSI\n4\n21\n22\n6.9098\n0.0053\n1.693\nSI\n8\n21\n22\n6.9098\n0.0125\n6.048\nSI\n4\n20\n26\n3.6785\n0.0430\n0.07722\nSI\n8\n20\n26\n3.6785 6.1770e-4\n0.118\nSI\n4\n16\n26\n2.8896\n0.0218\n0.2362\nSI\n8\n16\n26\n2.8896\n0.0340\n0.0838\nSI\n4\n5\n26\n1.9365\n0.2367\n1.305\nSI\n8\n5\n26\n1.9365\n0.0085\n0.7084\n\nStd error\nII\n0.6231\n0.2986\n0.3168\n0.3362\n0.2658\n0.3061\n0.1455\n0.3858\n0.1535\n0.506\n0.1641\n0.573\n0.1529\n2.901\n3.775\n0.4262\n0.1209\n0.3097\n0.165\n2.072\n0.9323\n\nThe identified models have been analyzed focussing on overall system dynamics and on\nfast system dynamics. Considering overall system dynamics, models have been selected based on\nthe criterion that provided the mean error (µε,ΙΙ) between the results of the SI model simulation\nand the HAMBase/Matlab reference model is smaller than 0.12, relatively accurate results have\nbeen obtained by the SI model simulations. The models that fulfil the criterion have been bolded\nin Table 4. Moreover, Table 4 shows that the accuracy of a model is both dependent of the\nmathematical order and the identification set points that have been used for simulation. First of\nall, the influence of the mathematical order on model accuracy is considered. Considering the\nmean error (µε,Ι) between the SI model simulation results and the validation data, the accuracy of\nthe model increases with increasing mathematical order. The research showed that the accuracy\nof a model can be determined by examining mean error I (µε,Ι).\nSecond, the influence of the identification set points on model accuracy is considered.\nTable 4 shows that when the input signal contains insufficient input power to excite the system,\nthe SI model contains insufficient transfer information regarding input-output behaviour. This\nlacking information results in an inaccurate prediction of the indoor air temperature (µε,ΙΙ). For\nexample, the results of an SI model, which has been identified based on identification set points\n\n23 | P a g e\n\n\fof 21oC for heating and 22oC for cooling, are relatively inaccurate (µε,ΙΙ = 6.948). The relatively\nhigh Crest factors Cf,Ti show that the input signal contains little input power.\nIn Figure 12, the indoor air temperatures predicted by the SI model (based on\nidentification set points of 21oC and 22oC) and the HAMBase/Matlab reference model are\npresented. Both models have been simulated with set points for heating and cooling of 18oC and\n22oC. The SI model contains insufficient transfer information regarding input-output behaviour,\nresulting in an inaccurate prediction of the indoor air temperature. Moreover, a considerable\ndeviation between the dynamics represented by the SI model and the reference model is present.\nThe example shows that however an accurate model is expected (mean error I (µε,Ι) is relatively\nsmall), the model is inaccurate because the input power into the system is too low.\n\nFigure 12: Comparison between indoor air temperatures predicted by the SI model and the\nHAMBase/Matlab reference model. The SI model has been identified with identification set\npoints of 21oC and 22oC and simulated with set points of 18 oC and 22 oC.\nThe research shows that an SI model that has been identified based on identification set\npoints Th,ID and Tc,ID is not generally applicable for simulation with different set points for\nheating and cooling. Table 4 illustrates that for this specific type of building the identification set\npoint for heating is limited to a range between 16 oC and 20 oC as well as the identification set\npoint for cooling is limited to a range between 22 oC and 26 oC.\n\n24 | P a g e\n\n\fIn conclusion, system identification is useful for the prediction of overall building\ndynamics (dynamic behaviour of the building within the identification time step) and control of\noverall building dynamics. The accuracy of the derived SI model is dependent of both the choice\nof identification set points and the mathematical order of the derived model. In addition, the\nmain advantage is a reduction in computing time and the possibility of performing a simulation\nin a MATLAB environment, which makes model coupling to other building models and tools\npossible. However, system identification is not applicable for the identifying of fast building\ndynamics, which lie within the identification time step and can not be used for indoor air\ntemperature control within this time step.\nThese observations result in the following guidelines for the successful application of\nsystem identification models for the prediction of overall dynamics. First of all, before\nidentification of the model the input signal should be examined, focusing on the sample\nfrequency as well as the input power present in the input signals. The calculation of the Crest\nfactor (Equation (2)) is a good way of giving insight into the input power. Second, the SI model\nis identified and simulated. Third, the simulation results are compared with the original data set,\nbased on mean error I (µε,Ι). Provided the model contains sufficient transfer information\nregarding input-output behaviour (Cf < 4) and the model is relatively accurate (µε,Ι < 0.05 oC), the\nmodel is suitable for the prediction of the indoor air temperature in a building.\nThe research shows that system identification offers good perspectives for the prediction\nof the overall dynamical behaviour regarding the indoor air temperature in a building. The\nresearch proceeds with the analysis of HAM processes. The inclusion of the dynamical\nbehaviour regarding the moisture contents in the building is reported in Section 4.\n\n25 | P a g e\n\n\f4. HAM MODEL\nSection 3 describes the identification of building dynamics focusing on indoor air\ntemperature. This section focuses on the application of system identification regarding Heat, Air\nand Moisture (HAM) transport. A system identification model is used to predict indoor air\ntemperature as well as indoor air humidity. The application for temperature and humidity control\nhas been researched.\nA similar modelling methodology as depicted in Figure 1 is applied. First, (I.)\nHAMBase/Matlab is used to perform a HAM simulation with time steps of one hour, based on\nclimate data of the year 1976. Identification set points for heating and cooling of 18oC (Th,ID) and\n22oC (Tc,ID) and identification set points for humidification and dehumidification of 40%\n(RHhum,ID) and 70% (RHdehum,ID) relative humidity. Models’ input and output variables are the\nindoor and outdoor air temperature, the indoor and outdoor air relative humidity and the power\nsupplied to the HVAC installation. The input and output variables are presented in Figure 13.\nBefore identification of building dynamics, the input signal is examined. The sample frequency\nof the data is 2.8.10-4 Hz. Based on the Nyquist-Shannon theorem, the model is expected to\nrepresent building dynamics for frequencies smaller than 2.8.10-4 Hz. The amount of input power\ninto the system is determined by calculation of the Crest factors Cf,To, Cf,,RHo, Cf,Qsolar, Cf,Ti, Cf,RHi,\nCf,Qh/Qc, and Cf,Q(de)hum (Equation (2)). The Crest factors are respectively 3.09 (Cf,T0), 5.47\n(Cf,Qsolar), 5.76 (Cf,,Rho), 1.71 (Cf,Ti), 2.39 (Cf,RHi), 2.16 (Cf,Qh/Qc), and 5.28 (Cf,Q(de)hum). Second, an\nSI model has been identified based on the simulation results obtained from HAMBase/Matlab.\nThird, the SI model has been simulated in MATLAB using climate data of the year 2000 and\nwith set points of 18oC (Th) for heating, 22oC (Tc) for cooling, 40% (RHhum) for humidification\nand 70% (RHdehum) for dehumidification.\n\n26 | P a g e\n\n\fFigure 13: Input and output variables of the system identification model: the air temperature, the\nair relative humidity and the power supplied to the HVAC installation.\nThe HAM model of the building with feedback control is shown in Figure 14. Both\nindoor air temperature and indoor air humidity are controlled by the HVAC installation. The\nmodel has been simulated in MATLAB with time steps of one hour (II.) and compared with the\nsimulation results of the HAMBase/Matlab reference model.\nThe simulation results of both the SI model and the reference model have been presented\nin Figures 15 to 17. Figure 15 shows the indoor air temperature in the building predicted by the\nSI model and the reference model (HAMBase/Matlab). Figure 16 shows the indoor air absolute\nhumidity. The relative humidity of the air in the building is presented in Figure 17. Comparing\noverall dynamics, an average error of respectively 0.24oC (µII,T), 0.21.10-5 kg/kg (µII,X) and 4.2%\nRH (µII,RH) is observed. A spectral analysis shows that both the SI model results and the results\nobtained from HAMBase/Matlab agree well with each other. However, the average error of the\npredicted indoor air relative humidity (µII,RH) is relatively high. The error is caused by the\nnumerical calculation of the relative indoor air humidity, which is based on the predicted indoor\nair temperature and absolute humidity. The error in of the indoor air relative humidity is a\nsummation of the error of the indoor air temperature and the absolute humidity. Therefore, the\nerror of the predicted indoor air relative humidity gives no insight in the quality and accuracy of\nthe developed SI model.\nIn conclusion, the research shows that system identification offers good perspectives for\nthe modelling of the HAM processes in a building. The simulation of an SI model in MATLAB\nenables the designer of building and HVAC installation to predict the indoor air temperature as\nwell as indoor air humidity. In addition, main advantages are that the building model runs in a\nMATLAB environment and computing time is reduced. First of all, the implementation of the\nbuilding model in an MATLAB environment offers the user the possibility of coupling the\nbuilding model to other components and simulation modules. Second, the simulation of the SI\nmodel of a building in MATLAB is less time consuming compared to the simulation of the\nbuilding model in standard simulation software (for example ESP-r). Focusing on the advantages\nof the MATLAB environment, the next section describes the identification of a building model\nbased on discrete time data obtained from an ESP-r building simulation.\n\nIndoor Air\nTemperature\n\nOutdoor Air\nTemperature\nSolar Gains\nIndoor Air\nHumidity\n\nx(n+1) = A.x(n)+B.u(n)\ny(n) = C.x(n)+D.u(n)\n\nIndoor Air\nHumidity\n\nMoisture\nProduction\nHVAC\ninstallation\n\n27 | P a g e\n\n\fFigure 14: HAM model of the building with feedback control. Indoor air temperature and indoor\nair humidity are controlled by the HVAC installation.\n\nFigure 15: Comparison between indoor air temperature predicted by the HAMBase/Matlab\nreference model simulation and the results from the SI model simulation (µII,T = 0.24oC; σII,T =\n0.33oC).\n\n28 | P a g e\n\n\fFigure 16: Comparison between indoor air absolute humidity predicted by the HAMBase/Matlab\nreference model simulation and the results from the SI model simulation (µII,X = 0.21.10-5 kg/kg;\nσII,X = 0.28.10-5 kg/kg).\n\n29 | P a g e\n\n\fFigure 17: Comparison between indoor air relative humidity predicted by the HAMBase/Matlab\nreference model simulation and the results from the SI model simulation (µII,RH = 4.2% RH;\nσII,RH = 2.7% RH).\n.\n\n30 | P a g e\n\n\f5. MODEL IDENTIFICATION BASED ON IMPLEMENTED EXTERNAL DATA\nThis section describes the modeling of building dynamics based on a building\nperformance simulation in ESP-r. First, the indoor air temperature in an office building,\nconsisting of an office, a reception, and a roof space, is simulated in ESP-r. The office building is\nstandard available in the ESP-r library and the geometry of the building is presented in Figure\n18. Building dynamics have been simulated for one year with a discrete time step of one hour\nand set points for heating and cooling of 12oC and 20oC. Second, the results have been exported\nand input and output variables have been analyzed, focusing on the input power of the system.\nInput and output variables of the SI model are depicted in Figure 19. The Crest factors (Cf) with\nrespect to the outdoor air temperature, the solar gains, the internal heat gains, the heating/cooling\npower, and the indoor air temperature are respectively 3.21(Cf,To), 5.91 (Cf,Qsolar), 1.90 (Cf,Qint),\n4.54 (Cf,Qh/Qc), and 2.82 (Cf,Ti). The MATLAB System Identification Toolbox has been used for\nmodel identification. Since all Crest factors are relatively small, it is expected that sufficient\ninput power is delivered into the system for identification of an accurate SI model. Third, the\nobtained model SI is used for the design of climate control and a similar control strategy as\ndepicted in Figure 8 has been developed. Lastly, the building model is implemented and\nsimulated in MATLAB.\n\nFigure 18: ESP-r office building consisting of an office, a reception and a roof space.\n\n31 | P a g e\n\n\fFigure 19: Input and output variables of the SI model.\nThe results of the simulation of the SI building model in MATLAB are shown in Figure\n20. The building model has been simulated with set points for heating and cooling of 12oC and\n20oC. Considering overall dynamics, the SI model predicts the indoor air temperature to be for\napproximately 96% of the time within the set point range. Comparing both models of building\ndynamics, the average error of the predicted indoor air temperature is 0.13oC (µII,T). The research\nshows that the SI model is applicable for the obtaining of information regarding building\ndynamics. The computing time for the simulation of the SI model of the building is less than a\nminute. This is considerably fast compared to the computing time of approximately 15 minutes,\nneeded for the ESP-r building simulation. The reduction of computing time offers perspectives\nfor the simulation of large buildings using an SI model.\n\n32 | P a g e\n\n\fFigure 20: Comparison between the results obtained by ESP-r simulation and the results from the\nSI model simulation. (µII,T = 0.13oC; σII,T = 0.98oC)\n\n33 | P a g e\n\n\f6. CONCLUSIONS AND DISCUSSION\nThis study reports the research of the applicability of system identification for\nidentification of building dynamics and climate control. Considering building simulation, the\nmain problems are that a simulation of a large complex building using standard building\nsimulation software is time consuming and simulation results often lack information regarding\nfast dynamic behaviour (in the order of seconds) of the building, since most software uses a\ndiscrete time step. The applicability of system identification to reduce computing time for the\nsimulation of large complex buildings as well as the applicability of system identification to\nidentify building dynamics based on discrete time data (one hour) for climate control design has\nbeen researched.\nIt is concluded that system identification is applicable for the identification of building\ndynamics with a frequency that is smaller as the maximum sample frequency as used for\nidentification. System identification offers good perspectives for climate control design, but the\napplication is limited. A summary of the observed limitations of the applicability of system\nidentification with respect to the researched case studies is presented in Table 5.\nTable 5: Case studies and observed limitations\nCase\nInput\nOutput\nStudy\n(Identification)\n(Application)\nDiscrete\nfreeContinuous freeI\nfloating indoor air floating indoor air\ntemperature\ntemperature\nDiscrete\nfree- Continuous\nand\nII\nfloating indoor air discrete controlled\ntemperature\nindoor\nair\ntemperature\nOn/off controlled On/off controlled\nIII\ndiscrete indoor air discrete indoor air\ntemperature\ntemperature\nOn/off controlled On/off controlled\nIV\ndiscrete indoor air continuous indoor\ntemperature\nair temperature\nOn/off controlled On/off controlled\nV\ndiscrete indoor air discrete indoor air\ntemperature\ntemperature\n\nPossible Limitations\nYes\nNo\n\nYes\nNo\nYes\n\nLacking\ninput-output\ntransfer\ninformation\nCrest factor\nTime step,\nlacking fast\ndynamics.\nCrest factor\n\nThe study showed that:\no System identification is useful for the identification of free-floating building\ndynamics and application of the SI model in the same free-floating configuration.\no System identification is not applicable for the design of a climate control strategy\nbased on discrete data containing (free-floating) inside air temperature, outside air\ntemperature and solar gains. The research showed that the SI model lacks (1)\nvaluable information for smaller frequencies than the sample frequency of the\ninput signal used for identification and (2) information regarding the transfer from\nheating/cooling power to indoor air temperature.\n34 | P a g e\n\n\fo System identification is useful for the identification of building dynamics based\non a simulation with on/off-controlled indoor air temperature and application of\nthe SI model in the same on/off-controlled configuration. Moreover, the\napplication of the model is limited to a situation using a similar (discrete) time\nstep, a similar configuration of the building, and similar set points as has been\nused for model identification. With respect to the identification set points, a value\nof the Crest factor between 1 and 4 (Cf,Ti < 4) is required to enable the\nidentification of an accurate SI model.\no An SI model that has been identified based on a discrete time step of one hour is\nnot applicable for simulation in continuous time. The research showed that it is\nnot possible to capture fast dynamics within the time step that is used for\nidentification of the SI model.\no An SI model that has been identified based on identification set points Th,ID and\nTc,ID is not generally applicable for simulation with different set points for heating\nand cooling. Therefore, examination of the input signal is required, focusing on\nthe input power present in the input signals. The calculation of the Crest factor\n(Equation (2)) is a good way of giving insight into this waveform property.\nProvided the model contains sufficient transfer information regarding input-output\nbehaviour (Cf,Ti < 4) and the model is relatively accurate (µε,Ι < 0.05 oC), the\nmodel is suitable for the prediction of overall building dynamics.\nFurthermore, the research shows that system identification offers good perspectives for the\nmodelling of HAM processes in a building. The simulation of an SI model in MATLAB enables\nthe designer of a building and HVAC installation to predict the indoor air temperature as well as\nthe indoor air humidity.\nThe research shows that the main advantages of system identification models compared\nto the modelling of building dynamics and the design of climate control using standard building\nsimulation software are, that:\no SI models run in a MATLAB environment, in which many building simulation\ntools have been developed. This offers good perspectives regarding the coupling\nof the SI model to other building models, installation components and simulation\nmodules (SIMBAD, COMSOL Multiphysics).\no The computing time regarding the simulation of an SI model is reduced\nsignificantly compared to the computing time needed for the simulation of a\nsimilar building model using standard building simulation software.\n\n35 | P a g e\n\n\fNOMENCLATURE\n\nQh\nQc\nQ(de)hum\nε\nP(T)\nP(ε)\nµ\nσ\nCf\nCf,i\n\ncoefficients of the polynomial functions z-1\nbackward shift operator\nmatrixes of the state-space system\ntime [s]\noutput variable of the state-space system\ninput variable of the state-space system\nstate variable of the state-space system\ntime derivative of state variable x\ntime step\ntemperature [oC]\nindoor air temperature [oC]\noutside air temperature [oC]\nset point for heating [oC]\nset point for cooling [oC]\nidentification set point for heating [oC]\nidentification set point for cooling [oC]\nair humidity [kg/kg]\nindoor air humidity [kg/kg]\noutside air humidity [kg/kg]\nair relative humidity [%]\nindoor air relative humidity [%]\noutside air relative humidity [%]\nelectricity consumption [kW]\nsolar gains, total heat gain of the solar radiance\nsupplied to the building [W]\nheating power [W]\ncooling power [W]\n(De)humidification power [W]\nerror [oC]\ndistribution P of the error T\ndistribution P of the error ε\nmean\nstandard deviation\nCrest factor\nCrest factor with respect to variable i.\n\nSubscripts\nSI\n\npredicted by SI model\n\na, b, c, d, e, f\nz-1\nA, B, C, D\nt\ny\nu\nx\nx’\nn\nT\nTi\nTo\nTh\nTc\nTh,ID\nTc,ID\nX\nXi\nXo\nRH\nRHi\nRHo\np\nQsolar\n\n36 | P a g e\n\n\fREFERENCES\nCunningham, M.J., 2001, Inferring ventilation and moisture release rates from field\npsychometric data only using system identification techniques, Building and\nEnvironment, vol. 36, no. 1, p. 129-138.\nLowry, G., Lee, M.-W., 2004, Modelling the passive thermal response of a building using sparse\nBMS data, Applied Energy, vol. 78, no. 1, p. 53-62.\nMadsen, H., Holst, J., 1995, Estimation of continuous-time models for the heat dynamics of a\nbuilding, Energy and Buildings, vol. 22, no. 1, p. 67-79.\nMechaqrane, A., Zouak M., 2004, A comparison of linear and neural network ARX models\napplied to a prediction of the indoor temperature of a building, Neural Computing &\nApplications, vol. 13, no. 1, p. 32–37.\nPape, F.L.F., Mitchell, J.W., Beckman W.A., 1991, Optimal control and fault detection in\nheating, ventilating and air-conditioning systems. ASHRAE Transactions, vol. 97, no. 1,\np. 729-736.\nCrawley, D.B., Hand, J.W., Kummert, M., Griffith, B.T., 2005, Contrasting the capabilities of\nbuilding energy performance simulation programs, U.S. Department of Energy,\nWashington, D.C., U.S.A.\nASHRAE Handbook - Fundamentals, 2005, American Society of Heating, Refrigerating and AirConditioning Engineers, Atlanta.\nBrigham, E.O., 2004, The Fast Fourier Transform and its applications,\nPrentice Hall.\nGirod, B., Rabenstein, R., Stenger, A., 2001, Signals and Systems,\nWiley.\nLjung, L., 1997, MATLAB System identification toolbox user’s guide, Natick (MA),\nMathWorks.\nDe Wit, M.H., 2004, HAMBase A model for the simulation of the thermal and hygric\nperformance of building and systems, Technische Universiteit Eindhoven.\nhttp://sts.bwk.tue.nl/hambase/.\nData Model Summary ESP-r Version 9 Series, 2001, Energy Systems Research Unit, University\nof Strathclyde.\nhttp://www.esru.strath.ac.uk/\n\n37 | P a g e\n\n\f"
        ],
        [
         "8",
         "8",
         "cs.CE",
         "Computational Engineering",
         "1612.09087v2.pdf",
         "Efficient isogeometric thin shell formulations for soft biological\nmaterials\nFarshad Roohbakhshan and Roger A. Sauer\n\n1\n\nAachen Institute for Advanced Study in Computational Engineering Science (AICES), RWTH Aachen\nUniversity, Templergraben 55, 52056 Aachen, Germany\n\narXiv:1612.09087v2 [cs.CE] 24 Oct 2017\n\nPublished2 in Biomechanics and Modeling in Mechanobiology,\nDOI: 10.1007/s10237-017-0906-6\nSubmitted on 23. December 2016 2016, Accepted on 27. March 2017, Published online on 12. April 2017\n\nAbstract: This paper presents three different constitutive approaches to model thin rotationfree shells based on the Kirchhoff–Love hypothesis. One approach is based on numerical integration through the shell thickness while the other two approaches do not need any numerical\nintegration and so they are computationally more efficient. The formulation is designed for\nlarge deformations and allows for geometrical and material nonlinearities, which makes it very\nsuitable for the modeling of soft tissues. Furthermore, six different isotropic and anisotropic\nmaterial models, which are commonly used to model soft biological materials, are examined for\nthe three proposed constitutive approaches. Following an isogeometric approach, NURBS-based\nfinite elements are used for the discretization of the shell surface. Several numerical examples\nare investigated to demonstrate the capabilities of the formulation. Those include the contact\nsimulation during balloon angioplasty.\nKeywords: Angioplasty, contact modeling, isogeometric analysis, Kirchhoff–Love shell, soft\nbiological materials, thin rotation-free shells\n\n1\n\nIntroduction\n\nMany biological systems are thin structures, composed of nonlinear soft materials, which can\neasily undergo large deformations. In many cases, such structures do not resist any bending\nmoments (Humphrey, 1998); thus, a membrane formulation (e.g. Roohbakhshan et al., 2016)\nis efficient and robust to predict the mechanical response. However, if the bending effects are\nnot negligible, a shell formulation is required. For thin structures, where the transverse shear\nstrains can be neglected, rotation-free formulations based on the Kirchhoff–Love hypothesis are\nthe best choice. Here, we introduce a new approach to model thin biological shells3 constructed\nfrom nonlinear constitutive laws, without any need for numerical integration. Following Sauer\nand Duong (2017) and Duong et al. (2017), the model is formulated in a curvilinear coordinate system, without resorting to a transformation from/to the Cartesian coordinate system.\nFurthermore, the geometry, kinematic variables and weak form of the governing equation are\ndiscretized within the framework of isogeometric analysis (IGA) in order to take advantage of\nthe C 1 -continuity NURBS-based interpolation, which is a necessary condition for the Kirchhoff–\nLove shells.\n1\n\ncorresponding author, email: sauer@aices.rwth-aachen.de\nThis pdf is the personal version of an article whose final publication is available at http://www.springer.com/\n3\nHere, we distinguish between membranes and shells as two thin structures with different mechanical characteristics. Membranes bear only in-plane stresses but shells bear bending moments as well. However, the term\n“(bio)membrane” is also used for structures that are mechanical shells (cf. Tepole et al., 2015).\n2\n\n1\n\n\fThe finite element modeling and analysis of thin soft tissues has been the subject of extensive\nresearch although only the membrane forces are considered in general (e.g. Humphrey et al.,\n1992; Humphrey, 1998; Prot et al., 2007; Kroon and Holzapfel, 2009; Abdessalem et al., 2011;\nRausch and Kuhl, 2013, 2014; Roohbakhshan et al., 2016) and mostly planar tissues are studied\n(e.g. Flynn et al., 1998; Sun and Sacks, 2005; Holzapfel and Ogden, 2009; Jacobs et al., 2013;\nFan and Sacks, 2014). The first isogeometric Kirchhoff–Love shell, specially formulated for soft\ntissues, was introduced by Tepole et al. (2015). It is based on numerical integration through\nthe shell thickness. Furthermore, Kiendl et al. (2015) and Duong et al. (2017) have suggested\ntwo different isogeometric formulations for the modeling of the rotation-free thin shells with\narbitrary nonlinear hyperelastic materials. Both approaches can be used for the modeling of\nbiological shells. The former requires numerical integration through the shell thickness. In\ncontrast, the latter allows for both projected shell models that are extracted from existing 3D\nmaterial models using numerical integration and shell models that are directly formulated on a\n2D manifold, like the Koiter model or the Canham model.\nIn the present work, we extend the earlier work of Duong et al. (2017), which allows an arbitrary\nchoice of the membrane and bending strain energies. For specific applications, like biological\nshells, a physically well-defined link between the bending and membrane parts is needed. Here,\nfor any given 3D material model, a systematic approach is introduced to derive the corresponding\n2D shell formulation, which (1) requires no numerical integration through the thickness, (2)\nprovides a natural link between membrane and bending parts and (3) admits many isotropic\nand anisotropic material models. To show the accuracy of the new approach, a simplified version\nof the projected shell formulation of Duong et al. (2017) is used for reference.\nThe presented work adds novelties to the existing literature on the computational modeling of\nsoft biological shells:\n• First and foremost, it provides two new approaches that do not require any numerical\nthickness integration and that are therefore computationally more efficient.\n• Second, the resultant stresses and bending moments are expressed in terms of the first\nand second fundamental forms of the shell mid-surface, which allows flexible coupling of\nthe bending and membrane modes of the shell (see Sec. 3.3).\n• Third, an efficient and accurate treatment of the compression/extension switch, to exclude\ncompressed fibers from the constitutive law, is introduced. Such a switch, which is used for\nthe anisotropic material models like the Gasser–Ogden–Holzapfel (GOH) model (Gasser\net al., 2006), is needed to guarantee the polyconvexity of the strain energy density function\nin order to avoid non-physical responses (Balzani et al., 2006).\nThe remaining part of this paper is organized as follows: Sec. 2 provides a short summary of\nthe rotation-free thin shell theory, including the kinematics and weak form of the governing\nequations. Sec. 3 discusses the three constitutive approaches to model thin shells in detail.\nThose are the numerically-projected (NP) shell model, which is based on numerical integration\nthrough the shell thickness, and the analytically-projected (AP) and directly-decoupled (DD)\nshell models, which need no numerical integration. In Sec. 4, those three shell models are\nspecifically derived for different isotropic and anisotropic material models, which are commonly\nused for soft tissues. Several numerical experiments are presented in Sec. 5 to illustrate the\ncapabilities of the new model. Sec. 6 concludes the paper.\n\n2\n\n\f2\n\nThin shell theory\n\nThis section summarizes the nonlinear theory of rotation-free thin shells. Further details can\nbe found in e.g. Naghdi (1982), Steigmann (1999) and Sauer and Duong (2017). Here, first the\nkinematics of thin shells based on the Kirchhoff–Love hypothesis is reviewed. Those kinematics\ncan either be derived from 3D kinematics or be formulated directly on the shell mid-surface.\nThis is followed by a brief discussion of the weak form of the governing equation. Last, the weak\nform is linearized in order to be solved by the Newton-Raphson method. Upon this theoretical\nfoundation, three different shell models are constructed in the next section.\n\n2.1\n\nKinematics of Kirchoff–Love shells\n\nA thin shell is a structure that can be presented as a 2D manifold defined by the shell midsurface. Alternatively, the shell can be described as a thin 3D continuum, which is confined by\nan upper and a lower surface. Here, a framework is presented that can capture both approaches.\nFirst, the shell mid-surface is described and then the description is extended to the other shell\nlayers according to the Kirchhoff–Love hypothesis.\nIn the deformed configuration, the shell mid-surface S is described by the mapping\nx = x(ξ α ) ,\n\n(α = 1, 2) ,\n\n(1)\n\nwhere ξ α are the convective coordinates defined in a parametric domain. According to this\nsurface description, the co-variant tangent vectors are aα = ∂x/∂ξ α and the contra-variant\ntangent vectors are defined by aα = aαβ aβ , where the co-variant components of the metric\ntensor are aαβ = aα · aβ and the contra-variant components are [aαβ ] = [aαβ ]−1 . Then, from\nthe tangent vectors aα , the normal vector of the surface S is given by n = (a1 × a2 )/ka1 × a2 k.\nAnother important object associated with a surface is the curvature tensor b = bαβ aα ⊗ aβ ,\nwhere bαβ := n·aα,β are the co-variant components of the curvature tensor. The mean curvature\nof the deformed surface is H := 12 aαβ bαβ . Likewise, the shell mid-surface can be described in\nits reference configuration, denoted by S0 .\n∗\n\nSuch a surface description can be extended to any shell layer S within the shell thickness. Based\non the Kirchhoff–Love assumptions, the position x̃ of any material point in the deformed shell\nbody is related to a corresponding point x on the shell mid-surface S as\nx̃(ξ α , ξ) = x(ξ α ) + ξ n ,\n\n(2)\n\nwhere ξ ∈ [− T2 , T2 ] is the out-of-plane coordinate and T is the initial shell thickness.\nRemark 2.1. Henceforth, the variables\nof 3D continua are distinguished by a tilde and the cor∗\nresponding variables of a shell layer S, located at ξ within the shell thickness, are distinguished\nby an asterisk. The variables of the shell mid-surface S, located at ξ = 0, have no mark. All the\nvariables in the reference and current configurations are denoted by uppercase and lowercase\nletters, respectively.\nRemark 2.2. Greek indices take values in {1, 2}, where the Einstein summation convention\nis assumed. Further, one needs to distinguish between variables with Greek upper and lower\nindices as contra-variant and co-variant objects, respectively. Latin indices that appear later\ncan be any positive integer and may be arbitrarily used in upper or lower positions.\n\n3\n\n\f∗\n\nLikewise to the shell mid-surface, the tangent vectors on a shell layer S can be expressed as\ng α :=\n\n∂ x̃\n= aα − ξ bγα aγ ,\n∂ξ α\n\n(3)\n\nwhich give the co-variant components of the metric tensor of a shell layer\ngαβ := g α · g β = aαβ − 2 ξ bαβ\n\n(4)\n\nif the second and higher order terms are neglected. The contra-variant components are then\n[g αβ ] = [gαβ ]−1 , which give the contra-variant tangent vectors g α = g αβ g β . Similarly, corre∗\n\nsponding variables on S0 are defined in the same fashion.\nThe mapping between the reference configuration and the current configuration\nis characterized\n∗\nby the deformation tensor F̃ := ∂ x̃/∂ X̃. On the shell layer S, the deformation gradient\n∗\n√\ncan be decomposed into in-plane and out-of-plane components as F̃ = F + g33 n ⊗ N and\ncorrespondingly as F̃ := F +λ3 n⊗N on S, where the surface deformation tensors are F = aα ⊗\n∗\n√\nAα and F = g α ⊗ Gα . Here, g33 and λ3 measure the out-of-plane stretches at ξ ∈ [−T /2, T /2]\n√\nand ξ = 0, respectively. In general, λ3 is the average of g33 over the thickness. Such a layerwise decomposition is also applied to the other kinematical variables. For instance, the volume\nchange, measured by the determinant of deformation gradient, is given by\nJ˜ := det F̃ = J λ3 ,\n\n√\nJ˜ := det F̃ = J g33 ,\n∗\n\n(5)\n\n∗\n\nfor S and S, respectively, where the surface changes are determined by\np\np\n∗\n∗\nJ := det F = a/A , J := det F = g/G .\n\n(6)\n\nHere, g := det[gαβ ] and a := det[aαβ ] are defined in the current configuration and they are\ncorrespondingly denoted by G and A in the reference configuration. The other important\ntensors to describe the deformation are the Cauchy–Green deformation tensors and the Green–\nLagrange strain tensor. The right Cauchy–Green deformation tensors for a Kirchhoff–Love shell\nare\n∗\n(7)\nC̃ = C + λ23 N ⊗ N , C̃ = C + g33 N ⊗ N ,\nwhere the right Cauchy–Green deformation tensors of the shell mid-surface and a layer within\nthe shell thickness are, respectively,\nC := F T F = aαβ Aα ⊗ Aβ ,\n∗\n∗\n∗\nC := F T F = gαβ Gα ⊗ Gβ .\n\n(8)\n\nAccordingly, the first three invariants of C̃ are\n∗\n\nI˜1 := tr C̃ = I1 + λ23 ,\n\nI˜1 := tr C̃ = I1 + g33 ,\n\nwith\n\n∗\n\n∗\n\nI1 := tr C = Aαβ aαβ , I1 := tr C = Gαβ gαβ ,\ni\n\u00012\n∗\n∗\n1h\nI˜2 :=\ntr C̃ − tr C̃ 2 = λ23 I1 + J 2 , I˜2 = g33 I1 + J 2\n2\nand\nI˜3 := det C̃ = J˜2 = λ23 J 2 ,\n\n∗\n\nI˜3 := det C̃ = g33 J 2 .\n\n(9)\n(10)\n(11)\n(12)\n\nThe Green–Lagrange strain tensors then are\nẼ :=\n\n\u0001\n1\nC̃ − 1 = E + E33 N ⊗ N ,\n2\n\nẼ :=\n4\n\n\u0001\n∗\n∗\n1\nC̃ − 1 = E + E33 N ⊗ N ,\n2\n\n(13)\n\n\f∗\n\nwhere 1 is the usual identity tensor in R3 . Further, E and E are the in-plane components of\nthe Green–Lagrange strain tensor given by\n\u0001\n1\nC − I = Eαβ Aα ⊗ Aβ ,\n2\n∗\n∗\u0001\n∗\n1 ∗\nE :=\nC − I = Eαβ Gα ⊗ Gβ ,\n2\nE :=\n\n∗\n\n(14)\n\n∗\n\nwhere I = Aαβ Aα ⊗ Aβ and I = Gαβ Gα ⊗ Gβ are the surface identity tensors on S and S,\nrespectively, and thus\nEαβ =\n\n1\naαβ − Aαβ ) ,\n2\n\n∗\n\nEαβ =\n\n1\ngαβ − Gαβ ) .\n2\n\n(15)\n\nThe out-of-plane components of the Green–Lagrange strain tensor (13) are then\nE33 =\n\n\u0001\n1 2\nλ3 − 1 ,\n2\n\n∗\n\nE33 =\n\n\u0001\n1\ng33 − 1 .\n2\n\n(16)\n\nMoreover, Eqs. (4), (14) and (15) imply that the distribution of the in-plane strain across the\nshell thickness is linear, i.e.\n∗\n(17)\nEαβ = Eαβ − ξ Kαβ ,\nwhere we have defined\nKαβ = bαβ − Bαβ .\n\n2.2\n\n(18)\n\nGoverning weak form\n\nHaving described the kinematics of thin shells, now the weak form of the governing equation of\na thin shell is introduced. Here, a brief review is presented. Further details of the stress and\nmoment tensors as well as the balance laws and strong form can be found in Sauer and Duong\n(2017).\nNeglecting the inertial effects, for any admissible variation δx ∈ V, the weak form of the\ngoverning equation is formulated in terms of the internal and external virtual work contributions\nas\nGint − Gext = 0 ∀ δx ∈ V .\n(19)\nThe internal virtual work is\nZ\nGint =\nS0\n\n1\nδaαβ τ αβ dA +\n2\n\nZ\nS0\n\nδbαβ M0αβ dA ,\n\n(20)\n\nwhere τ αβ is the Kirchhoff stress and M0αβ is the moment tensor defined in the reference configuration. They are associated with their counterparts in the current configuration by τ αβ = J σ αβ\nand M0αβ = J M αβ . The external virtual work is\nZ\nZ\nZ\n\u0003\nGext =\nδx · f da +\nδx · t ds +\nδn · mτ ν ds + [δx · mν n ,\n(21)\nS\n\n∂t S\n\n∂m S\n\nwhere f = f α aα + pext n is a prescribed body force on S with pext as the external pressure.\nFurther, t, mτ and mν are distributed forces and moments prescribed on the boundary and\nν = να aα is the normal to ∂m S, where the bending moment mτ is applied.\n\n5\n\n\f2.3\n\nLinearization of the weak form\n\nAs the weak form (19) is nonlinear, it needs to be linearized in order to be solved by the Newton–\nRaphson method. The linearized internal virtual work contribution is (Sauer and Duong, 2017)\nZ\n\u0001\n1\n1\n∆Gint =\nδaαβ cαβγδ ∆aγδ + dαβγδ ∆bγδ dA\n2\nS 2\nZ 0\n\u0001\n1\nδbαβ eαβγδ ∆aγδ + f αβγδ ∆bγδ dA\n+\n(22)\n2\nS\nZ 0\n\u0001\n1\n+\nτ αβ ∆δaαβ + M0αβ ∆δbαβ dA ,\n2\nS0\nwhere\ncαβγδ := 2\n\n∂τ αβ\n,\n∂aγδ\n\ndαβγδ :=\n\n∂τ αβ\n,\n∂bγδ\n\neαβγδ := 2\n\n∂M0αβ\n,\n∂aγδ\n\nf αβγδ :=\n\n∂M0αβ\n∂bγδ\n\n(23)\n\nare the material tangent tensors. The linearized external virtual work contribution, ∆Gext , is\ngiven in Appendix B. The discretized weak form, which gives the FE force vectors and the FE\ntangent matrices, can be found in Duong et al. (2017) for a NURBS-based FE implementation.\n\n3\n\nShell constitution: Three modeling approaches\n\nAs shown in Tab. 1, in general, there are two different structural modeling approaches in\nshell theory (Bischoff et al., 2004). In the projection approach, a shell is assumed to be a 3D\ncontinuum, thus the stress resultants are derived rarely analytically and mostly by numerical\nintegration through the shell thickness (e.g. Duong et al., 2017). In the direct surface approach,\na shell is considered as a 2D manifold, defined on the mid-surface of the shell continuum and\nthus the stresses and moments can be directly derived from a well-postulated 2D strain energy\ndensity function (e.g. Sauer and Duong, 2017). Furthermore, the degenerated solid approach\ncan also be used, which is in fact not based on a shell theory but rather is a method to reduce\nthe dimension of 3D finite elements (Bischoff et al., 2004).\nHere, a model of the first approach, namely the numerically-projected (NP) shell model, and a\nmodel of the second approach, namely the directly-decoupled (DD) shell model are introduced.\nFurther, the analytically-projected (AP) shell model is presented, which combines elements of\nboth approaches and provides an algorithm to analytically evaluate the integration through the\nshell thickness. For the NP shell model, it is assumed that the in-plane strains vary linearly\nacross the shell thickness, which considerably simplifies the formulation yet it is accurate only\nfor thin shells. For the DD shell model, by extending the formulation of Duong et al. (2017) for\na combined Koiter/Neo-Hooke shell, a systematic algorithm is introduced to consistently find\nthe bending counterparts for any given membrane formulation.\nAs shown in detail in the next sessions, the NP shell model is a fully nonlinear shell formulation.\nThe AP shell model is a first-order approximation of the NP shell model, in which the membrane\nand bending forces are still coupled. The DD shell model combines a fully nonlinear membrane\nwith a linear elastic bending model.\nFurther theoretical background and implementation details for the DD and the NP shell models\ncan be found in the earlier works of the authors. Here, the principal concepts and the new\nextensions are introduced.\n6\n\n\fNumerically-projected\n(NP)\n\nDirect surface approach\nAnalytically-projected\n(AP)\n\nDirectly-decoupled\n(DD)\n\nStress\n\nStrain\n\nGeometry\n\nModel\n\nProjection approach\n\nTable 1: Three different modeling approaches for defining the constitution of thin shells\n\n3.1\n\nNumerically-projected (NP) shell model\n\nIn this approach, the strain energy density function of a 3D shell continuum is projected onto\nthe mid-surface of the shell so that the stress and bending moment resultants can be found by\nan appropriate integration through the shell thickness, which has to be evaluated numerically\nin general. The projected strain energy density function is\nZ T\n2\n(24)\nW = W (aαβ , bαβ ) =\nW̃ (gαβ , g33 ) dξ ,\n− T2\n\n∗\n\nwhere W̃ = W̃ (gαβ , g33 ) is the 3D strain energy density function defined on a shell layer S at\n∗\nξ ∈ [−T /2, T /2]. Then, the Kirchhoff stress on S is derived by the variation δ W̃ = 12 τ̃ αβ δgαβ +\n1 33\nδg33 , which gives\n2 τ̃\n∂ W̃\n∂ W̃\n(25)\nτ̃ αβ := 2\n, τ̃ 33 := 2\n.\n∂gαβ\n∂g33\nWriting the variation of the projected strain energy W as\nZ T\n2\n1\n(26)\nδ W̃ (gαβ , g33 ) dξ = τ αβ δaαβ + M0αβ δbαβ ,\nδW =\n2\n−T\n2\n\nthe resultant stress and moment tensors are\n∂W\nτ αβ := 2\n,\n∂aαβ\n\nM0αβ :=\n\n∂W\n.\n∂bαβ\n\n(27)\n\nPlugging Eq. (167) into Eqs. (24), (25) and (27), one can find the resultant stresses and bending\nmoments as\nZ T\nτ αβ\n\n2\n\n=\n\nτ̃ αβ dξ ,\n\n− T2\n\nM0αβ\n\nZ\n= −\n\nT\n2\n\n− T2\n\n7\n\n(28)\nξ τ̃\n\nαβ\n\ndξ .\n\n\fRemark 3.1. Assuming the plane-stress condition, i.e. τ̃ 33 = 0, the out-of-plane squared stretch\ng33 is eliminated by static condensation (Bischoff et al., 2004; Echter, 2013; Kiendl et al., 2015;\nDuong et al., 2017).\nFrom Eqs. (28), (23) and (4), the material tangents follow as\nZ T\n2\ncαβγδ =\nc̃αβγδ dξ ,\n− T2\n\ndαβγδ\n\n=\n\neαβγδ\n\nZ\n=−\n\nT\n2\n\nξ c̃αβγδ dξ ,\n\n(29)\n\n− T2\n\nf αβγδ\n\nZ\n=\n\nT\n2\n\nξ 2 c̃αβγδ dξ ,\n\n− T2\n∗\n\nwhere, on S, we have introduced the elasticity tensor\nc̃αβγδ := 2\n\n3.2\n\n∂ τ̃ αβ\n.\n∂gγδ\n\n(30)\n\nAnalytically-projected (AP) shell model\n\nIf the shell thickness is small enough compared to the curvature radii of the shell, one can use a\nfirst order Taylor expansion to analytically evaluate the integrals in Eqs. (28) and (29). For this\npurpose, all the kinematical objects, stresses and bending moments are linearized w.r.t. the outof-plane coordinate ξ. The linearization of the kinematical parameters are given in Appendix C.\nIn this section, the stresses, bending moments and material tangents are derived for two cases:\n(1) The whole thickness of the shell contributes to the strain energy density function and (2)\nonly a portion of the shell thickness, e.g. [T1 T2 ] ⊂ [−T /2 T /2], is active. The former case is\nthe typical condition of thin shells while the latter happens for instance if the material bears\nonly compression (e.g. concrete) or only tension (e.g. collagen fibers).\nUsing a Taylor expansion of τ̃ αβ about ξ = 0, we have\nτ̃ αβ = τ̂ αβ + ξ τ̂,3αβ + O(ξ 2 ) ,\n\n(31)\n\nwhere we have defined\n\u0010\n\u0011\nτ̂ αβ := τ̃ αβ\n\nξ=0\n\nτ̂,3αβ :=\n\n,\n\n\u0010 ∂ τ̃ αβ \u0011\n∂ξ\n\nξ=0\n\n.\n\n(32)\n\nRemark 3.2. Henceforth, a hat is used to denote the quantities calculated at ξ = 0, i.e. •ˆ =\n∗\n(•)ξ=0 . In general, such quantities can be defined for each shell layer. In particular, they can be\ndimensionally linked to a counterpart in the membrane theory (e.g. aαβ = ĝαβ and τ αβ = T τ̂ αβ )\nor there might be no corresponding quantity in the membrane theory (e.g. for τ̂,3αβ ).\n3.2.1\n\nFully-stressed cross-section\n\nPlugging Eq. (31) into Eq. (28) and integrating analytically, the resultant stresses and bending\nmoments are\nτ αβ = T τ̂ αβ ,\n(33)\nT 3 αβ\nτ̂,3 .\nM0αβ = −\n12\n8\n\n\fThe tangent matrices are derived from Eq. (33) as\n∂τ αβ\n∂τ αβ\n= T ĉαβγδ ,\ndαβγδ :=\n= T dˆαβγδ ,\n∂aγδ\n∂bγδ\nT 3 αβγδ\n∂M0αβ\nT 3 ˆαβγδ\n∂M0αβ\n= −\nĉ,3\n, f αβγδ :=\n= −\nd\n,\n:= 2\n∂aγδ\n12\n∂bγδ\n12 ,3\n\ncαβγδ := 2\neαβγδ\n\n(34)\n\nwhere we have defined\nĉαβγδ\n\n3.2.2\n\n∂ τ̂ αβ\n,\n:= 2\n∂aγδ\n\n∂ τ̂ αβ\ndˆαβγδ :=\n,\n∂bγδ\n\nĉαβγδ\n,3\n\n:= 2\n\n∂ τ̂,3αβ\n∂aγδ\n\n,\n\ndˆαβγδ\n:=\n,3\n\n∂ τ̂,3αβ\n∂bγδ\n\n.\n\n(35)\n\nPartially-stressed cross-section\n\nAs already mentioned, in many applications, the strain energy density function and accordingly\nthe in-plane stresses are nonzero only in a portion of the shell thickness, i.e.\nZ T2\nτ̃ αβ dξ ,\nτ αβ =\nT1\n\nM0αβ\n\nZ\n\n(36)\n\nT2\n\n= −\n\nξ τ̃\n\nαβ\n\ndξ .\n\nT1\n\nPlugging Eq. (31) into Eq. (36), one can analytically calculate the stress and the bending\nmoment resultants as\n\u0001\n\u0001\n1 2\nτ αβ =\nT2 − T1 τ̂ αβ +\nT2 − T12 τ̂,3αβ ,\n2\n(37)\n\u0001\n\u0001\n1\n1 3\nM0αβ =\nT12 − T22 τ̂ αβ +\nT1 − T23 τ̂,3αβ .\n2\n3\nHowever, the derivation of the material tangents is not so simple since T1 and T2 are not\nconsidered to be generally fixed and they may vary with x, i.e.\nT1 = T1 (x) = T1 (aαβ , bαβ ) ,\n\n(38)\n\nT2 = T2 (x) = T2 (aαβ , bαβ )\nand they are defined based on the constitution and application. Thus, introducing\nU1αβ :=\n\n∂T1\n,\n∂aαβ\n\nU2αβ :=\n\n∂T2\n,\n∂aαβ\n\nV1αβ :=\n\n∂T1\n,\n∂bαβ\n\nV2αβ :=\n\n∂T2\n,\n∂bαβ\n\nthe material tangents are derived as\n\u0001\n\u0001\n1\ncαβγδ = T2 − T1 ĉαβγδ + T22 − T12 ĉαβγδ\n+ 2 τ̃2αβ U2γδ − 2 τ̃1αβ U1γδ ,\n,3\n2\n\u0001\n\u0001\n1\n+ τ̃2αβ V2γδ − τ̃1αβ V1γδ ,\ndαβγδ = T2 − T1 dˆαβγδ + T22 − T12 dˆαβγδ\n,3\n2\n\u0001\n\u0001\n1 2\n1\neαβγδ =\nT1 − T22 ĉαβγδ + T13 − T23 ĉαβγδ\n+ 2 T1 τ̃1αβ U1γδ − 2 T2 τ̃2αβ U2γδ ,\n,3\n2\n3\n\u0001\n\u0001\n1\n1 2\nf αβγδ =\nT1 − T22 dˆαβγδ + T13 − T23 dˆαβγδ\n+ T1 τ̃1αβ V1γδ − T2 τ̃2αβ V2γδ .\n,3\n2\n3\n\n(39)\n\n(40)\n\nHere, τ̃1αβ and τ̃2αβ are the stresses corresponding to the lower and upper limits, respectively,\nwhich are defined as\n\u0001\nτ̃1αβ := τ̃ αβ ξ=T1 = τ̂ αβ + T1 τ̂,3αβ ,\n(41)\n\u0001\nτ̃2αβ := τ̃ αβ ξ=T2 = τ̂ αβ + T2 τ̂,3αβ .\n9\n\n\fRemark 3.3. It should be noted that the tangent tensors given by Eqs. (34) and (40) may not\nbe symmetric as the linearization and variation are treated differently.\nTab. 2 summarizes the procedure to derive a AP shell model from any given 3D material model.\n1)\n2)\n3)\n\nFor any 3D constitution, with a given strain energy function W̃ , derive τ̃ αβ\naccording to Eq. (25).\nDetermine τ̂ αβ from Eqs. (32.1) and ĉαβγδ and dˆαβγδ from Eq. (35).\nDetermine τ̂,3αβ from Eq. (32.2) using the linearized kinematical variables\n(see Appendix C). Compute ĉαβγδ and dˆαβγδ from Eq. (35).\n,3\n\n4.a)\n4.b)\n\n,3\n\nM0αβ\n\nFor fully-stressed shells: τ αβ and\nand their corresponding tangents are\nfound from Eq. (33) and (34).\nFor partially-stressed shells:\n4.b.1) Find the effective thickness [T1 , T2 ] and its corresponding tensors,\ngiven by Eq. (39), using Appendices C and D.\n4.b.2) Determine τ αβ and M0αβ and their corresponding tangents from\nEqs. (37) and (40).\nTable 2: Summary of the analytically-projected shell formulation\n\n3.3\n\nDirectly-decoupled (DD) shell model\n\nIn this approach, the stresses and moments are directly derived from a 2D strain energy density\nfunction; therefore, there is no need for numerical integration thought the shell thickness. The\nmodel completely decouples the membrane and bending forces. It predicts the stretching deformation of the shell through the metric tensor while the bending of the shell depends only on\nthe curvature tensor (Ciarlet, 2005; Sauer and Duong, 2017). The classic shell formulations of\nthis kind, namely Koiter shell model and Canham model, are described in detail e.g. by Ciarlet\n(2005) and Sauer and Duong (2017). Recently, Duong et al. (2017) have proposed a mixed\nformulation that combines the bending stored energy of a Koiter shell and the strain energy of\na compressible Neo–Hookean membrane. Here, a systematic approach is introduced to find an\nappropriate and consistent bending energy for any given isotropic or anisotropic membrane formulation so that their combination results in a directly-decoupled shell model with a polyconvex\n2D strain energy density function as\nW = W (aαβ , bαβ ) = WM (aαβ ) + WB (bαβ ) ,\n\n(42)\n\nwhere WM and WB are the membrane and bending parts, respectively.\nRemark 3.4. Compared to the combined Koiter/Neo-Hooke shell model of Duong et al.\n(2017), the presented formulation (i) provides a physically well-defined link between the bending and membrane parts, (ii) is extended to many isotropic material models and (iii) allows for\nanisotropic behavior, which is of great importance for the modeling of soft tissues.\nMany biological materials, such as soft tissues, can easily undergo large deformations while\nbeing extremely stretched. Thus, the membrane formulation should allow for large material\nand geometrical nonlinearities. This implies that, for the membrane part, a nonlinear stressstrain relationship is required; however, for the bending part, a linear stress-strain relationship\nis sufficient for most applications even when the shell exhibits large deformations.\n10\n\n\fFor membranes, the strains are constant over the thickness, i.e. gαβ = aαβ and g33 = λ23 . Thus,\na 3D material model can be reduced to a 2D membrane one as (Roohbakhshan et al., 2016)\nZ T\n2\n(43)\nWM (aαβ , λ3 ) =\nW̃ (gαβ , g33 ) dξ = T Ŵ (aαβ , λ3 ) ,\n− T2\n\nwhere\n\nh\ni\nŴ (aαβ , λ3 ) := W̃ (gαβ , g33 )\n\nξ=0\n\n(44)\n\nis the 3D strain energy density function in terms of the mid-surface metric tensor aαβ and\nnormal stretch λ3 . From Eq. (43), it follows that\nτ αβ := 2\nand\ncαβγδ := 2\n\n∂WM\n∂ Ŵ\n= 2T\n∂aαβ\n∂aαβ\n\n∂τ αβ\n∂ 2 Ŵ\n= 4T\n∂aγδ\n∂aαβ ∂aγδ\n\n(45)\n\n(46)\n\nare the components of the membrane elasticity tensor. Likewise to the NP shell model (see\nRemark 3.1), the membrane out-of-plane stretch λ3 is eliminated by the plane-stress assumption,\ni.e. τ̃ 33 := λ−1\n3 ∂W/∂λ3 = 0.\nThe corresponding linear stress-strain relation is derived from a linearized material model as\nWlin (aαβ , bαβ ) =\nwhere\n\n1 αβγδ\nT 2 αβγδ\nc0\nEαβ Eγδ +\nc\nKαβ Kγδ ,\n2\n24 0\n\n(47)\n\n\u0010\n\u0011\ncαβγδ\n:= cαβγδ\n0\n\n(48)\n\nS0\n\nare the components of the elasticity tensor before deformation and cαβγδ is given by Eq. (46).\nThen, the rear part of Eq. (47) can be used for the 2D strain energy density function corresponding to the bending deformations as\nWB = WB (bαβ ) =\n\nT 2 αβγδ\nc\nKαβ Kγδ .\n24 0\n\n(49)\n\nFrom Eq. (49), the bending moments and their corresponding tangents are\nM0αβ :=\n\n∂WB\nT 2 αβγδ\nKγδ ,\n=\nc\n∂bαβ\n12 0\n\n(50)\n\n∂M0αβ\nT 2 αβγδ\n=\nc\n.\n∂bγδ\n12 0\n\n(51)\n\nf αβγδ :=\n\nRemark 3.5. As the bending and membrane parts are decoupled, here in contrast to the NP\nshell model, dαβγδ = eαβγδ = 0 (Sauer and Duong, 2017).\nRemark 3.6. The presented decoupled membrane-bending equations are derived provided that\n(1) the material is symmetric w.r.t. the shell mid-surface and (2) the shell thickness is considerably smaller than the other dimensions and the radii of curvature. If these two conditions\nare violated, in addition to the stretching and bending strains, other mixed terms (i.e. strain\ngradients) are present in Eq. (47); therefore, the membrane and bending parts cannot be easily\ndecoupled.\nTab. 3 summarizes the procedure to formulate a DD shell model for any given 3D material\nmodel.\n11\n\n\f1)\n2)\n\n3)\n\nFor any 3D constitution, with a given strain energy function W̃ ,\npostulate a 2D counterpart as WM + WB .\nFor the membrane part WM , compute\n2.1) the membrane stresses τ αβ from Eqs. (43) and (45),\n2.1) the membrane elasticity tensor cαβγδ from Eq. (46).\nFor the bending part WB , compute\n3.1) cαβγδ\nfrom Eq. (48),\n0\n3.2) the bending moments M0αβ from Eq. (50),\n3.3) f αβγδ from Eq. (51).\nTable 3: Summary of the directly-decoupled shell formulation\n\n4\n\nMaterial models\n\nHaving introduced the thin shell theory and three different approaches to model shells, various\nisotropic and anisotropic constitutive laws can be examined now. For each material model, the\nthree different approaches from Sec. 3 (i.e. the NP, AP and DD shell models), are derived. All\nthe introduced material models are considered to be incompressible since most types of soft\nbiological materials, in particular soft tissues, are regarded as incompressible (Holzapfel, 2001).\nHere, the incompressibility constraint\ng̃ := 1 − J˜ = 0\n\n(52)\n\nis enforced strictly through the Lagrange multiplier method. Thus, the incompressible 3D stored\nenergy W̃inc (C̃) is augmented by the contribution from the Lagrange multiplier as\n˜ = W̃inc (C̃) + p̃ g̃ ,\nW̃ (C̃, J)\n\n(53)\n\nwhere the unknown Lagrange multiplier p̃ is a hydrostatic pressure. For ∗shells and membranes,\nit is analytically determined from the plane-stress condition as p̃ = 2 J −2 ∂ W̃inc (C̃)/∂g33 . In\naddition to physical reasons, the plane-stress condition is therefore advantageous for thin shells.\nIn the DD shell model, the incompressibility constraint is treated analogously for the membrane\npart. The incompressibility constraint is added to the corresponding incompressible 2D stored\nenergy Winc (C̃) through the Lagrange multiplier method (Sauer et al., 2014; Sauer, 2016)\n˜ = Winc (C̃) + p g̃ ,\nWM (C̃, J)\n\n(54)\n\nwhich is similar to the 3D formulation, cf. Eq. (53). The unknown Lagrange multiplier p = T p̃\ncan also be analytically found from the plane-stress condition. For the bending part, the effect\nof incompressibility constraint is condensed into cαβγδ\n.\n0\nIn Secs. 4.1 and 4.2, the stress, bending moment and tangent tensors for different isotropic and\nanisotropic material models are derived. To avoid repetition, the derivations are not explained\nin detail. As summarized in Tab. 4, for the NP shell model, one needs to derive τ̃ αβ and c̃αβγδ\nspecifically for any given material model. Then, the stress and moment tensors and their corresponding tangents are determined by plugging the specific τ̃ αβ and c̃αβγδ into Eqs. (28) and (29),\nrespectively.\nFor the AP shell model, τ̂ αβ , τ̂,3αβ , ĉαβγδ , dˆαβγδ , ĉαβγδ\nand dˆαβγδ\nare needed. Then, the stress\n,3\n,3\nand moment tensors and their corresponding tangents follow from step 4 in Tab. 2.\nFor the DD shell model, having derived τ αβ and cαβγδ specifically for each material model, one\ncan compute cαβγδ\n, M0αβ and f αβγδ from step 3 in Tab. 3.\n0\n12\n\n\fShell model\nNumerically-projected (NP)\nAnalytically-projected (AP)\nDirectly-decoupled (DD)\n\nRequired constitutive variables\nτ̃ αβ (25) and c̃αβγδ (30)\nτ̂ αβ (32.1), τ̂,3αβ (32.2) and ĉαβγδ , dˆαβγδ , ĉαβγδ\nand dˆαβγδ\n(35)\n,3\n,3\nαβγδ\nτ αβ (45), cαβγδ (46), c0\n(48) and f αβγδ (51)\n\nTable 4: Constitutive variables of the NP, AP and DD shell models\n\n4.1\n\nIsotropic models\n\nSoft biomaterials are commonly modeled with incompressible hyperelastic constitutive models\nthat have been introduced for rubber-like materials. Although soft tissues are constructed from\nelastin and collagen fibres, the anisotropic part might be neglected and a purely isotropic model\ncan be used. Examples are the modeling of liver, kidney, bladder and rectum, lungs, uterus,\netc. (Chagnon et al., 2015). This section discusses a few isotropic constitutive models that are\ncommonly used for biomaterials and soft tissues (Martins et al., 2006; Wex et al., 2015). Both\nkinds of constitutive laws, i.e. material models with polynomial and exponential forms of strain\nenergy functions, are included in the presented examples.\n4.1.1\n\nIncompressible Neo–Hooke (NH)\n\nThe incompressible Neo–Hookean (NH) model is the most common hyperelastic constitution\nfor rubber–like and soft biological materials. It is constructed from the first invariant of the\nright Cauchy–Green deformation tensor; therefore, it requires only one material constant to be\nset.\n4.1.1.1\n\nNP shell model\n\nThe strain-energy density function of a 3D incompressible Neo-Hookean solid is\n\u0010\n\u0011\n˜ = c̃1 I˜1 − 3 + p̃ g̃ ,\nW̃ (I˜1 , J)\n2\n\n(55)\n\nwhere c̃1 = µ̃ is the infinitesimal 3D shear modulus. The stress and elasticity tensors needed for\nthe projection/integration procedure in Eqs. (28) and (29) can be found in Duong et al. (2017)\nas\n\u0012\n\u0013\n1 αβ\n∂ W̃\nαβ\nαβ\n(56)\nτ̃ := 2\n= c̃1 G − ∗ g\n,\n∂gαβ\nJ2\n\u0011\n∂ τ̃ αβ\n2 c̃1 \u0010 αβ γδ\n(57)\nc̃αβγδ := 2\n= ∗\ng g − g αβγδ .\n∂gγδ\nJ2\nThe fourth-order tensors g αβγδ , aαβγδ and bαβγδ , which are used henceforth, are given in Appendix A.\n4.1.1.2\n\nAP shell model\n\nFrom Eqs. (32.1) and (56), we have\nτ̂ αβ\n\n\u0012\n\u0013\n1 αβ\nαβ\n= c̃1 A − 2 a\n.\nJ\n13\n\n(58)\n\n\fThus, from Eqs. (32.2) and (56), one can obtain\n\u0012\ni\u0013\n1 h αβ\nαβ\nαβ\nαβ\nτ̂,3 = 2 c̃1 B − 2 b + 2 (H − H0 ) a\n,\nJ\n\n(59)\n\nwhere H0 := 21 Aαβ Bαβ is the mean curvature on S0 . The linearization of kinematic variables\nw.r.t. the through-the-thickness coordinate ξ can be found in Appendix C. The corresponding\nmaterial tangents, defined in Sec. 3.2, are\n\u0011\n2 c̃1 \u0010\nĉαβγδ = 2 aαβ aγδ − aαβγδ ,\ndˆαβγδ = 0 ,\nJ\n(60)\n\u0011\n4 c̃1 \u0010 αβ γδ\nαβγδ\nαβγδ\nαβ γδ\nαβγδ\nαβγδ\nˆ\n+ 4 (H − H0 ) ĉ\n,\nd,3\n= −ĉ\n.\nĉ,3\n= 2 b a +a b\nJ\n4.1.1.3\n\nDD shell model\n\nThe 2D incompressible Neo–Hookean strain energy (e.g. Sauer et al., 2014; Sauer, 2016) is\n\u0001\nc1\nWM (I1 , J) =\nI1 − 2 + p g̃ ,\n(61)\n2\nwhere c1 = T c̃1 = µ/2 is physically related to the 2D shear modulus µ as µ = T µ̃. The in-plane\nstress components now are\n\u0012\n\u0013\n1 αβ\nαβ\nαβ\nτ = c1 A − 2 a\n,\n(62)\nJ\nwith\n\n\u0011\n2 c1 \u0010\n∂τ αβ\n= 2 aαβ aγδ − aαβγδ ,\n∂aγδ\nJ\n\ncαβγδ := 2\n\nwhere aαβγδ is given by Eq. (170). Correspondingly, in the reference configuration,\n\u0001\n:= 2 c1 Aαβ Aγδ + c1 Aαγ Aβδ + Aαδ Aβγ .\ncαβγδ\n0\n\n(63)\n\n(64)\n\nThus, the bending energy WB can be found by plugging Eq. (64) into Eq. (49), which gives the\nbending moment\n(65)\nM0αβ = f αβγδ (bγδ − Bγδ ) ,\nwhere\n\n\u0011i\n\u0010\nT2 h\n(66)\n.\n2 c1 Aαβ Aγδ + c1 Aαγ Aβδ + Aαδ Aβγ\n12\nRemark 4.1. As many material models, introduced in the following sections, are based on the\nfirst invariant of the right Cauchy–Green deformation tensor, their corresponding stress and\nelasticity tensors include expressions similar to an incompressible Neo-Hookean material. Thus,\nαβ\nfor the sake of simplicity, we introduce normalized τ̃NH\nand c̃αβγδ\nNH for the NP shell model by\nsetting c̃1 = 1 in Eqs. (56) and (57), which gives\nf αβγδ :=\n\nαβ\nτ̃NH\n:= Gαβ −\n\nc̃αβγδ\nNH := 2\n\n1\n∗\n\nJ2\n\ng αβ ,\n\nαβ\n\u0011\n∂ τ̃NH\n2 \u0010\n= ∗ g αβ g γδ − g αβγδ .\n∂gγδ\nJ2\n\n(67)\n(68)\n\nαβ\nFor the DD shell model, the normalized τNH\nand cαβγδ\nNH are derived by setting c1 = 1 in Eqs. (62)\nand (63), which yields\n1\nαβ\n(69)\nτNH\n:= Aαβ − 2 aαβ ,\nJ\n\n14\n\n\fcαβγδ\nNH := 2\n\nαβ\n\u0011\n∂τNH\n2 \u0010\n= 2 aαβ aγδ − aαβγδ .\n∂aγδ\nJ\n\nAccordingly, in the reference configuration,\n\u0010\n\u0011\nαβγδ\n= 2 Aαβ Aγδ + Aαγ Aβδ + Aαδ Aβγ\ncαβγδ\n:=\nc\nNH0\nNH\nS0\n\n\u0010\n\u0011\nαβ\nαβ\nand trivially τNH0\n:= τNH\n\nS0\n\n(70)\n\n(71)\n\n= 0.\n\nSimilarly, the normalized stresses and tangent tensors for the AP shell model are defined according to Eqs. (58), (59) and (60) as\nαβ\nτ̂NH\n\nĉαβγδ\nNH\ndˆαβγδ\nNH\n4.1.2\n\nαβ\n= τNH\n,\n\u0012\ni\u0013\n1 h αβ\nαβ\nαβ\nαβ\nτ̂NH,3 = 2 B − 2 b + 2 (H − H0 ) a\n,\nJ\n\u0011\n4 \u0010 αβ γδ\nαβγδ\nαβ γδ\n= cαβγδ\n,\nĉ\n=\nb\na\n+\na\nb\n+ 4 (H − H0 ) ĉαβγδ\nNH\nNH,3\nNH ,\nJ2\nαβγδ\n= 0,\ndˆαβγδ\nNH,3 = −ĉNH .\n\n(72)\n\n(73)\n\nIncompressible Mooney–Rivlin (MR)\n\nThe Mooney–Rivlin (MR) model is one of the oldest and most accurate constitutive laws developed for large deformations of isotropic materials (Martins et al., 2006; Wex et al., 2015). It is\nbased on the first and second invariants of the right Cauchy–Green deformation tensor, which\nrequires two material constants to be specified.\n4.1.2.1\n\nNP shell model\n\nThe incompressible 3D strain-energy density function of the Mooney–Rivlin type is\n\u0001 c̃1\nc̃2\nW̃ I˜1 , I˜2 , J˜ = (I˜1 − 3) + (I˜2 − 3) + p̃ g̃ ,\n2\n2\n\n(74)\n\nwhere c̃1 and c̃2 are stress-like parameters that should be found from experiments. The components of the Kirchhoff stress tensor thus are\n\u0011\n∗\n∗\nc̃2 \u0010\nαβ\nτ̃ αβ = c̃1 τ̃NH\n+ ∗ Gαβ − I1 g αβ + c̃2 J 2 g αβ ,\n(75)\nJ2\nwhich gives\n\u0010\n\u0011\n\u0001\n\u0001\n∗\n∗\n2 c̃2\nαβ γδ\nc̃αβγδ = c̃1 + c̃2 I1 c̃αβγδ\ng + g αβ Gγδ + 2 c̃2 J 2 g αβγδ + g αβ g γδ .\nNH − ∗2 G\nJ\n4.1.2.2\n\n(76)\n\nAP shell model\n\nFollowing Eq. (75),\nc̃2\n(Aαβ − I1 aαβ ) + c̃2 J 2 aαβ ,\nJ2 \u0010\n\u0011\nαβ\n= c̃1 τ̂NH,3\n+ c̃2 τ̂Iαβ + τ̂IIαβ ,\n\nαβ\nτ̂ αβ = c̃1 τNH\n+\n\nτ̂,3αβ\n\n15\n\n(77)\n\n\fwhere\n\ni\n1 h\nαβ\nαβ\nαβ\nαβ\nˆ1,3 aαβ ,\n4\n(H\n−\nH\n)\n(A\n−\nI\na\n)\n+\n2\n(B\n−\nI\nb\n)\n−\nI\n0\n1\n1\nJ2 \u0002\n\u0003\n= 2 J 2 bαβ − 2 (H − H0 ) aαβ .\n\nτ̂Iαβ =\nτ̂IIαβ\n\n(78)\n\nThus, the tangent tensors are dˆαβγδ = 0,\nc̃2\n(Aαβ aγδ + aαβ Aγδ ) + 2 c̃2 J 2 (aαβ aγδ + aαβγδ ) ,\nJ2\n\u0010\n\u0011\n\u0002\n\u0003\nαβ\nαβ\n= c̃1 ĉαβγδ\n+\n2\nc̃\nτ̂\n−\nτ̂\naγδ + 4 c̃2 J 2 bαβγδ + aαβ bγδ − 2 (H − H0 ) aαβγδ\n2\nNH,3\nII\nI\nh\ni\n\u0011\nc̃2 \u0010\n+ 4 2 I1 aαβ bγδ − Aαβ bγδ − aαβ B γδ − 2 (H − H0 ) aαβ + bαβ Aγδ\nJ\ni\nc̃2\nc̃2 h\n− 4 2 I1 bαβγδ − 2 2 4 (H − H0 ) I1 + Iˆ1,3 aαβγδ\nJ\nJ\n\nĉαβγδ = (c̃1 + c̃2 I1 ) cαβγδ\nNH − 2\nĉαβγδ\n,3\n\nand\n\n\u0010\n\u0011\nˆαβγδ − c̃2 I1 cαβγδ + 2 c̃2 Aαβ aγδ + aαβ Aγδ\ndˆαβγδ\n=\nc̃\nd\n1\n,3\nNH,3\nNH\nJ2\n\u0001\n2\nαβ\nγδ\nαβγδ\n− 2 c̃2 J a a + a\n.\n\n4.1.2.3\n\n(79)\n\n(80)\n\n(81)\n\nDD shell model\n\nFor this model, the incompressible 2D stored energy is\n\u0001 c1\nc2\nWM I˜1 , I˜2 , J˜ = (I˜1 − 2) + (I˜2 − 2) + p g̃ ,\n2\n2\nwhere c1 := T c̃1 and c2 := T c̃2 . Likewise to Eqs. (75) and (76), it can be shown that\n\u0011\nc2 \u0010\nαβ\n+ 2 Aαβ − I1 aαβ + c2 J 2 aαβ\nτ αβ = c1 τNH\nJ\n\n(82)\n\n(83)\n\nand\ncαβγδ = (c1 + c2 I1 ) cαβγδ\nNH −\n\n\u0001\n\u0001\n2 c2\nαβ γδ\nαβ γδ\n2\nαβγδ\nαβ γδ\nA\na\n+\na\nA\n+\n2\nc\nJ\na\n+\na\na\n.\n2\nJ2\n\nIn the reference configuration, I1 = 2, J = 1 and aαβ = Aαβ , thus\n\u0001\ncαβγδ\n= c1 + c2 cαβγδ\n0\nNH0 .\n4.1.3\n\n(84)\n\n(85)\n\nIncompressbile Fung\n\nThe strain energy function of this model has an exponential form in terms of the first invariant of\nthe Cauchy–Green deformation tensor. This model has been proposed first by Fung (1967) and\nwas then further investigated by Demiray (1972)4 . Later, Humphrey and Yin (1987) extended\nthe formulation by including an anisotropic contribution of fibers to model passive cardiac\ntissue.\n\n4\nIn literature, this model is mostly called as “Fung” model while it is also named “Fung–Demiray” (e.g. by\nWex et al., 2015) or “Demiray” model (e.g. by Gasser et al., 2006).\n\n16\n\n\f4.1.3.1\n\nNP shell model\n\nThe incompressible version of the Fung model is\nn\nh\ni\no\n˜ = c̃1 exp c2 (I˜1 − 3) − 1 + p̃ g̃ ,\nW̃ (I˜1 , J)\n2 c2\nwhich gives\nαβ\nτ̃ αβ = D̃1 τ̃NH\n,\n\u0010\n\u0011\nαβ γδ\nc̃αβγδ = D̃1 c̃αβγδ\n+\n2\nc\nτ̃\nτ̃\n,\n2\nNH\nNH NH\n\n(86)\n\n(87)\n(88)\n\nwith\nD̃1 :=\n\n4.1.3.2\n\nh\ni\n∂ W̃\n= c̃1 exp c2 (I˜1 − 3) .\n∂ I˜1\n\n(89)\n\nAP shell model\n\nPlugging Eq. (87) into Eq. (32), we have\nαβ\nτ̂ αβ = D̂1 τNH\n,\n\u0012\n\u0014\n\u0015\n\u0013\n4\nαβ\nαβ\nαβ\nτ̂,3 = D̂1 τNH,3 + c2 Iˆ1,3 + 2 (H − H0 ) τNH ,\nJ\n\nwhere Iˆ1,3 is given by Eq. (186) (see Appendix C) and\n\u0014 \u0012\n\u0013\u0015\n1\nD̂1 := c̃1 exp c2 I1 + 2 − 3\n.\nJ\nThe corresponding material tangents are dˆαβγδ = 0 and\n\u0011\n\u0010\nαβ γδ\n,\nτ\n+\n2\nc\nτ\nĉαβγδ = D̂1 cαβγδ\n2\nNH NH\nNH\n\u0014\n\u0012\n\u0013\n\u0015\n4\nαβγδ\nαβγδ\nγδ\nαβγδ\nˆ\n= D̂1 ĉNH,3 + c2 I1,3 + 2 (H − H0 ) cNH\nĉ,3\n+ 2 c2 τ̂,3αβ τNH\nJ\n\u0012\n\u0013\n\u0003\n1 \u0002\nαβ\nγδ\nγδ\nγδ\n+ 4 c2 D̂1 τNH B − 2 2 (H − H0 ) a + b\n,\nJ\nαβ γδ\ndˆαβγδ\n= D̂1 dˆαβγδ\n,3\nNH,3 − 2 c2 D̂1 τNH τNH .\n4.1.3.3\n\n(90)\n\n(91)\n\n(92)\n\nDD shell model\n\nThe corresponding membrane strain energy function is\nn\nh\ni\no\n˜ = c1 exp c2 (I˜1 − 3) − 1 + p g̃ ,\nWM (I˜1 , J)\n2 c2\nwhere c1 = T c̃1 . Similarly, we have\n\ncαβγδ\nand\nD1 :=\n\nαβ\nτ αβ = D1 τNH\n,\n\u0010\n\u0011\nαβ γδ\n= D1 cαβγδ\n+\n2\nc\nτ\nτ\n2\nNH\nNH NH\n\nh\ni\n∂WM\n= c1 exp c2 (I˜1 − 3) .\n∂ I˜1\n\n(93)\n\n(94)\n(95)\n(96)\n\nαβ\nIn the reference configuration, D1 = c1 and τNH0\n= 0, which results in\n\ncαβγδ\n:= c1 cαβγδ\n0\nNH0 .\n17\n\n(97)\n\n\f4.2\n\nAnisotropic models\n\nThe fibrous structure of soft tissues adds anisotropic features to their mechanical behavior.\nIn order to capture those, different anisotropic hyperelastic models are introduced here. The\nvarious isotropic material models, introduced in Sec. 4.1, depend on a combination of the first\n˜ Similarly,\nthree invariants of the Cauchy–Green deformation tensor, i.e. I˜1 , I˜2 and I˜3 := J.\nthe anisotropic constitutive laws introduced in this section depend on extra invariants, which\nare related to the principal direction of the fibers. The anisotropy can also be measured by\ncomponents of the Green–Lagrange tensor (Chagnon et al., 2015), which is not discussed here.\nConsidering that the principal direction of the ith family of fibers is L̃i in the reference configuration, the structural tensor M̃ i can be expressed according to the kinematics of Kirchhoff–Love\nshells as\n∗\n∗\n(98)\nM̃ i := L̃i ⊗ L̃i = M i + L33\ni N ⊗N .\nThe in-plane component of structural tensor is\n∗\n\n∗\n\n∗\n\n∗\n\nM i := Li ⊗ Li = Lαβ\ni Gα ⊗ Gβ ,\nwhere\n\n∗\n\n∗\n\nLi := Lαi Gα ,\n\n∗\n\n∗\n\n∗\n\nα β\nLαβ\ni := Li Li ,\n\n∗\n\nLαi := L̃i · Gα .\n\nThe out-of-plane component of structural tensor is then\n\u00012\n∗\nL33\n.\ni := L̃i · N\n\n(99)\n(100)\n\n(101)\n\nThe first invariant of the structural tensor, which is used for most anisotropic models, is5\n\u0001\n∗\n∗\n(102)\nI˜4i := tr C̃ M̃ i = L̃i · C̃ L̃i = I4i + g33 L33\ni ;\nhowever, other invariants can also be used (Chagnon et al., 2015). Likewise, the in-plane\ninvariant is defined as\n∗\n∗ \u0001\n∗\n∗ ∗\n∗\n∗\n(103)\nI4i := tr C M i = Li · C Li = gαβ Lαβ\ni .\nIn the same fashion, for the membrane formulation, the corresponding quantities are defined on\nthe shell mid-surface as\n(104)\nM̃ i = M i + L33\ni N ⊗N ,\nwhere\nM i := Li ⊗ Li = Li αβ Aα ⊗ Aβ ,\nLi := Lαi Aα ,\n\nα β\nLαβ\ni := Li Li\n\n(105)\n(106)\n\nand Lαi = Li · Aα . Thus, the invariants are reformulated as\nI˜4i = I4i + λ23 L33\ni ,\n\n(107)\n\n\u0001\nI4i := tr C M i = Li · CLi = aαβ Lαβ\ni .\n\n(108)\n\nwith\nRemark 4.2. For thin membrane\nand shells, it is more realistic\nto assume that fibers are\n∗\n∗\n33 = 0. This implies that I˜i = I i on each shell layer through\ndistributed layer-wise, i.e. L33\n=\nL\n4\n4\ni\ni\nthe thickness and I˜4i = I4i on the mid-surface. Here, for the examples shown in Sec. 5, it is\n∗\n33\nassumed that L33\ni = Li = 0.\n5\nSome scholars (e.g. Gasser et al., 2006) use I˜4 and I˜6 for I˜41 and I˜42 if two family of fibers are considered.\nHere, we use I˜4 if only one family of fibers is included and I˜4i for more families of fibers.\n\n18\n\n\fAnisotropic hyperelastic material models are mostly developed based on the assumption that\nthe material is constructed from an “isotropic” matrix reinforced with several fibers with a\ngiven principal orientation, which induce “anisotropy”. Hence, the strain energy function W̃ is\ncomposed of an isotropic part W̃m and an anisotropic part W̃f as\nW̃ = W̃m\n\nnf\n\u0001 X\n\u0001\n˜\n˜\n˜\nI1 , I2 , J +\nW̃f I˜1 , I˜2 , · · · , I˜4i , I˜5i , · · · ,\n\n(109)\n\ni=1\n\nwhere nf is the number of fiber families. The anisotropic part may only include the invariants of\nthe structural tensors, like in anisotropic Mooney–Rivlin model (Sec. 4.2.1), or it may combine\nthem with the invariants of the right Cauchy–Green deformation tensor, like in Gasser–Ogden–\nHolzapfel model (Sec. 4.2.2). For a detailed survey of anisotropic models for biological tissues,\nsee Chagnon et al. (2015).\n4.2.1\n\nAnisotropic Mooney–Rivlin (AMR)\n\nThe anisotropic Mooney–Rivlin (AMR) material model can be obtained by generalizing the\nformulation of Rivlin and Saunders (1951) in terms of the invariants of the structural tensor as\n(Kaliske, 2000)\n\u0001 X\n\u0001j\nW̃f I˜4i =\nc̃j I˜4i − 1 ,\n(i = 1, ..., nf ) .\n(110)\nj≥2\n\nIn this study, for the isotropic part, an incompressible Mooney–Rivlin constitution is considered\n(see Sec. 4.1.2). For the anisotropic part, nf families of fibers with a quadratic potential are\nincluded as\n\u0001\n\u0001\nW̃ = W̃m I˜1 , I˜2 , J˜ + W̃f I˜1 , I˜2 , ..., I˜nf\n4\n\n4\n\n4\n\nn\n\n=\n\nf\nX\n\u00012\nc̃1 ˜\nc̃2\n(I1 − 3) + (I˜2 − 3) +\nc̃3i I˜4i − 1 + p̃ g̃ .\n2\n2\n\n(111)\n\ni=1\n\n4.2.1.1\n\nNP shell model\n\nThe total Kirchhoff stress is\nαβ\nτ̃ αβ = τ̃m\n+ τ̃fαβ ,\n\nwhere\nτ̃fαβ := 2\n\nnf\nX\n\u0001∗\n∂ W̃f\n=2\nc̃3i I˜4i − 1 Lαβ\ni\n∂gαβ\n\n(112)\n\n(113)\n\ni=1\n\nαβ\naccording to Eq. (111) and τ̃m\nis given by Eq. (75). The total layer-wise elasticity tensor is\nαβγδ\nthen c̃αβγδ = c̃αβγδ\n+\nc̃\n,\nwhere\nc̃αβγδ\nis given by Eq. (76) and\nm\nm\nf\n\nc̃αβγδ\nf\n\n=4\n\nnf\nX\n\n∗\n\n∗\n\nγδ\nc̃3i Lαβ\ni Li .\n\n(114)\n\ni=1\n\n4.2.1.2\n\nAP shell model\n\nLikewise to Eq. (112), the total stress is split into the isotropic and anisotropic contributions as\nαβ\nτ̂ αβ = τ̂m\n+ τ̂fαβ ,\n\n19\n\n(115)\n\n\fαβ\nwhere τ̂m\nis given by Eq. (77.1) and\n\nτ̂fαβ = 2\n\nnf\nX\n\n\u0001\nc̃3i I4i − 1 Lαβ\ni .\n\n(116)\n\ni=1\n\nSimilarly, the first-order approximated terms are\nαβ\nαβ\nτ̂,3αβ = τ̂m,3\n+ τ̂f,3\n,\n\n(117)\n\nαβ\nwhere τ̂m,3\nfollows from Eq. (77.2) and\nαβ\nτ̂f,3\n\n=2\n\nnf\nX\n\nh\n\u0001 αβ i\ni\ni\n.\nLαβ\nc̃3i Iˆ4,3\ni + I4 − 1 L̂i,3\n\n(118)\n\ni=1\n\nˆi\nHere, L̂αβ\ni,3 and I4,3 are given by Eqs. (187) and (188), respectively. In the same fashion, the\nfibers and the matrix contribute to the corresponding tangents, e.g. ĉαβγδ = ĉαβγδ\n+ ĉαβγδ\n.\nm\nf\nαβγδ\nαβγδ\nαβγδ\nˆ\nˆ\nThe isotropic tensors ĉαβγδ\n,\nĉ\nand\nd\nare\ngiven\nby\nEqs.\n(79-81)\nand\nd\n=\n0.\nThe\nm\nm\nm,3\nm,3\nanisotropic tangents are then\n= 4\nĉαβγδ\nf\n\nnf\nX\n\nγδ\nc̃3i Lαβ\ni Li ,\n\n= 0,\ndˆαβγδ\nf\n\ni=1\n\nĉαβγδ\nf,3\n\n= 4\n\nnf\nX\n\nc̃3i\n\n\u0010\n\nLαβ\ni\n\nL̂γδ\ni,3\n\n+\n\nL̂αβ\ni,3\n\nLγδ\ni\n\n\u0011\n\ndˆαβγδ\nf,3\n\n,\n\n= −4\n\n(119)\nc̃3i Lαβ\ni\n\nLγδ\ni\n\n.\n\ni=1\n\ni=1\n\n4.2.1.3\n\nnf\nX\n\nDD shell model\n\nThe strain energy of the corresponding membrane formulation is\n\u0001\n\u0001\nWM = Wm I˜1 , I˜2 , J˜ + Wf I˜41 , I˜42 , ..., I˜4nf\nnf\nX\n\u00012\nc1 ˜\nc2 ˜\n= (I1 − 3) + (I2 − 3) +\nc3i I˜4i − 1 + p g̃ .\n2\n2\n\n(120)\n\ni=1\n\nAccordingly, the total stress is\nαβ\n+ τfαβ ,\nτ αβ = τm\n\n(121)\n\nαβ\nwhere τm\nis given by Eq. (83) and\n\nτfαβ\n\n=2\n\nnf\nX\n\n\u0001\nc3i I˜4i − 1 Lαβ\ni .\n\n(122)\n\ni=1\n\nThe corresponding tangent tensor is cαβγδ = cαβγδ\n+ cαβγδ\n. The isotropic part cαβγδ\nis defined\nm\nm\nf\nby Eq. (84) and the anisotropic part is\ncαβγδ\nf\n\n=4\n\nnf\nX\n\nγδ\nc3i Lαβ\ni Li ,\n\n(123)\n\ni=1\n\nwhich gives\n\nnf\nX\n\u0001 αβγδ\nγδ\ncαβγδ\n=\nc\n+\nc\nc\n+\n4\nc3i Lαβ\n1\n2\n0\ni Li .\nNH0\ni=1\n\n20\n\n(124)\n\n\f4.2.2\n\nGasser–Ogden–Holzapfel (GOH)\n\nThe Gasser–Ogden–Holzapfel (GOH) material model is an anisotropic hyperelastic material\nmodel, which is used to model soft tissues with distributed collagen fibers, and is mainly developed for the modeling of cardiovascular arteries (Gasser et al., 2006). This model is constructed\nof an isotropic part, which represents the elastin matrix of soft tissue and is modeled by the\nincompressible Neo-Hookean material model, and an anisotropic part due the collagen network,\nwhich is based on the structural tensors of two families of fibers. Here, a 3D generalized structural tensor (3D GST) is considered; however, for thin structures, 2D generalized structural\ntensors (2D GST) can also be used (Tonge et al., 2013).\n4.2.2.1\n\nNP shell model\n\nThe strain-energy density function is considered as\n\nwith\n\n˜ = W̃m (I˜1 ) + W̃f (J˜1 , J˜2 ) + p̃ g̃ ,\nW̃ (I˜1 , J˜41 , J˜42 , J)\n4\n4\n\n(125)\n\nµ̃ ˜\nI1 − 3) ,\n2\n2\nX\n\u0002\n\u0003\nk̃1i \b\nW̃f =\nexp k2i (J˜4i − 1)2 − 1 ,\n2 k2i\n\n(126)\n\nW̃m =\n\ni=1\n\nwhere\n\n\u0001\n∗\nJ˜4i = C̃ : H̃ i = κi I1 + g33 + (1 − 3 κi ) I˜4i ,\n\n(i = 1, 2) ,\n\n(127)\n\nare the invariants of the 3D generalized structural tensor H̃ i . H̃ i is introduced by Gasser et al.\n(2006) to extend the structural tensor M̃ i , cf. Eq. (98), by accounting for dispersion in fibers\nas\n(128)\nH̃ i := κi 1 + (1 − 3 κi ) M̃ i = κi 1 + (1 − 3 κi ) L̃i ⊗ L̃i ,\n(i = 1, 2) ,\nwhere κi ∈ [0, 1/3] is the parameter determining the degree of dispersion (Gasser et al., 2006)\n∗\nand 1 is the usual identity tensor in R3 . Assuming L33\ni = 0, the layer-wise 3D generalized\nstructural tensor becomes\n∗\n\n∗\n\nH̃ i = κi 1 + (1 − 3 κi ) Li ⊗ Li ,\n\n(i = 1, 2) .\n\n(129)\n\nSimilar to the other anisotropic models, the stress tensor has two components as\nαβ\nτ̃ αβ = τ̃m\n+ τ̃fαβ ,\n\n(130)\n\nαβ\nαβ\nτ̃m\n= µ̃ τ̃NH\n\n(131)\n\nwhere\nis the isotropic contribution. The anisotropic contribution of the fibers is then\nτ̃fαβ = 2\n\n2\nX\n\nẼi R̃iαβ ,\n\n(132)\n\ni=1\n\nwhere we have defined\nR̃iαβ :=\n\n∗\n∂ J˜4i\nαβ\n= κi τ̃NH\n+ (1 − 3 κi ) Lαβ\ni ,\n∂gγδ\n\n21\n\n(i = 1, 2)\n\n(133)\n\n\fand\nẼi :=\n\n\u0001\n\u0002\n\u0003\n∂ W̃f\n= k̃1i J˜4i − 1 exp k2i (J˜4i − 1)2 ,\ni\n˜\n∂ J4\n\n(i = 1, 2) .\n\n(134)\n\nFrom Eqs. (131) and (132), the symmetric elasticity tensor is\nc̃αβγδ\n\n2\n2\n\u0010\n\u0011\nX\nX\n∂ τ̃ αβ\nαβγδ\n:= 2\n= µ̃ + 2\nκi Ẽi c̃NH + 4\nD̃i R̃iαβ R̃iγδ ,\n∂gγδ\ni=1\n\n(135)\n\ni=1\n\nwith\nh\nh\n\u00012 i\n\u00012 i\n∂ Ẽi\ni\ni\n˜\n˜\n= k̃1i 1 + 2 k2i J4 − 1\nexp k2i J4 − 1\n,\nD̃i :=\n∂ J˜4i\n4.2.2.2\n\n(i = 1, 2) .\n\n(136)\n\nAP shell model\n\nαβ\nOn the shell mid-surface S, τ̂ αβ = τ̂m\n+ τ̂fαβ , where\nαβ\nαβ\nτ̂m\n= µ̃ τNH\n,\n\nτ̂fαβ = 2\n\n2\nX\n\nÊi Riαβ .\n\n(137)\n\ni=1\n\nHere, we have defined\nαβ\nRiαβ := κi τNH\n+ (1 − 3 κi ) Lαβ\ni ,\n\u0001\n\u0002\n\u0003\ni\ni\nÊi = k̃1i Jˆ − 1 exp k2i (Jˆ − 1)2\n4\n\n(138)\n(139)\n\n4\n\nand\nJˆ4i = κi (I1 + J −2 ) + (1 − 3 κi ) I4i ,\n\n(140)\n\n(i = 1, 2) ,\n\nwhich depend only on the mid-surface parameters. The first-order terms are then\n2 \u0010\n\u0011\nX\nαβ\nαβ\nαβ\nαβ\ni\n=2\nÊi Ri,3\n+ D̂i Jˆ4,3\nRiαβ ,\n,\nτ̂f,3\nτ̂m,3\n= µ̃ τ̂NH,3\n\n(141)\n\ni=1\n\nwhere we have defined\ni = κ (Iˆ + 4 H J −2 ) + (1 − 3 κ ) Iˆi ,\nJˆ4,3\ni 1,3\ni 4,3\n\n(142)\n\nαβ\nαβ\nR̂i,3\n:= κi τ̂NH,3\n+ (1 − 3 κi ) L̂αβ\ni,3\n\n(143)\n\nand\nD̂i :=\n\nh\nh\n\u00012 i\n\u00012 i\n∂ Êi\ni\ni\nˆ\nˆ\n=\nk̃\n1\n+\n2\nk\nJ\n−\n1\nexp\nk\nJ\n−\n1\n,\n1i\n2i\n2i\n4\n4\n∂ Jˆi\n\n(i = 1, 2) ,\n\n(144)\n\n4\n\nˆi\non S. Further, Iˆ1,3 , L̂αβ\ni,3 and I4,3 are given by Eqs. (186), (187) and (188), respectively (See\nAppendices C and D).\nThe components of the anisotropic tangent tensors are dˆαβγδ\n= 0 and\nf\nĉαβγδ\nf\n\n= 2\n\n2\nX\n\nκi Êi cαβγδ\nNH\n\n+4\n\ni=1\n\nĉαβγδ\n= 4\nf,3\n\n2\nX\n\nD̂i Riαβ Riγδ ,\n\ni=1\n\n2 \u0010\nX\n\nαβ γδ\nγδ\ni\nD̂i R̂i,3\nRi + F̂i Jˆ4,3\nRiαβ Riγδ + D̂i Riαβ Jai\n\ni=1\n\n+ 2\ndˆαβγδ\n= 2\nf,3\n\n2\nX\n\n\u0011\n(145)\n\nκi\n\ni=1\n2 \u0010\nX\n\n\u0010\n\nÊi ĉαβγδ\nNH,3\n\n+\n\ni\nD̂i Jˆ4,3\ncαβγδ\nNH\n\nγδ\nD̂i Riαβ Jbi\n+ κi Êi dˆαβγδ\nNH,3\n\ni=1\n\n22\n\n\u0011\n\n\u0011\n\n.\n\n,\n\n\fHere, we have defined\nF̂i :=\n\nh\n\u00012 i\n\u00012 i\n\u0001h\n∂ D̂i\ni\ni\ni\nˆ\nˆ\nˆ\nJ\n−\n1\nJ\n−\n1\nexp\nk\nJ\n−\n1\n3\n+\n2\nk\n=\n2\nk̃\nk\n2i\n2i\n1i 2i\n4\n4\n4\n∂ Jˆi\n\n(146)\n\n\u0014\ni\n\u0011\u0015\n∂ Jˆ4,3\n1 \u0010 αβ\nαβ\nαβ\n:=\n+ (1 − 3 κi ) L̂αβ\n= 2 κi B − 2 b + 2 H a\ni,3 ,\n∂aγδ\nJ\ni\n∂ Jˆ4,3\nαβ\n:=\n= −2 κi τ̂NH\n− 2 (1 − 3 κi ) Lαβ\ni .\n∂bγδ\n\n(147)\n\n4\n\nand\nγδ\nJai\nγδ\nJbi\n\n4.2.2.3\n\nDD shell model\n\nFor the membrane part, the projected membrane formulation of Roohbakhshan et al. (2016) is\nadopted. In this setup, the generalized structural tensor H̃ i , is defined on the shell mid-surface\nas\n\u0001\n(148)\nH̃ i := κi 1 + 1 − 3 κi Li ⊗ Li ,\n(i = 1, 2) ,\nwhere Li = Lαi Aα ans Lαi = Li · Aα . Thus, the first invariant of the generalized structural\ntensor is\n(149)\nJ˜4i = C̃ : H̃ i = κi (I1 + λ23 ) + (1 − 3 κi ) I4i ,\n(i = 1, 2) .\nαβ\nSimilar to the other shell models, the in-plane Kirchhoff stress is split as τ αβ = τm\n+τfαβ , where\nαβ\nαβ\nand\nτm\n= µ τNH\n2\nX\n(150)\nEi Riαβ .\nτfαβ = 2\ni=1\n\nHere,\n\nRiαβ\n\nis given by Eq. (138) and\nEi :=\n\n\u0001\n\u0002\n\u0003\n∂WM\n= k1i J˜4i − 1 exp k2i (J˜4i − 1)2 ,\ni\n˜\n∂ J4\n\n(i = 1, 2) .\n\n(151)\n\n2\n2\n\u0010\n\u0011\nX\nX\n∂τ αβ\n= µ+2\nκi Ei cαβγδ\n+\n4\nDi Riαβ Riγδ ,\nNH\n∂aγδ\n\n(152)\n\nThe total elasticity tensor is then\ncαβγδ := 2\n\ni=1\n\ni=1\n\nwhere we have defined\nh\ni\n\u0002\n\u0003\nDi := k1i 1 + 2 k2i (J˜4i − 1)2 exp k2i (J˜4i − 1)2 .\n\n(153)\n\nIn the reference configuration, I4i = 1, which gives Di = k1i , and Rαβ = (1 − 3 κi ) Lαβ\ni . Further,\nJ˜4i = 1, which results in Ei = 0. Hence, considering Eqs. (65) and (66), we get\ncαβγδ\n:= µ cαβγδ\n0\nNH0 + 4\n\n2\nX\n\nγδ\nk1i (1 − 3 κi )2 Lαβ\ni Li .\n\ni=1\n\n23\n\n(154)\n\n\f4.2.3\n\nGOH model with compression/tension switch\n\nThe anisotropic strain energy density function of Eq. (126.2) is polyconvex if I˜4i > 1 (Balzani\net al., 2006; Prot et al., 2007). This issue is also addressed by Gasser et al. (2006) who argue\nthat their model will predict non-physical behavior if I˜4i < 1. This is due to the fact that the\nfibers bear no compressive force; therefore, they are active only if being extended. Thus, such\nmodels can be equipped with a compression/tension switch to exclude the compressed fibers.\nFor instance, the GOH model should be modified as (cf. Melnik et al., 2015)\nW̃f =\n\n2\nX\n\n∗\n\nHi\n\ni=1\n\n\u0002\n\u0003\nk̃1i \b\nexp k2i (J˜i4 − 1)2 − 1 ,\n2 k2i\n\n∗\n\n(155)\n\n∗\n\nwhere on the shell layer S, the compression/tension switch Hi is formulated by the Heaviside\nstep function\n\u001a\n1, x>0,\nH(x) :=\n(156)\n0, x≤0\nas\n\u0001\n∗\n∗\n∗\n(157)\nHi = Hi (gαβ ) = H I4i − 1 .\nIn the same fashion, for the membrane constitution, we have\nWf =\n\n2\nX\ni=1\n\nHi\n\n\u0003\n\u0002\nk1i \b\nexp k2i (J˜i4 − 1)2 − 1 ,\n2 k2i\n\nin which, the compression/tension switch Hi is defined on the shell mid-surface as\n\u0001\nHi = Hi (aαβ ) = H I4i − 1 .\n\n(158)\n\n(159)\n\nRemark 4.3. As discussed by Holzapfel and Ogden (2015), the compression/tension switch has\n∗\nalso been implemented in terms of I˜4i instead of I4i or I4i . But this may give erroneous results.\nIn the following, the application of the compression/tension switch to the GOH material model\nis discussed for the three introduced shell models.\n4.2.3.1\n\nNP shell model\n\nIn this approach, one can directly plug the switch definition (156) into the strain energy density\nfunction (155). Similarly, the in-plane stress and elasticity tensors should be augmented with\nthe compression/tension switch as\nτ̃fαβ = 2\n\n2\nX\n\n∗\n\nHi Ẽi R̃iαβ\n\n(160)\n\ni=1\n\nand\nc̃αβγδ := 2\n\n2\n2\n\u0010\n\u0011\nX\nX\n∗\n∗\n∂ τ̃ αβ\nHi D̃i R̃iαβ R̃iγδ .\n= µ̃ + 2\nκi Hi Ẽi c̃αβγδ\n+\n4\nNH\n∂gγδ\ni=1\n\n(161)\n\ni=1\n\nHowever, the numerical integration should be performed more carefully to assure that there are\nenough number of Gaussian quadrature points in the locations the switch is active.\n24\n\n\f4.2.3.2\n\nAP shell model\n\nIf the compression/tension switch is considered, the anisotropic stresses might be non-zero only\nwithin a portion of the shell thickness; however, the isotropic stresses are non-zero across the\nwhole thickness. Thus, for the isotropic part, one can use the formulation introduced in Sec. 3.2.1\nwith the corresponding stress and tangent tensors given in Sec. 4.2.2.2. For the anisotropic part,\na partially-stressed shell formulation (see Sec. 3.2.2) is required. Hence, the stress and moment\ntensors due to the fibers are\nτfαβ\nαβ\nMf0\n\n=\n\n=\n\n2 \u001a\u0010\nX\ni=1\n2\nX\ni=1\n\nT2i\n\n−\n\nT1i\n\n\u0011\n\nτ̂fiαβ\n\n\u00012 i αβ\n1 h i \u00012\n+\nT2 − T1i\nτ̂fi,3\n2\n\n\u001b\n,\n(162)\n\n\u001a h\n\u001b\n\u0001\n\u0001 i αβ 1 h i \u00013\n\u0001 i αβ\n1\ni 2\ni 2\ni 3\nT2 − T1\nτ̂fi +\nT2 − T1\nτ̂fi,3 ,\n2\n3\n\nwhere we have defined\nτ̂fiαβ := 2 Êi Riαβ ,\n\n\u0011\n\u0010\nαβ\nαβ\ni Rαβ\n,\n+ D̂i Jˆ4,3\nτ̂fi,3\n:= 2 Êi Ri,3\ni\n\ni = 1, 2 ,\n\n(163)\n\naccording to Eqs. (137.2) and (141.2). As the anisotropic part is partially-stressed, one needs to\nfind the thickness interval [T1i , T2i ] ∈ [−T /2, T /2], where I˜4i > 1. The algorithm to find T1i and\nT2i is given in Appendix D. Likewise to the stress and moment tensors, the material tangents\nare also derived following the formulation of Sec. 3.2.2. For instance,\ncαβγδ\nf\n\n2 \u001a\u0010\nX\n\n\u00012 i αβγδ\n1 h i \u00012\n=\n−\n+\nT2 − T1i\nĉfi,3\n2\ni=1\n2 \u0010\n\u0011\nX\nαβ γδ\nαβ γδ\n+ 2\nτ̃2i\nU2i − τ̃1i\nU1i ,\nT2i\n\nT1i\n\n\u0011\n\nĉαβγδ\nfi\n\n\u001b\n(164)\n\ni=1\nαβ\nαβ\nwhere ĉαβγδ\nand ĉαβγδ\nfi\nfi,3 are given by Eq. (145); τ̃1i and τ̃2i are defined according to Eq. (41)\nαβ\nαβ\nand U1i\nand U2i\ncan be found in Appendix D. The other material tangent tensors can be\nderived similarly.\n\n4.2.3.3\n\nDD shell model\n\nThe compression/tension switch is not fully consistent with the DD shell model since the assumptions made to derive Eqs. (47) and (49) are not necessarily valid if the switch is applied.\nIn fact, as the material model is no longer symmetric w.r.t. the shell mid-surface, due to the unsymmetric structure of the switch, the expressions with mixed term, i.e. Eαβ Kγδ , do not vanish,\ne.g. in Eq. (47). Hence, the membrane and bending strains cannot be fully decoupled. Nonetheless, if the directly-decoupled approach is followed, in the reference configuration, I4i = 1, which\nimplies that Hi = 0. Thus, the anisotropic part do not contribute to the bending energy and this\nreduces Eqs. (154) to a purely isotropic formulation, i.e. cαβγδ\n= µ cαβγδ\n0\nNH0 . Put differently, the\ndirectly-decoupled approach cannot capture the effects of the compression/tension switch if the\nbending moments are dominant or the anisotropic forces are much stronger than the isotropic\nones; however, it is accurate if only the membrane forces are influential (Roohbakhshan et al.,\n2016).\n\n25\n\n\f5\n\nNumerical examples\n\nIn this section, for each of the introduced material models, different numerical examples are\nconsidered to study the performance of the three presented shell models, i.e. the numericallyprojected (NP), analytically-projected (AP) and directly-decoupled (DD) shell models. First, a\nuniaxial tension test is performed to compare the membrane response of different shell models.\nSecond, the pure bending of a cantilever subjected to a given rotation on its free end is considered, which shows how the models behave if the bending forces are dominant. Third, a square\nplate under pressure is studied to examine the coupled membrane and bending modes. Then,\nthe formulation is tested for two specific applications: Large indentation of a strip under a rigid\nspherical indenter and an angioplasty example, which involves contact between two deformable\nbodies.\nFor the NP shell model, the through-the-thickness integration is evaluated by two Gaussian\nquadrature points unless specified otherwise. Furthermore, for all the examples, the material\nconstants are set according to the Tab. 5. For the anisotropic material models (GOH and\nAMR), two families of fibers are considered, i.e. nf = 2. For the GOH material model, κi ∈\n{0.0, 0.226, 1/3} following Gasser et al. (2006).\nNH\nMR\nFung\nAMR\nGOH\n\nc̃1 = 10 [kPa]\nc̃1 = 10 [kPa]\nc̃1 = 10 [kPa]\nc̃1 = 10 [kPa]\nµ̃ = 10 [kPa]\n\nc̃2 = 2 c̃1 [kPa]\nc2 = 10\nc̃2 = 2 c̃1 [kPa]\nk̃1i = 100 c̃1 [kPa]\n\nc̃3 = 100 c̃1 [kPa]\nk2i = 500\n\nTable 5: Material constants of the considered material models\n\n5.1\n\nUniaxial tension test\n\nTo examine the presented shell models for the case that membrane forces are dominating, a\nrectangular strip of T × W × L = 0.3 × 3 × 9 [mm3 ] is pulled as shown in Fig. 1.a. On the\npulled edged, the displacements in e2 direction are enforced to be equal. The pulling force F\nis applied at the corner of the same edge. The strip is meshed by 6 × 18 quadratic NURBS\nelements (see Fig. 1.a).\n\n(a)\n\n(b)\n\nFigure 1: Uniaxial tension test: (a) Reference configuration with boundary conditions and (b)\ndeformed configuration for the GOH model (with κi = 0) colored by I1 := tr C.\n\n26\n\n\fFor the anisotropic materials, the principal directions of fibers are defined as\nL̃i = sin θi e1 + cos θi e2 ,\n\n(165)\n\n(i = 1, 2) ,\n\nwhere e1 and e2 are the unit vectors of the Cartesian coordinate system (shown in Fig. 1.a).\nFor this example, θ1 , θ2 = ±45◦ and ±30◦ for\nthe AMR and GOH models, respectively. As\n∗\n33\nalready mentioned, here it is assumed that L33\ni = Li = 0. Henceforth, the displacements in\ne1 , e2 and e3 directions are denoted by u, v and w, respectively.\nFig. 2 shows the displacement of point A (shown in Fig. 1.a) versus the applied total force.\nThe applied force is normalized by EA, where E = 3 c̃1 corresponds to an infinitesimal Young’s\nmodulus and A = W T is the cross section area. As expected, for all the isotropic and anisotropic\nmaterials, the AP and DD shell models give exactly the same results as the NP shell model.\n\n(a)\n\n3.0\n2.0\n1.0\n0.0\n0.0\n\n0.5\n\n1.0\n1.5\nForce [EA]\n\n2.0\n\n(b)\n\n2.4\n\nvA (NP)\nvA (AP)\nvA (DD)\n\nDisplacement [L]\n\n4.0\n\n0.4\n\n3.0\nvA (NP)\nvA (AP)\nvA (DD)\n\nDisplacement [L]\n\nDisplacement [L]\n\n5.0\n\n1.8\n1.2\n0.6\n0.0\n0.0\n\n0.5\n\n1.0\n1.5\nForce [EA]\n\n(d)\n\n0.3\n\n0.4\n\nvA (NP)\nvA (AP)\nvA (DD)\n\nDisplacement [L]\n\nDisplacement [L]\n\n0.4\n\n0.2\n0.2\n0.1\n0.0\n0.0\n\n2.5\n\n5.0\n7.5\nForce [EA]\n\n10.0\n\n(e)\n\n2.0\n\nvA (NP)\nvA (AP)\nvA (DD)\n\n0.3\n0.2\n\n0.3\n0.2\n0.1\n0.0\n0.0\n\n0.5\n\n1.0\n1.5\nForce [EA]\n\n2.0\n\n5i = 1=3\n\n5i = 0:226\n\n0.1\n0.0\n\n(c)\n\nvA (NP)\nvA (AP)\nvA (DD)\n\n5i = 0:0\n0\n\n25\n\n50\n75\nForce [EA]\n\n100\n\nFigure 2: Uniaxial tension test – the displacement of the tip vs. the applied force: (a) NH, (b)\nMR, (c) Fung, (d) AMR and (e) GOH material model for the three constitutive approaches\n(NP, AP and DD) presented in Sec. 3.\n\n5.2\n\nCantilever bending\n\nThe cantilever has the same geometry and mesh properties as the strip of Sec. 5.1 although\nhere T = W/20. On the clamped edge (see Fig. 3.a), the rotations are restricted following a\npenalty formulation. On the free end, the surface normal n is constrained to be equal to the\ngiven normal n̄ using the constraint of Duong et al. (2017). Here, n̄ = cos α e3 − sin α e2 , where\nα is the angle of rotation around e1 . In the reference configuration, n̄ = N and α = 0 (see\nFig. 3.a). Here, the maximum rotation is set to α = 90◦ (see Fig. 3.b).\nThe total bending moment corresponding to this rotation is determined following the constraint\nformulation. The corresponding bending moment is normalized by E I/L, where I = W T 3 /12\nis the second moment of area of the cross section. The orientation of the fibers is defined based\non Eq. (165). Here, θ1 , θ2 = ±45◦ for the AMR model and θ1 , θ2 = ±30◦ for the GOH model.\n\n27\n\n\f(a)\n\n(b)\n\nFigure 3: Cantilever bending test: (a) Reference configuration with boundary conditions and\n(b) deformed configuration for the GOH model (with κi = 0.226 and the compression/tension\nswitch) colored by I1 := tr C.\n\nIn Figs. 4 and 5, the corresponding bending moment is plotted against the applied rotation.\nSimilar to the previous example, for the Neo–Hookean, Mooney–Rivlin and Fung material models, which are isotropic, as well as for the anisotropic Mooney–Rivlin material model, the AP\nand DD shell models are as accurate as the NP shell model (see Fig. 4).\n\n1.2\n0.8\n0.4\n0.0\n\n0\n\n(a)\n\n15\n\n30 45 60\nRotation, ,/\n\n75\n\n90\n\n4.8\n\nMoment [EI=L]\n\n1.6\n\n2.0\nNP\nAP\nDD\n\n3.6\n2.4\n1.2\n0.0\n\n0\n\n(b)\n\n15\n\n30 45 60\nRotation, ,/\n\n75\n\n90\n\n45.0\nNP\nAP\nDD\n\n1.6\n\nMoment [EI=L]\n\n6.0\nNP\nAP\nDD\n\nMoment [EI=L]\n\nMoment [EI=L]\n\n2.0\n\n1.2\n0.8\n0.4\n0.0\n\n0\n\n(c)\n\n15\n\n30 45 60\nRotation, ,/\n\n75\n\n90\n\nNP\nAP\nDD\n\n36.0\n27.0\n18.0\n9.0\n0.0\n\n0\n\n(d)\n\n15\n\n30 45 60 75\nRotation, ,/\n\n90\n\nFigure 4: Cantilever bending test – the corresponding bending moment vs. the applied rotation:\n(a) NH, (b) MR, (c) Fung and (d) AMR material model for the three constitutive approaches\n(NP, AP and DD) presented in Sec. 3.\n\nFig. 5 shows the results for the Gasser–Ogden–Holzapfel material model. If the compression/tension switch is excluded (see Figs. 5.a-5.c), all the three introduced shell models behave\nvery similarly. In this case, for the NP shell model, 2 Gaussian quadrature points are sufficient\nto evaluate the integration through the shell thickness. However, if the compression/tension\nswitch is included (see Figs. 5.d-5.f), the DD shell model cannot capture the switch effect since\nthe material model is no longer symmetric w.r.t. the shell mid-surface. Although if the material\nmodel is completely isotropic (i.e. setting κi = 1/3), the fibers are excluded and trivially the\nswitch has no effect on the constitutive equations (see Fig. 5.f). By increasing the anisotropy\n(i.e. κi → 0), the AP and NP shell models behave very similarly although the DD shell model\ndeviates from the correct solution (see Figs. 5.d-5.e).\nIf the compression/tension switch is included, more Gaussian quadrature points are needed\nto capture the discontinuity of switch through the shell thickness. For the results shown in\nFigs. 5.d-5.f, 5 quadrature points are used, which is computationally more expensive compared\n28\n\n\f0.9\n0.6\n0.3\n0\n\n15\n\n30 45 60\nRotation, ,/\n\n75\n\n90\n\n(d)\n\n1.0\n0.5\n0\n\n15\n\n30 45 60\nRotation, ,/\n\n75\n\n90\n\n4.5\n3.0\n1.5\n0\n\n15\n\n30 45 60\nRotation, ,/\n\n75\n\n90\n\n(e)\n\n1.2\n0.8\n0.4\n0\n\n15\n\n30 45 60\nRotation, ,/\n\n75\n\n90\n\n30 45 60\nRotation, ,/\n\n75\n\n90\n\n2.0\nNP\nAP\nDD\n\n4.0\n3.0\n2.0\n1.0\n0.0\n\nNP\nAP\nDD\n\n1.6\n\n0.0\n\n(c)\n\nMoment [EI=L]\n\n6.0\n\n0.0\n\n1.5\n\n5.0\nNP\nAP\nDD\n\nMoment [EI=L]\n\nMoment [EI=L]\n\n7.5\n\n2.0\n\nNP\nAP\nDD\n\n2.0\n\n0.0\n\n(b)\n\n#101\n\nMoment [EI=L]\n\n2.5\n\nNP\nAP\nDD\n\n1.2\n\n0.0\n\n(a)\n\n#102\n\nMoment [EI=L]\n\nMoment [EI=L]\n\n1.5\n\n0\n\n15\n\n30 45 60\nRotation, ,/\n\n75\n\n90\n\n(f)\n\nNP\nAP\nDD\n\n1.6\n1.2\n0.8\n0.4\n0.0\n\n0\n\n15\n\nFigure 5: Cantilever bending test for the GOH material model – the corresponding bending\nmoment vs. the applied rotation for the three constitutive approaches NP, AP and DD: (a-c)\nwithout and (d-f) with the compression/tension switch. Further, for (a) and (d) κi = 0.0, for\n(b) and (e) κi = 0.226 and for (c) and (f) κi = 1/3.\n\nto the cases that no switch is considered. This issue is further investigated in Fig. 6, which\nshows how the NP shell model approaches the AP shell model by increasing the number of\nGaussian quadrature points.\n0.8\n\n9.0\nNP (ngp = 2)\nNP (ngp = 3)\nNP (ngp = 5)\nAP\n\n6.0\n\n0.7\n\nRel. err (M )\n\nMoment [EI=L]\n\n7.5\n\n4.5\n3.0\n1.5\n\n(a)\n\n0.0\n\n0.5\nNP (ngp = 2)\nNP (ngp = 3)\nNP (ngp = 5)\n\n0.4\n0.3\n0.1\n\n0\n\n15\n\n30 45 60\nRotation, ,/\n\n75\n\n90\n\n(b)\n\n0.0\n\n0\n\n15\n\n30 45 60\nRotation, ,/\n\n75\n\n90\n\nFigure 6: Cantilever bending test for the GOH material model (with κi = 0.0 and the compression/tension switch): The corresponding bending moment (a) and its error (b) vs. the applied\nrotation for the NP and AP shell model with different number of Gaussian quadrature points\nngp considered across the thickness.\n\nFurthermore, as the shell thickness decreases, the AP shell model becomes more accurate.\nFig. 7.a shows the displacement of the tip versus the applied rotation for different thicknessto-width (T /W ) ratios. Fig.7.c shows how the corresponding bending moments change. Here,\nfor all the cases modeled by the NP shell model, 5 Gaussian quadrature points are considered\nthrough the shell thickness. Fig. 7.b and 7.d show the corresponding relative errors evaluated\nw.r.t. the solution of the NP shell model. As can be observed, the AP shell model becomes\ninaccurate for thick shells; however, such shells are not covered by the Kirchhoff-Love hypothesis.\n29\n\n\f6.0\n4.5\n\n0.5\n0.3\n0.1\n\n0.0\n\n-0.1\n\n(a)\n\n30 45 60\nRotation, ,/\n\n75\n\n90\n\n0\n\n(b)\n\n15\n\n30 45 60\nRotation, ,/\n\n#101\nT =W\nT =W\nT =W\nT =W\nT =W\nT =W\n\n0.8\n\n0.7\n\n1.5\n15\n\n0.9\n\nT =W = 0:1\nT =W = 0:05\nT =W = 0:01\n\n0.9\n\n3.0\n\n0\n\n#10!2\n\n75\n\n90\n\n0.6\n0.5\n\n18.0\n= 0:1 (NP)\n= 0:1 (AP)\n= 0:05 (NP)\n= 0:05 (AP)\n= 0:01 (NP)\n= 0:01 (AP)\n\n0.3\n0.2\n0.0\n\n#10!2\nT =W = 0:1\nT =W = 0:05\nT =W = 0:01\n\n15.0\n\nRel. err (M )\n\n7.5\n\n1.1\n= 0:1 (NP)\n= 0:1 (AP)\n= 0:05 (NP)\n= 0:05 (AP)\n= 0:01 (NP)\n= 0:01 (AP)\n\nRel. err (wA )\n\nDisplacement (wA ) [L]\n\nT =W\nT =W\nT =W\nT =W\nT =W\nT =W\n\nMoment [EI=L]\n\n#10!1\n\n9.0\n\n12.0\n9.0\n6.0\n3.0\n\n0\n\n(c)\n\n15\n\n30 45 60\nRotation, ,/\n\n75\n\n90\n\n0.0\n\n0\n\n(d)\n\n15\n\n30 45 60 75\nRotation, ,/\n\n90\n\nFigure 7: Cantilever bending test for the GOH material model (with κi = 0.0 and the compression/tension switch): Comparison of the NP and AP shell models with different thickness to\nwidth ratios (T /W ).\n\n5.3\n\nA clamped plate under pressure\n\nLarge deformation of a clamped plate under live pressure is a challenging example. Such an\nexample is used here to compare the capabilities of the three introduced shell models to capture\nthe membrane and bending forces together. As shown in Fig. 8.a, a square plate, with T ×L×L =\n0.25 × 10 × 10 [mm3 ], is clamped with appropriate boundary conditions. As the problem is\nsymmetric, only 1/4 of the whole system is modeled and symmetry constraints are applied\nalong the corresponding boundaries. On the clamped and symmetry edges, the rotations are\nfixed following the constraint formulation of Duong et al. (2017). The plate quarter is meshed\nby 6 × 6 quadratic NURBS-based elements. Furthermore, for both the anisotropic material\nmodels, the fibers are oriented according to Eq. (165) with θ1 , θ2 = ±45◦ .\n\n(a)\n\n(b)\n\nFigure 8: Clamped plate under pressure: (a) Reference configuration (1/4 system) with boundary conditions and (b) deformed configuration (full system) for the GOH model (with κ = 1/3\nand the compression/tension switch) colored by I1 := tr C.\n\nFigs. 9 and 10 represent the deflection of the mid point A (shown in Fig. 8.a) under the applied\nlive pressure. As expected, all the three presented shell models predict similar displacements.\nFurther, the results for the Gasser–Ogden–Holzapfel material model with and without the compression/tension switch are shown in Figs. 10.a-10.c and Figs. 10.d-10.f, respectively. Here, for\nthe cases modeled by the NP shell model, 3 Gaussian quadrature points are considered across\nthe thickness if the compression/tension switch is excluded and 5 Gaussian quadrature points\nare used if the switch is included.\n30\n\n\f0.50\n\n0.40\n\n0.40\n\n0.40\n\n0.40\n\n0.30\n0.20\nwA (NP)\nwA (AP)\nwA (DD)\n\n0.10\n0.00\n0.0\n\n0.3\n\n(a)\n\n0.6 0.9 1.2 1.5\nPressure [E] #10!2\n\n0.30\n0.20\nwA (NP)\nwA (AP)\nwA (DD)\n\n0.10\n0.00\n0.0\n\n(b)\n\n1.0\n\n2.0 3.0 4.0 5.0\nPressure [E] #10!2\n\n0.30\n0.20\nwA (NP)\nwA (AP)\nwA (DD)\n\n0.10\n0.00\n0.0\n\n(c)\n\n1.2\n\nDisplacement [L]\n\n0.50\n\nDisplacement [L]\n\n0.50\n\nDisplacement [L]\n\nDisplacement [L]\n\n0.50\n\n2.4 3.6 4.8 6.0\nPressure [E] #10!2\n\n0.30\n0.20\nwA (NP)\nwA (AP)\nwA (DD)\n\n0.10\n0.00\n0.0\n\n(d)\n\n0.3\n\n0.6 0.9 1.2\nPressure [E]\n\n1.5\n\n0.50\n\n0.24\n\n0.40\n\n0.40\n\n0.18\n0.12\n0.06\n0.00\n0.0\n\n(d)\n\n1.0\n\n2.0 3.0 4.0 5.0\nPressure [E] #10!1\n\n0.30\n0.20\n0.10\n0.00\n0.0\n\n(b)\n\nwA (NP)\nwA (AP)\nwA (DD)\n\n1.0\n\n2.0 3.0 4.0 5.0\nPressure [E] #10!1\n\n0.30\n0.20\n\n0.20\n\n0.30\n\n0.50\n\n0.16\n\n0.24\n\n0.40\n\n0.12\n0.08\nwA (NP)\nwA (AP)\nwA (DD)\n\n0.04\n0.00\n0.0\n\n1.0\n\n2.0 3.0 4.0 5.0\nPressure [E] #10!2\n\n(e)\n\n0.18\n0.12\nwA (NP)\nwA (AP)\nwA (DD)\n\n0.06\n0.00\n0.0\n\n1.0\n\n2.0 3.0 4.0 5.0\nPressure [E] #10!2\n\n(f)\n\nwA (NP)\nwA (AP)\nwA (DD)\n\n0.10\n0.00\n0.0\n\n(c)\n\nDisplacement [L]\n\nDisplacement [L]\n\n(a)\n\nwA (NP)\nwA (AP)\nwA (DD)\n\nDisplacement [L]\n\n0.50\n\nDisplacement [L]\n\n0.30\n\nDisplacement [L]\n\nDisplacement [L]\n\nFigure 9: Clamped plate under pressure – the displacement of the middle point vs. the applied\npressure: (a) NH, (b) MR, (c) Fung and (d) AMR material model for the three constitutive\napproaches (NP, AP and DD) presented in Sec. 3.\n\n0.2\n\n0.4 0.6 0.8 1.0\nPressure [E] #10!1\n\n0.30\n0.20\nwA (NP)\nwA (AP)\nwA (DD)\n\n0.10\n0.00\n0.0\n\n0.2\n\n0.4 0.6 0.8 1.0\nPressure [E] #10!1\n\nFigure 10: Clamped plate under pressure for the GOH material model – the displacement of\nthe middle point vs. the applied pressure for the three constitutive approaches NP, AP and DD:\n(a-c) without and (d-f) with the compression/tension switch. Further, for (a) and (d) κi = 0.0,\nfor (b) and (e) κi = 0.226 and for (c) and (f) κi = 1/3.\n\n5.4\n\nIndentation of a sheet\n\nIn vitro and in silico indentation tests are widely used to empirically and numerically determine\nthe mechanical characteristics of soft tissues (Zhang et al., 1997; Liu et al., 2004; Choi and\nZheng, 2005; McKee et al., 2011; Lu et al., 2012). For instance, puncture testing has been\napplied frequently for the mechanical characterization of the human fetal membrane tissue\n(Bürzle et al., 2014).\nHere, the indentation of a square sheet is simulated. The sheet has the same dimensions and\nmaterial properties as the plate of Sec. 5.3. As shown in Figs. 11.a and 11.b, two types of\nboundary conditions are considered, i.e. the outer edges are either fixed or clamped. The sheet\n31\n\n\fis pressed by an indenter with a rigid spherical cap (see Figs. 11.c and 11.d). The indenter\nradius is R = L/6, where L is the width of sheet. Here, the sheet is meshed by 6 × 6 quadratic\nNURBS-based elements. In the contact area, the mesh is finer. The size of the finest element\nis 1/4 of the coarsest one. The sheet constited of the GOH material model with the constants\ngiven in Tab. 5. Following Sauer and De Lorenzis (2015), the contact computations is based\non an unbiased penalty formulation applied at the quadrature points of the isogeometric finite\nelements. The penalty parameter is set to \u000fc = 108 E T , where E = 3 µ̃.\n\n(a)\n\n(c)\n\n(b)\n\n(d)\n\nFigure 11: Indentation test: Reference configuration (quarter system) with (a) clamped and (b)\nfixed outer edges. Deformed configuration (full system) with (c) clamped and (d) fixed outer\nedges for the GOH model (with the compression/tension switch and κi = 0.226) colored by\nI1 := tr C.\nIn Fig. 12, the vertical component of the total contact force is plotted against the indentation\ndepth for different indenter radius R. As expected, both the AP and NP shell models perform\nsimilarly.\n\n(a)\n\n#103\n\n3.0\n\n4.0\n\nFixed (NP)\nFixed (AP)\nClamped (NP)\nClamped (AP)\n\nContact force [E I=L2 ]\n\nContact force [E I=L2 ]\n\n4.0\n\n2.0\n1.0\n0.0\n0.0\n\n0.1\n0.2\n0.3\nIndentation depth [L]\n\n0.4\n\n(b)\n\n#102\n\n3.0\n\nFixed (NP)\nFixed (AP)\nClamped (NP)\nClamped (AP)\n\n2.0\n1.0\n0.0\n0.0\n\n0.1\n0.2\n0.3\nIndentation depth [L]\n\n0.4\n\nFigure 12: Indentation test - contact force vs. the indentation depth: GOH model with the\ncompression/tension switch and (a) κi = 0.226 and (b) κi = 1/3.\n\n32\n\n\f5.5\n\nAngioplasty\n\nBalloon angioplasty is the typical treatment to widen obstructed arteries or veins (Humphrey,\n2013). This procedure has been studied computationally by many scholars (e.g. Holzapfel et al.,\n1996; Rogers et al., 1999; Holzapfel et al., 2002; Gasser and Holzapfel, 2007; Gervaso et al., 2008;\nPant et al., 2012) in order to optimize the internal pressure, mechanical properties and location\nof the balloon.\n\n(a)\n\n(b)\n\n(c)\n\n(d)\n\nFigure 13: Balloon angioplasty: (a) Reference configuration (1/8 system) with boundary conditions and deformed configuration colored by the circumferential stretch λθ for (a) µ̃b = 2 µ̃a ,\n(b) µ̃b = 10 µ̃a and (c) µ̃b = 20 µ̃a .\n\nHere, the angioplasty procedure is simulated as shown in Fig. 13. A portion of an artery with\nthe dimensions Ta × Ra × L = 0.5 × 5 × 30 [mm3 ] is inflated by a balloon with initial radius\nRb = 0.9 Ra and thickness Tb = 0.1 Rb . The balloon is initially pre-stretched by λp = 1.1. The\nartery is modeled by the GOH material model with κi = 0. The material constants are taken\nfrom Tab. 5, which is similar to the properties of the adventitia of an artery (Gasser et al.,\n2006). Two families of fibers are considered with\nL̃i = sin θi sin ψ e1 + sin θi cos ψ e2 + cos θi e3 ,\n\n(i = 1, 2) ,\n\n(166)\n\nwhere θi ± 45◦ and ψ is the angular coordinate around e3 axis. The NP and AP shell models are\nused for the artery and the compression/tension switch is included. For the NP shell model, 5\nGaussian quadrature points are used for the numerical integration through the thickness. The\nballoon is modeled by an incompressible Neo-Hookean membrane (Sauer et al., 2014) with the\nshear moduli µ̃b = 2, 10 and 20 µ̃a , where µ̃a is the shear modulus of the anisotropic part of the\nartery material model (see Tab. 5). The balloon is inflated up to V = 3 V0 , where V0 is the initial\nvolume of the balloon. The contact constraint is enforced following the penalty formulation of\nSauer and De Lorenzis (2015). The penalty parameter is set to \u000fc = 107 E Ta , where E = 3 µ̃a .\nIn Fig. 14.a the internal pressure of the balloon is plotted against its volume for different values\nof µ̃b . Fig. 14.b shows the average circumferential stretch, λθ , computed in the middle of\nthe artery (see the circumferential dashed line in Fig. 13.a) against the volume of the inflated\nballoon. As expected, the results of the AP and NP shell models are very close even though\nthe artery is quite thick (T /R = 0.1).\n\n33\n\n\f1.32\n\n1.00\n\n1.26\n\n0.60\n0.40\n\n7\n~b\n7\n~b\n7\n~b\n7\n~b\n7\n~b\n7\n~b\n\n= 27\n~a (AP)\n= 27\n~a (NP)\n= 10 7\n~a (AP)\n= 10 7\n~a (NP)\n= 20 7\n~a (AP)\n= 20 7\n~a (NP)\n\n1.13\n\n7\n~b\n7\n~b\n7\n~b\n7\n~b\n7\n~b\n7\n~b\n\n= 27\n~a (AP)\n= 27\n~a (NP)\n= 10 7\n~a (AP)\n= 10 7\n~a (NP)\n= 20 7\n~a (AP)\n= 20 7\n~a (NP)\n\n1.06\n\n0.20\n\n(a)\n\n1.19\n63\n\nPressure [E]\n\n0.80\n\n0.00\n1.33 1.66 2.00 2.33 2.67 3.00\nVolume [V0 ]\n\n(b)\n\n1.00\n1.33 1.66 2.00 2.33 2.67 3.00\nVolume [V0 ]\n\nFigure 14: Balloon angioplasty: (a) The internal pressure and (b) the average circumferential\nstretch, λθ , in the middle of the artery vs. the volume of the inflated balloon.\n\n6\n\nConclusion\n\nThis paper presents different rotation-free shell formulations to model thin structures composed\nof soft biological materials. The formulation is designed for large deformations and allows\nfor geometrical and material nonlinearities, which makes it very suitable for the modeling of\nsoft tissues. The formulation is based on the Kirchhoff–Love hypothesis; thus, it needs only\ndisplacement degrees of freedom. Following an isogeometric approach, NURBS-based finite\nelements are used for the FE discretization and the FE solution, which satisfies the necessary\nC 1 -continuity of solution for rotation-free shells.\nThree different approaches to model thin shells are introduced: The numerically-projected (NP)\nshell model, which uses numerical integration through the shell thickness, and the analyticallyprojected (AP) and directly-decoupled (DD) shell models, which do not need any numerical\nthrough-the-thickness integration. The NP shell model is the most general approach; however, it can be computationally expensive e.g. for anisotropic constitutive laws like the Gasser–\nOgden–Holzapfel material model (Gasser et al., 2006). For such materials, one may need many\nquadrature points across the shell thickness to capture discontinuities of the stress across the\nthickness. This has motivated us to develop the AP shell model, which is computationally\nmore feasible. If the shell thickness is considerably smaller than the in-plane dimensions, for\nan initially-planar shell, or the radii of curvature, for an initially-curved shell, the NP and AP\nshell models perform similarly. Furthermore, the DD shell model is presented, which is directly\ndefined on a 2D manifold. This formulation assumes that the material properties and the constitutive law are symmetric w.r.t. the shell mid-surface. Apart from this restriction, the DD\nshell model is the most efficient approach.\nFurthermore, the exclusion of compressed fibers is considered for each type of the three shell\nmodels. As shown by different examples, the introduced compression/tension switch works very\nwell for both the NP and AP shell models. The DD shell model, however, cannot capture the\neffect of the switch if the bending forces are dominant.\nAltogether, the presented formulations can be characterized by increased computational efficiency and algorithmic complexity. Accordingly, an appropriate formulation should be chosen\nby a trade-off between efficiency and complexity for any specific application, structure and\nconstituent.\nThe introduced shell models are specifically derived for different isotropic and anisotropic material models, which are commonly used for soft biological materials. For both the isotropic\nand anisotropic models, two types of strain energy density function are examined: Polynomial\n34\n\n\fforms (i.e. Neo–Hooke, Mooney–Rivlin, and anisotropic Mooney–Rivlin material models) and\nexponential forms (i.e. Fung and Gasser–Ogden–Holzapfel material models). The procedure can\nbe easily applied to other material models. Furthermore, the robustness and accuracy of the\npresented shell models is demonstrated by different examples, which examine pure membrane\nmodes (see the uniaxial tension test), pure bending modes (see the cantilever bending test)\nand mixed modes (see the pressured clamped plate) of the shell deformation. Moreover, the\napplicability of the shell models is demonstrated by two examples: The indentation of a sheet\nunder a rigid spherical indenter and an angioplasty example that involves contact between two\ndeformable bodies.\nAcknowledgement\nFinancial support from the German Research Foundation (DFG) through grant GSC 111 is\ngratefully acknowledged. The authors also thank Reza Ghaffari for checking the manuscript\ncarefully.\n\nA\n\nVariation of kinematic variables\n\nFollowing the approach of Sauer and Duong (2017), the variation of kinematic variables are\nexpressed in terms of the metric tensor aαβ , which captures the stretching deformations, and\nthe curvature tensor bαβ , which captures the bending deformations. These variations are then\nused to derive the stresses and linearize the governing equations.\nFrom Eq. (4), the variation of gαβ is\nδgαβ = δaαβ − 2 ξ δbαβ\n\n(167)\n\nδg αβ = g αβγδ δgγδ ,\n\n(168)\n\nand since [g αβ ] = [gαβ ]−1 ,\nwith\ng αβγδ :=\n\n\u0011\n∂g αβ\n1\u0010\n= − g αγ g βδ + g αδ g βγ .\n∂gγδ\n2\n\n(169)\n\nAs shown by Sauer and Duong (2017), δaαβ = aαβγδ δaγδ , where\naαβγδ :=\n\n\u0011\n∂aαβ\n1\u0010\n= − aαγ aβδ + aαδ aβγ\n∂aγδ\n2\n\n(170)\n\nand δbαβ = bαβγδ δaγδ − aαβγδ δbγδ , where\nbαβγδ :=\n\n\u0010\n\u0011 \u0010\n\u0011\n∂bαβ\n= 2H aαβ aγδ + aαβγδ − aαβ bγδ + bαβ aγδ .\n∂aγδ\n\n(171)\n\nBesides, it can be proven that\n∗\n\nJ\nδJ = aαβ δaαβ ,\n2\n\nJ\nδ J = g αβ δgαβ .\n2\n∗\n\n(172)\n\nThe variation of the first invariant of the right Cauchy–Green tensor is\nδ I˜1 = δI1 + 2 λ3 δλ3 ,\n35\n\n∗\n\nδ I˜1 = δ I1 + δg33 ,\n\n(173)\n\n\fwhere the in-plane components are\n∗\n\nδI1 = Aαβ δaαβ ,\n\nδ I1 = Gαβ δgαβ .\n\n(174)\n\nSimilarly, the variation of the other invariants can be found as\nδ I˜2 = λ23 δI1 + 2J δJ + 2 I1 λ3 δλ3 ,\n∗\n\n∗\n\n∗\n\n(175)\n\n∗\n\nδ I˜2 = g33 δ I1 + 2J δ J + I1 δg33\nand\n\n∗\n\nδ I˜3 = 2 λ23 J δJ + 2 J 2 λ3 δλ3 ,\n\nB\n\n∗\n\n∗\n\nδ I˜3 = 2 g33 J δ J + J 2 δg33 .\n\n(176)\n\nLinearization of the external virtual work\n\nIf the in-plane components of the body force, i.e. f α aα , are neglected, the linearized external\nvirtual work contribution is (Sauer and Duong, 2017)\n∆Gext = ∆Gextp + ∆Gextt + ∆Gextm ,\n\n(177)\n\nwhere (Sauer et al., 2014)\nZ\n∆Gextp =\n\np δx · (n ⊗ aα − aα ⊗ n) ∆aα da .\n\n(178)\n\nS\n\nDenoting the convective coordinate along the curve ∂m S as ξ \u000f , where \u000f = 1 or \u000f = 2, the covariant base vector at x ∈ ∂m S is a\u000f := ∂x/∂ξ \u000f . The corresponding contra-variant base vector\nis then a\u000f := a\u000fα aα (α = 1, 2). This gives (Duong et al., 2017)\nZ\n\u0001\n∆Gextm =\nδaα · ν β n ⊗ aα + ν α aβ ⊗ n ∆aβ ds\n∂ S\nZ m\n(179)\n\u0001\n1\nα\n\u000f\n−\nm\nν\nδa\n·\nn\n⊗\na\n∆a\nds\nτ\nα\n\u000f\n2\n∂m S ka\u000f k\nand\n\nZ\nδx · ∆t ds +\n\n∆Gextt =\nwhere ka\u000f k =\n\nC\n\n√\n\nZ\n\n∂t S\n\n∂t S\n\n\u0001\n1\n\u000f\nδx\n·\nt\n⊗\na\n∆a\u000f ds ,\nka\u000f k2\n\n(180)\n\na\u000f · a\u000f . If the traction t is not a follower load (of the displacement), ∆t = 0.\n\nOut-of-plane linearization of kinematic variables\n\nIn this section, on the shell mid-surface, the kinematical variables are linearized w.r.t. ξ. From\nEq. (4), we have\n∂gαβ\n(181)\ngαβ,3 :=\n= −2 bαβ .\n∂ξ\nCombining Eqs. (168) and (181) gives\nαβ\ng,3\n:=\n\n∂g αβ\n= −2 g αβγδ bγδ .\n∂ξ\n\n36\n\n(182)\n\n\fIt can be shown that g αβγδ bγδ = −bαβ if ξ = 0. Thus, on the shell mid-surface,\n\u0010\n\u0011\nαβ\nαβ\nĝ,3\n:= g,3\n= 2 bαβ .\nξ=0\n\n(183)\n\nSimilar quantities can be derived for Gαβ and Gαβ in the reference configuration. Plugging\nEq. (181) into Eq. (172), we have\n∗\n\n\u0010\n\u0011\n∗\n∂J\nJ,3 :=\n= J Gαβ Bαβ − g αβ bαβ ,\n∂ξ\n∗\n\n(184)\n\nwhich gives\n\u0010∗ \u0011\nJˆ,3 := J,3\n\nξ=0\n\n= 2 J (H0 − H)\n\n(185)\n\non the shell mid-surface. In a similar fashion, the first invariant of the right Cauchy–Green\ntensor is linearized as\n\u0010 ∂ I∗ \u0011\n\u0001\n1\n(186)\n= 2 aαβ B αβ − bαβ Aαβ .\nIˆ1,3 :=\n∂ξ ξ=0\n\nD\n\nFirst-order compression/tension switch for fibers\n\nFor the principal directions of anisotropic materials, it can be shown that\nL̂αβ\ni,3\n\n:=\n\n\u0010 ∂ L∗ αβ \u0011\ni\n\n∂ξ\n\nξ=0\n\n= 2 Lαi Lβi,3 ,\n\n(187)\n\nwhere Lαi,3 := B αβ Liβ and Liα := Li · Aα . Thus, on the mid-surface, we have\ni\nIˆ4,3\n\n:=\n\n\u0010 ∂ I∗i \u0011\n4\n\n∂ξ\n\nξ=0\n\nαβ\n= −2 bαβ Lαβ\ni + aαβ L̂i,3 .\n\n(188)\n\n∗\n\nUsing a first order Taylor expansion, on a shell layer at ξ, I4i can be related to the similar\nquantity I4i on the mid-surface as\n∗\ni ,\n(189)\nI4i = I4i + ξ Iˆ4,3\n∗\n\nwhich gives ξ0i = ξ0i (x), where I˜4i = I4i = 16 , as\nξ0i :=\n\n1 − I4i\n.\nIˆi\n\n(190)\n\n4,3\n\nThen, T1i and T2i are defined according to the algorithm shown in Tab. 6. Furthermore, for the\nmaterial tangents of Sec. 3.2.2, one needs to linearize T1i and T2i as\nαβ\nU1i\n:=\n\n∂T1i\n,\n∂aαβ\n\nαβ\nU2i\n:=\n\n∂T2i\n,\n∂aαβ\n\nV1iαβ :=\n\n∂T1i\n,\n∂bαβ\n\nV2iαβ :=\n\n∂T2i\n,\n∂bαβ\n\n(191)\n\nwhich depend on\n\u0010 \u0011−2 h\ni\n\u0001\n∂ξ0i\ni\ni Lαβ + 1 − I i L̂αβ ,\n= − Iˆ4,3\nIˆ4,3\n4\ni\ni,3\n∂aαβ\n\u0010\n\u0011\ni\n−2\n\u0001\n∂ξ0\ni\n:=\n= 2 Iˆ4,3\n1 − I4i Lαβ\ni\n∂bαβ\n\nYiαβ :=\nZiαβ\n6\n\n∗\n\n33\nHere, it is assumed that L33\ni = Li = 0 (see Remark 4.2)\n\n37\n\n(192)\n\n\fas shown in Tab. 7.\nξ0 < −\n\nT\n2\n\ni >0\nIˆ4,3\n\nT\nT\nT1i = − , T2i =\n2\n2\n\ni <0\nIˆ4,3\n\nN.A.\n\ni =0\nIˆ4,3\n\n−\n\nT\nT\n≤ ξ0 ≤\n2\n2\n\nT1i = ξ0i , T2i =\n\nT\n< ξ0\n2\n\nT\n2\n\nT\nT1i = − , T2i = ξ0i\n2\n\nN.A.\nT\nT\nT1i = − , T2i =\n2\n2\n\nI4i > 1\n\nI4i ≤ 1\n\nT\nT\nT1i = − , T2i =\n2\n2\n\nN.A.\n\nTable 6: Algorithm to find [T1i , T2i ], where I˜4i > 1.\n\nξ0 < −\ni >0\nIˆ4,3\n\ni =0\nIˆ4,3\n\n−\n\nαβ\nU1i\n= V1iαβ = 0\nαβ\nU2i\n\ni <0\nIˆ4,3\n\nT\n2\n\n=\n\nV2iαβ\n\n=0\n\nN.A.\n\nT\nT\n≤ ξ0 ≤\n2\n2\n\nT\n< ξ0\n2\n\nαβ\nU1i\n= Yiαβ , V1iαβ = Ziαβ\nαβ\nU2i\n\n=\n\nV2iαβ\n\nN.A.\n\n=0\n\nαβ\nU1i\n= V1iαβ = 0\n\nU1i = V1i = 0\n\nαβ\nU2i\n= Yiαβ , V2iαβ = Ziαβ\n\nαβ\nU2i\n= V2iαβ = 0\n\nI4i > 1\n\nI4i ≤ 1\n\nαβ\nαβ\nU1i\n= V1iαβ = U2i\n= V2iαβ = 0\n\nN.A.\n\nTable 7: Algorithm to linearize T1i and T2i .\n\nConflict of Interest\nThe authors declare that they have no conflict of interest.\n\nReferences\nAbdessalem, J., Kallel, I. K., and Fakhreddine, D. (2011). Theory and finite element implementation of orthotropic and transversely isotropic incompressible hyperelastic membrane.\nMultidiscip. Model. Mater. Struct., 7(4):424–439.\nBalzani, D., Neff, P., Schröder, J., and Holzapfel, G. A. (2006). A polyconvex framework for soft\nbiological tissues. Adjustment to experimental data. Int. J. Solids Struct., 43(20):6052–6070.\nBischoff, M., Wall, W. A., Bletzinger, K.-U., and Ramm, E. (2004). Models and finite elements for thin-walled structures. In Stein, E., de Borst, R., and Hughes, T. J. R., editors,\nEncyclopedia of Computational Mechanics. Vol. 2: Solids and Structures. Chapter 3. Wiley.\n38\n\n\fBürzle, W., Mazza, E., and Moore, J. J. (2014). About puncture testing applied for mechanical\ncharacterization of fetal membranes. J. Biomech. Engrg., 136(11):111009.\nChagnon, G., Rebouah, M., and Favier, D. (2015). Hyperelastic energy densities for soft biological tissues: A review. J. Elast., 120(2):129–160.\nChoi, A. and Zheng, Y. (2005). Estimation of Young’s modulus and Poisson’s ratio of soft\ntissue from indentation using two different-sized indentors: Finite element analysis of the\nfinite deformation effect. Med. Biol. Engrg. Comput., 43(2):258–264.\nCiarlet, P. G. (2005). An introduction to differential geometry with applications to elasticity.\nJ. Elast., 78-79:3–201.\nDemiray, H. (1972). A note on the elasticity of soft biological tissues. J. Biomech., 5(3):309–311.\nDuong, T. X., Roohbakhshan, F., and Sauer, R. A. (2017). A new rotation-free isogeometric\nthin shell formulation and a corresponding C 1 -constraint for patch boundaries. Comp. Meth.\nAppl. Mech. Engrg., 316:43–83.\nEchter, R. (2013). Isogeometric analysis of shells. PhD thesis, Institut für Baustatik und\nBaudynamik der Universität Stuttgart.\nFan, R. and Sacks, M. S. (2014). Simulation of planar soft tissues using a structural constitutive\nmodel: Finite element implementation and validation. J. Biomech., 47(9):2043–2054.\nFlynn, D., Peura, G., Grigg, P., and Hoffman, A. (1998). A finite element based method to\ndetermine the properties of planar soft tissue. J. Biomech. Engrg, 120(2):202–210.\nFung, Y. (1967). Elasticity of soft tissues in simple elongation. Amer. J. Physio., 213(6):1532–\n1544.\nGasser, T. C. and Holzapfel, G. A. (2007). Finite element modeling of balloon angioplasty by\nconsidering overstretch of remnant non-diseased tissues in lesions. Comput. Mech., 40(1):47–\n60.\nGasser, T. C., Ogden, R. W., and Holzapfel, G. A. (2006). Hyperelastic modelling of arterial\nlayers with distributed collagen fibre orientations. J. Roy. Soc. Interface, 3(6):15–35.\nGervaso, F., Capelli, C., Petrini, L., Lattanzio, S., Di Virgilio, L., and Migliavacca, F. (2008).\nOn the effects of different strategies in modelling balloon-expandable stenting by means of\nfinite element method. J. Biomech., 41(6):1206–1212.\nHolzapfel, G. A. (2001). Biomechanics of soft tissue. In Lemaitre, J., editor, The handbook of\nmaterials behavior models, volume 3, pages 1049–1063. Academic Press, San Diego, USA.\nHolzapfel, G. A., Eberlein, R., Wriggers, P., and Weizsäcker, H. W. (1996). Large strain analysis\nof soft biological membranes: Formulation and finite element analysis. Comp. Meth. Appl.\nMech. Engrg., 132(1):45–61.\nHolzapfel, G. A. and Ogden, R. W. (2009). On planar biaxial tests for anisotropic nonlinearly\nelastic solids. A continuum mechanical framework. Math. Mech. Solids, 14(5):474–489.\nHolzapfel, G. A. and Ogden, R. W. (2015). On the tension–compression switch in soft fibrous\nsolids. Eur. J. Mech. A/Solids, 49:561–569.\n\n39\n\n\fHolzapfel, G. A., Stadler, M., and Schulze-Bauer, C. A. (2002). A layer-specific threedimensional model for the simulation of balloon angioplasty using magnetic resonance imaging\nand mechanical testing. Ann. Biomed. Engrg., 30(6):753–767.\nHumphrey, J. (1998). Computer methods in membrane biomechanics. Comput. Meth. Biomech.\nBiomed. Eng., 1(3):171–210.\nHumphrey, J., Strumpf, R., and Yin, F. (1992). A constitutive theory for biomembranes:\napplication to epicardial mechanics. J. Biomech. Engrg., 114(4):461–466.\nHumphrey, J. and Yin, F. (1987). On constitutive relations and finite deformations of passive\ncardiac tissue: I. A pseudostrain-energy function. J. Biomech. Engrg., 109(4):298–304.\nHumphrey, J. D. (2013). Cardiovascular solid mechanics: cells, tissues, and organs. Springer\nScience & Business Media, New York.\nJacobs, N. T., Cortes, D. H., Vresilovic, E. J., and Elliott, D. M. (2013). Biaxial tension of\nfibrous tissue: using finite element methods to address experimental challenges arising from\nboundary conditions and anisotropy. J. Biomech. Engrg., 135(2):021004.\nKaliske, M. (2000). A formulation of elasticity and viscoelasticity for fibre reinforced material\nat small and finite strains. Comp. Meth. Appl. Mech. Engrg., 185(2):225–243.\nKiendl, J., Hsu, M.-C., Wu, M. C., and Reali, A. (2015). Isogeometric Kirchhoff–Love shell\nformulations for general hyperelastic materials. Comp. Meth. Appl. Mech. Engrg., 291:280–\n303.\nKroon, M. and Holzapfel, G. A. (2009). Elastic properties of anisotropic vascular membranes\nexamined by inverse analysis. Comput. Methods Appl. Mech. Eng., 198(45–46):3622 – 3632.\nLiu, Y., Kerdok, A. E., and Howe, R. D. (2004). A nonlinear finite element model of soft tissue\nindentation. In Cotin, S. and Metaxas, D., editors, Proceed. Int. Sympos. Medical Simulation,\nISMS 2004, pages 67–76. Springer, Berlin, Heidelberg.\nLu, M.-H., Mao, R., Lu, Y., Liu, Z., Wang, T.-F., and Chen, S.-P. (2012). Quantitative imaging\nof Young’s modulus of soft tissues from ultrasound water jet indentation: A finite element\nstudy. Comput. Math. Methods Med., 2012.\nMartins, P., Natal Jorge, R., and Ferreira, A. (2006). A comparative study of several material models for prediction of hyperelastic properties: Application to silicone-rubber and soft\ntissues. Strain, 42(3):135–147.\nMcKee, C. T., Last, J. A., Russell, P., and Murphy, C. J. (2011). Indentation versus tensile\nmeasurements of Young’s modulus for soft biological tissues. Tissue Engrg. Part B Rev.,\n17(3):155–164.\nMelnik, A. V., Da Rocha, H. B., and Goriely, A. (2015). On the modeling of fiber dispersion in\nfiber-reinforced elastic materials. Int. J. Nonlin. Mech., 75:92–106.\nNaghdi, P. M. (1982). Finite deformation of elastic rods and shells. In Carlson, D. E. and\nShields, R. T., editors, Proceedings of the IUTAM Symposium on Finite Elasticity, pages\n47–103, The Hague. Martinus Nijhoff Publishers.\nPant, S., Bressloff, N. W., and Limbert, G. (2012). Geometry parameterization and multidisciplinary constrained optimization of coronary stents. Biomech. Model. Mechanobiol.,\n11(1-2):61–82.\n40\n\n\fProt, V., Skallerud, B., and Holzapfel, G. A. (2007). Transversely isotropic membrane shells\nwith application to mitral valve mechanics. Constitutive modelling and finite element implementation. Int. J. Numer. Methods Eng., 71(8):987–1008.\nRausch, M. K. and Kuhl, E. (2013). On the effect of prestrain and residual stress in thin\nbiological membranes. J. Mech. Phys. Solids, 61(9):1955 – 1969.\nRausch, M. K. and Kuhl, E. (2014). On the mechanics of growing thin biological membranes.\nJ. Mech. Phys. Solids, 63:128–140.\nRivlin, R. S. and Saunders, D. (1951). Large elastic deformations of isotropic materials. VII.\nExperiments on the deformation of rubber. Phil. Trans. Roy. Soc. Lon. A: Math. Phys. Engrg\nSci., 243(865):251–288.\nRogers, C., Tseng, D. Y., Squire, J. C., and Edelman, E. R. (1999). Balloon–artery interactions\nduring stent placement: A finite element analysis approach to pressure, compliance, and stent\ndesign as contributors to vascular injury. Circ. Res., 84(4):378–383.\nRoohbakhshan, F., Duong, T. X., and Sauer, R. A. (2016). A projection method to extract\nbiological membrane models from 3D material models. J. Mech. Behav. Biomed. Mater.,\n58:90–104.\nSauer, R. A. (2016). A contact theory for surface tension driven systems. Math. Mech. Solids,\n21(3):305–325.\nSauer, R. A. and De Lorenzis, L. (2015). An unbiased computational contact formulation for\n3D friction. Int. J. Numer. Meth. Engrg., 101:251–280.\nSauer, R. A. and Duong, T. X. (2017). On the theoretical foundations of solid and liquid shells.\nMath. Mech. Solids, 22(3):343–371.\nSauer, R. A., Duong, T. X., and Corbett, C. J. (2014). A computational formulation for\nconstrained solid and liquid membranes considering isogeometric finite elements. Comput.\nMethods Appl. Mech. Engrg., 271:48–68.\nSteigmann, D. J. (1999). On the relationship between the Cosserat and Kirchhoff–Love theories\nof elastic shells. Math. Mech. Solids, 4:275–288.\nSun, W. and Sacks, M. S. (2005). Finite element implementation of a generalized fung-elastic\nconstitutive model for planar soft tissues. Biomech. Model. Mechanobiol., 4(2-3):190–199.\nTepole, A. B., Kabaria, H., Bletzinger, K.-U., and Kuhl, E. (2015). Isogeometric Kirchhoff–Love\nshell formulations for biological membranes. Comp. Meth. Appl. Mech. Engrg., 293:328–347.\nTonge, T. K., Voo, L. M., and Nguyen, T. D. (2013). Full-field bulge test for planar anisotropic\ntissues: Part II – A thin shell method for determining material parameters and comparison\nof two distributed fiber modeling approaches. Acta Biomater., 9(4):5926–5942.\nWex, C., Arndt, S., Stoll, A., Bruns, C., and Kupriyanova, Y. (2015). Isotropic incompressible\nhyperelastic models for modelling the mechanical behaviour of biological tissues: A review.\nBiomed. Engrg./Biomed. Tech., 60(6):577–592.\nZhang, M., Zheng, Y., and Mak, A. F. (1997). Estimating the effective Young’s modulus of\nsoft tissues from indentation tests—nonlinear finite element analysis of effects of friction and\nlarge deformation. Med. Engrg. Phys., 19(6):512–517.\n\n41\n\n\f"
        ],
        [
         "9",
         "9",
         "cs.CE",
         "Computational Engineering",
         "1703.07231v1.pdf",
         "Control and Limit Enforcements for VSC\nMulti-Terminal HVDC in Newton Power Flow\nHaoyu Yuan\nPeak Reliability\n4850 Hahns Peak Dr., Suite 120\nLoveland, CO 80538\nEmail: hyuan@peakrc.com\n\nHantao Cui, Fangxing Li\n\narXiv:1703.07231v1 [cs.CE] 19 Mar 2017\n\nDept. of Electrical Engineering and Computer Science\nUniversity of Tenneseee\nKnoxville, TN 37996\nEmail: {hcui7, fli6}@utk.edu\n\nAbstract—This paper proposes a novel method to automatically\nenforce controls and limits for Voltage Source Converter (VSC)\nbased multi-terminal HVDC in the Newton power flow iteration\nprocess. A general VSC MT-HVDC model with primary PQ or\nPV control and secondary voltage control is formulated. Both\nthe dependent and independent variables are included in the\npropose formulation so that the algebraic variables of the VSC\nMT-HVDC are adjusted simultaneously. The proposed method\nalso maintains the number of equations and the dimension of\nthe Jacobian matrix unchanged so that, when a limit is reached\nand a control is released, the Jacobian needs no re-factorization.\nSimulations on the IEEE 14-bus and Polish 9241-bus systems are\nperformed to demonstrate the effectiveness of the method.\nIndex Terms—Multi-terminal HVDC, Voltage Source Converter, Newton power flow, reactive power limit\n\nBus i,Vi\nIshi\n\nBus j,Vj\n\nPshi+Qshi\n\nIshj\n\nZshi\n\nBus k,Vk\n\nPshj+Qshj\n\nPshk+Qshk\n\nZshj\n\n+\nVshi\n-\n\nZshk\n\n+\nVshj\n-\n\n+\nVshk\n-\n\nIdci\n\nIdcj\nDC line ij\n\nVdci\n\nIshk\n\nIdck\nDC line jk\n\nVdcj\n\nVdck\n\nDC line ik\n\nI. I NTRODUCTION\nThe transmission network of the electric power system is\nundergoing a transformation with more renewable penetration\nthrough power converters. Voltage Sourced Converters (VSC),\ncomparing to Silicon-Controlled Rectifiers, are advantageous\nwith converter state control, independent active and reactive\npower control, and contingent power support capability. A\nnumber of VSC-based FACTS controllers such as STATCOM,\nSSSC and UPFC, have been deployed.\nThe imbalance between renewable generations and local\npower consumptions brings up the challenge of transmitting electricity continentally among asynchronous systems.\nBuilding a multi-terminal HVDC (MT-HVDC) overlay on the\nexisting AC transmission grid is promising to improve transfer\ncapacity and resilience of the grid, owing to VSC being able\nto make multi-terminal connections easily.\nTo understand the mechanism of VSC MT-HVDC and its\nimpacts on the AC system, both the steady-state and transient\nprocess need to be modeled. Previous work has been carried\nout on the steady-state models of VSC HVDC such as UPFC\n[1], IPFC [2], generalized models [3], [4], [5], [6], and\ngeneralized models with controls [7], [8]. The dynamic models\nfor transient analysis have been widely studied [9], [10], [11].\nPower flow analysis of systems with VSC is the basics for\ninitializing the dynamic equations, however, the handling of\ncontrols limits are insufficient for software implementation.\nMathematically, the VSC controls versus the voltage and\ncurrent limits is the choice of effective equations in the power\n\nFig. 1. Generalized system with VSC MT-HVDC\n\nflow analysis. In a typical AC power flow problem, when a PV\ngenerator is switch to a PQ load, a voltage angle variable is\nintroduced along with a reactive power balancing equation.\nThe same idea applies to network with VSC MT-HVDC,\nhowever, a typical implementation requires to change the size\nof the Jacobian matrix once a limit is violated.\nIn this paper, an automatic limit enforcement method for\nVSC MT-HVDC is proposed by including both the independent and dependent variables in the set. The number of\nequations and variables stays unchanged during the iteration\nprocess. The rest of this paper covers a generalized VSC MTHVDC model with controls, incorporation of VSC MT-HVDC\ninto Newton power flow, and the numerical results.\nII. G ENERALIZED VSC MT-HVDC M ODEL\nA. VSC MT-HVDC Equivalent Circuit\nIn power flow analysis, only the steady-state equations\nof VSC are considered. Fig. 1 shows a VSC MT-HVDC\nequivalent circuit in shunt connection. The converters i, j, k\nare connected to an AC network bus i, j, k with a coupling\ntransformer having an equivalent impedance of Zsh. They also\nconnect to DC nodes i, j, k linked by DC lines.\nIn this scheme, the converters at Buses i, j are considered\nas primary converters which are capable of controlling the\n\n\factive power and reactive power flow from the AC buses\nindependently. The converter at Bus k is a secondary converter\ncapable of controlling the AC bus voltage Vk and the DC\nvoltage V dck . Therefore, converter k is slack to balance power\nexchange among the converters. This scheme can be extended\nto N-terminal HVDC networks where the first N −1 converters\nare primary and the N th converter is secondary.\nB. VSC MT-HVDC Power Flow Equations\nThe VSC MT-HVDC equivalent circuit is modeled in the\nphasor domain, i.e., the converters are represented at the\nfundamental frequency by the voltage phasors Vshm =\nV shm ∠θshm (m = i, j, k). The power injection from the AC\nbus m to the coupling transformer is given by:\nSshm = Vm × Ish∗m = Vm ×\n\n\u0012\n\nVm − V shm\nZshm\n\n\u0013∗\n(1)\n\nwhere Vm is the voltage magnitude of bus m, Ishm is the\ncurrent in the coupling transformer, and Zshm the equivalent\nimpedance of the transformer. Zshm = Rshm + jXshm ,\nwhere Rshm and Xshm are the resistance and reactance of\nthe transformer. The real and imaginary parts of (1) correspond\nto the active and reactive power injections:\nP shm =gshm Vm2 − gshm Vm V shm cos(θm − θshm )\n− bshm Vm V shm sin(θm − θshm )\nQshm = − bshm Vm2 − gshm Vm V shm sin(θm − θshm )\n+ bshm Vm V shm cos(θm − θshm )\n\n=Re(−Vshm Ish∗m )\n=gshm V sh2m − gshm Vm V\n\nI = −YVdc\n\n(2)\n\n(3)\n\n(6)\n\nwhere I = [Ii , Ij , Ik ]T is the DC nodal current injections from\nlines, Vdc = [V dci , V dcj , V dck ]T is the DC node voltage\nmagnitudes, and Y is the DC conductance matrix following\nthe AC admittance matrix definition. Equation (6) can be\nwritten in tensorial form as:\nX\nIdcm = −\nYmn · Vn\n(7)\nn\n\nwhere ∀m = i, j, k, n = {i, j, k}, and Ymn is the element at\n(m, n) in the conductance matrix Y.\nThe current injection from into node m is given as:\nIdcm = P dcm /Vm\n\n(8)\n\nwhere P dcm is the power injection from VSC into the DC\nnetwork given by (5). The current injections from all devices\ninto node m follow Kirchoff’s law and sum up to zero.\nFinally, all converters are subject to physical voltage and\ncurrent limits. The limits are rated values of the converters\nwhich need to be handled carefully in the power flow formulation. Voltage limits are given by:\nV shmin\n≤ V shm ≤ V shmax\nm\nm\n\nwhere Zshm = 1/(gshm + jbshm ), m = i, j, k.\nThe power balancing equation of each converter involves\nthe throughput power, converter losses and the actual output.\nThe active power through the converter, P dc0m , is given by:\nP dc0m\n\nThe current injection from the DC network to the nodes\ngiven in the following matrix form:\n\n(9)\n\nThe current flow through the VSC equals the current\nthrough the coupling transformer, given by:\np\nVm2 + V sh2m − 2Vm V shm cos(θm − θshm )\nIshm =\n|Zshm |\n(10)\nNote the throughput current is not an independent variable\nin the equations but a function of several variables. In the final\nsolution, it is limited within the rating:\nIshm ≤ Ishmax\nm , m = i, j, k\n\n(11)\n\nC. Voltage and Power Flow Control of Converters\nshm cos(θm − θshm ) (4)\n\n+ bshm Vm V shm sin(θm − θshm )\nwhich consists of two parts: the neat power injection to the\nDC network P dcm and converter losses P lm . The converter\nloss term can be further split into three components: a constant\npower term, a constant voltage term, and a constant impedance\nterm. In other words, the loss terms are independent, linearly\nand quadratically dependent on the converter current:\nP lm = P dc0m − P dcm\n= a + b · Ishm + c · Ish2m\n\n(5)\n\nThe next set of equations is the DC network equations. The\nDC network model composes of DC nodes and DC lines.\nThe DC power flow pattern is dictated by the network line\nresistances and the DC node voltages following Kirchoff’s\nlaws. Therefore, a voltage variable and a current injection\nequation is added for each node.\n\nIn an N-terminal VSC HVDC network, the N − 1 primary converters are capable of controlling either PQ or PV\nindependently, while the N th secondary converter can control\nthe voltage magnitude on the AC bus and DC node. Power\nor voltage control of VSC in power flow analysis forces the\ncontrolled variable at the desired value. These controls are\nvalid if neither voltage or current constraint is binding. That\nis, the controlled variables are let equal the desired values,\ndepending on the control mode.\n1) Primary VSC - PQ Control: The primary converter controls the active and reactive power injections at the connected\nAC bus independently. This is given by:\n0 = P shm − P shcm\n\n(12)\n\n0 = Qshm − Qshcm\n\n(13)\n\nwhere P shcm and Qshcm are the desired active power and\nreactive power on bus m, m = i, j.\n\n\fIf either voltage or current limit of a PQ-controlled primary\nconverter is reached, the voltage will be set to the limit to\nrelease the reactive power control. If the other limit is also\nreached afterwards, the active power control will be released.\n2) Primary VSC - PV Control: The converter controls the\nactive power injection and bus voltage magnitude on the AC\nbus. The control is given by (12) and (14):\n0 = Vm − Vmc\n\n(14)\n\nwhere Vmc is the desired voltage magnitude of bus m, m = i, j.\nIf voltage or current limit is reached, the converter voltage\nwill be set at the limit, and the AC voltage control will be\ndropped first, and then the active power control.\n3) Secondary VSC - Voltage Control: The secondary VSC\nacts as a reference/slack bus for active power balancing in\nthe DC network. DC nodal voltage on the DC slack node is\ncontrolled to the reference value given by:\n0 = V dck − V dcck\n\n(16)\n\nwhere Vkc is the desired voltage magnitude on the secondary\nVSC connected bus k. AC network voltage control will be\nreleased if either voltage limit or current limit is reached.\nD. Summary of VSC Power Flow Model\nThe generalized VSC MT-HVDC model with controls and\nlimits contains the following equations:\n1) AC Bus Power Outputs: (2) and (3)\n2) VSC Power Injections and Losses: (4) and (5)\n3) VSC Limits: (9) and (11),\n4) Primary Converter Controls: (12), (13) or (12), (14)\n5) Secondary Converter Controls: (15), (16)\n6) DC Network Current Injections: (8)\nIII. F ORMULATING INTO N EWTON P OWER F LOW\nThe power flow problem is to find the zero of a set of nonlinear equations starting from an adequate initial guess. The\ngeneral form of the power flow equations is given as follows:\ng(y) = 0\n\n0 = V − V0\n\n(18)\n\nOrganize the equations by grouping together active power\nmismatches, reactive power mismatches, voltage deviation (for\nPV and slack buses), and angle deviation (for the slack bus\nonly), the equations can be written in the following form:\n\n(15)\n\nPower injections on the connected AC bus is hence slack and\nuncontrollable. The voltage magnitude on the connected AC\nbus is controlled, given by:\n0 = Vk − Vkc\n\nto be added. Checking the reactive power limit can be easily\ndone by a if-then logic, however, the addition of equations\nwill change the size of matrices and requires re-factorization\nof the Jacobian, which is time consuming.\nInclusion of PV reactive power output in the power flow\nequation set is proposed in [6] which retains the size of the\nJacobian matrix. For each PV-connected bus, a variable Q for\nthe reactive power output is added, so is an equation for the\nreactive power mismatch on that bus. Also added is a variable\nV for the voltage magnitude and an equation for the voltage\nmismatch given by (18), where V0 is the desired value.\n\n(17)\n\nwhere y is the steady-state algebraic variables. The state variables of the differential equations will be initialized afterwards.\nA. Newton Method with Automatic Reactive Power Limit\nBefore considering VSC MT-HVDC, the power flow equations are revisited. The commonly adopted equations are nodal\npower mismatches which include the active power mismatch\nfor PQ- and PV-connected buses, and reactive power mismatch\nfor PQ-connected buses. Reactive power limits of PV generators are checked after each iteration. If a limit is reached, the\nPV generator will be converted to a PQ load to fix the reactive\npower output at the limit, and a reactive mismatch equation has\n\ng = [gpT , gqT , gvT , gθT ]T = 0\n\n(19)\n\nThe linearized equation for each Newton iteration can be\nwritten as:\n\n\n\n\n\n0\ngp,pg\n∆θ\n∆P\ngp,θ gp,v\n\n\n ∆Q \ngq,qg\n0\ngq,θ gq,v\n  ∆V \n\n\n = −\n\n\n ∆Ve \nQg \n\n\n0\ngv,v\nε 0\nPg\n∆θe\ngθ,θ\n0\n0 ε\n(20)\nwhere gp,θ = ∇Tθ gp , gp,v = ∇Tv gp , gq,θ = ∇Tθ gq , gq,v =\n∇Tv gq , gv,v = ∇Tv gv , gθ,θ = ∇Tθ gθ . Note that gp,pg = ∇Tpg gp\nand gq,qg = ∇Tqg gq are the derivatives of gp and gq with respect\nto the specific generator output, and \u000f is a diagonal matrix of\nsmall values (10−6 ) to avoid singularity in matrix factorization.\nEach time before evaluating all the equations, the reactive\npower limits are checked for violations. If violation happens,\nthe corresponding reactive power output will be set to the limit,\nand, more importantly, the voltage mismatch equations will be\nforced at 0, which invalidate the voltage control on PV buses.\nThis process does not affect the size or shape of the Jacobian\nmatrix, hence the symbolic factorization can be re-used.\nOne observation from the Jacobian matrix (20) is that,\nexcept for the upper-left block corresponding to the bus\npower injection mismatch equations, the values in matrix are\nconstant. In other words, only the upper-left block needs to\nbe updated at every iteration using the Newton method. Since\nthe matrix size does not change, some variations of Newton\nmethod which does not update the Jacobian at every step, e.g.\nthe Dishonest Newton method can be applied.\nB. Incorporation of VSC MT-HVDC Model\nThe equations and the Jacobian matrix need to be extended\nto incorporate the VSC MT-HVDC model described in Section\nII-D. The introduced variable set of VSC are given by (21),\nwhose increments are appended to the right-hand side of (20).\nX2 = [θsh, V sh, P sh, Qsh, P dc0 , P dc, V dc, Ish]\n\n(21)\n\n\fThe introduced equation set of VSC is given by (22), which\nare appended to the left-hand side of (20). The components\nin (22) corresponds to equations (2), (3), (12), (13), (4), (5),\n(8), (10), respectively. All terms in each equation are moved\nto one side to evaluate the mismatch for each iteration.\nT\nT\nT\nT\nT\nT\nT\ng2 = [gPT sh ,gQsh\n, gPT shc , gQsh\nc , gP dc0 , gP l , gP dc , gIsh ]\n(22)\n\nIn addition to (21) and (22), for each VSC in voltage control\nmode, (14) or (16) is not explicitly used. Rather, the connected\nAC bus is converted to a PV-type bus, where a voltage variable\nand voltage mismatch equation gV c is added like (18).\nThe corresponding Jacobian matrix is obtain by taking the\nderivative of each equation with respect to each variable. Note\nthat the power outflos from the VSC connected AC buses are\nadded to the AC network equations, namely ∆P and ∆Q, the\nderivatives of ∆P and ∆Q with respect to P sh and Qsh needs\nto be evaluated. Similarly, the derivatives of (22) with respect\nto AC voltage magnitude and angle need to be included.\nC. Automatic VSC Control and Limit Enforcing\nIn the framework of Newton method with automatic reactive\npower limit described in Section III-A, the voltage and current\nlimits of VSC MT-HVDC model can be automatically handled\nwithout changing the size of Jacobian matrix. The approach\nis described as follows:\n1) No Limit Violation: If there is no limit violation at this\niteration, all controls are maintained, which means enforcing\nthe corresponding control equation. For example, if the PQ\ncontrol of a primary converter is effective, gP shc and gQshc\nneed to be evaluated for mismatches.\n2) Limit Violations: If either voltage or current limit is\nreached, the reactive power control or voltage control is first\nreleased. The violated term is set to the limit value, and the\ncorresponding equation gQshc or gV c is set to 0 using (23).\nIf the other limit is reached after releasing the first controlled\nvariable, the active power control will be released. For the\nsecondary converter, AC voltage control is released first, and\nthen the DC node voltage control.\n∆\n\n∆\n\ngQshc = 0 or gV c = 0\n\n(23)\n\nThe voltage and current limits of VSC can be checked at\nevery iteration, but it is more effective to start enforcing the\nlimits when the mismatch is relative small.\nIV. C ASE S TUDIES AND R ESULTS\nThe proposed VSC MT-HVDC control and limit enforcement method is simulated on IEEE 14-bus and Polish 9241bus systems. Simulations are performed on a Python-based\nsoftware package, Andes [12], using CVXOPT 1.1.8 for sparse\nmatrix operations and KLU for fast sparse matrix factorizations. A generic Newton-Raphson method with a convergence\ntolerance of 10−8 . Limit enforcement is enabled since the\nfourth iteration, and a maximum of one PV can be converted\nin each iteration. All the case studies are carried out on a\nlaptop computer with a i5-6200U processor and 8GB RAM.\n\nTABLE I\nVSC MTDC SYSTEM DATA\nVSC\nn\n\nBus\nm\n\nCtrl 1\n\nCtrl 1\nValue\n\nCtrl 2\n\nCtrl 2\nValue\n\nVsh\nmax\n\nVsh\nmin\n\nIsh\nmax\n\n1\n2\n3\n4\n\n1\n3\n12\n14\n\nP\nP\nP\nVm\n\n0.2\n0.2\n0.2\n1.035\n\nQ\nQ\nVm\nVdc\n\n0.1\n0.1\n1.05\n1.0\n\n1.0\n1.08\n1.1\n1.1\n\n0.95\n1.02\n0.9\n0.9\n\n1\n1\n1\n1\n\nTABLE II\nS CENARIO 1: BASE CASE WITH qG2 ≤ 0.4\nBus\nm\n\nv\n[pu]\n\nθ\n[rad]\n\npG\n[pu]\n\nqG\n[pu]\n\npL\n[pu]\n\nqL\n[pu]\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n1.06\n1.0442\n1.01\n1.0183\n1.0199\n1.07\n1.0618\n1.09\n1.0562\n1.0512\n1.057\n1.0552\n1.0504\n1.0357\n\n0\n-0.0015\n-0.0039\n-0.0032\n-0.0027\n-0.0043\n-0.0041\n-0.0041\n-0.0046\n-0.0046\n-0.0045\n-0.0046\n-0.0046\n-0.0049\n\n2.3239\n0.4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n-0.1535\n0.4\n0.2400\n0\n0\n0.1243\n0\n0.1744\n0\n0\n0\n0\n0\n0\n\n0\n0.217\n0.942\n0.478\n0.076\n0.112\n0\n0\n0.295\n0.09\n0.035\n0.061\n0.135\n0.149\n\n0\n0.127\n0.19\n-0.039\n0.016\n0.075\n0\n0\n-0.046\n0.058\n0.018\n0.016\n0.058\n0.05\n\nOn the IEEE 14-bus system, we consider three scenarios:\n1) Base case with qG2 ≤ 0.4.\n2) Base case with qG2 ≤ 0.4, DC networks, and VSC\n3) Base case with qG2 ≤ 0.4, DC networks, and VSC with\nreduced Ish limit on VSC 3.\nIn the modified test case, generator reactive power is limited\nto 0.4 pu, 0.4 pu, 0.24 pu and 0.24 pu, respectively. The four\nVSCs are connected to the 14-bus system on Buses 1, 3, 12,\nand 14, and their DC output is connected to a circular DC\nnetwork where each DC line has a resistance of 1 pu. The\ncontrol methods and the parameters are listed in Table I. All\nthe loss coefficients are neglected.\nThe base case solution to the original 14-bus system can be\nfound in [6]. The solution to the first scenario is listed in Table\nII, where reactive violation on bus 2 is enforced at the fourth\niteration to fix the reactive power generation at its maximum.\nIt takes 7 iterations in 0.0069 second to reach the tolerance.\nScenario 2 considers the four VSC MT-HVDC and a DC\nnetwork. The bus-wise solution to this scenario is listed in\nTable III, while the results of the VSC converters are given in\nTable IV, where the grey cells are the effective limits. Note\nthe power flow into of VSC, namely Psh and Qsh , are counted\ninto the load on the connected buses.\nThree limits are violated at the fourth iteration: qG2 ,\nV shmax1 and V shmin2 , therefore, the reactive power control on VSC 1 and VSC 2 are dropped. On the contrary, as\nthe iteration continues, qG2 returned within its limit and the\nvoltage on Bus 3 remained at 1.1 pu. The solution process\ntakes 14 iterations in 0.0571 second to finish. For comparison,\n\n\fTABLE III\nS CENARIO 2: BASE CASE WITH qG2 ≤ 0.4, VSC MT-HVDC NETWORK\n\nTABLE V\nS CENARIO 3: VSC MT-HVDC R ESULTS WITH Ish3 ≤ 0.19\n\nBus\nm\n\nv\n[pu]\n\nθ\n[rad]\n\npG\n[pu]\n\nqG\n[pu]\n\npL\n[pu]\n\nqL\n[pu]\n\nBus\nm\n\nNode\nn\n\nP sh\n[pu]\n\nQsh\n[pu]\n\nP dc\n[pu]\n\nVm\n[pu]\n\nV sh\n[pu]\n\nIsh\n[pu]\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n1.06\n1.045\n1.01\n1.023\n1.0246\n1.07\n1.0591\n1.09\n1.0467\n1.0436\n1.0534\n1.05\n1.0524\n1.035\n\n0\n-0.0799\n-0.226\n-0.1575\n-0.1331\n-0.2026\n-0.1757\n-0.1757\n-0.1852\n-0.1932\n-0.1999\n-0.2382\n-0.1983\n-0.1027\n\n2.3044\n0.4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n0.4664\n0.3620\n0.1764\n0\n0\n0.1799\n0\n0.1915\n0\n0\n0\n0\n0\n0\n\n0.2\n0.217\n1.142\n0.478\n0.076\n0.112\n0\n0\n0.295\n0.09\n0.035\n0.261\n0.135\n-0.4844\n\n0.6169\n0.127\n0.0712\n-0.039\n0.016\n0.075\n0\n0\n-0.0422\n0.058\n0.018\n-0.1025\n0.058\n0.3010\n\n1\n3\n12\n14\n\n1\n2\n3\n4\n\n0.2\n0.2\n-0.0105\n-0.4640\n\n0.6169\n-0.1188\n-0.2051\n0.2093\n\n-0.1811\n-0.2344\n-0.0718\n0.4888\n\n1.06\n1.01\n1.0809\n1.035\n\n1\n1.02\n1.1\n1.0203\n\n0.6118\n0.2303\n0.19\n0.4918\n\nTABLE IV\nS CENARIO 2: VSC MT-HVDC POWER , VOLTAGE AND CURRENT RESULTS\nBus\nm\n\nNode\nn\n\nP sh\n[pu]\n\nQsh\n[pu]\n\nP dc\n[pu]\n\nVm\n[pu]\n\nV sh\n[pu]\n\nIsh\n[pu]\n\n1\n3\n12\n14\n\n1\n2\n3\n4\n\n0.2\n0.2\n0.2\n-0.6333\n\n0.6169\n-0.1188\n-0.1849\n0.25102\n\n-1.1811\n-0.2345\n-0.2344\n0.6524\n\n1.06\n1.01\n1.05\n1.035\n\n1\n1.02\n1.0596\n1.0189\n\n0.6118\n0.2303\n0.2214\n0.6583\n\ncase is solved in 9 iterations in 1.236 seconds, while the VSC\nMT-HVDC case takes 12 iterations in 3.703 seconds. The\nslow-down is mainly due to the increased iterations after the\nfirst and only V shmin violation of VSC 5 on bus 4000.\nV. C ONCLUSIONS\nIn this paper, an automatic control and limit enforcement\nmethod for VSC MT-HVDC is elaborated. By using additional\nequations in the Newton power flow routine, voltage and current limits can be enforced during the iterations by switching\nout the control equations and substituting in the limits. Case\nstudies verified the proposed method for handling multiple\nviolations during iterations.\nFuture work involves handling non-convergence in limit\nviolation cases and releasing the package, Andes, on GitHub.\nR EFERENCES\n\na case with wide ranges of voltage and higher current limits\nconverges in 9 iterations in 0.015 second. Obviously, handling\nof the violations increases iterations and the calculation time.\nScenario 3 studies the impact of Ish limit of VSC 3 on\nthe power flow and voltage on Bus 12. In this scenario, we\nenforce Ish3 ≤ 0.19 pu, which is smaller than the solution\nin Table IV. As a consequence, the VSC current limit will be\nviolated and the voltage control on Bus 12 will be dropped.\nThe power flow solution takes 29 iterations in 0.051 second to\nfinish, and the results are listed in Table V. The limit violations\nare reported as follows:\n1) At iteration 4, V shmax1 , V shmin2 and Ishmax3 are\nviolated. qG2 is also violated.\n2) At iteration 6, qG3 is violated.\n3) At iteration 23, V shmax3 is violated.\nThis is an extreme case where both the voltage and active\npower controls on VSC 3 are dropped. The active power drawn\non Bus 12 are sharply reduced to meet the current limit.\nDue to the power balancing equations in the DC network,\nif Ishmax3 is too small, for example, Ishmax3 ≤ 0.18, the\npower flow iteration will not converge. This happens when\nall the primary VSC active power control are valid, while\nthe secondary cannot maintain the power balance in the DC\nnetwork without violating its limit.\nThe Polish 9248-bus test system from MATPOWER is\nalso studied. The base 9248-bus system is first solved, and\na modified system with a 10-terminal VSC HVDC network is\nsolved. PV reactive power checking for the system is turned\noff, while the limit enforcements of VSC remain on. The base\n\n[1] A. Nabavi-Niaki and M. R. Iravani, “Steady-state and dynamic models of\nunified power flow controller (UPFC) for power system studies,” IEEE\nTransactions on Power Systems, vol. 11, no. 4, pp. 1937–1943, 1996.\n[2] Y. Zhang, Y. Zhang, and C. Chen, “A novel power injection model of\nIPFC for power flow analysis inclusive of practical constraints,” IEEE\nTransactions on Power Systems, vol. 21, no. 4, pp. 1550–1556, 2006.\n[3] X.-P. Zhang, “Multiterminal Voltage-Sourced Converter-Based HVDC\nModels for Power Flow Analysis,” IEEE TRANSACTIONS ON POWER\nSYSTEMS, vol. 19, no. 4, 2004.\n[4] M. Baradar and M. Ghandhari, “A multi-option unified power flow\napproach for hybrid AC/DC grids incorporating multi-terminal VSCHVDC,” IEEE Transactions on Power Systems, vol. 28, no. 3, pp. 2376–\n2383, 2013.\n[5] L. Gengyin, Z. Ming, H. Jie, L. Guangkai, and L. Haifeng, “Power flow\ncalculation of power systems incorporating VSC-HVDC,” in Power System Technology, 2004. PowerCon 2004. 2004 International Conference\non, vol. 2. IEEE, 2004, pp. 1562–1566.\n[6] F. Milano, Power system modelling and scripting. Springer Science &\nBusiness Media, 2010.\n[7] W. Wang and M. Barnes, “Power flow algorithms for multi-terminal\nVSC-HVDC with droop control,” IEEE Transactions on Power Systems,\nvol. 29, no. 4, pp. 1721–1730, 2014.\n[8] J. Beerten, S. Cole, and R. Belmans, “Generalized steady-state VSC\nMTDC model for sequential AC/DC power flow algorithms,” IEEE\nTransactions on Power Systems, vol. 27, no. 2, pp. 821–829, 2012.\n[9] S. Cole, J. Beerten, and R. Belmans, “Generalized dynamic VSC MTDC\nmodel for power system stability studies,” IEEE Transactions on Power\nSystems, vol. 25, no. 3, pp. 1655–1662, 2010.\n[10] E. Prieto-Araujo, F. D. Bianchi, A. Junyent-Ferre, and O. GomisBellmunt, “Methodology for droop control dynamic analysis of multiterminal VSC-HVDC grids for offshore wind farms,” IEEE Transactions\non power delivery, vol. 26, no. 4, pp. 2476–2485, 2011.\n[11] J. Beerten, S. Cole, and R. Belmans, “Modeling of multi-terminal VSC\nHVDC systems with distributed DC voltage control,” IEEE Transactions\non Power Systems, vol. 29, no. 1, pp. 34–42, 2014.\n[12] H. Cui, “Andes - A Python-based tool for power system research,”\n2016. [Online]. Available: https://github.com/cuihantao/andes\n\n\f"
        ],
        [
         "10",
         "10",
         "cs.CE",
         "Computational Engineering",
         "1002.0170v2.pdf",
         "Spectral Analysis of Virus Spreading in Random Geometric Networks\n\narXiv:1002.0170v2 [cs.MA] 2 Feb 2010\n\nVictor M. Preciado and Ali Jadbabaie\n\nAbstract— In this paper, we study the dynamics of a viral\nspreading process in random geometric graphs (RGG). The\nspreading of the viral process we consider in this paper is\nclosely related with the eigenvalues of the adjacency matrix\nof the graph. We deduce new explicit expressions for all the\nmoments of the eigenvalue distribution of the adjacency matrix\nas a function of the spatial density of nodes and the radius of\nconnection. We apply these expressions to study the behavior of\nthe viral infection in an RGG. Based on our results, we deduce\nan analytical condition that can be used to design RGG’s in\norder to tame an initial viral infection. Numerical simulations\nare in accordance with our analytical predictions.\n\nresult that relates the behavior of an initial infection with the\nspectral radius of the adjacency matrix. In Section III, we\nstudy the eigenvalue spectrum of random geometric graphs.\nWe derive explicit expressions for the expected spectral\nmoments in the case of one- and two-dimensional RGG’s.\nIn Section IV, we use these expressions to study the spectral\nradius of RGG’s. Our results allow us to design RGG’s\nwith the objective of taming epidemic outbreaks. Numerical\nsimulations in Section IV validate our results.\n\nI. I NTRODUCTION\n\nIn this section, we briefly describe random geometric\ngraphs and introduce several useful results concerning their\nstructural properties (see [7] for a thorough treatment). We\nthen describe the spreading model introduced in [10] and\nshow how to study the behavior of an infection in the network\nfrom the point of view of the adjacency eigenvalues.\n\nThe analysis of spreading processes in large-scale complex\nnetworks is a fundamental dynamical problem in network\nscience. The relationship between the dynamics of epidemic/information spreading and the structure of the underlying network is crucial in many practical cases, such as\nthe spreading of worms in a computer network, viruses in a\nhuman population, or rumors in a social network. Several\npapers approached different facets of the virus spreading\nproblem. A rigorous analysis of epidemic spreading in a\nfinite one-dimensional linear network was developed by\nDurrett and Liu in [3]. In [10], Wang et al. derived a sufficient\ncondition to tame an epidemic outbreak in terms of the\nspectral radius of the adjacency matrix of the underlying\ngraph. Similar results were derived by Ganesh et al. in [4],\nestablishing a connection between the behavior of a viral\ninfection and the eigenvalues of the adjacency matrix of the\nnetwork.\nIn this paper, we study the dynamics of a viral spreading\nin an important type of proximity networks called Random\nGeometric Graphs (RGG). RGG’s consist of a set of vertices\nrandomly distributed in a given spatial region with edges\nconnecting pairs of nodes that are within a given distance\nr from each other (also called connectivity radius). In this\npaper, we derive new explicit expressions for the expected\nspectral moments of the random adjacency matrix associated\nto an RGG. Our results allow us to derive analytical conditions under which an RGG is well-suited to tame an infection\nin the network.\nThe paper is structured as follows. In Section II, we\ndescribe random geometric graphs and introduce several\nuseful results concerning their structural properties. We also\npresent the spreading model in [10] and review an important\nThis work was supported by ONR MURI N000140810747, and AFOR’s\ncomplex networks program.\nThe authors are with the Department of Electrical and Systems Engineering, University of Pennsylvania, 3451 Walnut Street,\n\n{preciado,jadbabai}@seas.upenn.edu\n\nII. V IRUS S PREADING\n\nIN\n\nR ANDOM G EOMETRIC G RAPHS\n\nA. Random Geometric Graphs\nConsider a set of n nodes, Vn = {v1 , ..., vn }, respectively\nlocated at random positions, χn = {x1, ..., xn }, where xi\nare i.i.d. random vectors uniformly distributed on the ddimensional unit torus, Td . We use the torus for convenience,\nto avoid boundary effects. We then connect two nodes vi , v j ∈\nVn if and only if xi − x j ≤ r, where r is the so-called\nconnectivity radius. In other words, a link exists between\nvi and v j if and only if v j lies inside the sphere of radius\nr (n) centered at vi . We denote this spherical region by Si (r),\nand the resulting random geometric graph by G (χn ; r). We\ndefine a walk of length k from v0 to vk as an ordered set of\n(possibly repeated) vertices (v0 , v1 , ..., vk ) such that vi ∼ vi+1 ,\nfor i = 0, 1, ..., k − 1; if νk = ν0 the walk is said to be closed.\nThe degree di of a node vi is the number of edges\nconnected to it. In our case, the degrees are identical random\nvariables with expectation [7]:\nE [di ] = nV (d) rd ,\n\n(1)\n\nwhere V (d) \u000eis the volume of a d-dimensional unit sphere,\nV (d) = π d/2 Γ (d/2 + 1), and Γ (·) is the Gamma function.\nThe clustering coefficient is a measure of the number of\ntriangles in a given graph, where a triangle is defined by\nthe set of edges {(i, j) , ( j, k) , (k, i)} such that i ∼ j ∼ k ∼ i.\nFor one- and two-dimensional RGG’s we can derive an\nexplicit expression for the expected number of triangles,\nE [ti ], touching a particular node vi (details are provided in\nSection III).\nThe adjacency matrix of an undirected graph G, denoted\nby A(G) = [ai j ], is defined entry-wise by ai j = 1 if nodes\ni and j are connected, and ai j = 0 otherwise. (Note that\n\n\faii = 0 for simple graphs.) Denote the eigenvalues of a n × n\nsymmetric adjacency matrix A(G) by λ1 ≤ ... ≤ λn . The k-th\norder moment of the eigenvalue spectrum of A(G) is defined\nas:\n1 n\nmk (G) = ∑ λik\nN i=1\n(which is also called the k-th order spectral moment).\nWe are interested in studying asymptotic properties of the\nsequence G(χn ; r (n)) for some sequence {r (n) : n ∈ N}. In\n[7], two particularly interesting regimes are introduced: the\nthermodynamic limit with nr (n)d → α ∈ (0, ∞), so that the\nexpected degree of a vertex tends to a constant, and the\n\u0010\n\u00111/d\nconnectivity regime with r (n) → γ logn n\nwith a constant\nγ , so that the expected degree of the nodes grows as c log n.\nIn this paper, we focus on studying the spectral moments\nin the connectivity regime. In Section III, we derive explicit\nexpressions for the expected spectral moments of G ( χn ; rn )\nfor any network size n. We then use this information to bound\nthe spectral radius of the adjacency matrix of G(χn ; r (n)).\nB. Spectral Analysis of Virus Spreading\nIn this section, we briefly review an automaton model\nthat describes the dynamics of a viral infection in a specific network of interactions. This model was proposed and\nanalyzed in [10], where a connection between the growth of\nan initial infection in the network and the spectral radius of\nthe adjacency matrix was established. This model involves\nseveral parameters. First, the infection rate β represents the\nprobability of a virus at an infected node i spreading to\nanother neighboring node j during a time step. Also, we\ndenote by δ the probability of recovery of any infected node\nat each time step. For simplicity, we consider β and δ to be\nconstants for all the nodes in G. We also denote by pi [k] the\nprobability that node i is infected at time k. The evolution\nof the probability of infection is modeled by means of the\nfollowing system of non-linear difference equation:\npi [k + 1] = [1 −\n\n∏ (1 − β p j [k])] + (1 − δ ) pi [k] ,\n\n(2)\n\nj∈Ni\n\nfor i = 1, ..., n, where Ni denotes the set of nodes connected\nto node i. We are interested in studying the dynamics of the\nsystem for a low-density level of infection, i.e., β p j [k] ≪\n1. In this regime, a sufficient condition for a small initial\ninfection to die out is [10]:\n\nλmax (A(G)) <\n\nδ\n.\nβ\n\n(3)\n\nOne can prove that (3) is a sufficient condition for local\nstability around the disease-free state. Thus, we can use\ncondition (3) to design networks with the objective of taming\ninitial low-density infections.\nIII. S PECTRAL A NALYSIS OF R ANDOM G EOMETRIC\nG RAPHS\nIn this paper, we study the eigenvalue distribution of the\nrandom adjacency matrix associated to G(χn ; r (n)) for n →\n∞. In this section, we characterize eigenvalue distribution\n\nusing its sequence of spectral moments. In our derivations,\nwe use an interesting graph-theoretical interpretation of the\nspectral moments [1]: the k-th spectral moment of G is\nproportional to the number of closed walks of length k in G.\nThis result allows us to transform the algebraic problem of\ncomputing spectral moments of the adjacency matrix into the\ncombinatorial problem of counting closed walks in the graph.\nIn the following subsection, we compute the expected value\nof the number of closed walks of length k in G(χn ; r (n)).\nA. Spectral Moments of One-Dimensional RGG’s\nAs we mentioned above, we can compute the k-th spectral\nmoment of a graph by counting the number of closed walks\nof length k. In the case of an RGG G(χn ; r (n)), this number\nis a random variable. In this subsection, we introduce a novel\ntechnique to compute the expected number of closed walks\nof length k. For clarity, we introduce our technique for the\nfirst three expected spectral moments k = 1, 2, 3. We then use\nthese results to induce a general expression for higher-order\nmoments in one-dimensional RGG’s.\nThe first-order spectral moment is equal to the number\nof closed walks of length k = 1. Since G(χn ; r) is a simple\ngraphs with no self-loops, we have that m1 (G(χn ; r)) is a\ndeterministic quantity equal to 0.\nWe now study the expected second moment,\nE [m2 (G(χn ; r))], by counting the number of closed\nwalks of length two. In simple graphs, the only possible\nclosed walks of length two are those that start at a given\nnode vi , visit a neighboring node v j ∈ Ni , and return back\nto vi . Hence, the number of closed walks of length two\nstarting at vi is equal to di . Thus, from (1), we have\nE [m2 ] =\n\n1 n\n∑ E [di] = nV (d) rd ,\nn i=1\n\nwhere this result is valid for any dimension d ≥ 1.\nThe third spectral moment is proportional to the number\nof closed walks of length three in the graph. We now derive\nan expression for the expected number of triangular walks\nstarting at a given node vi in a one-dimensional RGG. Since\nall nodes are statistically equivalent, our result is valid for\nany other starting node. For simplicity in our calculations,\nwe consider that vi is located at the origin. A triangular walk\nstarting at node vi exists if and only if there exist two nodes\nv j anv vk such that x j ≤ r, |xk | ≤ r, and xk − x j ≤ r. Also,\nsince the random distribution of vertices on T1 is uniform\n(with density n), the probability of nodes v j and vk being\nrespectively located in the differential lengths [x j + dx j ) and\n[xk + dxk ) is equal to n2 dx j dxk . Hence, one can compute the\nexpected number of triangular walks starting at node vi as\nE [ti ] =\n\nZ Z\n\n(x j ,xk )∈H2 (r(n))\n\nn2 dx j dxk ,\n\nwhere\n\b\nH2 (r) = (x j , xk ) ∈ T2 s.t. x j ≤ r,\nxk − x j ≤ r, |xk | ≤ r .\n\n(4)\n\n\fThus, E [ti ] can be computed as n2 Vol[H2 (r (n))] (where\nVol(H) denotes the volume contained by the polyhedron H.)\nNotice that H2 (r) can be defined by a set of linear inequalies;\nhence, H2 (r) is a convex polyhedron that depends on r.\nFurthermore, the set of linear inequalities in (4) presents a\nhomogeneous dependency with respect to the parameter r.\nTherefore, we can write Vol(H2 (r)) as r2 Vol(H2 (1)). Finally,\none can easily compute the volume of H2 (1) to be equal\nto 3. Thus, the expected third spectral moment of a onedimensional RGG is given by\nE [m3 ] =\n\n1 n\n∑ E [ti ] = 3n2r2 .\nn i=1\n\nIn the following, we extend the above technique to compute higher-order expected spectral moments. Denote by\n(k)\nWi the number of closed walks of length k starting at node\n(k)\nvi in G(χn ; r (n)). Regarding Wi , we derive the following\nresult.\nTheorem 1: The expected number of closed walks of\n(k)\nlength k, Wi , in a random geometric graph, G(χn ; r), on\n1\nT is given by\n\u0013\nh\ni\nk−2 \u0012\nk−1\n1\n(k)\nk−1\nE Wi\n= (nr)\n∑ j − 1 Ek−1, j ,\n2 (k − 1)! j=1\nwhere Ek−1, j are the Eulerian numbers 1 .\nProof: Consider a particular closed walk, wk =\n(v1 , v2 , v3 , ..., vk , v1 ), of length k starting and ending at node\nv1 (which we locate at zero for computational convenience).\nA walk wk exists if and only if there exists a set of k − 1\nnodes, {v2 , v3 , ..., vk } , such that |x1 | ≤ r, x j+1 − x j ≤ r for\nj = 2, ..., k − 1, and |xk | ≤ r. Since the distribution of vertices\non T1 is uniform (with density n) one can compute the\n(k)\nexpectation of Wi as\ni Z\nh\n(k)\n=\nnk−1 dx2 ...dxk ,\nE Wi\n(x2 ,...,xk )∈Hk−1 (r(n))\n\nwhere\n\nn\nHk−1 (r) = (v2 , v3 , ..., vk ) ∈ Tk−1 s.t. |v2 | ≤ r,\n\n(5)\n\nx j+1 − x j ≤ r for j = 2, ..., k − 1,\n|xk | ≤ r} .\n\n(k)\n\nThus, E[Wi ] can be computed as nk−1 Vol[Hk−1 (r)], where\nHk−1 (r) is a convex polyhedron defined by a set of linear\ninequalities. Finally, note that the homogeneous structure of\nthe system of linear inequalities defining Hk−1 (r) allows us\nto write Vol(Hk−1 (r)) = rk−1 Vol(Hk−1 (1)). Therefore,\nh\ni\n(k)\nE Wi\n= (nr)k−1 Vol (Hk−1 (1)) .\n(6)\n\nThe volume of Hk−1 (1) is a particular number, independent\nof the RGG parameters, i.e., n and r. Furthermore, we have\nfound an explicit analytical expression for the volume of\nHk (1) for any k ≥ 1. Although we do not provide details of\n1 The Eulerian number E (n,k) gives the number of permutations of\n{1,2,...,n} having k permutation ascents [5].\n\nour derivation, due to space limitations, an explicit expression for the volume of Hk (1) is given by [9]:\n\u0012\n\u0013\nk\n2 k−1\nEk, j ,\n(7)\nVol (Hk (1)) = ∑\nk! j=1 j − 1\nwhere Ed,k denotes the Eulerian numbers. Substituting (7) in\n(6) we obtain the statement of our lemma.\nIn [6], Lasserre proposed an algorithm to compute the\nvolume of a polyhedron defined by a set of linear inequalities. We can use this algorithm to verify the validity of (5).\nApplying this algorithm to the set of inequalities in (5), we\ncompute the following volumes for k = 1, ..., 10:\nH1 = 2, H2 = 3, H3 = 5.333..., H4 = 9.58333...,\nH5 = 17.6000..., H6 = 32.70555..., H7 = 61.3587...,\nH8 = 115.947..., H9 = 220.3238..., H10 = 420.825...\nThese numerical values match perfectly with our analytical\nexpression in Theorem 1.\nIf nr (n) = Ω (log n) (i.e., the average degree grows\nas log n, or faster), one can prove that E [mk ] =\n\u0001\u0001\n(k)\n1 + O log−1 n\nE[Wi ]. Hence, from (6) and (7), we\nhave the following closed-form expression for the asymptotic\nexpected spectral moments:\n\u0013\nk−2 \u0012\nk−1\n1\nE [mk ] ≍ (nr)k−1\n∑ j − 1 Ek−1, j . (8)\n2 (k − 1)! j=1\nIn the following table we compare the analytical result\nin (8) with numerical realizations of the empirical spectral\nmoments. In our simulations, we distribute n = 1000 nodes\nuniformly in T1 and choose a connectivity radius r = 0.01\n(which results in an average degree E[di ] = 20). The second,\nthird, and forth column in the following table represent\nthe analytical expectations of the spectral moments, the\nempirical average of the spectral moments from 10 random\nrealizations of the RGG, and the corresponding empirical\ntypical deviation, respectively.\nk\n1\n2\n3\n4\n\nE [mk ]\n0\n20\n300\n5,733\n\nEmpirical Average\n1.38e-16\n19.9326\n297.284\n5,956.30\n\nTypical Deviation\n1.3e-15\n0.0976\n4.3598\n196.94\n\nOur numerical results present an excellent match with our\nanalytical predictions.\nB. Spectral Moments of Two-Dimensional RGG’s\nIn this subsection, we derive expressions for the first three\nexpected spectral moments of G(χn ; r (n)) when the nodes\nare uniformly distributed in T2 . The expressions for the\nfirst and second expected spectral moments are m1 = 0 and\nE [m2 ] = π nr2 . The third spectral moment is proportional to\nthe number of closed walks of length three in the graph. In\nthe two-dimensional case, we count the number of triangular\nwalks using a technique that we illustrate in Fig. 1. In this\nfigure, we plot two nodes vi and v j . The parameters ρ and\nφ in Fig. 1 denote the distance and angle between these\n\n\fFig. 1. This figure illustrates the technique proposed in Section III.B to\ncount the number of triangular walks in a two-dimensional RGG.\n\ntwo nodes, i.e., ρ , x j − xi and φ = ∡ (x j − xi ). An edge\nbetween vi and v j exists if an only if v j is located inside the\ncircle Si (r). In this setting, the probability of existence of a\ntriangle touching both vi and v j is equal to the probability of\na third node vk being in the shaded area Al (see Fig. 1). This\narea is the result of intersecting the circles Si (r) and S j (r),\nand the resulting probability is equal to n Al . The intersecting\nregion Al is a symmetric lens which area can be computed\nas a function of ρ and r as follows:\n\u001a 2 −1 ρ \u0001 ρ p\n4r2 − ρ 2, for ρ ≤ r,\n2r cos\n2r − 2\nAl (ρ ; r) =\n0,\nfor ρ > r.\n(9)\nTherefore, we can compute the expected number of triangles\nby integrating over the set of all possible positions of v j , i.e.,\nη ∈ [0, r] and φ ∈ [0, 2π ), as follows\nE [ti ] =\n\nZ r Z 2π\n\nρ =0 φ =0\n\nn2 Al (ρ ; r) ρ d ρ d φ .\n\nConsequently, we have the following expression for the third\nexpected spectral moment E [m3 ] = n1 ∑ni=1 E [ti ] = E [ti ].\nIn the following, we extend the technique introduced above\n(k)\nto compute closed walks of arbitrary length. Denote by Wi\nthe number of closed walks of length k starting at node v1\nin G(χn ; r (n)). The idea behind our technique is illustrated\nin Fig. 2, where we represent a particular closed walk of\nlength 6. We denote this walk by wk = (v1 , v2 , ..., vk−1 , vk , v1 ).\nWe define the following set of relative distances and angles\nbetween every pair of connected vertices: ρi , kxi+1 − xik\nand φi = ∡ (xi+1 − xi ) for i = 1, ..., k − 2. We also define the\nfollowing parameter\nk−2\n\n∑ ρ j eiα j\n\nj=1\n\n,\n\n√\n(i = −1) which is the resulting distance between nodes\nvk−1 and v1 given a particular set of distances and angles\n{(ri , φi )}i=1,...,k−2 (see Fig. 2). In this setting, the conditional\nprobability of existence of a walk wk = (v1 , v2 , ..., vk−1 , vk , v1 )\ngiven the set of relative positions, {(ri , φi )}i=1,...,k−2 , is equal\nto the probability of vk being in the shaded area Al in Fig.\n2. We have an expression for this area in (9), where ρ is\ndefined in (12). Finally, we can compute the expectation of\n(k)\nWi by performing an integration over the set of all possible\npositions (i.e., ρ j ∈ [0, r] and φ j ∈ [0, 2π ) for j = 2, ..., k − 1),\nas follows\n\n(10)\n\nAfter substituting (9) in (10), we can explicitly solve the\nresulting integral to be\n√ !\n\u00012\n\u00012\n3 3\nE [ti ] = π −\n(11)\nπ nr2 ≈ 5.78 nr2 .\n4\n\nρ=\n\nFig. 2. This figure illustrates the technique proposed in Section III.B to\ncount the number of closed walks of length k in a two-dimensional RGG.\n\n(12)\n\nZ\nh\ni\n(k)\nE Wi\n= nk−1\n\n(η ,ϕ )∈Ck−2\n\nAl (ρ ; r)\n\nk−1\n\n∏ η j dη\n\ndϕ ,\n\nj=2\n\nwhere η = (ρ2 , ..., ρk−1 ), ϕ = (φ2 , ..., φk−1 ), and Ck−2 =\n{(η , ϕ ) : η ∈ [0, r]k−2 and ϕ ∈ [0, 2π )k−2 }. Although a\nclosed-form for the above expression can only be computed\nfor k ≤ 3, we can always find a good approximation via\nnumerical integration. For example,\n\u00013 the integration for k = 4\n(4)\ngives us E[Wi ] ≈ 14.2511 nr2 .\nIn the following table, we compare our analytical results\nwith numerical realizations of the empirical spectral moments of a two-dimensional RGG. In our simulations, we\n2\ndistribute n = 1000 nodes\np uniformly on T and choose a\nconnectivity radius r = 50/π n ≈ 0.1784 (which results in\nan average degree E[di ] = 50). The second, third, and forth\ncolumns in the following table represent the analytical expectation of the spectral moments, the empirical average from 10\nrandom realizations, and the corresponding empirical typical\n\n\fdeviation, respectively.\nk\n1\n2\n3\n4\n\nE [mk ]\n0\n50\n1,464.1\n59,452\n\nEmpirical Average\n-9.2e-16\n50.0820\n1,475.8\n60,127\n\nTypical Deviation\n1.1e-15\n0.3908\n37.3777\n2,955.3\n\nOur numerical results present an excellent match with our\nanalytical predictions.\nIn the following section, we use the results introduced in\nthis section to study the spreading of an infection in a random\ngeometric network.\nIV. S PECTRAL A NALYSIS OF V IRUS S PREADING\nIn this section, we use the expressions for the expected\nspectral moments to design random geometric networks to\ntame an initial viral infection in the network. In our design\nproblem, we consider that the size of the network n and\nthe parameters in (2), i.e., β and δ , are given. Hence, our\ndesign problem is reduced to studying the range of values of\nr for which the RGG is well-suited to tame an initial viral\ninfection.\nA sufficient condition for local stability around the\ndisease-free state was given in (3). Thus, we have to find\nthe range of values of r for which the associated spectral\nradius λmax is smaller than the ratio δ /β . In the following\nsubsection, we show how to derive an analytical upper\nbound for the spectral radius based on the expected spectral\nmoments.\nA. Analytical Upper Bound for the Spectral Radius\nIn order to upper-bound the spectral radius, we use\nWigner’s high-order moment method [11]. This method\nprovides a probabilistic upper bound based on the asymptotic\nbehavior of the k-th expected spectral moments for large k.\nWe present the details for a one-dimensional RGG, although\nthe same technique can be applied to RGG’s in higher\ndimensions. For a one-dimensional RGG in the connectivity\nregime, we derived an explicit expression for the expected\nspectral moments in (8). A logarithmic plot of Vol(Hk (1))\nfor k = 1, 2, ..., 9 unveils that Vol(Hk (1)) → β1 ck1 for largeorder moments (a line in logarithmic scale), where, from a\nnumerical fitting, we find that β1 = 0.35 and c1 = 1.9192.\nTherefore, from (8) we have\nE [mk ] ≍ β1 (c1 nr)k ,\nfor large k.\nFor even-order expected spectral moments (i.e., k = 2s for\ns ∈ N), the following holds\nE [m2s ] =\n\n1 n\n1\n2s\n].\n∑ E[λi2s ] ≥ n E[λmax\nn i=1\n\nDefine f (n) = n1−δ log n; thus, for any ε , δ > 0 (and c1 =\n1.9192), we can apply Markov´s inequality as follows\n2s ]\n\u0001\nE[λmax\n2s\n≥ (c1 nr + ε r f (n))2s\n≤\nP λmax\n(c1 nr + ε r f (n))2s\nn E [m2s ]\n,\n≤\n(c1 nr + ε r f (n))2s\n\nFig. 3. Comparison between the empirical spectral radius of an RGG\n(circles in the plot) and the values of our analytical upper bound (solid\n¯\nline) for n = 1000 and r (n) = d/2n,\nwith expected degrees d¯ = [10:1:100].\n\nFor large s, one can prove that [8]\n\u0013\n\u0012\nε\nP (λmax ≥ c1 nr + ε r f (n)) ≤ nβ1 exp − sn−δ log n .\nc1\nAssuming that s grows as β2 nδ , for β2 , δ > 0, we have\n\u0013\n\u0012\nβ2 ε\nlog n = o (1) ,\nP (λmax ≥ c1 nr + ε r f (n)) ≤ nβ1 exp −\nc1\nfor all sufficiently large ε . Thus,\n\u0011\n\u0010\nlim P λmax < c1 nr + ε rn1−δ log n = 1.\nn→∞\n\n(13)\n\nIn other words, λmax is upper-bounded by cnr + ε rn1−δ log n\nwith probability 1 for n → ∞. In practice, for a large (but\nfinite) n, we can use 1.9192 nr as an upper bound of λmax .\nIn Fig. 4, we plot the empirical spectral radius of an RGG\n¯\nwith n = 1000 and r (n) = d/2n,\nwith expected degrees d¯ =\n[10:1:100] (circles in the figure). We also plot the values of\nour analytical upper bound, 1.9192 nr, in solid line.\nThe technique introduced in this subsection is also valid\nfor RGG’s in higher-dimensions. In general, one can prove\nthat for a d-dimensional RGG that the expected spectral\n\u0001k\nmoment grows as E [mk ] → βd cd nrd . Applying Wigner’s\nhigh-order moment method to this sequence, one can derive\na probabilistic upper bound similar to (13). In particular, we\nhave that λmax < cd nrd for large n with high probability. In\nthe following subsection, we use our results to design the\nconnectivity radius of an RGG in order to tame an initial\nviral infection.\nB. Spectral Radius Design\nOnce the spectral radius is upper-bounded, our design\nproblem becomes trivial. Since (3) represents a sufficient\ncondition for local stability around the disease-free state, we\nhave the following condition to tame an initial viral infection\nfor a d-dimensional RGG:\n\nλmax (G(χn ; r)) < cd nrd <\n\nδ\n,\nβ\n\n\fFig. 4. Color map representing the evolution of the probabilities of infection\npi [n] for i = 1,...,1000 in an RGG with n = 1000 nodes, connectivity\nradius r = 0.005, rate of infection β = 0.020, and recovery rate δ = 0.018.\nEach horizontal line represents the value of pi [n] for a particular i. In this\ncolor map, blue represents a zero value, green and yellow tones represent\nintermediate values, and red represents values close to one. In this case, we\nobserve an epidemic outbreak.\n\nwhich implies the following design condition for the connectivity radius:\n\u0012\n\u00131/d\nδ\nr<\n,\n(14)\nβ cd n\n\nwhere cd is a positive constant that depends on the dimension\nof Td . For example, in the one-dimensional case, we have\nc1 = 1.9192; hence, (14) becomes r < δ / (1.9192 β n). We\nnow validate this result with several numerical simulations\nof a viral infection in a one-dimensional RGG.\nConsider an RGG with n = 1000 nodes and a connectivity\nradius of r = 0.005 (which implies an average degree of\n10). The resulting spectral radius in this RGG is λmax =\n17.2629. In our numerical simulations, we choose the initial\nprobability of infection to be pi [0] ∼ 0.01Unif[0, 1]; hence,\napproximately 1% of the nodes in the network are initially\ninfected. In our first experiment, we choose a rate of infection\nβ = 0.020, and a recovery rate δ = 0.018. Since the sufficient\ncondition for viral control in (14) is not satisfied, we cannot\nguarantee an initial infection to be tamed. In Fig. 5 we show\nan image of the evolution of the probability of infection for\nthis case. This figure is a color map for the simultaneos\nevolution of pi [n] for i = 1, ..., 1000. Each horizontal line\nrepresents the value of pi [n] for a particular i. In this color\nmap, blue represents a zero value, green and yellow tones\nrepresent intermediate values, and red represents values close\nto one. On the other hand, if we increase the recovery rate to\nδ = 0.35 keeping the rest of parameters fixed, we have that\nδ /β = 17.50 > λmax and we satisfy condition (3). Hence,\nthe probability of infection of every node is guaranteed to\nconverge towards zero. In Fig. 6, we observe the color map\nfor the evolution of the probability of infection in this case,\nwhere we clearly observe how pi [n] → 0 for all i. Hence,\nthis latter RGG is well-suited to tame initial viral infections.\nV. C ONCLUSIONS\nIn this paper, we have studied the spreading of a viral\ninfection in a random geometric graph from a spectral point\n\nFig. 5. Color map representing the evolution of the probabilities of infection\npi [n] when we increase the recovery rate to δ = 0.35 (the rest of parameters\nare the same as we used for Fig. 5). We observe how the probability of\ninfection of every node converges towards zero in this case.\n\nof view. We have focused our attention on studying the\neigenvalue distribution of the adjacency matrix. We have\nderived, for the first time, explicit expressions for the spectral\nmoments of the adjacency matrix as a function of the density\nof nodes and the connectivity radius. We have then applied\nour results to the problem of viral spreading in a network\nwith a low-density infection. Using our expressions, we have\nderived upper bounds for the spectral radius of the adjacency\nmatrix. Finally, we have applied this upper bound to design\nrandom geometric graphs that are well-suited to tame an\ninitial low-density infection. Our numerical results match our\npredictions with high accuracy.\nR EFERENCES\n[1] N. Biggs, Algebraic Graph Theory, 2nd Edition. Cambridge University\nPress, 1993.\n[2] P. Blackwell, M. Edmondson-Jones, and J. Jordan, “Spectra of Adjacency Matrices of Random Geometric Graphs,” Preprint.\n[3] R. Durrett and X.-E. Liu, “The Contact Process on a Finite Set,”\nAnnals of Probability, vol. 16, pp. 1158-1173, 1988.\n[4] A. Ganesh, L. Massoulie, and D. Towsley, “The Effect of Network\nTopology on the Spread of Epidemics,” Proc. IEEE INFOCOM ’05,\npp. 1455-1466, 2005.\n[5] R.L. Graham, D.E. Knuth, and O. Patashnik, Concrete Mathematics:\nA Foundation for Computer Science, Second Edition, Addison-Wesley,\n1994.\n[6] J.B. Lasserre, “An Analytical Expression and an Algorithm for the\nVolume of a Convex Polyhedron in Rn ,” J. Optim. Theor. Appl., vol.\n39, pp. 363–377, 1983.\n[7] M. Penrose, Random Geometric Graphs, Oxford University Press,\n2003.\n[8] V.M. Preciado, Spectral Analysis for Stochastic Models of Large-Scale\nComplex Dynamical Networks, Ph.D. dissertation, Dept. Elect. Eng.\nComput. Sci., MIT, Cambridge, MA, 2008.\n[9] V.M. Preciado and A. Jadbabaie, “Moment-Based Spectral Analysis\nof Random Geometric Graphs,” in preparation.\n[10] Y. Wang, D. Chakrabarti, C. Wang, and C. Faloutsos, “Epidemic\nSpreading in Real Networks: An Eigenvalue Viewpoint,” Proc. Int.\nSymp. Reliable Distributed Systems, pp. 25-34, 2003.\n[11] E.P. Wigner, “On the Distribution of the Roots of Certain Symmetric\nMatrices,” Ann. Math., vol. 67, pp. 325–327, 1958.\n\n\f"
        ],
        [
         "11",
         "11",
         "cs.CE",
         "Computational Engineering",
         "1408.3622v2.pdf",
         "arXiv:1408.3622v2 [cs.NA] 18 Aug 2014\n\nComputer Science Technical\nReport CSTR-14-10\nAugust 19, 2014\nHong Zhang and Adrian Sandu and Paul Tranquilli\n\nApplication of approximate matrix\nfactorization to high order linearly\nimplicit Runge-Kutta methods\nComputational Science Laboratory\nComputer Science Department\nVirginia Polytechnic Institute and State University\nBlacksburg, VA 24060\nPhone: (540)-231-2193\nFax: (540)-231-6075\nEmail: sandu@cs.vt.edu\nWeb: http://csl.cs.vt.edu\n\nInnovative Computational Solutions\n\n\fApplication of approximate matrix factorization\nto high order linearly implicit Runge-Kutta\nmethods\nHong Zhang∗\n\nAdrian Sandu†\n\nPaul Tranquilli‡\n\nAbstract\nLinearly implicit Runge-Kutta methods with approximate matrix factorization can solve efficiently large systems of differential equations that\nhave a stiff linear part, e.g. reaction-diffusion systems. However, the use\nof approximate factorization usually leads to loss of accuracy, which makes\nit attractive only for low order time integration schemes. This paper discusses the application of approximate matrix factorization with high order\nmethods; an inexpensive correction procedure applied to each stage allows\nto retain the high order of the underlying linearly implicit Runge-Kutta\nscheme. The accuracy and stability of the methods are studied. Numerical experiments on reaction-diffusion type problems of different sizes and\nwith different degrees of stiffness illustrate the efficiency of the proposed\napproach.\n\n1\n\nIntroduction\n\nA frequently used approach to solve partial diﬀerential equations (PDEs) is the\nmethod of lines, where the spatial derivative terms are discretized ﬁrst using\ntechniques such as ﬁnite diﬀerences, ﬁnite volumes or ﬁnite elements, and then\nintegrating the resulting system of ordinary diﬀerential equations (ODEs) in\ntime. Discretization of PDEs with linear terms in space leads to a semi-linear\nODE system of the form\ny ′ = F (y, t) = L y + f (y, t) ,\n\ny ∈ RN\n\n(1)\n\nwhere L is a spatial linear operator and f (y, t) is nonlinear. We consider the case\nwhere the linear term has a fast characteristic time scale and the nonlinear term\n∗ Computational Science Laboratory, Department of Computer Science, Virginia Polytechnic Institute and State University, Blacksburg, VA 24061 (zhang@vt.edu)\n† Computational Science Laboratory, Department of Computer Science, Virginia Polytechnic Institute and State University, Blacksburg, VA 24061 (sandu@cs.vt.edu)\n‡ Computational Science Laboratory, Department of Computer Science, Virginia Polytechnic Institute and State University, Blacksburg, VA 24061 (ptranq@vt.edu)\n\n1\n\n\fhas a slow characteristic time scale. Due to the Courant-Friedrichs-Levy (CFL)\ncondition, the step size of an explicit time integrator is restricted by the fastest\ntime scale. To alleviate the time step constraint imposed by the stiﬀ linear term\nwith a reasonably low computational cost, linearly implicit methods (particular\ncases of implicit-explicit methods) treat the stiﬀ linear term implicitly while the\nnonlinear term explicitly.\nVarious families of linearly implicit integration methods have been proposed\nand successfully applied to solve PDEs with linear dispersion and dissipation\n[11, 8, 3, 2]. Fully implicit schemes solve at each step a linear system where the\nmatrix involves the Jacobian of the right hand side function. Eﬃcient schemes\nspecialized in the solution of (1) such as linearly implicit methods use only the\npart of the Jacobian associated with the stiﬀ linear term, resulting in linear\nsystems of the form\n(I − h γ L) x = ℓ, x, ℓ ∈ RN ,\n(2)\nwhere h is the step size, γ is a parameter of the integration scheme, and the\nright hand side ℓ is determined by the method.\nThe matrix L is usually sparse but has a large bandwidth, especially when\nhigh order spatial discretization schemes are applied. Consequently the LU\nfactorization of the matrix in (2) can be very costly in large scale problems.\nOne approach to increase eﬃciency is to apply iterative solvers to (2), as in [17],\nbut there are associated challenges related to preconditioning and convergence.\nAn alternative approach to increase the computational eﬃciency is approximate matrix factorization (AMF). Assuming that the matrix is a sum of simpler\nmatrices\nR\nX\nLr\n(3)\nL=\nr=1\n\nAMF replaces the system matrix (2) with a product of simpler, and easier to\nfactorize, matrices\ne=\nI− hγL ≈ I − hγL\n\nR\nY\n\n(I − h γ Lr ) .\n\n(4)\n\nr=1\n\ne as\nThe approximation formula (4) deﬁnes implicitly the matrix L\ne =L+\nL\n\nR\nX\n\nk=2\n\nX\n\n(−hγ)k−1\n\nLi1 Li2 . . . Lik .\n\n(5)\n\n1≤i1 <i2 <···<ik ≤R\n\nFor example, consider L to be the discrete two-dimensional Laplace operator.\nIt can be written in the form (3) where L1 and L2 correspond to the derivatives\nalong the x direction and y direction, respectively. The AMF approximation\ncorresponds to the alternating directions factorization [14, 16]\ne := (I − h γ L1 ) (I − h γ L2 )\nI − hγL\n2\n\n\fwhere\n\ne = L − h γ L1 L2 .\nL\n\n(6)\n\nThe idea of using AMF to speed up calculations in implicit time integration\nhas appeared multiple times in the literature. Sandu [18] discussed a family\nof methods named ELADI, which are Rosenbrock-W schemes that make use\nof AMF to speed up calculations. The order of the resulting discretization remains unchanged since Rosenbrock-W schemes can accommodate any Jacobian\napproximation. Houwen et al. provided a survey for AMF methods applied in\nthe context of several diﬀerent linear integration schemes, and provide stability results for such schemes [19]. The discussion is limited to methods of low\norder, two and three, presumably due to the inaccurate nature of the AMF\napproximation. Gonzalez [10] proposed a way to apply AMF-reﬁnements to\nsecond-order and third-order Rosenbrock-type methods for solving advectiondiﬀusion-reaction PDEs. But their methods are limited to the low or medium\nprecision level, and no generalization to higher order is supplied. In [5] Beck\net al. compared the eﬃciency of AMF versus Krylov based approaches to the\nsolution of linear systems in the context of Newton iterations arising in Radau\n[13] and Peer [6] integration methods. These methods avoid the issue of order\ndegradation, through the use of integration schemes in which the Jacobian of\nthe spatial discretization does not appear explicitly. They conclude that AMF\nmethods are extremely eﬃcient, particularly when low accuracy solutions are\nsought. Berzins et al. [7, 1] presented a method for solving the linear system\nin a Newton iteration arising from several classes of time integration methods\nincluding theta methods, backward diﬀerential formulas, and implicit-explicit\n(IMEX) multistep methods. They performed and analysis of the error arising\nfrom operator splitting and provided a method to control timesteps such as to\nguarantee Newton convergence when using AMF. Since AMF is only used to\nspeed up the solution of the nonlinear equations, the error does not aﬀect the\norder of accuracy at the time stepping level. Calvo and Gerisch [9] applied\nAMF to a form of linearly implicit Runge-Kutta (LIRK) methods that avoids\nthe computation of matrix-vector products. First order of convergence is obtained by using a third-order LIRK method, and improved to second order by\nadding a correction to the solution at each time step.\nNone of the existing methods that take advantage of AMF can provide highly\naccurate results and there is still room for improvement in eﬃciency. The goal\nof this work is to achieve high accuracy while maintaining a low computational\ncost. The focus is on using AMF with linearly implicit Runge-Kutta methods of\nhigh order. The main contribution of this paper is to account for the inherent\ninaccuracy of AMF through low cost reﬁnements of the stage values and to\nrecover the accuracy of the underlying time discretization.\nThe remainder of this paper is organized as follows. Section 2 introduces\nLIRK methods and the existing approaches to apply AMF. Section 3 presents\na new strategy to incorporate AMF by using an inexpensive stage reﬁnement\nprocedure. An error analysis explains how this strategy solves the accuracy\ndegradation issue that aﬀects existing approaches. Stability issues are also in-\n\n3\n\n\fvestigated. Section 4 reports numerical rests for a variety of test problems of\ndiﬀerent dimensions and diﬀerent degrees of stiﬀness, and illustrates the convergence behavior and eﬃciency of the approach. Conclusions are drawn in Section\n5.\n\n2\n\nLinearly implicit Runge-Kutta methods\n\nA general linearly implicit Runge-Kutta (LIRK) scheme proposed by Calvo, de\nFrutos, and Novo [8] is obtained by applying the IMEX Runge-Kutta methods\n[4, 15]\nYi\nyn+1\n\n= yn + h\n= yn + h\n\ni−1\nX\n\nj=1\ns\nX\n\nai,j f (Yj ) + h\nbj f (Yj ) + h\n\ni\nX\n\nj=1\ns\nX\nj=1\n\nj=1\n\nb\nai,j g(Yj ),\n\nbbj g(Yj ),\n\n(7a)\n(7b)\n\nto solve (1) where the stiﬀ component is linear, g(y) := Ly:\n(I − h b\nai,i L) Yi\n\n=\n\nyn+1\n\n=\n\nℓi := yn + h\n\ni−1\nX\n\nai,j f (Yj ) + h\n\nj=1\n\nj=1\n\nyn + h\n\ns\nX\n\ni−1\nX\n\nbj f (Yj ) + h\n\ns\nX\nj=1\n\nj=1\n\nb\nai,j L Yj ,\n\nbbj L Yj .\n\n(8a)\n(8b)\n\nOrder conditions for LIRK methods are derived in [8], and simpliﬁcations of\nthe IMEX Runge-Kutta conditions are possible due to the special form of the\nnonlinear term.\nThe coeﬃcients b\nai,i in practical methods are chosen to be equal to the same\nvalue γ for computational eﬃciency, as this allows to reuse the same LU factorization in all stages. A linear transformation of variables allows for a reformulation of the stage equations (8a) in a form that avoids explicit multiplications\nby the matrix L\n(I − h γ L) Ui\n\n=\n\nyn +\n\ni−1\ni−1\nX\nX\nb\nai,j − ai,j\nai,j F (Yj ),\nYj + h\nγ\nj=1\nj=1\n\n(9a)\n\nYi\n\n=\n\nUi −\n\ni−1\nX\nb\nai,j − ai,j\nYj .\nγ\nj=1\n\n(9b)\n\nMoreover, it is convenient to choose pairs of methods with the same weights\nbj = bbj in which case the next step solution (8b) is\nyn+1 = yn + h\n\ns\nX\nj=1\n\n4\n\nbj F (Yj ).\n\n(9c)\n\n\fCalvo and Gerisch [9] studied the use of LIRK methods with AMF. The\ne in (9a), or equivapproximation is obtained by replacing I − hγ L with I − hγ L\ne\nalently, by using the matrix L instead of L in (8)\n\u0010\n\u0011\ne Yei\nI − hγL\n\nyen+1\n\n=\n=\n\nyn + h\nyn + h\n\ni−1\nX\n\nj=1\ns\nX\nj=1\n\nai,j f (Yej ) + h\nbj f (Yej ) + h\n\ni−1\nX\n\nj=1\ns\nX\nj=1\n\ne Yej ,\nb\nai,j L\n\nbbj L\ne Yej .\n\nIt can also be regarded as a direct application of the LIRK method (8) to solve\nthe perturbed ODE system\n\u0010\n\u0011\ne ye + f (e\ne ye\nye′ = L\ny ) = F (e\ny) − h γ L − L\n(10)\n\ninstead of the original system (1). The ﬁrst-order behavior of this approach has\nbeen explained in [9] by the fact that the perturbation added in (10) changes\nthe solution over one time step by O(h2 ).\nTo recover second order Calvo and Gerisch [9] apply corrections to the numerical solution obtained by LIRK with AMF. One such correction has the\nform\n\u0010\n\u0011−1 \u0010\n\u0011\ne\ne yn .\nyn+1 = yen+1 + h2 γ I − h γ L\nL−L\n(11)\n\nThe matrix inverse in the correction term uses the same LU factorization as the\nsolution. The presence of the matrix inverse in the correction term ensures the\nlinear stability of the new solution (11). It is also noted in [9] that “regaining\norder three using corrections similar to (11) is not feasible with a computational\ncost comparable with the cost of recovering order two”.\n\n3\n\nLIRK-AMF methods with stage refinement\n\nWe consider a diﬀerent way to incorporate AMF into LIRK methods, which\nforms the basis of all approaches presented in this paper. In order to keep the\nright-hand side of the original ODE system (1), we only approximate I − h γ L\ne when computing the Runge-Kutta stages:\nwith I − h γ L\n\u0010\n\u0011\ne Yei\nI − hγL\n\nyen+1\n\n=\n=\n\nℓei := yn + h\nyn + h\n\ns\nX\nj=1\n\ni−1\nX\nj=1\n\nai,j f (Yej ) + h\n\nbj f (Yej ) + h\n\ns\nX\nj=1\n\ni−1\nX\nj=1\n\nbbj L Yej .\n\nb\nai,j L Yej ,\n\n(12a)\n(12b)\n\nThus the new method uses an inexact Jacobian for the implicit part in the LIRK\nscheme. The change of the left hand side in (12a) will aﬀect the solution, and\na correction is needed to restore accuracy.\n5\n\n\fConsider the solution of the original stage equation (8a)\n(I − h γ L) Yi − ℓi = 0\nby simpliﬁed Newton iterations of the form:\n\u0011\n\u0010\n\u0011−1 \u0010\n(k−1)\n(k)\n(k−1)\ne\n− I − hγL\nYi = Yi\n− ℓi .\n· (I − h γ L) Yi\n\n(13)\n\nFor example, the direct solution is:\n(0)\n\nYi\n\n\u0010\n\n=\n\ne\nI− hγL\n\n\u0011−1\n\nℓi ,\n\nand the solution after one reﬁnement iteration is:\n\u0011\n\u0010\n\u0011−1 \u0010\n(0)\n(1)\n(0)\ne\n= Yi − I − h γ L\nYi\n· (I − h γ L) Yi − ℓi\n\u0010\n\u0011−1 \u0010\n\u0011\n(0)\ne\ne − h γ L Y (0) .\n= Yi − I − h γ L\n· hγL\ni\n\n(14)\n\nNext we analyze the linear system solution errors and investigate how this\nerrors propagate to aﬀect the solution at the next step.\n\n3.1\n\nError analysis\n\nConsider the exact stage solution\nYi = (I − h γ L)−1 ℓi .\nThe linear system solution error after k iterations is deﬁned as\n(k)\n\nεi\n\n(k)\n\n= Yi\n\n− Yi .\n\nFrom (13) we obtain\n(k)\n\nεi\n\n\u0010\n\u0011−1\n(k−1)\n(k−1)\ne\n− I− hγL\n= εi\n· (I − h γ L) εi\n\u0010\n\u0011−1 \u0010\n\u0011\ne\ne − (I − h γ L) ε(k−1)\n=\nI− hγL\n· I− hγL\ni\n\u0010\n\u0011−1 \u0010\n\u0011\ne\ne − h γ L ε(k−1) .\n= − I − hγL\n· hγL\ni\n\n(15a)\n(15b)\n(15c)\n\nNonstiff or moderately stiff case. In the nonstiﬀ or moderately stiﬀ case\ne − Lk = O(h) and\nwe have khLi k = O(h), therefore kL\n\u0010\n\u0011−1 \u0010\n\u0011\n\u0001\ne\ne − h γ L = O h2 .\nI − hγL\n· hγL\n\nConsequently from (15c) the error decrease is\n(k+1)\n\nεi\n\n=\n\n(k)\n\nO(h2 ) εi\n\n⇒\n\n6\n\n(k)\n\nεi\n\n\u0001\n= O h2k+2 .\n\n\fHighly stiff case. For the highly stiﬀ case khLi k ≫ 1. We make the assumption that, for any h there exists 0 < ρ(h) < 1 such that the following matrix\nnorm is uniformly bounded for any step size smaller than h:\n\u0010\n\u0011−1 \u0010\n\u0011\ne\ne − h γ L ≤ ρ(h) < 1 , ∀h : 0 ≤ h ≤ h.\nI − hγL\n· hγL\nIn the highly stiﬀ case the error decrease equation (15c) leads to\n(k)\n\nεi\n\n(k−1)\n\n⇒\n\n= ρ εi\n\n(k)\n\nεi\n\n(0)\n\n= ρk ε i .\n\nWe expect that the convergence rate will decrease with increasing step sizes,\ni.e., ρ(h) → 1 when h → ∞.\nExample 3.1 (Dimensional splitting of the discrete diﬀusion operator on a\nCartesian grid). Consider the two-dimensional diﬀusion operator with periodic\nboundary conditions on a domain of size LX ×LY . It is discretized on an M ×N\ngrid of size ∆x, ∆y. We perform a dimensional splitting. The error equation\n(15c) can be written as\n(k)\n\n(I − h γ L1 ) (I − h γ L2 ) εi\n\n(k−1)\n\n= (h γ L1 ) (h γ L2 ) εi\n\n.\n\n(16)\n\nConsider the discrete frequencies\n−\n\nM\nM\n≤m≤\n− 1,\n2\n2\n\n−\n\nN\nN\n≤n≤\n− 1,\n2\n2\n\nm̃ =\n\n2πm\n,\nLX\n\nñ =\n\n2πn\n.\nLY\n\nA discrete Fourier transform applied to (16) gives the following error equation\nfor each spatial mode (m, n) of the error:\n2\n2\n(k−1)\n(1 + h γ m̃2 )(1 + h γ ñ2 ) ε̂(k)\nm,n = (−h γ m̃ )(−h γ ñ ) ε̂m,n\n\nLet\n\n\u00132\n\u00132\n\u0012\n\u0012\nh\nh\n2πm\n2πn\n,\nz\n=\n.\n2\n∆x2\nM\n∆y 2\nN\nThe evolution of the mode (m, n) of the error is\nz1 =\n\nε̂(k+1)\nm,n =\n\n(γz1 )(γz2 )\nε̂(k)\n(1 + γz1 )(1 + γz2 ) m,n\n\nThe error ampliﬁcation factor for the (m, n) mode is\nRm,n =\n\n(γz1 )(γz2 )\n,\n(1 + γz1 )(1 + γz2 )\n\n|Rm,n | < 1,\n\n|Rm,n |\n\nz1 ,z2 →∞\n\n−→\n\n1.\n\nTherefore we expect that more iterations will be required for stiﬀ problems. The\nAMF with correction will work well for mildly stiﬀ problems. It will work well\nfor stiﬀ problems only when the solution is smooth, and the high order modes\nare approaching zero. Similar conclusions are drawn for the three-way splitting\nof a three dimensional diﬀusion problem\nRm,n.k =\n\n(γz1 )(γz2 ) + (γz1 )(γz3 ) + (γz2 )(γz3 ) + (γz1 )(γz2 )(γz3 )\n.\n(1 + γz1 )(1 + γz2 )(1 + γz3 )\n7\n\n\fRemark 3.1. The accuracy analysis in Calvo and Gerisch’s paper [9] considers\nthe non-stiﬀ case. For very stiﬀ systems the correction term (11) is\n\u0010\n\u0011−1 \u0010\n\u0011\ne\ne yn = O(h)\nh I− hγL\nhγL − hγL\n\nand the remaining error term is O(h2 ), therefore the corrected solution is ﬁrst\norder.\n\n3.2\n\nPropagation of linear system errors\n\nThe computation of stage values via (12) and (13) propagates the linear system\nerrors from one stage to another. To account for the total error consider the\nmethods (8) and (12) and let\nδYi = Yei − Yi .\n\n(17)\n\nWe assume that these errors are small. The exact stage equations (8a) read\n(I − h γ L) Yi\n\n= yn + h\n\ni−1\nX\n\nai,j f (Yj ) + h\n\ni−1\nX\nj=1\n\nj=1\n\nb\nai,j L Yj .\n\nThe AMF stage equations are solved inexactly and read\n\u0011\n\u0010\n=\n(I − h γ L) Yei − εi\n\nyn + h\n\ni−1\nX\nj=1\n\nai,j f (Yej ) + h\n\ni−1\nX\nj=1\n\nb\nai,j L Yej ,\n\nwhere εi is the error due to the simpliﬁed Newton approximation of the system\nsolution. Using (17) we express this in terms of the solution of the exact stages\n(I − h γ L) (Yi + δYi − εi ) =\n\ni−1\nX\n\nyn + h\n\nai,j f (Yj + δYj )\n\nj=1\n\n+h\n\ni−1\nX\nj=1\n\n=\n\nb\nai,j L (Yj + δYj )\n\ni−1\nX\n\nyn + h\n\nai,j f (Yj ) + h\n\nj=1\n\nj=1\n\n+h\n\ni−1\nX\n\ni−1\nX\n\nai,j fj′ δYj + h\n\ni−1\nX\nj=1\n\nj=1\n\nb\nai,j L Yj\n\nb\nai,j L δYj ,\n\nwhere we use the mean value theorem\nf (Yj + δYj ) − f (Yj ) = fj′ · δYj ,\n\n8\n\nfj′ =\n\nZ\n\n0\n\n1\n\nfy (Yj + s δYj ) ds.\n\n\fThe nonstiﬀ/moderately stiﬀ assumption about the nonlinear terms f implies\nthat these average Jacobians are of moderate size,\nkfj′ k = O(1) ,\n\n∀j.\n\n(18)\n\nAfter subtracting the exact stage equations we are left with the error relation\n\u0011−1 \u0010\n\u0011\n\u0010\nb⊗L ε\nb ⊗ L − hA ⊙ F′\nI − hΓ\n(19)\nδY = I − h A\n\nb\nb and (A ⊙ F ′ )i,j = ai,j f ′ . For nonstiﬀ or moderately stiﬀ\nwhere Γ=diag(\nA)\nj\nproblems khLk = O(h), khF ′ k = O(h), and for small step sizes we have\nkδY k = (1 + O(h)) kεk.\nFor highly stiﬀ systems khLk → ∞ and we have\n\u0010\n\u0011−1 \u0010\n\u0011\nb ⊗ (L/kLk)\nb ⊗ (L/kLk) ε\nδY = A\nΓ\n\ntherefore\n\nkδY k = O(1) kεk.\nIn both the nonstiﬀ and the stiﬀ cases the stage error is of the size of the linear\nsystem solution error.\nFrom the exact step equation the error in the solution is\nδyn+1\n\n= h\n\ns\nX\n\nbj fj′\n\nδYj + h\n\ns\nX\nj=1\n\nj=1\n\nbbj L δYj .\n\nNon-stiff or moderately stiff problems. In the non-stiﬀ or moderately\nstiﬀ case where kLk = O(1) we have\n\u0001\n\u0001\n\u0001\n⇒ ke\nyn+1 − yn+1 k ∼ O h2k+3 .\n⇒ δY ∼ O h2k+2\nε ∼ O h2k+2\nFor k = 0 and k = 1 correction iterations we have the following results.\n\nTheorem 3.1. If a LIRK method of order higher than 2 is applied to a nonstiﬀ\nor moderately stiﬀ case of (1) with the AMF technique according to (12), then\nthe order of the method will reduce to second order.\nTheorem 3.2. If a LIRK method of order 3 or 4 is applied to a nonstiﬀ or\nmoderately stiﬀ case of (1) with the AMF technique according to (12), and one\ncorrection iteration (14) is applied to each stage value Yi , the order of the method\nis the same as that of the original method.\nRemark 3.2. Taking more iterations in the correction procedure may further\nreduce the linear system solution errors.\nRemark 3.3. Correspondingly two iterations of the correction procedure are\nneeded for a LIRK method \u0001of order 5 or 6 since k = 2 yields an error in the\nsolution of magnitude O h7 . The idea can be extended to arbitrarily high order\nmethods.\n9\n\n\fVery stiff problems. In the highly stiﬀ case a more complex analysis based\non (19) is called for since khLk can be large and khL · δYj k can also become\nlarge. We consider LIRK methods with a stiﬄy accurate implicit component\nbbi = b\nas,i . We have that\nYs\n\n= yn + h\n\nyn+1\n\n= yn + h\n\ni−1\nX\n\nj=1\ns\nX\n\nas,j f (Yj ) + h\nbj f (Yj ) + h\n\n= Ys + h\n\nj=1\ns\nX\nj=1\n\nj=1\n\ns\nX\n\ns\nX\n\nb\nas,j L Yj ,\n\nbbj L Yj\n\n(bj − as,j ) f (Yj ).\n\nj=1\n\nThe corresponding error equation\nδyn+1\n\n= δYs + h\n\ns\nX\n\n(bj − as,j ) fj′ δYj\n\nj=1\n\nand the non-stiﬀ condition (18) reveal that the error in the solution is of the\nsame size as the error in the linear solvers\nke\nyn+1 − yn+1 k ∼ kδY k ∼ kεk.\n\n3.3\n\nStability considerations\n\nFollowing Calvo and Gerisch [9] we perform stability analysis using the following\nscalar test problem:\nhf (y) = iwy,\n\ne = z1 + z2 − γz1 z2 .\nhL\n\nhL = z = z1 + z2 ,\n\nThe LIRK method (8) applied to the test problem (20) gives:\nyn+1\n\n=\n\nR(z, iw) =\nR(∞, iw) =\n\nR(z, iw) yn ,\n1 + (iw b + zbb)T\nb−1 1.\n1 − bbT A\n\n\u0010\n\n\u0011−1\nb − iwA\nI − zA\n1,\n\nThe LIRK+AMF method (12) applied to the test problem (20) gives:\n(1 − γz1 )(1 − γz2 ) Yei\n\n=\n\n(1 + γ 2 z1 z2 ) Yei\n\n=\n\nyn + iw\n\ni−1\nX\n\nyn + iw\n\ni−1\nX\n\nj=1\n\nj=1\n\n10\n\nai,j Yej + (z1 + z2 )\nai,j Yej + (z1 + z2 )\n\ni−1\nX\nj=1\n\ni\nX\nj=1\n\nb\nai,j Yej ,\nb\nai,j Yej ,\n\n(20)\n\n\fand therefore\nYei\n\n\u0010\n\n=\n\nyen+1\n\n\u0011−1\nb − iwA\n(1 + γ 2 z1 z2 )I − z A\n1 yn ,\n\n= yn + iw\n\ns\nX\nj=1\n\nbj Yej + (z1 + z2 )\n\ns\nX\nj=1\n\nbbj Yej .\n\nConsequently, for very stiﬀ linear components the overall scheme is weakly stable:\nyn+1\n\n=\n\nR(z1 , z2 , iw) yn ,\n\nR(z1 , z2 , iw) =\n\n1 + (iw b + zbb)T\n\nR(∞, ∞, iw) =\n\n1.\n\n\u0010\n\n\u0011−1\nb − iwA\n(1 + γ 2 z1 z2 )I − z A\n1,\n\nFor the scheme with one reﬁnement step we have:\n(1)\n\n(1 − γz1 ) (1 − γz2 ) Yi\n\n=\n\n1 + iw\n\ni−1\nX\nj=1\n\n\u0001 (1)\n1 + γ 2 z1 z2 Yi\n(1 − γz1 ) (1 − γz2 ) Yei\n\nand therefore\nτ\n\n=\n\nYe\n\n=\n\nyen+1\n\n=\n\n=\n\n1 + iw\n\ni−1\nX\nj=1\n\n=\n\nai,j Yej + z\nai,j Yej + z\n\ni−1\nX\nj=1\n\ni\nX\nj=1\n\nb\nai,j Yej ,\nb\nai,j Yej ,\n\n\u0001 (1)\n1 − γ (z1 + z2 ) + 2 γ 2 z1 z2 Yi ,\n\n1 − γ (z1 + z2 ) + 2 γ 2 z1 z2\n,\n(1 − γz1 ) (1 − γz2 ) (1 + γ 2 z1 z2 )\n\u0010\n\u0011−1\nb\nτ −1 I − iwA − (z1 + z2 ) A\n1 yn ,\n\u0010\n\u0011T \u0010\n\u0011−1\nb\nyn + iwb + zbb · τ −1 I − iwA − (z1 + z2 ) A\n1 yn .\n\nThe corresponding stability function is\nyn+1\nR(z1 , z2 , iw)\nR(∞, ∞, iw)\n\n= R(z1 , z2 , iw) yn ,\n\u0010\n\u0011T \u0010\n\u0011−1\nb\n= 1 + iwb + zbb · τ −1 I − iwA − z A\n1,\n= 1.\n\nWe have that for very stiﬀ components the scheme with one level of reﬁnement is\nweakly stable. The reﬁnement does not improve the overall stability properties\nof the scheme, only the accuracy. The correction step of Calvo and Gerisch [9]\nleads to L-stable, ﬁrst order methods for very stiﬀ problems.\n\n11\n\n\f4\n\nNumerical experiments\n\nWe perform numerical experiments with the following methods:\n• LIRK3(4): the original LIRK methods of orders three and four, respectively, proposed by Calvo, de Frutos, and Novo [8]. The implicit parts are\nL-stable and stiﬄy accurate;\n• LIRK3(4)AMF: the LIRK methods using AMF as (12);\n• LIRK3(4)AMFR1: the LIRK methods using AMF together with one\niteration reﬁnement;\n• LIRK3(4)AMFR2: the LIRK methods using AMF together with two\niterations reﬁnement.\nIn all the experiments, the error is computed in the relative L2 norm as follows\nE=\n\nku − uref k2\n,\nkuref k2\n\n(21)\n\nwhere u is the numerical solution at the ﬁnal time, and uref is the reference\nsolution at the same point obtained using Matlab’s ode15s solver with very\ntight tolerances (AbsTol=RelTol=3 × 10−14 ).\n\n4.1\n\nAn Allen-Cahn type problem\n\nWe use the PDE test problem of Allen-Cahn type from [9]:\nut = ∆u + u − u3 + f,\n\n(22)\n\nwhere f is chosen to make the exact solution of the equation be\nu(t, x, y) = et sin(πx) sin(πy).\nThe spatial domain is (x, y) ∈ [0, 1] × [0, 1] and the time interval is t ∈ [0, 1]\n(units). The initial conditions and Dirichlet boundary conditions are calculated\nfrom the exact solution.\nThe spatial discretization uses second order central ﬁnite diﬀerences of the\nLaplacian on a uniform grid of size M × M\n\u0013\n\u0012\nj\ni\n, i, j = 1, . . . , M.\n(23)\n,\n(xi , yi ) =\nM +1 M +1\nIn our tests we consider the case M = 59. The discrete solution elements\nUij (t) ≈ u(t, xi , yj ) are ordered into a vector\nT\n\nz = (U11 , U12 , . . . , U1M , . . . , UM1 , UM2 , . . . , UMM ) .\n\n12\n\n\fThe resulting ODE system can then be written into the form (1) with the discrete diﬀusion term being the linear part. The largest magnitude of the eigenvalues of the Jacobian for the diﬀusion term is approximately 2.88 × 104 . The\ndiscrete Laplacian operator L = Lx + Ly is split into two matrices corresponding\nto derivatives along x and y directions, respectively:\n\n\n−2 1\n\n 1 −2 1\n\n\n\n\n..\n..\n..\nLx = DM ⊗ IM , Ly = IM ⊗ DM , DM = \n , (24)\n.\n.\n.\n\n\n\n1 −2 1 \n1 −2\nwhere the symbol ⊗ denotes the tensor product and IM is an identity matrix of\ndimension M × M .\nFigure 1(a) plots the convergence results for all the methods tested. As\nexpected, both LIRK3AMF and LIRK4AMF show second order, and give less\naccurate results for the same time step than the underlying LIRK methods.\nAll the LIRK methods with AMF and reﬁnement perform equally well as the\noriginal LIRK methods; LIRK3AMFR1 produces slightly better results, and the\nfull order of the underlying LIRK methods has been recovered.\nFigure 1(b) shows the corresponding work-precision diagrams. LIRK methods with AMF are not very competitive in terms of eﬃciency. One reﬁnement\niteration improves LIRK3AMF and LIRK4AMF signiﬁcantly. LIRK3AMFR1\nand LIRK4AMFR1 are clearly the most eﬃcient methods among the methods of\nthe same order. A second iteration does not improve accuracy, but spends compute time, and makes LIRK3(4)AMFR2 schemes slightly less eﬃcient. Calvo\nand Gerisch’s approach [9] can achieve second order with AMF, and may only\nbe attractive for low accuracy requirements. The approach with stage reﬁnement proposed herein is competitive at all accuracy levels due to the recovery\nof the full order and the addition of the relatively cheap reﬁnement procedure.\nIt should be noted that the separation of L into Lx and Ly in (24) allows for a\nreduction of the large linear systems with 2M small ones of dimension M × M ,\nwhich can lead to considerable savings in the computational cost. The trick is\nnot used with this example simply for the sake of enabling a fair comparison\nwith the results in [9].\n\n4.2\n\nBrusselator problem\n\nWe next consider the two-dimensional Brusselator reaction-diﬀusion equation\n[12, Sec. IV.10]\nut\nvt\n\n=\n=\n\n1 + u2 v − (B + 1) u + α∆u,\n2\n\nB u − u v + α∆v,\n\n2\n\nwhere (x, y) ∈ [0, 1] , t ∈ [0, 1.5], with the Neumann boundary conditions\n∂v\n= 0.\n∂n\n\n∂u\n= 0,\n∂n\n13\n\n(25a)\n(25b)\n\n\f−1\n\n0\n\n10\n\n10\nLIRK3 order=3.1\n\nLIRK4 order=3.9\n\nLIRK3AMF order=2.0\n\n−2\n\n10\n\nLIRK4AMF order=2.0\n−2\n\nLIRK3AMFR1 order=3.0\n\nLIRK4AMFR1 order=3.9\n\n10\n\nLIRK3AMFR2 order=3.1\n\n−3\n\nLIRK4AMFR2 order=3.9\nError\n\nError\n\n10\n\n−4\n\n−4\n\n10\n\n10\n\n−6\n\n10\n\n−5\n\n10\n\n−6\n\n−8\n\n10\n\n10\n\n20 28 40 57 80 113160\nNo. of steps\n\n20 28 40 57 80 113160\nNo. of steps\n\n(a) Temporal errors vs. number of steps\n−1\n\n0\n\n10\n\n10\nLIRK3\n\nLIRK4\n\nLIRK3AMF\n\n−2\n\n10\n\nLIRK4AMF\n−2\n\nLIRK3AMFR1\n\nLIRK4AMFR1\n\n10\n\nLIRK3AMFR2\n\n−3\n\nLIRK4AMFR2\nError\n\nError\n\n10\n\n−4\n\n−4\n\n10\n\n10\n\n−6\n\n10\n\n−5\n\n10\n\n−6\n\n10\n\n−8\n\n2\n\n10\n\n3\n\n10\nCPU time [in ms]\n\n10\n\n4\n\n10\n\n2\n\n10\n\n3\n\n4\n\n10\n10\nCPU time [in ms]\n\n5\n\n10\n\n(b) Temporal errors vs. CPU time\n\nFigure 1: Results for the 2D Allen-Cahn type problem (22).\nThe problem is discretized with a second order central ﬁnite diﬀerence scheme\non a uniform mesh (23). The stiﬀness of this problem increases with the value of\nα and number of grid points M . We test LIRK+AMF methods with or without\niterative corrections on two diﬀerent cases.\nCase 1. A nonstiﬀ stiﬀ case described by α = 0.001, B = 3 and the initial\nconditions\nu(x, y, 0) = 0.5 + y, v(x, y, 0) = 1 + 5x.\nwith M = 39 grid points used in each dimension. This gives an ODE system is\nof dimension N = 3, 042.\nCase 2. A stiﬀ case, in which the settings follow [12, Sec. IV.10] where α =\n0.1, B = 3.4, and the initial conditions are deﬁned as\nu(x, y, 0) = 22y(1 − y)3/2 ,\n\nv(x, y, 0) = 22x(1 − x)3/2 .\n\nWe choose M = 127 for the three-way splitting test and M = 199 for the twoway splitting test so that the resulting ODE systems are of size N = 31, 752 and\n14\n\n\fN = 79, 202 respectively. The time varying Jacobian for the reaction term used\nin the three-way splitting test makes the linearization challenging especially\nwhen the problem is very stiﬀ. For a large M , e.g. M = 150, the error caused\nby the linearization leads to failure in solving the linear systems with direct\nmethods after a few time steps. So we select a relatively smaller value of M for\nthe three-way splitting test. The details on the splitting setup are given later\nin this section.\nTable 1 shows the dominant eigenvalues for these test cases which shed light\non the degree of stiﬀness. Note that the Jacobian matrix for the reaction term\nhas complex eigenvalues which makes this test problem more challenging than\nthe previous one.\nAfter spatial discretization the PDE is turned into a semi-linear ODE system\nof the form\nz ′ = Ldif z + Lrea (t) z +R,\n| {z } | {z }\nreaction\n\ndiffusion\n\nwhere z is the combined vector for the variables u and v, Ldif and Lrea (t) are\nthe Jacobian of the diﬀusion term and reaction term respectively, and R is the\nrest of the terms such as boundary treatment. The diﬀusion term is stiﬀ while\nthe reaction term is nonstiﬀ. We ﬁrst apply LIRK methods with the diﬀusion\ntreated implicitly and the other terms explicitly. Next we include the Jacobian\nof the reaction terms in the linear part and apply a three-way splitting strategy.\nThe LU decompositions are performed per time step using sparse Gaussian\nelimination in MATLAB.\nTable 1: The dominant eigenvalues (largest in magnitude) of each component.\nCase\n1 (M = 39)\n2 (M = 127)\n3 (M = 199)\n\nLx\n\nLx + Ly\n\nLy\n1\n\n1.28 × 10\n6.55 × 103\n1.60 × 104\n\n1\n\n1.28 × 10\n6.55 × 103\n1.60 × 104\n\nLrea (t0 )\n1\n\n2.56 × 10\n1.31 × 104\n3.20 × 104\n\n1.05 × 101\n2.01 × 101\n2.01 × 101\n\nTwo-way splitting. A directional splitting is applied to the diﬀusion term\nwhich is written as the sum of derivatives in the x-direction and y-direction,\nLdif = Lx + Ly . See (24) for the structure of Lx and Ly . This splitting allows to\nreduce the linear algebra eﬀort to solving 2M tridiagonal systems of dimension\nM , all of which share the same matrix I − hγ DM , and use reordered right-hand\nsides.\nFigure 2(a) shows the convergence plots of diﬀerent methods for Case 1.\nBoth LIRK3AMF and LIRK4AMF give second order. With one reﬁnement\niteration the results become as accurate as those of the original LIRK methods.\nThe largest allowable time steps are almost the same for all methods, implying\ngood stability properties of LIRK methods with AMF. Figure 2(b) presents the\ncorresponding work-precision diagrams. It can be seen that LIRK methods with\none reﬁnement iteration yield the best eﬃciency. To achieve the same accuracy\n15\n\n\flevel, LIRK3AMFR1 is about 2.2 times faster than LIRK3 and LIRK4AMFR1\nis about 1.6 times faster than LIRK4.\n−2\n\n−2\n\n10\n\n10\n\n−4\n\n10\n−4\n\nError\n\nError\n\n10\n\n−6\n\n10\n\n2.0\n\n2.0\n\n−6\n\n10\n\nLIRK3\n\nLIRK4\n−8\n\n10\n\nLIRK3AMF\nLIRK3AMFR1\n−8\n\n15\n\n25\n\n50\n\n−10\n\n100\n200\nNo. of steps\n\n4.0\n\nLIRK4AMFR1\n\n3.0\n\nLIRK3AMFR2\n\n10\n\nLIRK4AMF\n\n10\n\n400\n\nLIRK4AMFR2\n15\n\n25\n\n50\n\n100\n200\nNo. of steps\n\n400\n\n(a) Temporal error vs. number of steps\n−2\n\n−2\n\n10\n\n10\nLIRK3\nLIRK3AMF\n\n−4\n\nLIRK3AMFR1\n−4\n\n10\n\n10\n\nError\n\nError\n\nLIRK3AMFR2\n−6\n\n10\n\n−6\n\n10\n\nLIRK4\n−8\n\n10\n\nLIRK4AMF\nLIRK4AMFR1\n\n−8\n\n10\n\nLIRK4AMFR2\n\n−10\n\n2\n\n10\n\n3\n\n10\nCPU time [in ms]\n\n10\n\n4\n\n10\n\n2\n\n10\n\n3\n\n10\nCPU time [in ms]\n\n4\n\n10\n\n(b) Temporal error vs. CPU time\n\nFigure 2: Results for the 2D Brusselator system(25), Case 1, with M = 39. AMF\nis applied with a two-way splitting of the Jacobian. 15, 20, 25, 50, 100, 200, 400\nequal steps are used for the time integration of the system on the interval [0, 1].\nThe left-most points (highlighted by a circle) on each curve indicates the maximal allowable time steps.\nFigure 3 shows the convergence and work-precision diagrams for diﬀerent\nmethods for the large scale stiﬀ case with M = 199. Generally LIRK methods\nwith AMF and two reﬁnement iterations give more accurate results than those\nwith AMF and one reﬁnement iteration. The reﬁnement works well and recovers the theoretical orders of the corresponding LIRK methods. The errors for\nmethods with two reﬁnement iterations approach the LIRK results at a faster\nrate than methods with just one reﬁnement iteration. This diﬀers from the\nresults of the ﬁrst case, but is in line with the theoretical prediction. Another\nnotable advantage of the AMF technique is the gain in term of stability. If no\nreﬁnement is employed, the maximal time step size can be at least two times\nlarger than that allowed by the original LIRK methods for both third-order\nand fourth-order schemes. However, the gain disappears or shrinks when the\n16\n\n\freﬁnement procedure is added.\nIn the eﬃciency comparison, LIRK methods with AMF and two reﬁnement\nare the most eﬀective for solutions more accurate than approximately 10−4 .\nLIRK methods with AMF provide a good compromise between accuracy and\nspeed since they can use a maximal allowable time step size that is at least\ntwo times larger than LIRK methods, and run signiﬁcantly faster than LIRK\nmethods.\n0\n\n0\n\n10\n\n10\n\n−2\n\n−2\n\n10\n1.7\n\nError\n\nError\n\n10\n\n−4\n\n10\n\n2.8\n3.7\n\nLIRK3\n−6\n\n10\n\n500\n\n800\n\nLIRK4AMFR1\n10\n\n8000\n\n3.5\n\nLIRK4AMFR2\n\n−8\n\n2000\n4000\nNo. of steps\n\n4.7\n\nLIRK4AMF\n\n2.4\n\nLIRK3AMFR2\n\n−8\n\n3.5\n\nLIRK4\n−6\n\n10\n\nLIRK3AMF\nLIRK3AMFR1\n\n10\n\n2.1\n\n−4\n\n10\n\n500\n\n800\n\n2000\n4000\nNo. of steps\n\n8000\n\n(a) Temporal error vs. number of steps\n0\n\n0\n\n10\n\n10\n\n−2\n\n−2\n\n10\n\nError\n\nError\n\n10\n\n−4\n\n10\n\n−4\n\n10\n\nLIRK3\n−6\n\n10\n\nLIRK4\n−6\n\n10\n\nLIRK3AMF\n\nLIRK4AMF\n\nLIRK3AMFR1\n−8\n\n10\n\nLIRK4AMFR1\n\nLIRK3AMFR2\n1\n\n10\n\nLIRK4AMFR2\n\n−8\n\n2\n\n3\n\n10\n10\nCPU time [in sec]\n\n4\n\n10\n\n10\n\n5\n\n10\n\n1\n\n10\n\n2\n\n10\n\n3\n\n4\n\n10\n10\nCPU time [in sec]\n\n5\n\n10\n\n(b) Temporal error vs CPU time\n\nFigure 3: Results for the 2D Brusselator system (25), Case 2, with M = 199.\nAMF is applied with a two-way splitting of the Jacobian. 400, 500, 600, 800,\n1000, 1500, 2000, 4000, 8000 equal steps are used for the time integration of the\nsystem on the interval [0, 1]. The left-most points (highlighted by a circle) on\neach curve indicate the maximal allowable time steps.\n\nThree-way splitting. We use a three-way splitting of the linear part\nL = Lx + Ly + Lrea (t)\nwhere the Jacobian of the reaction terms is treated implicitly. Lrea (t) contains\nfour blocks, each of which is a diagonal matrix. It is updated at each time step.\n17\n\n\fThe linear system associated with I − hγ Lrea can be reduced to M 2 smaller\nsystems of dimension two which can be solved separately at each grid point. We\nchoose to solve the large sparse system directly as an explicit decoupling does\nnot lead to a clear performance gain for a serial implementation.\nThe results are shown in Figure 4, and 5. For Case 1, the reﬁnement procedure can successfully improve the order from 2 to the theoretical order. AMF\nwithout reﬁnement is not competitive in terms of eﬃciency, but AMF with reﬁnement yields some performance gain for third-order schemes and comparable\nresults with LIRK methods for fourth-order schemes. This is due to the fact\nthat the system is relatively small and additional cost is brought in to solve the\nlinear system associated with Lrea (t).\nFor the stiﬀ case, the Jacobian for the implicit part L makes the linear\nsystem diﬃcult to solve with direct methods. One LU decomposition of the\nsystem may take over 5000 seconds. To improve the performance of LIRK\nmethods, we make use of the reordering algorithm symamd in MATLAB before\nsolving the linear systems. The reordering can also help reduce the bandwidth\nof the sparse matrices as similar to purpose of the splitting schemes we used.\nThe convergence orders for the stiﬀ case are very close to the two-way splitting test results. In terms of eﬃciency, there is still considerable performance\ngain for AMF with reﬁnement, especially for AMF with two reﬁnement iterations. The savings in CPU time may mainly come from reducing the big system\ninto multiple small systems, which is another advantage of the application of\nAMF.\n\n5\n\nConclusions\n\nWe have applied approximate matrix factorization to high order linearly implicit\nRunge-Kutta methods for solving semi-linear systems of diﬀerential equations.\nThe factorization (splitting) error brought by AMF leads to severe order degradation, especially for high order Runge-Kutta methods. The existing approach\nto recover second order is based on correction applied to the next step solution\n[9]. In this work the full order of the underlying methods is recovered by correcting stage values via a reﬁnement procedure based on the idea of simpliﬁed\nNewton iterations.\nWe have performed error analysis for the linear system solutions with AMF,\nand investigated how this errors aﬀect the next step solution. In the non-stiﬀ\nand mildly stiﬀ case the full order of the underlying method can be recovered\nusing a ﬁxed, small number of reﬁnement iterations. In the very stiﬀ case\nthe number of iterations can be large since the convergence can deteriorate\nwith increasing stiﬀness. A stability analysis reveals that the stage reﬁnement\nprocedure does not improve the overall stability of the LIRK+AMF method.\nWhen AMF is used the resulting schemes are only weakly stable for very stiﬀ\nproblems. Consequently, this application of AMF is attractive for mildly stiﬀ\nproblems, but may not work well for very stiﬀ systems.\nNumerical experiments on a variety of test problems of diﬀerent sizes and\n\n18\n\n\f−2\n\n−2\n\n10\n\n10\n\n−3\n\n10\n\n−4\n\n10\n\n−4\n\n2.0\nError\n\nError\n\n10\n\n2.0\n−5\n\n−6\n\n10\n\n10\n\n10\n\n−8\n\n3.0\n\n10\n\nLIRK4AMF\n\nLIRK3AMFR1\n\nLIRK4AMFR1\n\nLIRK3AMFR2\n\n−7\n\n10\n\n20\n\n4.0\n\nLIRK4\n\nLIRK3\nLIRK3AMF\n\n−6\n\n50\n\n2.9\n\n100\n200\nNo. of steps\n\n400\n\n4.0\n\nLIRK4AMFR2\n\n−10\n\n10\n\n15\n\n25\n\n50\n\n100\n200\nNo. of steps\n\n400\n\n(a) Temporal error vs. number of steps\n−2\n\n−2\n\n10\n\n10\n\n−3\n\n10\n\n−4\n\n10\n\n−4\n\nError\n\nError\n\n10\n\n−5\n\n−6\n\n10\n\n10\n\nLIRK3\n10\n\nLIRK4AMF\n\nLIRK3AMFR1\n\nLIRK4AMFR1\n\nLIRK3AMFR2\n\n−7\n\n10\n\nLIRK4\n−8\n\n10\n\nLIRK3AMF\n\n−6\n\n2\n\n10\n\nLIRK4AMFR2\n\n−10\n\n3\n\n10\nCPU time [in ms]\n\n10\n\n4\n\n10\n\n2\n\n10\n\n3\n\n10\nCPU time [in ms]\n\n4\n\n10\n\n(b) Temporal error vs. CPU time\n\nFigure 4: Results for the 2D Brusselator system (25), Case 1, with M = 39.\nAMF is applied with a three-way splitting of the Jacobian. 15, 20, 25, 50,\n100, 200, 400 equal steps are used for the time integration of the system on\nthe interval [0, 1]. The left-most points (highlighted by a circle) on each curve\nindicates the maximal allowable time steps. The numbers inside the triangle\ngive the convergence order.\ndiﬀerent degrees of stiﬀness validate the theoretical ﬁndings on the accuracy\nand stability of high order linearly implicit Runge-Kutta methods when AMF\nis used. The results also show that the proposed approach can improve the\neﬃciency of high order linearly implicit Runge-Kutta methods signiﬁcantly and\nthus is attractive for solving large scale mildly stiﬀ systems such as diﬀusionreaction equations. Furthermore, our tests on the three-way splitting demonstrate that our methods can also eﬃciently deal with problems where stiﬀness\ncomes from both diﬀusion and reaction terms. Though we considered LIRK\nschemes up to order four, the general framework developed herein can be applied to higher order LIRK methods, and could be extended to study the use\nof AMF with implicit-explicit multistep methods and implicit-explicit general\nlinear methods.\n\n19\n\n\f0\n\n0\n\n10\n\n10\n\n−2\n\n10\n\n−2\n\n10\n\n−4\n\nError\n\nError\n\n10\n1.9\n\n−4\n\n10\n\n2.0\n\n−6\n\n10\n−6\n\n10\n\nLIRK4\n\nLIRK3AMF\n\n10\n\n4.6\n2.7\n\nLIRK3AMFR2\n\n−8\n\nLIRK4AMF\n\n−8\n\nLIRK3AMFR1\n10\n\n3.7\n\n3.3\n\nLIRK3\n\n−10\n\n200 300 500 1000 2000\nNo. of steps\n\n4000\n\n10\n\n8000\n\n5.1\n3.8\n\nLIRK4AMFR1\nLIRK4AMFR2\n200 300 500 1000 2000\nNo. of steps\n\n4000\n\n8000\n\n(a) Temporal error vs. number of steps\n0\n\n0\n\n10\n\n10\n\n−2\n\n10\n\n−2\n\n10\n\n−4\n\nError\n\nError\n\n10\n−4\n\n10\n\n−6\n\n10\nLIRK3\n−6\n\n10\n\nLIRK4\n\nLIRK3AMF\n\n10\n\nLIRK3AMFR1\nLIRK3AMFR2\n\n−8\n\n10\n\n0\n\n10\n\n1\n\n10\n\nLIRK4AMF\n\n−8\n\nLIRK4AMFR1\nLIRK4AMFR2\n\n−10\n\n2\n\n3\n\n10\n10\nCPU time [in sec]\n\n10\n\n4\n\n10\n\n1\n\n10\n\n2\n\n3\n\n10\n10\nCPU time [in sec]\n\n4\n\n10\n\n(b) Temporal error vs. CPU time\n\nFigure 5: Results for the 2D Brusselator system (25), Case 2, with M = 127.\nAMF is applied with a three-way splitting of the Jacobian. 100, 200, 300, 400,\n500, 1000, 2000, 4000, 8000 equal steps are used for the time integration of the\nsystem on the interval [0, 1]. The left-most points (highlighted by a circle) on\neach curve indicates the maximal allowable time steps.\n\n20\n\n\fAcknowledgements\nThis work has been supported in part by NSF through awards NSF CMMI–\n1130667, NSF CCF–1218454, NSF CCF–0916493, NSF DMS–1419003, AFOSR\nFA9550–12–1–0293–DEF, AFOSR 12-2640-06, and by the Computational Science Laboratory at Virginia Tech.\n\nReferences\n[1] I. Ahmad and M. Berzins. An algorithm for ODEs from atmospheric dispersion problems. Applied Numerical Mathematics, 25(23):137 – 149, 1997.\nSpecial Issue on Time Integration.\n[2] G. Akrivis and M. Crouzeix. Linearly implicit methods for nonlinear\nparabolic equations. Mathematics of Computation, 73(246):pp. 613–635,\n2004.\n[3] G. Akrivis, O. Karakashian, and F. Karakatsani. Linearly implicit methods\nfor nonlinear evolution equations. Numerische Mathematik, 94:403–418,\n2003.\n[4] U.M. Ascher, S.J. Ruuth, and R.J. Spiteri. Implicit-explicit Runge-Kutta\nmethods for time-dependent partial diﬀerential equations. Applied Numerical Mathematics, 25:151–167, 1997.\n[5] S. Beck, S. González-Pinto, S. Pérez-Rodrı́guez, and R. Weiner. A comparison of AMF- and Krylov-methods in Matlab for large stiﬀ ODE systems. Journal of Computational and Applied Mathematics, 262(0):292 –\n303, 2014. Selected Papers from NUMDIFF-13.\n[6] S. Beck, R. Weiner, H. Podhaisky, and B. Schmitt. Implicit peer methods\nfor large stiﬀ ode systems. Journal of Applied Mathematics and Computing,\n38(1-2):389–406, 2012.\n[7] M. Berzins and J.M. Ware. Solving convection and convection-reaction\nproblems using the method of lines. Applied Numerical Mathematics,\n20(12):83 – 99, 1996. Method of Lines for Time-Dependent Problems.\n[8] M.P. Calvo, J. de Frutos, and J. Novo. Linearly implicit Runge-Kutta methods for advection-reaction-diﬀusion equations. Applied Numerical Mathematics, 37(4):535–549, 2001.\n[9] M.P. Calvo and A. Gerisch. Linearly implicit Runge-Kutta methods\nand approximate matrix factorization. Applied Numerical Mathematics,\n53(2):183–200, 2005.\n[10] S. González-Pinto, D. Hernández-Abreu, and S. Pérez-Rodrı́guez.\nRosenbrock-type methods with inexact AMF for the time integration of\n\n21\n\n\fadvection-diﬀusion-reaction PDEs. Journal of Computational and Applied\nMathematics, 262:304–321, May 2014.\n[11] I. Grooms and K. Julien. Linearly implicit methods for nonlinear PDEs\nwith linear dispersion and dissipation. Journal of Computational Physics,\n230(9):3630 – 3650, 2011.\n[12] E. Hairer, S.P. Norsett, and G. Wanner. Solving Ordinary Diﬀerential\nEquations I. Nonstiﬀ Problems. Springer-Verlag, Berlin, 1993.\n[13] E. Hairer and G. Wanner. Solving Ordinary Diﬀerential Equations II: Stiﬀ\nand Diﬀerential-Algebraic Problems. Springer, 1993.\n[14] J. Douglas Jr. Alternating direction methods for three space variables.\nNumerische Mathematik, 4(1):41–63, 1962.\n[15] L. Pareschi and G. Russo. Implicit-explicit Runge-Kutta schemes for stiﬀ\nsystems of diﬀerential equations. In Recent trends in numerical analysis,\npages 269–288. Nova Science Publishers, Inc., 2000.\n[16] D. Peaceman and H. Rachford, Jr. The numerical solution of parabolic\nand elliptic diﬀerential equations. Journal of the Society for Industrial and\nApplied Mathematics, 3(1):28–41, 1955.\n[17] R.Weiner, B.A. Schmitt, and H. Podhaisky. ROWMAP-a ROW-code with\nKrylov techniques for large stiﬀ ODEs. Applied Numerical Mathematics,\n25:303–319, 1997.\n[18] A. Sandu. Numerical Aspects of Air Quality Modeling. PhD thesis, University of Iowa, 1997.\n[19] P.J. van der Houwen and B.P. Sommeijer. Approximate factorization for\ntime-dependent partial diﬀerential equations. Journal of Computational\nand Applied Mathematics, 128(1-2):447–466, 2001.\n\nA\n\nLIRK methods\n\nThe coeﬃcients of the LIRK3 method [8]:\n0\n\n0\n\nγ\n\n0\n\nγ\n\n1+γ\n2\n\n0\n\n1−γ\n2\n\nγ\n\n1\n\n0\n\nb2\n\nb3\n\nγ\n\n0\n\nb2\n\nb3\n\nγ\n\n2\n\n0\n\n0\n\nγ\n\nγ\n\n1+γ\n2\n\n1\n\n2\n\n1−γ\n2\n\n0\n− a32\n\n,\n\na32\n\n0\n\n0\n\n1 − a43\n\na43\n\n0\n\n0\n\nb2\n\nb3\n\nγ\n\nwhere b2 = − 3γ2 + 4γ − 41 and b3 = 3γ2 − 5γ + 45 . And the choice for the free\nparameter is γ = 0.435866521508459 and a32 = 0.35.\n22\n\n\fThe coeﬃcients of the LIRK4 method [8]:\n0\n1\n4\n\n0\n\n1\n4\n\n− 34\n\n0\n\n1\n2\n\n1\n4\n\n− 11\n20\n\n0\n\n17\n50\n\n1\n− 25\n\n1\n4\n\n− 12\n\n0\n\n371\n1360\n\n137\n− 2720\n\n15\n544\n\n1\n4\n\n0\n\n25\n24\n\n− 49\n48\n\n125\n16\n\n− 85\n12\n\n1\n4\n\n0\n\n25\n24\n\n− 49\n48\n\n125\n16\n\n− 85\n12\n\n1\n4\n\n1\n\n0\n\n0\n\n1\n4\n\n1\n4\n\n0\n\n− 14\n\n1\n\n0\n\n11\n− 20\n\n13\n− 100\n\n43\n75\n\n8\n75\n\n0\n\n− 12\n\n6\n− 85\n\n42\n85\n\n179\n1360\n\n15\n− 272\n\n0\n\n0\n\n79\n24\n\n− 85\n\n25\n2\n\n− 85\n6\n\n0\n\n0\n\n25\n24\n\n49\n− 48\n\n125\n16\n\n− 85\n12\n\n1\n4\n\n− 43\n\n23\n\n1\n\n.\n\n\f"
        ],
        [
         "12",
         "12",
         "cs.CE",
         "Computational Engineering",
         "1011.0491v1.pdf",
         "Aspects of multiscale modelling in a process algebra for\nbiological systems\nRoberto Barbuti\nGiulio Caravagna\nPaolo Milazzo Andrea Maggiolo–Schettini\nDipartimento di Informatica,\nUniversità di Pisa,\nLargo Pontecorvo 3, 56127 Pisa, Italy.\n{barbuti, caravagn, maggiolo, milazzo}@di.unipi.it\n\nSimone Tini\nDipartimento di Informatica e Comunicazione,\nUniversità dell’Insubria,\nVia Mazzini 5, 21100 Varese and Via Carloni 78, 22100 Como, Italy.\nsimone.tini@uninsubria.it\n\nWe propose a variant of the CCS process algebra with new features aiming at allowing multiscale\nmodelling of biological systems. In the usual semantics of process algebras for modelling biological\nsystems actions are instantaneous. When different scale levels of biological systems are considered\nin a single model, one should take into account that actions at a level may take much more time\nthan actions at a lower level. Moreover, it might happen that while a component is involved in one\nlong lasting high level action, it is involved also in several faster lower level actions. Hence, we\npropose a process algebra with operations and with a semantics aimed at dealing with these aspects\nof multiscale modelling. We study behavioural equivalences for such an algebra and give some\nexamples.\n\n1\n\nIntroduction\n\nFormal modelling notations of computer science are nowadays often applied to the description of biological systems. Such notations can be used to unambiguously describe the structure and the events\ngoverning the dynamics of the systems of interest, thus allowing development of analysis tools such as\nsimulators and of formal analysis techniques based, for instance, on model checking or on behavioural\nequivalences.\nAs examples of formalisms that have been applied to the description of biological systems we\nmention Bio-PEPA [15, 16], the (stochastic) π -calculus [29, 30, 32], Bioambients [31], the κ -calculus\n[17], the Language for Biological Systems (LBS) [27], and the Calculus of Looping Sequences (CLS)\n[22, 7, 8]. In these formalisms the dynamics of a biological system consists of a sequence of events (usually biochemical reactions) described either as communications between processes of a process algebra,\nor as applications of some rewrite rules. In the stochastic extension of these formalisms, the dynamics\nof a system is described by taking also into account the different rates of occurrence of the events. Rates\ndepend on some parameters associated with the events (e.g. kinetic constants of the corresponding biochemical reactions) and on the abundance (or concentration) of the entities (or reactants) that can cause\nsuch events. The rates are then used, as in Gillespie’s algorithm [18], to describe both the exponentially\ndistributed time elapsing between two subsequent events, and the probability of an event to occur.\nG.Ciobanu, M.Koutny (Eds.): Membrane Computing and\nBiologically Inspired Process Calculi 2010 (MeCBIC 2010)\nEPTCS 40, 2010, pp. 54–69, doi:10.4204/EPTCS.40.5\n\nc Barbuti et al.\nThis work is licensed under the\nCreative Commons Attribution License.\n\n\fBarbuti et al.\n\n55\n\nThis way of describing the dynamics of biological systems with sequences of events, however, assumes that the occurrence of one of such events can be described as an instantaneous change in the\nsystem state. In fact, even in the stochastic approach, the only notion of time that is considered is given\nby the frequency of the events, rather than by their duration. The duration of an event is usually ignored\nif it is negligible with respect to the time interval between two events, or hidden in such a time interval\nby chosing a rate for the event that is small enough to take into account both of its frequency and of its\nduration.\nPhenomena of interest in the study of biological systems often include processes at different levels\nof abstraction. A typical example is cell signalling, that involves gene regulation and protein interaction\nprocesses at the intra-cellular level, a signal diffusion process at the inter-cellular level, and some macroscopic change at the tissue level. The processes at the different levels influence each other. However, they\ninvolve system components of very different sizes and are characterized by events having very different\ndurations. This motivates the multiscale approach to modelling, whose application to biological systems\nseems to be promising [4, 2, 9, 13, 33, 34].\nThe description of the dynamics of biological systems by means of sequences of instantaneous events\nis not suitable for multiscale models. When different scale levels of biological systems are considered in\na single model, one should take into account that events at a level may take much more time than events\nat a lower level. Moreover, it might happen that while a component (e.g. a cell) is involved in a long\nlasting high level event (e.g. mitosis), it is involved also in several faster lower level events (e.g. protein\nsinthesis) that neither consume such a component nor interrupt the higher level event. Consequently, the\ndistinction between the time scales of the events, the possibility of having the same component involved\nin several events at different time scales, and the fact that the completion of some events may (or may\nnot) interfere with other events in which the same component is involved require new notions of system\ndynamics to be considered.\nIn this paper, we propose a process algebra with operations and with a semantics aimed at dealing\nwith these aspects of multiscale modelling. Actually, we aim at undertaking a foundational study. Hence,\nwe consider a minimal process algebra for the description of biological systems (a fragment of CCS [23]\nand of the Chemical Ground Form [14]) and we make the minimal changes we think to be necessary to\ndescribe the new aspects of interest.\nAs regards the syntax of the new process algebra (called Process Algebra with Preemptive and Conservative actions), we propose a new action prefixing operator that allows an action to be executed in a\nconservative (or non-consuming) manner, namely without removing the process that performed it. As\nregards the semantics, we define it as a labelled transition system by following the ST semantics approach\n(see [21, 19, 20, 12]) in which actions are not instantaneous, but described by two separate starting and\nending transitions. This will permit to have processes in which multiple actions are running in parallel\nand competing for their completion. Indeed, we change the usual interpretation of the summation operator (by making it slightly similar to a parallel composition) order to allow a process to be involved\nin several actions at the same time. The termination of an action in a summation may interrupt (in a\npreemptive way) the others that are concurrently executed in the same summation, depending on which\naction prefixing operator is used.\nThe semantics of the process algebra is given in a compositional way, and this allows us to study\nbehavioural equivalences. In particular, we define a notion of bisimulation for the process algebra and\nwe prove a congruence result for it. Some examples are given of use of the process algebra and of the\nbisimulation relation.\nThe paper is structured as follows. In Section 2 we introduce the syntax of the processes, process\nconfigurations and some auxiliary functions to define the semantics, given in Section 3 with an exam-\n\n\fAspects of multiscale modelling in a process algebra for biological systems\n\n56\n\nple. In Section 4 a behavioural equivalence for the processes is presented. Finally, we end with some\nconclusions and future work in Section 5.\n\n2\n\nA Process Algebra with Preemptive and Conservative actions\n\nIn this section we present the syntax of a Process Algebra with Preemptive and Conservative actions, in\nthe following shortly denoted as PAPC.\n\n2.1 The Syntax\nPAPC is a process algebra with dyadic communication in the style of CCS [23] and of the π -calculus\n[24, 25]. Hence, communications involve exactly two processes at a time. As in the cases of CCS and\nof the π -calculus, the description of biological systems with PAPC is based on a processes-as-molecules\nview: the modeling of a molecular species S occurring n times in a system includes n copies of a process\nPS modeling a single molecule.\nDifferently with respect to the approach of classical process algebras, where actions are instantaneous, in PAPC we assume that actions can consume time and that the instants of start and completion of\nan action can be detached. An action that has already started but not yet completed is said to be running.\nLet us assume an infinite set of actions Act ranged over by α , β , . . . and a function ( ) : Act → Act\nsuch that α = α . We also denote with Actτ the set of actions enriched with the special internal action τ ,\nActτ = Act ∪ {τ } . The abstract syntax of PAPC is a follows.\nDefinition Processes of PAPC are defined by the following grammar:\nP := 0\n\nα.P\n\nα :P\n\nP+P\n\nP|P\n\nA\n\nwhere α ∈ Act. We denote the set of all processes by P.\nAs usual, 0 denotes the classical idle process that can perform no action. PAPC processes can perform\nactions in Act in two different ways (represented by two different action prefixing operators). Both α .P\nand α : P can perform the action α . However, after performing such an action, the first process simply\nbehaves as P, while the second process continues its execution as P | α : P, namely it produces a copy\nof P, but it is also availabe again to perform α . This difference in the behaviour allows α .P to model a\nmolecule that can be involved in a reaction that transforms it into another molecule, and α : P to model\na molecule that can be involved in a reaction that does not consume nor transform it. Another difference\nbetween α .P and α : P is related with the fact that actions are not instantaneous and that a process can\nstart several actions concurrently. When an action used as in α .P completes, it interrupts all other running\nactions for the same process. This agrees with the intuition that α .P represents a transformation of the\ndescribed molecule into a different one. On the other hand, when an action used as in α : P completes, it\ndoes not interrupt the other running actions of the same process. Again, this agrees with the intuition that\nthe described molecule is neither consumed nor transformed into something different. We say that α is\npreemptive with respect to α .P and that it is conservative with respect to α : P. Notice that in PAPC the\ncomposition of these two approaches is possible, since an action α may be conservative for a process,\nand the complementary action α may be preemptive.\nA process P + Q is able to start actions of both P or Q, but notice that, because of the difference\nbetween conservative and preemptive actions and the fact that actions are not instantaneous, this summation operator does not correspond to the choice operator of classical process algebras. In fact, starting an\n\n\fBarbuti et al.\n\n57\n\naction in P (resp. Q) does not imply that process Q (resp. P) is discarded. As a consequence, P + Q may\nstart several actions, which are said to be in competition, meaning that they run concurrently until one of\nthem completes. When an action α in P (resp. Q) completes, then all actions in Q (resp. P) competing\nwith it have to be interrupted only if α is preemptive. If this is the case, then Q (resp. P) is discarded.\nDifferently, if a conservative action completes, then the summation is not discharged and, consequently,\nall the running actions in the summation are still running.\nA process P | Q is the parallel composition of processes P and Q. Handshaking is possible between\nany action α in P and its complementary action α in Q. The handshaking is to be performed to assign to\nan instance of communication a unique identifier which may be used to interrupt the communication. An\ninteresting case here is when an action α in P and its complementary action α in Q have been coupled\nwith an handshaking, have started and not yet terminated, and some preemptive action in P competing\nwith α completes, so that α must be interrupted. In fact, in this case also the complementary action α in\nQ must be interrupted.\nFinally, constants A are used to specify recursive systems. In general, systems are specified as a set of\ndef\nconstant defining equations of the form A = P. As usual, we assume that all processes in these equations\nare closed and guarded.\nSome further considerations are worth in order to introduce the differences between PAPC and the\nclassical process algebras. The capability of having competing actions is at the basis for the choices we\ndef\nmade in the definition of PAPC. A process P = P1 + ... + Pn can start multiple actions in parallel, but can\nbe involved in each action at most once at a time. Notice that, in classical process algebras, this is not\npossible since an action, when starts, determines the future process to transform P in. Hence, the choice\nis resolved at the time of the starting of an action. In this sense, the summation operator of PAPC is not\na classical choice for the reason that the competing actions compete for their completion, and, then, the\nsemantics of the completion, and hence the semantics of the PAPC summation, will depend on the type\nof the action to be completed, namely whether it is conservative or preemptive.\nConsequently, at any time of a computation, P could be in a configuration in which some of its actions\nare currently running. More precisely, the competition of the running actions is due to the fact that they\nare waiting to complete. Practically, the time for completion may be modeled by general distributions\nas in [12], or by delays as in [5, 6]. However, in this first definition of the algebra we do not consider\nquantitative timing and stochasticity.\nIn order to define the semantics of PAPC we need to model a process with possibly running actions.\nWe do this by introducing a notion of process configuration.\nDefinition Process configurations of PAPC are defined by the following grammar:\nCP := [α ]l .P\n\n[α ]l : P\n\nCP +CP\n\nCP | CP\n\nP\n\nwhere α ∈ Act and l ∈ N. We denote the set of all possible process configurations as C .\nAny process P ∈ P is also in a valid configuration, hence P ⊂ C . However, a process configuration\nmay contain actions denoted by a different prefix. In particular, the configuration [α ]l .P is the configuration reached by α . P after α has started, and [α ]l : P is the configuration reached by α : P after α\nhas started. For both the action prefixes, the new argument l ∈ N is a natural number that identifies the\nrunning action. Notice that these identifiers, which have to be unique, are computed by the handshaking\nperformed before the start of an action and, once a preemptive action is completed, they may be used\nto interrupt all other competing actions. Then, if one of these competing actions is α and it has started\nan handshaking with another action α in a process running in parallel, then also this action α will be\n\n\f58\n\nAspects of multiscale modelling in a process algebra for biological systems\n\ninterrupted. This can be obtained by assigning the same identifier to these two actions when the handshaking begins. By the definition of the semantics it will be clear how both the partners will share the\nsame identifier for the actions.\nLet us define by structural recursion an auxiliary function Id : C 7→ N as follows:\nId([α ]l .P) = Id([α ]l : P) = {l}\nId(CP +CP′ ) = Id(CP | CP′ ) = Id(CP ) ∪ Id(CP′ )\nId(P) = 0/ .\nThe value Id(CP ) denotes the set of the identifiers of the actions in the configuration CP that are running.\n′\n\ndef\n\nFor instance, given a configuration CP = [α ]l .P + β : Q| [γ ]l .T , the identifiers collected by function Id\nare given by Id(CP ) = {l, l ′ }.\nFinally, we define a function Action : N × C 7→ ℘(Act) such that Action(l,CP ) collects the set of\nactions currently running in CP and with assigned identifier l, if any. The function Action is defined as\nfollows:\nAction(l, [α ]l .P) = Action(l, [α ]l : P) = {α }\n′\n\n′\n\nAction(l, [α ]l .P) = Action(l, [α ]l : P) = 0/\n\nif l 6= l ′\n\nAction(l,CP +CP′ ) = Action(l,CP | CP′ ) = Action(l,CP ) ∪ Action(l,CP′ )\nAction(l, P) = 0/ .\nThe definition of this function is similar to the definition of Id. For instance, given the configuration\n′\ndef\nCP = [α ]l .P + β : Q| [γ ]l .T , the actions collected by function Action are given by Action(l,CP ) = {α },\n/\nAction(l ′ ,CP ) = {γ } and, for all l ′′ 6= l and l ′′ 6= l ′ , Action(l ′′ ,CP ) = 0.\nIn the next sections we define the semantics of PAPC by using the notions of process, process configurations and these auxiliary functions.\n\n3\n\nA Structural Operational Semantics for PAPC\n\nIn this section we define a Structural Operational Semantics (SOS) [28] for PAPC. The aim of the SOS is\nℓ\nto equip PAPC with a Labeled Transition System (LTS), namely a set of transitions of the form P →r P′\nrepresenting a move from P ∈ C to P′ ∈ C , with the label ℓ carrying some information about the move\nand the index r used to group transitions describing a particular aspect of the behavior of the processes.\npremises\nThe LTS is defined by a set of SOS transition rules of the form conclusion\n. Intuitively, each of these rules\nexplains how a move of a process is obtained from moves of its subprocesses. All our rules are in Figures\n1–6. We assume the standard way for assigning an LTS with such a set of transition rules (see, e.g., [1]).\nThe main features of the SOS we want are the following. Firstly, it must have a mechanism to\ninterrupt competing actions and this mechanism is activated by the completion of a preemptive action.\nSecondly, the style of the semantics must be ST-like, as this permits to easily observe detached events as\nthe start and the completion of an action.\nIn order to get this features, we define a relation for modeling the start of an action and the coupling of\nprocesses; this will be named as the handshaking relation. Furthermore, we define a completion relation\nfor modeling the finishing of both preemptive and conservative actions. These two relations will make\nuse of an interruption relation to model the interruption of currently running actions, as required by the\nnotion of preemptive actions.\n\n\fBarbuti et al.\n\n59\n\n1,α +\n\n1,α +\n\n(H1 ) α .P −−−→H [α ]1 .P\n\n(H2 ) α : P −−−→H [α ]1 : P\n\nl,α +\n\n(H3 )\n\nP −−→H P′\n\nP + Q −−→H P′ + Q\n\nl,α +\n\n(H4 )\n\nP −−→H P′\n\nl ∈ Id(Q) l ′ = min{N − Id(P + Q)}\nl ′ ,α +\n\nP + Q −−−→H P′ [l ′ /l] + Q\nl,α +\n\n(H5 )\n\nl 6∈ Id(Q) α ∈ Actτ\n\nP −−→H P′\n\nl,α +\n\n(H6 )\n\nl 6∈ Id(Q)\n\nl,α +\n\nP −−→H P′\n\nl,α +\n\nP | Q −−→H P′ | Q\nl ∈ Id(Q) l ′ = min{N − Id(P | Q)}\n\nα ∈ Actτ\n\nl ′ ,α +\n\nP | Q −−−→H P′ [l ′ /l] | Q\nl,α +\n\n(H7 )\n\nP −−→H P′\n\nl ′ ,α +\n\nQ −−−→H Q′\n\nl ′′ = min{N − Id(P | Q)}\n\nl ′′ ,τ +\n\nP | Q −−−→H P′ [l ′′ /l] | Q′ [l ′′ /l ′ ]\n\nFigure 1: The handshaking relation −\n→H ⊆ C × Θ+ × C .\n\nThe handshaking relation\nThis relation is used to model the starting of an action and the coupling of the processes starting complementary actions. The handshaking relation is −\n→H ⊆ C × Θ+ × C , where Θ+ contains labels θ + of the\nform\nθ + = (l, α + )\nwhere l ∈ N represents the identifier assigned to the started action α ∈ Actτ , and the use of the superscript\n“+” comes from the definition of the semantics in the ST style, in order to denote the start of an action.\nThe SOS rules in Figure 1 are at the basis of the definition of −\n→H . We implicitly assume the rules\nsymmetric to (H3 ), (H4 ), (H5 ), (H6 ).\nRules (H1 ) and (H2 ) model the starting of an action α . At any time a process with prefix α can\nstart action α moving to a configuration in which it cannot perform the same action anymore, i.e. the\nconfiguration [α ]1 .P or, analogously, the configuration [α ]1 : P. Such a configuration, together with\nthe one describing the process performing the complementary action, has to be uniquely identified by\na natural number representing the identifier of the just started action. At this step, the process simply\nchooses 1 as unique identifier. All our choices for assigning identifiers to actions are inspired to those\nof [11], which ensure that the portion of LTS rooted in a given process is finite. The rules for binary\noperators + and | will solve conflicts of colliding identifiers, if any. Notice that both preemptive actions\nand conservative actions start in the same way.\nRules (H3 ) and (H4 ) combine the start of an action with operator +. In rule (H3 ) the identifier l of\nthe action α started by process P has no conflicts with the identifiers of the competing actions running in\nprocess Q. Differently, in the case of rule (H4 ) a conflict does exist, which implies that a fresh identifier\nl ′ replaces l. Again, the policy by which we choose the new fresh identifier, along the line of [11], is such\nthat the resulting LTS is finite. More precisely, the use of the set N − Id(P + Q) is such that we consider,\nin the process of renaming an identifier in P′ , the only set of identifiers not used in a process P + Q and,\n\n\f60\n\nAspects of multiscale modelling in a process algebra for biological systems\n\n{l}\n\n(I1 ) [α ]l .P −→I α .P\n0/\n\n(I3 ) [α ]l .P −\n→I [α ]l .P\n0/\n\n→I α .P\n(I5 ) α .P −\n\n{l}\n\n(I2 ) [α ]l : P −→I α : P\n0/\n\n(I4 ) [α ]l : P →\n− I [α ]l : P\n0/\n\n(I6 ) α : P −\n→I α : P\n\nL\n\n(I7 )\n\nM\n\nP−\n→I P ′\n\nQ−\n→I Q ′\nL∪M\n\nP + Q −−→I P′ + Q′\n\nFigure 2: The interruption relation −\n→I ⊆ C × ℘(N) × C .\nfrom that, the choice of extracting the minimum value is such that the LTS is finite. We use this strategy\nin all the rules where we have to resolve some conflicts.\nRules (H5 ), (H6 ) and (H7 ) combine the start of an action with the operator |. Rules (H5 ) and (H6 )\nmodel an autonomous move by one of the two processes, and deal with identifiers as (H3 ) and (H4 ),\nrespectively. As in classical process algebras, we do not force P and Q to handshake, since P could\nhandshake with a further process composed in parallel with P | Q.\nNotice that here we may have a conflict even if in Q the action associated with the colliding identifier\nis the complementary action α . Rule (H7 ) models the handshaking by assigning to this particular instance\nof synchronization a new fresh identifier l ′′ chosen with the same policy used to resolve conflicts in the\nprevious rules. The renaming of both old identifiers with the newly generated is due to the fact that, in\ngeneral, the two processes will have two different candidate identifiers, i.e. l and l ′ . The system in this\ncase exhibits the internal action τ + . By applying this rule, the two processes terminated this handshaking\nphase.\n\nThe interruption relation\nThis relation, differently from those found in classical process algebras, is used to model the interruption\nof a set of actions currently running in a process. The interruption is caused by the completion of\ncompeting preemptive actions. The interruption relation is −\n→I ⊆ C ×℘(N)× C , where a label M ∈ ℘(N)\ncontains the identifiers of the actions that have been interrupted. The rules presented in Figure 2 are at\nthe basis of the definition of −\n→I .\nAt any time, a process either in configuration [α ]l .P or [α ]l : P may interrupt the action it is currently\nperforming. In these cases, treated with rules (I1 ) and (I2 ), it moves to a configuration in which the\ninterrupted action α may start again, namely to configuration α .P or α : P, respectively. In both the rules,\nthe identifier l of the interrupted action is exhibited as a label of this transition. Again, this information\nwill be used to interrupt also the partner of this action, as we are assuming that there is a partner in the\nsystem which, after terminating the handshaking phase, has been coupled with the same label l.\nIn some cases not all the actions have to be interrupted, so the processes in configuration [α ]l .P or\nl\n[α ] : P must be able also to non-deterministically decide whether to interrupt or not. This second case is\ndescribed by rules (I3 ) and (I4 ), which may seem controversial at first glance. In particular, it may not be\nclear why a process may independently decide whether to interrupt or not some of the currently running\nactions. The need of this autonomy for the process can be clarified by an example. Let us assume a\nprocess configuration (P + Σ) | (Q + Σ′ ) | S | R, where both P and Q successfully complete a preemptive\naction. The actions to be interrupted are those currently running in both Σ and Σ′ , namely those with\n\n\fBarbuti et al.\n\n61\n\nl,α − ,L\n\nl,α − ,/0\n\n(C1 ) [α ] .P −−−→CP P\nl\n\nP −−−→CP P′\n\n(C2 )\n\nl,α − ,L∪Id(Q)\n\nP + Q −−−−−−−→CP P′\nl,α − ,L\n\n(C3 )\n\nP −−−→CP P′\n\nM\n\nQ−\n→I Q ′\n\nM ⊇ (Id(Q) ∩ L) V = (L ∪ M) \\ (L ∩ Id(Q)) α ∈ Actτ\nl,α − ,V\n\nP | Q −−−−→CP P′ | Q′\nl,α − ,L\n\n(C4 )\n\nP −−−→CP P′\n\nl,α − ,M\n\nQ −−−−→CP Q′\n\nN = L∩M\n\nl,τ − ,(L∪M)\\N\n\nP | Q −−−−−−−−→CP P′ | Q′\n−\nFigure 3: The completion relation for preemptive actions −\n→CP ⊆ C × ΘCP\n×C.\n\nidentifiers denoted by Id(Σ) ∪ Id(Σ′ ). Let us assume that some of the actions that have to be interrupted\nin Σ and Σ′ were coupled with some actions in S. In this case, also these actions in S should be interrupted\nas well. Moreover, S may be involved in other actions currently running and coupled with actions in R.\nIndeed, these actions must not be interrupted. This means that from S the correct derivation with the\ninterruption relation, in general, will not exhibit as label Id(S), indeed it will exhibit a strict subset of\nId(S). This implies that S must be able to autonomously decide which actions to interrupt, and this can\nbe done by properly combining derivations of the interruption relation. The composition of the relations\nof the whole semantics will provide the correctness, namely the fact that all and only those to interrupt\nare actually interrupted.\nAlso, a process which is not performing any action, namely a process in a configuration α .P or α : P,\ndoes not interrupt any action, as stated by rules (I5 ) and (I6 ).\nFinally, rule (I7 ) simply collects the labels of the interrupted actions in a summation. Notice that this\nrelation is not defined for process configurations of the form PC | PC′ as the use of this relation is limited\nto the level of the summation.\n\nThe completion relation for preemptive actions\nThis relation is used to model the completion of a preemptive action. We will define completion relations\nalso for conservative actions as well as the combination of both preemptive and conservative actions.\n−\n−\nThe completion relation for preemptive actions is →\n− CP ⊆ C × ΘCP\n× C , with ΘCP\ncontaining labels\nof the form\nθ − := (l, α − , N)\nwhere l ∈ N represents the identifier that was assigned to the completed action α ∈ Actτ when it was\nstarted, N ∈ ℘(N) is the set of the identifiers of the competing actions that are interrupted by the termination of α , and the use of the superscript “-” comes from the definition of the semantics in the ST style.\nThe rules presented in Figure 3 are at the basis of the definition of −\n→CP . We implicitly assume rules\nsymmetric to (C2 ) and (C3 ).\nRule (C1 ) describes the completion of a preemptive action. When it completes, as the action is\npreemptive, the process is substituted by its continuation P. In the label, the identifier l is needed to\ncouple this process with the one performing the corresponding complementary action α , which will have\nthe same identifier l because of the handshaking, and 0/ states that no action is interrupted.\n\n\fAspects of multiscale modelling in a process algebra for biological systems\n\n62\n\nl,α − ,L,P′′\n\nl,α − ,/0,P\n\n(C5 ) [α ] : P −\n−−−−\n→CC α : P\nl\n\nl,α − ,L,P′′\n\n(C7 )\n\nP −−−−−→CC P′\nl,α − ,L,P′′\n\nP | Q −−−−−→CC P′ | Q\n\n(C6 )\n\nQ−\n→I Q ′\n\nl,α − ,L∪M,P′′\n\nP + Q −−−−−−−→CC P′ + Q′\n\nl,α − ,/0,P′′\n\n(C8 )\n\nM\n\nP −−−−−→CC P′\n\nP −−−−−→CC P′\n\nl,α − ,/0,Q′′\n\nQ −−−−−→CC Q′\n\nl,τ − ,/0\n\nP | Q −−−→CP P′ | Q′ | P′′ | Q′′\n\n−\nFigure 4: The completion relation for conservative actions −\n→CC ⊆ C × ΘCC\n×C.\n\nRule (C2 ) states that the completion of a preemptive action in P affects a summation P + Q so that\nall actions running in Q should be interrupted. This is obtained by adding to the set of labels of actions\ninterrupted L, the set of actions currently running in the process Q which disappears by the completion\nof the action in P, hence the exhibited set of labels becomes L ∪ Id(Q).\nRule (C3 ) states that the completion of a preemptive action in P affects a parallel composition P | Q\nso that all actions running in Q that are coupled with actions interrupted in P, must be interrupted as well.\nRule (C4 ) models the case in which both P and Q complete preemptive actions that were coupled. As\nin classical process algebras, the whole system P | Q exhibits an internal action τ . Some of the actions\nrequired to be interrupted outside P, may be also required to be interrupted by Q. Such a set is denoted\nby N and can be removed from the set of actions that can be interrupted outside P | Q. The remaining\nset of actions, which have to be still interrupted by further composition with the parallel operator outside\nP | Q, is the set of those belonging to P and not to Q, and viceversa.\n\nThe completion relation for conservative actions\n−\nThis relation is used to model the completion of a conservative action. This relation is −\n→CC ⊆ C × ΘCC\n×\n−\nC , where ΘCC contains labels of the form\n\nθ − := (l, α − , N, P)\nwhere l ∈ N represents the identifier assigned to the completed action α ∈ Actτ , N ∈ ℘(N) is the set of\nidentifiers of the interrupted actions, and P ∈ P is the continuation of the action which terminated and\nthat, syntactically, must be propagated at the level of a parallel composition. At first sight it could sound\nstrange that we need the component N. The idea is that we have to take care that a process Q terminating\na conservative action α could be composed in parallel with another process Q′ terminating the action\nα coupled with α . Now, if α is preemptive, there may be some running actions β in Q′ that should be\ninterrupted, which implies that if there is an action β in Q coupled with β , also β must be interrupted.\nFor this reason, such a β must appear in N. Of course, if also α is conservative, then in the transition by\nQ used to infer the transition of Q | Q′ , N will be empty.\nThe rules presented in Figure 4 are at the basis of the definition of relation →\n− CP . We implicitly\nassume rules symmetric to (C6 ) and (C7 ).\nRule (C5 ) deals with termination of a conservative action α in configuration [α ]l : P. The process\nbecomes able to perform the terminated action again, namely it rolls back to configuration α : P. Also,\nas expected by a conservative action, it produces the continuation P, which, because of the inductive\napproach of the SOS semantics, cannot appear at the same syntactic level of the configuration α : P.\nSpecifically, the continuation P will have to appear at the level of a parallel composition. To forward P\n\n\fBarbuti et al.\n\n63\n\nl,α − ,L,P′′\n\n(C9 )\n\nP −−−−−→CC P′\n\nl,α − ,M\n\nQ −−−−→CP Q′\n\nL⊆M\n\nl,τ − ,M\\L\n\nP | Q −−−−−→CP P′ | Q′ | P′′\nFigure 5: The completion relation for hybrid actions obtained by means of the other completion relations.\nℓ\n\n(R1 )\n\nP →r P ′\nℓ\n\nA →r P ′\n\ndef\n\nif A = P\n\nFigure 6: The standard rule for recursion.\nat the correct syntactic level, P is exhibited as a label of the transition. The empty set used in the label\ndenotes that no action is interrupted by completion of α .\nRule (C6 ) clearly justifies the terminology “conservative”. When a conservative action α completes\nin a process P being part of a configuration P + Q, it is required neither that actions in Q are interrupted,\nnor that Q is canceled. This is clearly different from what happens when a preemptive action is completed\n(rule (C2 )). More precisely, the set of actions interrupted in Q, namely M, is a subset of Id(Q) since here\nnot all the actions in Q have to be interrupted. Furthermore, the continuation of the action, namely P′′ in\nthe rule, is exhibited as a transition label, since also in this case we are not yet at the syntactic level of a\nparallel composition.\nRule (C7 ) describes the case in which P completes a conservative action α and the coupled action α\nis not in Q, since it runs in some other process running in parallel with P | Q.\nRule (C8 ) deals with the completion of two coupled conservative actions. As expected, the system\nexhibits an internal action τ , the label shows that no action has to be interrupted, and both the continuations appearing in the labels of the transitions of both processes P and Q, namely P′′ and Q′′ , are put in\nparallel with the continuations P′ and Q′ . Notice that this last derivation is a derivation for →CP rather\nthan →CC . The reason for this is that P′′ and Q′′ are already at the correct syntactic level and do not\nrequire to be lifted anymore.\n\nCompletion of both conservative and preemptive actions\nWe have to deal with the completion of two coupled actions α and α such that one of them is conservative\nand the other preemptive. To this purpose, we add the rule shown in Figure 5 and we implicitly assume\na symmetric rule.\nNotice that L ⊆ M expresses that the actions running in the process Q performing the conservative\nactions and that have to be interrupted are those that were coupled with actions running in the process P\nperforming the preemptive action. In fact, such a coupling is the only reason we have to interrupt actions\nrunning in Q.\n\nRecursive definitions\nAs far as the naming of processes is concerned, we define the standard rule for recursion, showed in\nFigure 6, for all the relations we defined.\n\n\fAspects of multiscale modelling in a process algebra for biological systems\n\n64\n\nA toy example\nIn this section we discuss the modeling of a multiscale system where we consider two populations. At\na higher level of abstraction we consider a cell C, and at a lower level a generic protein P. A cell C can\nbe involved in a process leading to its duplication. Also, it can be involved in some low–level reactions\n(i.e. DNA transcription inside its nucleus) leading to the creation, in the environment outside C, of a\nprotein of species P. Of course, we consider this model at a level of detail such that we do not need to\ntake into account any other possible population of either cells or proteins which could be involved in the\ndynamics.\nIn the context of chemically reacting systems such a system may be described by two populations C\nand P, and by two reactions R1 and R2 such that\nR1 : C −\n→ C +C\n\nR2 : C −\n→ C + P.\n\nReactions R1 and R2 model the non linear growth of cell C and the production of a protein P by a cell C,\nrespectively. Notice that, at this level of detail, the production of protein P depends on the cell C where\nall the details of the biological process leading to the creation of the protein are abstracted away. The\ninitial state of the system can be defined to have a precise initial number of cells C and proteins P in the\nenvironment.\nWe model now such a system in PAPC, and we show how the semantics models the behavior of the\npopulations. As PAPC is based on the paradigm processes-as-molecules, we start by assuming two types\nof processes for each species which, for clarity, are named C and P. As we want to model two reactions,\nwe assume the following set of actions {α , α , γ , γ } where α (resp. α ) and γ (resp. γ ) model reaction\nR1 and R2 , respectively. Also, as in PAPC the communication is dyadic and the reactions use a single\nreactant, we define two auxiliary process X α and X γ , used to model the communications on α and γ ,\nrespectively.\nReaction R1 creates two new different cells able to start again, if possible, the duplication process.\nIn the context of PAPC we model R1 by using preemptive actions for both α and α since the duplication\nof a cell interrupts all the low-level protein-transcription event inside the duplicated cell. Differently, the\nactions modeling reaction R2 are conservative since the protein-transcription event does not interrupt the\nduplication process started by a cell.\nThe PAPC processes are defined as\ndef\n\nC = α .(C | C) + γ : P\n\nX α = α .(X α | X α )\ndef\n\nXγ = γ : 0.\ndef\n\nNotice that we do not give a definition of the process P since it appears only as a product of the events\nwe want to model, and hence we are not interested in the interactions it may have in the system.\nProcess C as expected can perform two actions. Action α produces the two new copies of C. Such\nan action is performed by synchronizing with the auxiliary process X α which will produce two copies of\nitself to permit the duplication of the new cell. Indeed, the number of copies of the auxiliary process X α\nmust grow with the same law of growth for the cells C. Both C and X α behave as preemptive in α and\nα , as expected. Process C can also perform, by synchronizing with X γ , the action γ . The result of such\naction is to produce a new protein B without interrupting its running duplication, if any. The number of\nprocesses X γ in the system bounds the number of cells which can simultaneously produce a protein of\ntype B. This is obtained by producing, in X γ , the nil process. If this were a too strong constraint, it would\nhave been possible to add a X γ process in the continuation of the action α in C, as done for X α . For the\nsake of simplicity in this model we consider this constraint to be reasonable.\n\n\fBarbuti et al.\n\n65\n\nWe discuss now some features of the semantics of PAPC for a simple system S described by the\nprocess\nS = C | Xα | Xγ .\ndef\n\nIn S both the reactions may fire. We assume reaction R1 to fire first. To this extent, the semantics permits\nto observe handshaking derivations as\n1,α +\n\n1,α +\n\nX α −−−→H [α ]1 .(X α | X α ) .\n\nC −−−→H [α ]1 .(C | C) + γ : P\nThen the whole process S performs a derivation as\n1,τ +\n\nS −−→H [α ]1 .(C | C) + γ : P | [α ]1 .(X α | X α ) | X γ ≡ S′\nwhere the new process S′ is such that the action α is now running in C and in X α , with identifier 1. In S′\naction α can not start, but just complete, we assume reaction R2 to fire. The semantics permits to observe\nthe handshaking derivations\n1,γ +\n\n1,γ +\n\nX γ −−→H [γ ]1 : 0 .\n\nγ : P −−→H [γ ]1 : P\n\nThe composition of these derivations resolves the conflicts of the colliding identifiers such that the derivation for C will be\n2,γ +\n\n[α ]1 .(C | C) + γ : P −−→H [α ]1 .(C | C) + [γ ]2 : P\nand the whole system performs the following derivation\n2,τ +\n\nS′ −−→H [α ]1 .(C | C) + [γ ]2 : P | [α ]1 .(X α | X α ) | [γ ]2 : 0 ≡ S′′\nwhere in S′′ all the possible actions are running. We consider now two different cases: (a) R1 completes\nbefore R2 and (b) viceversa.\n(a) Reaction R1 completes before R2 : in this case action α (resp. α ) completes before action γ (resp.\nγ ), interrupting it. The semantics permits to derive transitions as\n1,α − ,/0\n\n[α ]1 .(C | C) −−−−→CP C | C\n\n1,α − ,/0\n\n[α ]1 .(X α | X α ) −−−−→CP X α | X α .\n\nActions in C and in X γ have to be interrupted, hence we derive\n1,α − ,{2}\n\n[α ]1 .(C | C) + [γ ]2 : P −−−−−→CP C | C\n\n{2}\n\n[γ ]2 : 0 −−→I X γ\n\nwhere {2} denotes the actions to be interrupted. Consequently, the whole process S′′ will perform\nthe transition\n1,τ − ,/0\n\nS′′ −−−→CP C | C | X α | X α | X γ\nwhere in the resulting process no actions are running, as expected, and there are two cells and two\nauxiliary processes X α .\n\n\fAspects of multiscale modelling in a process algebra for biological systems\n\n66\n\n(b) Reaction R2 completes before R1 : in this case action γ (resp. γ ) completes before action α (resp.\nα ). The semantics permits to derive transitions as\n2,γ − ,/0,0\n\n[γ ]2 : 0 −−−−→CC γ : 0\n\n2,γ − ,/0,P\n\n[γ ]2 : P −−−−→CC γ : P .\n\nNo actions have to be interrupted in any process, hence we derive\n0/\n\n→I [α ]1 .(C | C)\n[α ]1 .(C | C) −\n\n2,γ − ,/0,0\n\n[α ]1 .(C | C) + [γ ]2 : P −−−−→CC [α ]1 .(C | C) + γ : P .\n\nThe whole process S′′ will then perform the transition\n2,τ − ,/0\n\nS′′ −−−→CP [α ]1 .(C | C) + γ : P | [α ]1 .(X α | X α ) | X γ | P | 0\nwhere, as expected, in the resulting process only one action is still running (cell division) and a\nsingle protein P has been produced.\n\n4\n\nBisimulation equivalence for PAPC\n\nBisimulation equivalence is a central notion in concurrency theory. For processes with a higher order\nbehaviour, namely processes whose behaviour is described by portions of transition systems in which\nprocesses appear in the labels, the notion of bisimulation is usually replaced by a higher order notion of\nbisimulation [3, 10, 35], which can be rephrased in our setting as follows:\nDefinition A symmetric relation R ⊆ C × C is a bisimulation iff whenever (P, Q) ∈ R, then it holds\nthat:\nℓ\n\nℓ\n\n• if P →r P′ for any P′ ∈ C , r ∈ {H, I,CP} and label ℓ, then Q →r Q′ for some Q′ ∈ C such that\n(P′ , Q′ ) ∈ R.\nl,α − ,L,P′′\n\nl,α − ,L,Q′′\n\n• if P −−−−−→CC P′ for any P′ ∈ C , l ∈ N, α ∈ Act, L ⊆ N and P′′ ∈ C , then Q −−−−−→CC Q′ for\nsome Q′ , Q′′ ∈ C such that (P′ , Q′ ) ∈ R and (P′′ , Q′′ ) ∈ R.\nThe union of all bisimulations is, in turn, a bisimulation, which is denoted with ≈ and is called as “the\nbisimulation”.\nFor an algebric treatment of bisimulation equivalence and to reason in a compositional way, a bisimulation is required to be a congruence. By taking the standard notion of context C[ ], a bisimulation R is\na congruence with respect to all operations of the process algebra if and only if, given any pair (P, Q) ∈ R\nand any context C[ ], it holds that (C[P],C[R]) ∈ R.\nTheorem Bisimulation is a congruence w.r.t. all PAPC operations.\nProof. If we consider PAPC without recursion, then the proof comes for free. In fact, in [26] it is shown\nthat higher order bisimulation is a congruence with respect to all process algebra operations whose semantics is defined through transition rules respecting some syntactical constraints, and it can be checked\nthat the transition rules we use respect such constraints. The extension of the proof to the case of recursion is standard.\n\n\fBarbuti et al.\n\n67\n\nThe bisimulation relation for PAPC is a very fine behavioral equivalence. This can be seen as a disadvantage, since with behavioral equivalences it is often desirable to be able to equate as many processes\nas possible. On the other hand, the fact that bisimulation turns out to be fine may have another meaning,\nnamely that all the ingredients used in the process algebra play an important role. This does not happen, for instance, for the parallel composition in some variants of CCS where it can be reduced into an\nequivalent summation of processes.\nWe go through this last consideration via an example. Let us consider the following two PAPC\nprocesses\ndef\n\ndef\n\nC = α .(C | C)\n\nC′ = α : C′ .\n\nThe behaviour of the two processes is similar: both of them perform an action α and then continue\nas with copies of the initial process (in the case of C′ one of such two copies is not indicated in the\ncontinuation of α since it is obtained from the semantics of conservative actions). Even if the behaviour\nof the two processes seems to be the same, it is immediate to see that the two processes are not bisimilar.\nl,α +\n\nIn fact, they repeatedly perform the same handshaking transition −−→H , but followed by two different\nl,α − ,/0\n\nl,α − ,/0,C′\n\ncompletion transitions, namely −−−→CP and −−−−−→CC . This permits to state that, in general, C 6≈ C′ .\nThis is exactly what we expect from our bisimulation relation since, when the two processes are put\nin a summation context, their behavior would determine the behavior of the whole context. In fact,\nthe completion of the action in a process C + Σ would interrupt any action currently running in Σ and,\ndifferently, for the case of C′ + Σ no actions in Σ would be interrupted. This is due to the fact that the two\nprocesses perform the same action but with different prefix operators.\nIn order to see whether this is important, is enough to consider the toy example given in the previous\ndef\nsection where actions are modeled by a process C = α .(C | C) + γ : P. In that example the creation of\ntwo new cells should be an event which interrupts, when completes, the production of protein P by the\ndef\ncell. Of course, if the process used in the model would have been C = α : C + γ : P, then the completion\nof the duplication process for the cell would not have interrupted the production of P.\n\n5\n\nConclusions\n\nIn this paper we considered the problem of modeling biological systems in which different scale levels\nare taken into account resulting in the fact that actions at a higher level may take more time than actions\nat a lower one.\nIn order to model such systems we defined PAPC, a variant of the CCS process algebra in which a\nprocess may be simultaneously involved in one long lasting high level action and in several faster lower\nlevel actions. In order to model this, we added in the algebra two different prefix operators to model\nthe role of a process in an action. A process can either act as conservative or preemptive in an action,\nresulting in two different behaviors for the process and the other actions which currently are started and\nnot completed.\nWe gave a compositional Structural Operations Semantics for PAPC by means of different relations,\none for each of the possible events which may change the state of a process, namely the start and the\ncompletion of the actions. The semantics we gave is in ST style as this permits to observe the start and\nthe completion of an action as two detached events. This style of the semantics permits also to observe\nprocesses in configurations in which multiple actions are started and not completed.\n\n\f68\n\nAspects of multiscale modelling in a process algebra for biological systems\n\nWe also defined a notion of behavioral equivalence for PAPC processes based on the ideas of higherorder bisimulations for process calculi. We proved that our bisimulation is a congruence for all PAPC\noperators.\nIn the paper, we also showed some simple example of PAPC processes such that their semantics\npermits to observe the key features of the algebra. We also discussed the notion of bisimulation we\nintroduced by analyzing two simple PAPC processes.\nAs a future work we will apply PAPC to the modeling of multiscale systems in order to prove the\nutility of the formalism. Also, we may consider to enrich PAPC with biologically inspired operators\nto easily model complexation, de-complexation or more complex biological structures as membranes\nor compartments as it has been previously done with other calculi. Moreover, we may consider the\ndefinition of more biologically inspired notions of equivalence for PAPC processes.\n\nReferences\n[1] L. Aceto, W.J. Fokkink and C. Verhoef (2001): Structural operational semantics. Chapter in: J.A. Bergstra,\nA. Ponse and S.A. Smolka (Eds.): Handbook of Process Algebra, Elsevier, pp. 197–292.\n[2] T. Alarcon, H. M. Byrne and P. K. Maini (2005): A multiple scale model for tumour growth. MultiscaleModel\nSim. 3, 440–475.\n[3] E. Astesiano, A. Giovini and G. Reggio (1988): Generalized bisimulation in relational specifications. Proc.\nSTACS 98, Springer LNCS 294, 207–226.\n[4] G.S. Ayton, W.G. Noid and G.A. Voth (2007): Multiscale modeling of biomolecular systems: in serial and in\nparallel. Current Opinion in Structural Biology, 17, Issue 2, 192-198.\n[5] R. Barbuti, G. Caravagna, A. Maggiolo-Schettini and P. Milazzo (2009): On the Interpretation of Delays\nin Delay Stochastic Simulation of Biological Systems. 2nd Int. Workshop on Computational Models for Cell\nProcesses (CompMod’09), EPTCS 6, 17–29.\n[6] R. Barbuti, G. Caravagna, A. Maggiolo-Schettini and P. Milazzo (2010): Delay Stochastic Simulation of\nBiological Systems: A Purely Delayed Approach. Submitted.\n[7] R. Barbuti, G. Caravagna, A. Maggiolo-Schettini, P. Milazzo and G. Pardini (2008): The Calculus of Looping\nSequences. Chapter in: M.Bernardo, P.Degano and G.Zavattaro (Eds.): Formal Methods for Computational\nSystems Biology (SFM 2008), Springer LNCS 5016, 387–423.\n[8] R. Barbuti, A. Maggiolo-Schettini, P. Milazzo and G.Pardini (2008): Spatial Calculus of Looping Sequences.\nInt. Workshop From Biology to Concurrency and Back (FBTC’08), ENTCS 229(1), 21–39.\n[9] F. Billy, B. Ribba, O. Saut , H. Morre-Trouilhet, T. Colin, D. Bresch, J.P. Boissel, E.Grenier and J.P. Flandrois.\n(2009): A pharmacologically based multiscale mathematical model of angiogenesis and its use in investigating\nthe efficacy of a new cancer treatment strategy J.Theor.Biol. 260, 545-562.\n[10] G. Boudol (1989): Towards a lambda-calculus for concurrent and communicating systems. Proc. TAPSOFT,\nSpringer LNCS 351, 149–161.\n[11] M. Bravetti and R. Gorrieri (1999): Deciding and Axiomatizing ST Bisimulations for a Process Algebra with\nRecursion and Action Refinement. Tech, Rep. UBLCS-99-1, University of Bologna.\n[12] M. Bravetti and R. Gorrieri (2002): The theory of interactive generalized semi-Markov processes. Theoretical Computer Science 282 (1), 5–32.\n[13] H.M. Byrne, M.R. Owen., T. Alarcon, J. Murphy and P.K. Maini (2006): Modelling the response of vascular\ntumours to chemotherapy: a multiscale approach. Math.Mod.Meth. Appli. Sci. 15, 1219–1241.\n[14] L. Cardelli (2008): On Process Rate Semantics. Theoretical Computer Science 391(3) 190–215.\n[15] F. Ciocchetta and J. Hillston (2009): Bio-PEPA: a Framework for the Modelling and Analysis of Biochemical\nNetworks. Theoretical Computer Science 410 (33-34), 3065–3084.\n\n\fBarbuti et al.\n\n69\n\n[16] F. Ciocchetta and J. Hillston (2008): Calculi for Biological Systems. Chapter in: M.Bernardo, P.Degano and\nG.Zavattaro (Eds.): Formal Methods for Computational Systems Biology (SFM 2008), Springer LNCS 5016,\n265–312.\n[17] V. Danos, J. Feret, W. Fontana, R. Harmer and J. Krivine (2007): Rule-Based Modelling of Cellular Signalling. Proceedings of CONCUR07, Springer LNCS, 17–41.\n[18] D. Gillespie (1977): Exact Stochastic Simulation of Coupled Chemical Reactions.\nChemistry 81, 2340.\n\nJournal of Physical\n\n[19] M. Hennessy (1988): Axiomatising Finite Concurrent Processes. SIAM Journal of Computing 17(5),\n997–1017.\n[20] R.J. van Glabbeek (1990): The Refinement Theorem for ST Bisimulation Semantics. Proc. IFIP Working\nConference on Programming Concepts and Methods. North Holland.\n[21] R.J. van Glabbeek and F.W. Vaandrager (1987): Petri Net Models for Algebraic Theories of Concurrency.\nProc. PARLE, Springer LNCS 259, 224–242.\n[22] P. Milazzo (2007): Qualitative and Quantitative Formal Modeling of Biological Systems. Ph.D. Thesis,\nDepartment of Computer Science, University of Pisa.\n[23] R. Milner (1989): Communication and Concurrency.\nScience, ISBN 0-131-15007-3, 1989.\n\nPrentice Hall, International Series in Computer\n\n[24] R. Milner, J. Parrow and D. Walker (1992): A Calculus of Mobile Processes, I Inf. Comput. 100(1): 1–40.\n[25] R. Milner, J. Parrow and D. Walker (1992): A Calculus of Mobile Processes, II Inf. Comput. 100(1): 41–77.\n[26] M. Mousavi, M. Gabbay and M.A. Reniers (2005): SOS for Higher Order Processes. Proc. CONCUR 2005,\nSpringer LNCS 3653, 308–322.\n[27] M. Pedersen and G. Plotkin (2010): A Language for Biochemical Systems: Design and Formal Specification.\nTransactions on Computational Systems Biology XII, 5945(3), 77–145.\n[28] G.D. Plotkin (1981): A Structural Approach to Operational Semantics. Tech. Rep. DAIMI FN-19, Aarhus\nUniversity, Denmark.\n[29] C. Priami (1995): Stochastic π -Calculus. The Computer Journal 38 (7), 578–589.\n[30] C. Priami, A. Regev, E. Shapiro and W. Silverman (2001): Application of a stochastic name-passing calculus\nto representation and simulation of molecular processes. Information Processing Letters, 80:25–31.\n[31] A. Regev, E. M. Panina, W. Silverman, L. Cardelli and E. Y. Shapiro (2004): Bioambients: an abstraction\nfor biological compartments. Theoretical Computer Science, 325(1):141–167.\n[32] A. Regev, W. Silverman and E. Shapiro (2001): Representation and simulation of biochemical processes\nusing the pi-calculus process algebra. In Pacific Symposium on Biocomputing 6, 459–470. World Scienti?c\nPress.\n[33] B. Ribba, T. Colin and S. Schnell (2006): A multiscale mathematical model of cancer, and its use in analyzing\nirradiation therapies. Theor.Biol.Med.Model.3, 7.\n[34] Sloot, P. M. A. and A. G. Hoekstra (2009): Multi-scale modelling in computational biomedicine Brief.\nBioinform., doi:10.1093/bib/bbp038.\n[35] B. Thomsen (1995): A theory of Higher Order Communicating Systems. Inform. Comput. 116, 38–57.\n\n\f"
        ],
        [
         "13",
         "13",
         "cs.CE",
         "Computational Engineering",
         "1306.5365v1.pdf",
         "April 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\nws-aada˙HT˙v3.3\n\narXiv:1306.5365v1 [gr-qc] 23 Jun 2013\n\nAdvances in Adaptive Data Analysis\nc World Scientific Publishing Company\n\nON INVESTIGATING EMD PARAMETERS\nTO SEARCH FOR GRAVITATIONAL WAVES\n\nHIROTAKA TAKAHASHI\nDepartment of Management and Information Systems Science,\nNagaoka University of Technology, Niigata 940-2188, Japan and\nEarthquake Research Institute, The University of Tokyo, Bunkyo-Ku, Tokyo 113-0032, Japan\nhirotaka@kjs.nagaokaut.ac.jp\nKEN-ICHI OOHARA, MASATO KANEYAMA, YUTA HIRANUMA\nGraduate School of Science and Technology, Niigata University, Niigata 950-2181, Japan\nJORDAN B. CAMP\nLaboratory for Gravitational Physics, NASA Goddard Space Flight Center,\nGreenbelt, Maryland 20771, USA\n\nThe Hilbert-Huang transform (HHT) is a novel, adaptive approach to time series analysis.\nIt does not impose a basis set on the data or otherwise make assumptions about the\ndata form, and so the time–frequency decomposition is not limited by spreading due\nto uncertainty. Because of the high resolution of the time–frequency, we investigate the\npossibility of the application of the HHT to the search for gravitational waves. It is\nnecessary to determine some parameters in the empirical mode decomposition (EMD),\nwhich is a component of the HHT, and in this paper we propose and demonstrate a\nmethod to determine the optimal values of the parameters to use in the search for\ngravitational waves.\nKeywords: Hilbert-Huang Transform; Gravitational Wave Data Analysis; Sifting Stoppage Criteria.\n\n1. Introduction\nThe Hilbert-Huang transform (HHT), which consists of an empirical mode decomposition (EMD) followed by the Hilbert spectral analysis, was developed recently\nby [Huang et al. 1996; 1998; 1999]. It presents a fundamentally new approach to\nthe analysis of time series data. Its essential feature is the use of an adaptive timefrequency decomposition that does not impose a fixed basis set on the data, and\ntherefore, unlike Fourier or Wavelet analysis, its application is not limited by the\ntime-frequency uncertainty relation. This leads to a highly efficient tool for the investigation of transient and nonlinear features. The HHT is applied in various fields,\nincluding materials damage detection [Yang et al. (2004)] and biomedical monitoring [Novak et al. (2004); Huang et al. (2005)].\n1\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\n2\n\nws-aada˙HT˙v3.3\n\nHirotaka Takahashi, Ken-ichi Oohara et al.\n\nSeveral laser interferometric gravitational wave detectors have been designed and built to detect gravitational waves directly. They include LIGO\n[Abbott et al. (2009)] in the US, VIRGO [Accadia et al. (2011)] in Europe, and KAGRA (LCGT) [Somiya et al. (2012)] in Japan. The direct detection of gravitational\nwaves is important not only because it will help to investigate various unsolved\nastronomical problems and to find new objects that cannot be seen by other observational methods, but it will also be a new tool with which to verify general\nrelativity and other theories in a strong gravitational field. These detectors are sensitive over a wide frequency band, a range of between about 10 Hz and a few kHz,\nand they have the ability to observe the waveform of a gravitational wave, which\nwould contain astrophysical information. There are several kinds of data analysis\nschemes that are being developed and applied to observational data. Since gravitational waves are considered to be faint and gravitational wave detectors produce\na great variety of nonlinear and transient noise, an efficient data analysis scheme\nis required. The HHT has the promise of being a powerful new tool to extract the\nsignal from the noise of the detector.\nIn the HHT, the EMD first decomposes the data into intrinsic mode functions\n(IMFs), each representing a locally monochromatic frequency scale of the data.\nSumming over all the IMFs will recover the original data. Then, the Hilbert spectral\nanalysis derives the instantaneous amplitude (IA) and instantaneous frequency (IF)\nfrom the analytical complex representation of each IMF; the IMF itself and the\nHilbert transform of the IMF are the real and imaginary parts, respectively. The IA\nis obtained by taking the absolute value, and the IF is obtained by differentiating\nthe phase.\nWe consider the application of the HHT to the search for the signal of gravitational waves [Camp et al. 2007; 2009] [Stroeer et al. 2009; 2011] . It is necessary\nto determine some parameters in the EMD component of the HHT, and in this\npaper we propose and evaluate a method to determine the optimal values of the\nparameters to use in the search for gravitational waves.\nThis paper is organized as follows. In Sec. 2, we briefly give an overview of\nthe HHT. In Secs. 3 and 4, we propose and demonstrate our method, as described\nabove. We summarize our work in Sec. 5.\n2. Brief Description of the Hilbert–Huang Transform\nIn this section, we offer a brief introduction of the two HHT components: the Hilbert\nspectral analysis and the EMD. We will show that the Hilbert transform can lead\nto an apparent time-frequency-energy description of a time series. However, this\ndescription may not be consistent with physically meaningful definitions of IF and\nIA, since the Hilbert transform is based on Cauchy’s integral formula of holomorphic\nfunctions that tend to zero sufficiently quickly at infinity. The EMD, however, can\ngenerate components of the time series for which the Hilbert transform can lead\nto physically meaningful definitions of these two instantaneous quantities. Hence,\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\nws-aada˙HT˙v3.3\n\nOn Investigating EMD Parameters to Search for Gravitational Waves\n\n3\n\nthe combination of the EMD and the Hilbert transform provides a more physically\nmeaningful time-frequency-energy description of a time series.\nWe will assume that the input h(t) is given by sampling a continuous signal at\ndiscrete times, t = tj for j = 0, 1, · · · , N − 1.\n2.1. Hilbert spectral analysis\nThe purpose of the development of the HHT is to provide an alternative view of\nthe time-frequency-energy paradigm of data. In this approach, the nonlinearity and\nnonstationarity can be dealt with better than by using the traditional paradigm\nof constant frequency and amplitude. One way to express the nonstationarity is to\nfind the IF and IA, which is why the Hilbert spectral analysis was included as a\npart of the HHT.\nThe Hilbert transform of a function h(t) is defined by\nv(t) =\n\n1\nP\nπ\n\nZ\n\n∞\n−∞\n\nh(τ )\ndτ = h(t) ∗\nt−τ\n\n\u0012\n\n1\nπt\n\n\u0013\n\n,\n\n(1)\n\nwhere P and ∗ denote the Cauchy principal value of the singular integral and\nthe convolution, respectively. If a function h(t) belongs the Lebesgue space Lp for\n1 < p < ∞, the Hilbert transform is well-defined and F (t) = h(t) + iv(t) is the\nboundary value of a holomorphic function F (z) = F (t + iy) = aHT (t)eiθ(t) in the\nupper half-plane. Then the IA aHT (t) and the instantaneous phase function θ(t) are\ndefined by\naHT (t) =\n\nq\nh(t)2 + v(t)2\n\nand\n\nθ(t) = tan−1\n\n\u001a\n\nv(t)\nh(t)\n\n\u001b\n\n.\n\n(2)\n\n.\n\n(3)\n\nThe IF fHT (t) is given by\nfHT (t) =\n\n1\n1 dθ(t)\n=\n2π dt\n2πaHT (t)2\n\n\u0012\n\nh(t)\n\ndv(t)\ndh(t)\n− v(t)\ndt\ndt\n\n\u0013\n\nHowever, the IF obtained using this method is not necessarily physically meaningful unless the time series data h(t) is a monocomponent signal or a narrow-band\nsignal [Cohen (2005); Huang et al. (2005)]. For example, if h(t) is the sum of two\nsinusoidals, h(t) = a1 cos ω1 t + a2 cos ω2 t, where the amplitudes a1 and a2 are constants and ω1 and ω2 are positive constants, the IF varies with the time and may\nbecome negative although the signal is analytic. To explore the applicability of the\nHilbert transform, [Huang et al. (1998)] showed that the necessary conditions to\ndefine a meaningful IF are that the functions are symmetric with respect to the\nlocal zero mean and that they each have the same number of zero crossings and\nextrema. Thus they applied the EMD to the original data h(t) to decompose it into\nIMFs and a residual. A more detailed description is given in Sec. 2.2.\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\n4\n\nws-aada˙HT˙v3.3\n\nHirotaka Takahashi, Ken-ichi Oohara et al.\n\n2.2. Empirical mode decomposition and ensemble empirical mode\ndecomposition\nThe empirical mode decomposition (EMD) has an implicit assumption that, at any\ngiven time, the data may have many coexisting oscillatory modes of significantly\ndifferent frequencies, one superimposed on the other. For each of these modes, we\ndefine an intrinsic mode function (IMF) that satisfies the following conditions:\n(1) For all the IMFs of the data set, the number of extrema and the number of zero\ncrossings must either be equal or differ at most by one.\n(2) At any data point, the mean values of the upper and the lower envelopes defined\nby using the local maxima and the local minima, respectively, are zero.\nWith the above definition of an IMF, we can then decompose any function\nthrough the EMD, which, in a sense, is a sifting process using a series of high-pass\nfilters. The algorithm is summarized in the following outline and Fig. 1 shows a\nschematic example of EMD sifting:\n• h1 (t) = h(t)\n• for i = 1 to imax\n⊲ hi,1 (t) = hi (t)\n⊲ for k = 1 to kmax\n◦ Identify the local maxima and minima of hi,k (t) (Fig. 1a).\n◦ Ui,k (t) = the upper envelope joining the local maxima using\na cubic spline (Fig. 1b)\n◦ Li,k (t) = the lower envelope joining the local minima using\na cubic spline (Fig. 1b)\n◦ mi,k (t) = (Ui,k (t) + Li,k (t))/2 (Fig. 1b)\n◦ hi,k+1 (t) = hi,k (t) − mi,k (t) (Fig. 1c)\nExit from the loop k if a certain stoppage criterion, which will be\ndescribed below.\n⊲ IMFi (t) = ci (t) = hi,k (t) (Fig. 1d)\n⊲ hi+1 (t) = hi (t) − ci (t)\n• residual: r(t) = himax +1 (t)\nThe parameter imax specifies the number of IMFs to be extracted from h(t),\nwhich is usually based on the characteristics of the signal. The parameter kmax\nmust be sufficiently large, several thousand or more, since it determines when the\nmode decomposition stops even if the stoppage criterion has not been satisfied.\nThe EMD starts with identifying all the local extrema and then connecting all\nthe local maxima (minima) by a cubic spline to form the upper (lower) envelope. In\nAppendix A we review the details of the algorithm of extrema finder (XF 0, 1, and\n2) that we use to identify the local extrema. The upper and lower envelopes usually\nencompass all the data between them. Their mean is m1 (t). The difference between\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\nws-aada˙HT˙v3.3\n\nOn Investigating EMD Parameters to Search for Gravitational Waves\n\na\n\nc\n\n5\n\nb\n\nTime\n\nd\n\nTime\n\nTime\n\nTime\n\nFig. 1. Schematic example of EMD sifting.\n\nthe input h(t) and m1 (t) is the first proto-mode, h1 (t), that is, h1 (t) = h(t) −\nm1 (t). By construction, h1 is expected to satisfy the definition of an IMF. However,\nthat is usually not the case since changing a local zero from a rectangular to a\ncurvilinear coordinate system may introduce new extrema, and further adjustments\nare needed. Therefore, a repeat of the above procedure is necessary. The EMD serves\ntwo purposes:\n(1) To eliminate the background waves on which the IMF is riding;\n(2) To make the wave profiles more symmetric.\nThe process of the EMD has to be repeated as many times as is necessary to make\nthe extracted signal satisfy the definition of an IMF. In the iterating processes,\nh1 (t) is treated as a proto-IMF, which is then treated as data in the next iteration:\nh1 (t)−m11 (t) = h11 (t). After k iterations, the approximate local envelope symmetry\ncondition is satisfied, and h1k becomes the IMF c1 , that is, c1 (t) = h1k (t).\nThe approximate local envelope symmetry condition of the EMD is called the\nstoppage criterion. Several different types of stoppage criteria have been adopted.\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\n6\n\nws-aada˙HT˙v3.3\n\nHirotaka Takahashi, Ken-ichi Oohara et al.\n\nOne is a criterion determined by using the Cauchy type of convergence test, which\nwas used in [Huang et al. (1998)]:\n, N −1\nN\n−1\nX\nX\n2\n2\nm1k (tj )\nh1k (tj ) < ε,\n(4)\nj=0\n\nj=0\n\nwith a predetermined value ε. This stoppage criterion appears to be mathematically\nrigorous, but because how small is small enough begs an answer, it is difficult to\nimplement.\nThe second type of criterion, termed the S stoppage, was proposed in\n[Huang et al. 1999, 2003]. With this type of stoppage criterion, the EMD stops\nonly after the numbers of zero crossings and extrema are:\n(1) Equal or differ at most by one;\n(2) Stay the same for S consecutive times.\nExtensive tests by [Huang et al. (2003)] suggest that the optimal range for S should\nbe between 3 and 8, but the lower number is favored. Obviously, any selection is ad\nhoc, and a rigorous justification is needed. Thus in Sec. 3, we propose a policy to\njustify the stoppage criteria.\nThe first IMF should contain the finest scale or the shortest-period oscillation\nin the signal, which can be extracted from the data by h(t) − c1 (t) = r1 (t). The\nresidue, r1 , contains the longer-period oscillations. This residual is then treated as\na new data source and, in order to obtain the IMF of the next lowest frequency, it\nis subjected to the same process of the EMD as described above. The procedure is\nrepeatedly applied to all subsequent rn , and the result is rn−1 (t) − cn (t) = rn (t).\nThe decomposition process finally stops when the residue, rn , becomes a monotonic\nfunction or a function with only one extremum from which no more IMF can be\nextracted. Thus, the original data are decomposed into n IMFs and a residue, rn ,\nn\nX\nwhich can be either the adaptive local median or trend: h(t) =\ncl (t) + rn (t).\nl=1\n\nThe EMD can be applied to observed data in order to decompose it into signal and noise. In the original form of the EMD, however, mode mixing frequently\nappears. By definition, mode mixing occurs when either a single IMF consists of\nsignals of widely disparate scale, or when signals of a similar scale reside in different IMF components. It is a consequence of signal intermittency, which can\nnot only cause serious aliasing in the time-frequency distribution, but can also\nmake the individual IMFs devoid of physical meaning. To overcome this drawback,\n[Wu and Huang (2005)] proposed the ensemble EMD (EEMD), which defines the\ntrue IMF components as the mean of an ensemble of trials, each consisting of the\nsignal plus a white (Gaussian) noise of finite standard deviation (finite amplitude).\nThe EEMD algorithm contains the following steps:\n(1) Add a white (Gaussian) noise series to the targeted data;\n(2) Decompose the data with added white noise into IMFs;\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\nws-aada˙HT˙v3.3\n\nOn Investigating EMD Parameters to Search for Gravitational Waves\n\n7\n\n(3) Repeat steps (1) and (2) multiple times but with a different white (Gaussian)\nnoise series each time;\n(4) Obtain the ensemble means of the corresponding IMFs of the decompositions.\nThe standard deviation of the white (Gaussian) noise σe is not necessarily small.\nOn the other hand, the number of trials, Ne , must be large.\nWith the EMD, the signal usually appears in the IMF ci with a small value of\ni, typically i = 1, while it shifts to i = 3 for the EEMD. Since in the EEMD c1 (t)\nand c2 (t) contain only noise, we specify imax = 6 in this paper.\n3. Proposed Method\nWe consider the application of HHTs to the search for gravitational waves. There\nare several decisions that must first be made before conducting either the EMD\nor the EEMD. First we compare three kinds of extrema finders, XF 0, 1 and 2\nas algorithms to identify the local extrema, the details of which are described in\nAppendix A. We must also choose the stoppage criterion ε or S and, for the EEMD,\nthe standard deviation σe of the white (Gaussian) noise to be added to each trial.\nMoreover, we need to find the optimal value of some of these parameters. Thus, in\nthis section, we present a method to find the optimal values of the parameters.\n3.1. Setup for the simulation\nWe prepared analytic time series data by combining Gaussian noise with a sineGaussian signal, which is often used to model of gravitational wave bursts, as follows:\n\u0002\n\u0003\nh(t) = s(t) + n(t) = aSG exp −(t/τ )2 sin φ(t) + n(t),\n(5)\n\nwhere we let τ = 0.016 sec. For the frequency of the signal, we considered the two\ncases:\n(1) Constant frequency, where the phase φ(t) and frequency fSG are given by\nφ(t) = 6π t001\n\nand fSG =\n\n1 dφ\n= 300 Hz,\n2π dt\n\nwhere t001 ≡ 0.01t sec .\n(2) Time-dependent frequency, where φ(t) and fSG (t) are given by\n\u0001\n\u0001\nand fSG (t) = 300 + 48.0 t001 Hz.\nφ(t) = 2π 3.0 t001 + 0.24 t2001\n\n(6)\n\n(7)\n\nThe noise n(t) was generated by Gaussian random variates with mean zero and\nstandard deviation σ = 1.0. Figure 2 shows the signal s(t) of aSG = 3.12, the noise\nof σ = 1 and time series h(t) for aSG =\n3.12 (SNR = 20) and aSG = 1.56 (SNR =\nsX\n[s(tj )]2 /σ.\n10), where SNR is defined by SNR =\nj\n\nFor both the EMD and EEMD of the signal given by Eq. (5), we wish to determine the optimal extrema finder (XF 0, 1, or 2), the optimal value of σe and the\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\n8\n\nws-aada˙HT˙v3.3\n\nHirotaka Takahashi, Ken-ichi Oohara et al.\n\n4\n\nSignal\n\n2\n\nf = 300Hz\n\na (signal) = 3.12\n\n4\n\nSignal\n\n2\n\nf(t) = (300 + 48 t001)Hz\n\n0\n\n0\n\n-2\n\n-2\n\n-4\n4\n\n-4\n\nσ (noise) = 1\n\nNoise\n\n4\n\n2\n0\n\n0\n-2\n\n-4\n\n-4\na = 3.12, σ = 1\n\nSNR=20\n\n4\n\n2\n0\n\n0\n-2\n\n-4\n\na = 3.12, σ = 1\n\nSNR=10\n\na = 1.56, σ = 1\n\n-4\na = 1.56, σ = 1\n\nSNR=10\n\n4\n\n2\n\n2\n\n0\n\n0\n\n-2\n\n-2\n\n-4\n-0.1\n\nSNR=20\n\n2\n\n-2\n\n4\n\nσ (noise) = 1\n\nNoise\n\n2\n\n-2\n\n4\n\na (signal) = 3.12\n\n-4\n-0.05\n\n0\n\n0.05\n\n0.1\n\n-0.1\n\n-0.05\n\nt [sec]\n\n0\n\n0.05\n\n0.1\n\nt [sec]\n\nFig. 2. The signal and the Gaussian noise. The left and right figures are for the constant frequency\nfSG = 300Hz and the time-dependent frequency fSG = (300 + 48t001 )Hz, respectively. Two panel\nfrom the top in each figures show the signal of aSG = 3.12 and the noise of σ = 1, while examples\nof data for SNR=20 and 10 are shown below them.\n\noptimal stoppage criterion (ε or S). To examine the accuracy in calculation of the\nIF, for each of algorithms and parameters with SNR = 10 and 20, we calculated\nthe IF for 400 samples, each of which was generated by adding a Gaussian random\nvariate with a different seed to the 0.5 second data. The sampling frequency of the\ndata was 4096 Hz. A description of how we determined the accuracy of the IF is\ngiven in Sec.3.2.\nFor the EEMD, we chose the size of the ensemble to be Ne = 200. We tried\nother values of Ne , and we verified that the results change little even with Ne > 100\nbut that Ne ≈ 50 is too small.\n3.2. Method to examine the accuracy of the IF\nIn this subsection, we present a method to examine the accuracy of the IF, which\nwill determine the optimal values of the parameters.\nFirst, we performed the EMD and EEMD procedures for 400 samples of each\ndata set with the signal given by Eq. (5). We determined the optimal parameters\nfor different signal-to-noise ratios (SNR; SNR = 10 or 20), the algorithm of extrema\nfinder (XF 0, 1, or 2) to identify the local extrema, the stoppage criterion (S = 2, 4, 6\nand ε = 10−1 , 10−2 , 10−3 , 10−4 , 10−5 , 10−6 ) of the EMD, and the standard deviation\n(σe = 0.5, 1.0, 1.5, 2.0, 3.0, 5.0, 10.0, 20.0) of the white (Gaussian) noise to be added\nto each trial when we performed the EEMD.\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\nws-aada˙HT˙v3.3\n\nOn Investigating EMD Parameters to Search for Gravitational Waves\n\n3\n\n-4\n\nXF = 0, ε = 10 , σe = 2\nf = 300Hz\n\n2\n\n3\n\nsignal\n\n1\n\nsignal\n\n1\n\n3\n\n3\n\nIMF2\n\n2\n\n2\n\n1\n\n1\n\nIA\n\nIA\n\n-4\nXF = 0, ε = 10 , σe = 2\nf(t) = (300 + 48 t001)Hz\n\n2\n\n9\n\n3\n\nIMF3\n\n3\n\n2\n\n2\n\n1\n\n1\n\n3\n\nIMF2\n\nIMF3\n\n3\n\nIMF4\n\n2\n\nIMF4\n\n2\n\n1\n\n1\n-0.2\n\n-0.1\n\n0\n\n0.1\n\n0.2\n\n-0.2\n\n-0.1\n\n0\n\nt [sec]\n\n0.1\n\n0.2\n\nt [sec]\n\nFig. 3. The instantaneous amplitudes IA of each IMF obtained using (XF, σe , ε) = (0, 2.0, 10−4 )\nfor fSG = 300Hz (left) and fSG = (300 + 48t001 )Hz (right) with SNR=20. Note that only 30\nsamples are plotted.\n500\n\n500\nIMF3\n-4\nXF = 0, ε = 10 , σe = 2,\n\nIMF3\n-4\nXF = 0, ε = 10 , σe = 2, f(t) = (300 + 48 t001)Hz\n\nf = 300Hz\n400\n\nIF [Hz]\n\nIF [Hz]\n\n400\n\n300\n\n200\n\n300\n\n200\n\n-0.1\n\n-0.05\n\n0\n\n0.05\n\n0.1\n\n-0.1\n\n-0.05\n\n0\n\nt [sec]\n500\n\nIMF3\n-4\nXF = 0, ε = 10 , σe = 2, f(t) = (300 + 48 t001)Hz\n\nf = 300Hz\n400\n\nIF [Hz]\n\n400\n\nIF [Hz]\n\n0.1\n\n500\nIMF3\n-4\nXF = 0, ε = 10 , σe = 2,\n\n300\n\n200\n\n-0.02\n\n0.05\n\nt [sec]\n\n300\n\n200\n\n-0.015\n\n-0.01\n\n-0.005\n\n0\n\nt [sec]\n\n0.005\n\n0.01\n\n0.015\n\n0.02\n\n-0.02\n\n-0.015\n\n-0.01\n\n-0.005\n\n0\n\n0.005\n\n0.01\n\n0.015\n\n0.02\n\nt [sec]\n\nFig. 4. The instantaneous frequencies IF of IMF3 obtained using (XF, σe , ε) = (0, 2.0, 10−4 ) for\nfSG = 300Hz (left) and fSG = (300+48t001 )Hz (right) with SNR=20. The upper and lower figures\nshow the same IFs but for −0.1sec ≤ t ≤ 0.1sec and for −0.02sec ≤ t ≤ 0.02sec, respectively.\n\nFigure 3 shows the IA of each IMF for each data set using (SNR, XF, σe , ε) =\n(20, 0, 2.0, 10−4). Note that only 30 samples are plotted in this and the following\nfigures since the figures are not legible when all 400 samples are plotted. From\nFig. 3, it is apparent that IMF3 has a peak for this parameter set. However, which\nof IMFs catches the signal depends on the SNR and the parameters used in the\nEMD procedure. Figure 4 shows the IFs of IMF3 for these data. Each of the lower\nfigures shows a magnification of the upper one around the signal injection point\n(t = 0 sec). These figures indicate that the IF displays the characteristics of the\ninjected signal when the IA dominates over the noise level, while the IF is physically\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\n10\n\nws-aada˙HT˙v3.3\n\nHirotaka Takahashi, Ken-ichi Oohara et al.\n\nmeaningless during the other period.\nWe make the linear and quadratic regression for the instantaneous frequency\nfIMF (t) of each IMF using the least squares method with weights A2 (t), where A(t)\nis the IA of the IMF;\n\u0001\n(1) The linear regression: ffit (t) = a1 + b1 t001 Hz,\n\u0001\n(2) The quadratic regression: ffit (t) = a2 + b2 t001 + c2 t2001 Hz,\n\nwith fitting range −0.015 sec ≤ t ≤ 0.015 sec or −0.01 sec ≤ t ≤ 0.01 sec, that is,\n−1.5 ≤ t001 ≤ 1.5 or −1.0 ≤ t001 ≤ 1.0, respectively.\nFor indices of the accuracy of fitting, we calculate the following quantities;\n• The relative error of fitting against the exact frequency:\n\u0002\n\u0003\nWTSS ffit (t) − fSG (t)\n\u0002\n\u0003\n,\nρ = 100 ×\nWTSS fSG (t)\n\nwhere the weighted total sum of squares (WTSS) is defined by\n\u0002\n\u0003 X 2\nA (tj )f 2 (tj ).\nWTSS f (t) =\n\n(8)\n\n(9)\n\nj\n\n• The deviation of the IF for each IMF fIMF around the exact frequency:\n\u0002\n\u0003\nWTSS fIMF (t) − fSG (t)\n\u0002\n\u0003\nδ = 100 ×\n.\nWTSS fSG (t)\n\n(10)\n\n• The coefficient of determination:\n\n\u0002\n\u0003\nWTSS ffit (t) − fIMF (t)\n\u0002\n\u0003\nR =1−\n.\nWTSS fIMF (t)\n2\n\n(11)\n\nWhich IMF includes the signal depends on the parameters. IMF 1 always includes the signal for the EMD, while IMF 2, 3 or 4 includes the signal for the\nEEMD. Thus, we consider the IMF to include the signal if the relative error ρ is\nthe smallest for each parameter set.\nThe deviation δ indicates how widely fIMF fluctuates around the exact frequency.\nEven if the error of fitting ρ is small, the procedure is considered unstable when δ\nis large.\nThe coefficient of determination R2 is a measure of the goodness of fitting. In\ngeneral, R2 = 1 if the regression line perfectly fits the data and R2 = 0 indicates no\nrelationship between fIMF and t. That is, for the signal of time-dependent frequency,\nan R2 near 1 indicates better fit. For the signal of constant frequency, on the other\nhand, R2 approaches 0 as the fitting becomes better.\n4. Results\nIn this section, we present the results of the simulation based on Sec.3. We calculate\nthe IF by means of the HHT for 400 samples of each parameter set with each signal,\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\nws-aada˙HT˙v3.3\n\nOn Investigating EMD Parameters to Search for Gravitational Waves\n\n11\n\nTable 1. The comparison of the EMD and the EEMD. The coefficients of the linear regression (a1 ,\nb1 ) and the quadratic regression (a2 , b2 , c2 ), and the quantities ρ, δ and R2 defined by Eqs.(8)∼(11)\nfor signals of the constant frequency and the time-dependent frequency with SNR=20 and 10 are\nlisted. The results of the linear regression are shown in rows in which no value is listed in columns\nheaded ‘c’.\nFitting Range: −1.5 ≤ t001 ≤ 1.5;\nXF=0, S = 4, σe = 2.0 (for EEMD)\nConstant Frequency: fSG = 300Hz; a = 300, b = 0, c = 0\na\nb\nc\nρ\nδ\nSNR=20\nEMD\n300.4± 2.2\n0.2± 5.3\n1.0±0.8\n6.4±2.1\nEMD\n299.1± 3.5\n0.3± 7.8\n3.4±12.2\n1.7±1.3\n6.4±2.1\nEEMD\n299.6± 1.3\n−0.2± 2.4\n0.6±0.4\n2.9±0.6\nEEMD\n299.3± 2.0\n−0.1± 2.4\n0.7± 4.0\n0.9±0.4\n2.9±0.6\nSNR=10\nEMD\n307.0±18.4\n0.6±25.4\n6.1±4.2\n16.8±6.2\nEMD\n291.9±23.2\n2.3±33.8\n31.2±39.7\n9.2±5.5\n16.5±5.9\nEEMD\n301.5± 3.2\n−0.5± 5.5\n1.5±0.8\n5.3±1.3\nEEMD\n300.0± 4.5\n−0.1± 6.1\n3.6± 9.3\n2.1±1.1\n5.3±1.3\nTime-Dependent Frequency: fSG = (300 + 48t001 )Hz; a = 300, b = 48, c = 0\na\nb\nc\nρ\nδ\nSNR=20\nEMD\n301.1± 4.5\n45.2± 9.2\n1.4±1.3\n7.4±2.7\nEMD\n297.4± 7.3\n41.6±17.0\n9.3±17.4\n2.5±2.2\n7.4±2.6\nEEMD\n299.1± 1.2\n46.7± 2.7\n0.7±0.4\n3.2±0.7\nEEMD\n298.7± 2.2\n46.9± 2.7\n0.9± 4.4\n1.1±0.5\n3.2±0.7\nSNR=10\nEMD\n309.8±23.3\n27.9±32.1\n7.4±5.4\n17.7±6.5\nEMD\n290.9±26.0\n18.9±41.9\n34.9±44.6\n10.8±6.4\n17.4±6.2\nEEMD\n301.7± 3.5\n41.3± 7.4\n2.1±1.4\n5.9±1.8\nEEMD\n299.7± 4.9\n41.2± 8.2\n4.6±11.3\n2.8±1.7\n5.9±1.8\n\nR2\n0.02±0.03\n0.08±0.08\n0.04±0.05\n0.11±0.10\n0.09±0.12\n0.27±0.22\n0.06±0.06\n0.14±0.13\nR2\n0.65±0.22\n0.69±0.16\n0.92±0.04\n0.93±0.04\n0.24±0.22\n0.45±0.20\n0.72±0.17\n0.76±0.14\n\nmake the linear and quadratic regression and compare calculated coefficients with\nthe exact values, which are a1 = a2 = 300.0 and b1 = b2 = c2 = 0 for the signal\nof the constant frequency given by Eq.(6) and a1 = a2 = 300.0, b1 = b2 = 48.0\nand c2 = 0 for the signal of the time-dependent frequency given by Eq.(7). Here\nwe use the XF 0, 1 and 2 for the extrema finder and choose S = 2, 4, 6 or ε =\n10−1 , 10−2 , 10−3 , 10−4 , 10−5 , 10−6 for the stoppage criteria. For the EEMD, we\nalso used the standard deviation of the added white (Gaussian) noise of σe =\n0.5, 1.0, 1.5, 2.0, 3.0, 5.0, 10.0, 20.0.\nIn the following tables, we show the mean values and the standard deviations\nfor 400 samples of the coefficients of the fitting a, b and c, the relative error ρ, the\ndeviation of the IF δ, and the coefficient of determination R2 .\nFirst, to compare the EMD and the EEMD, the typical results of the linear\nand quadratic regression for signals of SNR=20 and 10 with the constant frequency\ndefined by Eq.(6) and the time-dependent frequency defined by Eq.(7) are shown\nin Table 1. The results of the linear regression are listed if the column headed c is\nblank, while the results of the quadratic regression are listed otherwise.\nThe mean values of coefficients for SNR=20 using the EMD acceptably agree\nwith the exact values, but the standard deviations of the coefficient and the value\nof δ tend to be large. It means that the IFs fluctuate widely and sometimes an\ninaccurate estimate of the IF will be given. A typical example is illustrated in\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\n12\n\nws-aada˙HT˙v3.3\n\nHirotaka Takahashi, Ken-ichi Oohara et al.\nXF = 0, S = 4, σe = 2\n\nXF = 0, S = 4, σe = 2\n\nSNR=20\n\nf = 300Hz\n\n400\n\nIF [Hz]\n\nIF [Hz]\n\n400\n\n300\n\nexact\nEMD\nLF: EMD\nEEMD\nLF: EEMD\n\n200\n\nSNR=20\n\nf(t) = (300 + 48 t001)Hz\n\n300\n\nexact\nEMD\nLF: EMD\nEEMD\nLF: EEMD\n\n200\n\nSNR=10\n\nSNR=10\n400\n\nIF [Hz]\n\nIF [Hz]\n\n400\n\n300\n\n200\n\n-0.015\n\n300\n\n200\n\n-0.01\n\n-0.005\n\n0\n\nt [sec]\n\n0.005\n\n0.01\n\n0.015\n\n-0.015\n\n-0.01\n\n-0.005\n\n0\n\n0.005\n\n0.01\n\n0.015\n\nt [sec]\n\nFig. 5. A sample of the instantaneous frequency IF obtained with the EMD and EEMD using (XF,\nσe , S) = (0, 2.0, 4) for fSG = 300Hz (left) and fSG = (300 + 48t001 )Hz (right) with SNR=20 (top)\nand 10 (bottom). The red (thin) and blue (thick) curves display the IF of IMF1 with EMD and the\nIF of IMF3 with EEMD, respectively. The dashed lines show the results of the linear regression.\n\nFig. 5. The IF obtained with the EMD fluctuate more widely than that with the\nEEMD, while the dashed lines, which represent the linear regression, match very\nwell with the frequency of injected signals, especially for SNR=20. The accuracy of\nthe EMD is inadequate for SNR=10. We found that the accuracy with the EMD\nis not improved even with other extrema finder or other stoppage criterion. On the\nother hand, the EEMD gives better results with smaller standard deviations for\nsignals for SNR=20. Even for SNR=10, the results are similar to or better than\nthose of the EMD for SNR=20. Thus, hereinafter we consider only the EEMD.\nSecondly, we compare the algorithm of extrema finder XF 0, 1 and 2. Some of\nthe fitting coefficients for the signal calculating using the EEMD with σe = 2.0 and\nthe stoppage criterion with S = 4 are listed in Table 2. There is little significant\ndifference among XF 0, 1 and 2 for simple signals as we considered here. As shown\nin Fig. 6, the difference between XF 0 and 1 is very small in particular. We found,\nhowever, that XF 2 sometimes becomes unstable with a small SNR and a strict\nstoppage criterion, that is, a small value of ε for the Cauchy type of convergence or\na large value of S for the S stoppage. Although we show the results for the linear\nregression and the fitting range of −1.5 ≤ t001 ≤ 1.5 with a specific parameter set\nin Table 2, it is generally the case with the quadratic regression, with fitting range\nof −1.0 ≤ t001 ≤ 1.0 or with other parameter sets.\nNext, let us consider effects of σe , the standard deviation of the white Gaussian\nnoise to be added to make ensembles for the EEMD. The coefficients of the linear\nregression for the signal of the constant frequency and the time-dependent frequency\ncalculating using the EEMD with XF=0, the S = 4 stoppage criterion and σe = 0.5\nthrough 20.0 are listed in Table 3. Those for the signal of the time-dependent\nfrequency are plotted in Fig. 7. Although the dependence of the accuracy on σe is\nrather weak, the best value of σe is near 3.0 for SNR=20, while it is 1.5 for SNR=10.\nEach of them corresponds to the amplitude aSG of the signal defined by Eq. (5),\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\nws-aada˙HT˙v3.3\n\nOn Investigating EMD Parameters to Search for Gravitational Waves\n\n13\n\nTable 2. The comparison of the extrema finder XF 0, 1 and 2. The coefficients of the linear\nregression (a1 , b1 ), and the quantities ρ, δ and R2 are listed.\nEEMD;\nXF = 0, S = 4;\nFitting Range: −1.5 ≤ t001 ≤ 1.5\nConstant Frequency: fSG = 300Hz; a = 300, b = 0, c = 0\nXF\na1\nb1\nρ\nδ\nSNR=20\n0\n299.6±1.3\n−0.2± 2.4\n0.6±0.4\n2.9±0.6\n1\n299.7±1.3\n−0.2± 2.5\n0.6±0.4\n2.9±0.6\n2\n300.4±1.3\n−0.0± 2.6\n0.7±0.4\n3.3±0.7\nSNR=20\n0\n301.5±3.2\n−0.5± 5.5\n1.5±0.8\n5.3±1.3\n1\n302.0±3.3\n−0.3± 5.7\n1.6±0.9\n5.4±1.4\n2\n305.9±4.5\n1.0± 8.7\n2.6±1.7\n7.0±2.2\n\nR2\n0.04±0.05\n0.04±0.05\n0.03±0.04\n0.06±0.06\n0.06±0.07\n0.06±0.08\n\nTime-Dependent Frequency: fSG = (300 + 48t001 )Hz; a = 300, b = 48, c = 0\nXF\na1\nb1\nρ\nδ\nR2\nSNR=20\n0\n299.1±1.2\n46.7± 2.7\n0.7±0.4\n3.2±0.7\n0.92±0.04\n1\n299.3±1.2\n46.8± 2.7\n0.7±0.4\n3.2±0.7\n0.92±0.04\n2\n300.4±1.3\n46.5± 3.1\n0.7±0.5\n3.5±0.9\n0.89±0.06\nSNR=10\n0\n301.7±3.5\n41.3± 7.4\n2.1±1.4\n5.9±1.8\n0.72±0.17\n1\n302.3±3.6\n41.3± 7.5\n2.1±1.5\n6.1±1.9\n0.71±0.17\n2\n307.9±6.1\n37.2±10.6\n3.5±2.3\n7.9±2.9\n0.57±0.23\n\nS= 4 , σe = 2\n\nSNR=20\n400\n\nf = 300Hz\n\nIF [Hz]\n\nIF [Hz]\n\n400\n\n300\n\nexact\nXF=0\nXF=1\nXF=2\n\n200\n\nS= 4 , σe = 2\nf(t) = (300 + 48 t001)Hz\n\nSNR=20\n\n300\n\nexact\nXF=0\nXF=1\nXF=2\n\n200\n\nSNR=10\n\nSNR=10\n400\n\nIF [Hz]\n\nIF [Hz]\n\n400\n\n300\n\n200\n\n-0.015\n\n300\n\n200\n\n-0.01\n\n-0.005\n\n0\n\nt [sec]\n\n0.005\n\n0.01\n\n0.015\n\n-0.015\n\n-0.01\n\n-0.005\n\n0\n\n0.005\n\n0.01\n\n0.015\n\nt [sec]\n\nFig. 6. A sample of the instantaneous frequency IF obtained with the EEMD using XF 0, 1 and 2\nfor fSG = 300Hz (left) and fSG = (300 + 48t001 )Hz (right) with SNR=20 (top) and 10 (bottom).\n\nthat is, aSG = 3.12 and 1.56 for SNR=20 and 10, respectively. It is the case with\nthe quadratic regression and/or with the fitting range of −0.1 ≤ t001 ≤ 0.1, too.\nThis result implies that we should perform the EEMD with some different values\nof σe to search and analyze a signal whose amplitude is not known in advance.\nFinally, we will compare stoppage criteria. The coefficients for the same signal as\nTable 1 calculated with XF 0, σe = 2.0 adopting the S stoppage criteria of S = 2, 4\nand 6, and the Cauchy type of convergence test with ε = 10−1 ∼ 10−6 are shown in\nTable 4. Inadequate accuracies are obtained with ε ≥ 10−2 . The accuracy sometimes\nget worse with more rigid criterion, or with small value of ε, especially for SNR=10.\nIt is because mode mixing occurs to a certain extent as shown in Fig. 8, which plots\nthe IAs of IMF3 and IMF4 calculated with ε = 10−4 , 10−5 and 10−6 , and S = 4 and\n6 for the SNR=10 signal of the time-dependent frequency. The fact that the IAs\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\n14\n\nws-aada˙HT˙v3.3\n\nHirotaka Takahashi, Ken-ichi Oohara et al.\n\nTable 3. The comparison of σe .\nEEMD;\nXF = 0, S = 4;\nFitting Range: −1.5 ≤ t001 ≤ 1.5\nConstant Frequency: fSG = 300Hz; a = 300, b = 0, c = 0\nσe\na1\nb1\nρ\nδ\nSNR=20\n0.5\n300.7± 1.7\n0.2± 4.4\n0.9±0.6\n5.4±1.6\n1.0\n299.5± 3.0\n−0.0± 3.7\n1.1±0.6\n4.8±1.6\n1.5\n299.2± 1.4\n−0.1± 2.6\n0.7±0.4\n3.1±0.8\n2.0\n299.6± 1.3\n−0.2± 2.4\n0.6±0.4\n2.9±0.6\n3.0\n300.1± 1.3\n−0.3± 2.5\n0.6±0.3\n2.8±0.6\n5.0\n300.9± 1.3\n−0.4± 2.6\n0.7±0.4\n3.1±0.7\n10.0\n302.6± 1.7\n0.3± 3.3\n1.1±0.6\n4.6±1.0\n20.0\n309.6± 4.1\n8.7± 7.7\n4.0±2.0\n10.2±2.3\nSNR=10\n0.5\n294.7± 8.1\n−0.4± 9.1\n3.2±2.0\n7.6±3.0\n1.0\n299.1± 3.1\n−0.5± 5.3\n1.4±0.8\n5.2±1.2\n1.5\n300.5± 3.1\n−0.5± 5.3\n1.4±0.8\n5.2±1.3\n2.0\n301.5± 3.2\n−0.5± 5.5\n1.5±0.8\n5.3±1.3\n3.0\n302.8± 3.5\n−0.1± 6.1\n1.7±1.0\n5.7±1.6\n5.0\n304.8± 4.3\n1.1± 8.1\n2.4±1.5\n6.7±2.1\n10.0\n311.4± 6.8\n7.9±11.7\n4.9±2.6\n11.2±3.0\nTime-Dependent\nσe\nSNR=20\n0.5\n1.0\n1.5\n2.0\n3.0\n5.0\n10.0\n20.0\nSNR=10\n0.5\n1.0\n1.5\n2.0\n3.0\n5.0\n10.0\n\n304\n\nf = (300 + 48 t001) Hz; XF = 0, S = 4\n\nFrequency: fSG\na1\n301.5± 2.8\n300.1± 6.2\n298.2± 1.6\n299.1± 1.2\n299.9± 1.2\n300.9± 1.3\n302.0± 1.7\n303.9± 3.2\n295.3±19.2\n298.2± 3.3\n300.4± 3.3\n301.7± 3.5\n303.2± 3.7\n304.8± 4.3\n307.8± 6.6\n\nR2\n0.02±0.03\n0.03±0.05\n0.04±0.06\n0.04±0.05\n0.04±0.05\n0.04±0.05\n0.03±0.04\n0.07±0.07\n0.06±0.08\n0.06±0.07\n0.05±0.06\n0.06±0.06\n0.06±0.07\n0.06±0.08\n0.08±0.09\n\n= (300 + 48t001 )Hz; a = 300, b = 48, c = 0\nb1\nρ\nδ\nR2\n44.9± 7.0\n1.2±1.1\n6.3±2.3\n0.67±0.19\n43.5± 6.0\n1.7±1.0\n5.9±1.9\n0.67±0.20\n46.3± 3.0\n1.0±0.5\n3.6±0.8\n0.90±0.06\n46.7± 2.7\n0.7±0.4\n3.2±0.7\n0.92±0.04\n46.6± 2.7\n0.7±0.4\n3.0±0.7\n0.92±0.04\n46.6± 2.7\n0.7±0.4\n3.1±0.7\n0.92±0.04\n46.7± 3.2\n1.0±0.6\n4.0±0.9\n0.88±0.06\n46.2± 5.0\n1.6±1.0\n6.9±1.4\n0.73±0.11\n31.0±16.0\n5.5±3.5\n10.6±4.5\n0.45±0.28\n41.3± 7.2\n2.1±1.4\n6.1±1.7\n0.73±0.15\n41.4± 7.2\n2.0±1.4\n5.9±1.7\n0.73±0.16\n41.3± 7.4\n2.1±1.4\n5.9±1.8\n0.72±0.17\n41.2± 7.6\n2.3±1.5\n6.2±1.9\n0.72±0.17\n41.5± 8.2\n2.5±1.7\n6.7±2.2\n0.70±0.18\n40.3± 9.9\n3.3±2.3\n8.9±2.6\n0.59±0.20\n\nSNR = 20\n56\n\nρ\n\nf = (300 + 48 t001)Hz; XF = 0, S = 4\n\nδ\n\nSNR = 20\n\n2\n\nR\n\n6\n2\n302\n\n1\n\n52\n\n2\n\n0.9\n\nR\n\n4\n\nb\na\n\n300\n\n48 b\n\n0.8\n1\n\na\n\nδ\nρ\n\n298\n\n44\n\n296\n\n40\n\n2\n\n0.7\n0.6\n\n0\nSNR = 10\n\nSNR = 10\n\n310\n\n305\n\n60\n\n5\n\n56\n\n4\n\n300\n\n48 b\n\nR\n\n8\n\n0.9\n0.8\n\n6\n\n52\na\n\n0.7\n\n3\n4\n\n44\n295\n\n0.5\n2\n\n1\n\n0.4\n\n36\n\n0\n0.5\n\n1.0\n\n1.5\n\n2\nσe\n\n3\n\n5\n\n10\n\n0.6\n\n2\n\n40\n\n290\n\n2\n\nδ\n\n0\n0.5\n\n1.0\n\n1.5\n\n2\nσe\n\n3\n\n5\n\n10\n\nFig. 7. The coefficients a1 and b1 (left), and the relative error ρ, the deviation of the IF δ and the\ncoefficient of determination R2 (right) of the linear regression for the signal of the time-dependent\nfrequency for various σe . The dots and the error bars indicate the mean value and the standard\ndeviation of 400 samples.\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\nws-aada˙HT˙v3.3\n\nOn Investigating EMD Parameters to Search for Gravitational Waves\n\n15\n\nTable 4. The comparison of stoppage criteria.\nEEMD;\nXF = 0, S = 4;\nFitting Range: −1.5 ≤ t001 ≤ 1.5\nConstant Frequency: fSG = 300Hz; a = 300, b = 0, c = 0\nS/ε\na1\nb1\nρ\nδ\nSNR=20\nS=2\n298.8± 1.5\n−0.1± 2.6\n0.8±0.4\n3.1±0.8\n4\n299.6± 1.3\n−0.2± 2.4\n0.6±0.4\n2.9±0.6\n6\n300.0± 1.3\n−0.2± 2.5\n0.6±0.3\n3.0±0.6\n−1\nε = 10\n302.1± 1.9\n−0.0± 4.0\n1.1±0.7\n3.7±1.2\n10−2\n299.9± 4.3\n−0.1± 3.6\n1.5±0.7\n4.7±1.2\n−3\n10\n299.0± 1.4\n−0.1± 2.5\n0.7±0.4\n2.9±0.7\n−4\n10\n300.2± 1.2\n−0.1± 2.5\n0.6±0.3\n3.1±0.7\n10−5\n301.3± 2.0\n−0.3± 3.1\n0.9±0.5\n4.3±1.0\n10−6\n299.5± 1.3\n−0.3± 2.4\n0.7±0.4\n2.7±0.7\nSNR=10\nS=2\n298.5± 3.2\n−0.8± 5.7\n1.5±0.9\n5.1±1.3\n4\n301.5± 3.2\n−0.5± 5.5\n1.5±0.8\n5.3±1.3\n6\n303.4± 3.6\n−0.3± 6.4\n1.9±1.1\n5.9±1.6\n−1\nε = 10\n311.8±16.7\n2.2±16.6\n6.8±3.9\n12.1±5.3\n−2\n10\n292.2± 5.4\n−1.1± 8.6\n3.3±1.9\n6.8±2.2\n10−3\n299.4± 3.1\n−0.7± 5.4\n1.4±0.8\n4.9±1.2\n10−4\n305.2± 4.1\n0.4± 7.9\n2.4±1.4\n6.6±2.0\n10−5\n295.1± 4.6\n−0.9± 6.0\n2.3±1.2\n5.4±1.6\n10−6\n301.4± 2.8\n−0.5± 5.2\n1.4±0.8\n4.7±1.1\n\nR2\n0.04±0.06\n0.04±0.05\n0.04±0.05\n0.04±0.05\n0.03±0.04\n0.05±0.06\n0.03±0.04\n0.02±0.03\n0.05±0.07\n0.06±0.07\n0.06±0.06\n0.06±0.06\n0.10±0.10\n0.08±0.10\n0.06±0.07\n0.06±0.07\n0.07±0.09\n0.06±0.07\n\nTime-Dependent Frequency: fSG = (300 + 48t001 )Hz; a = 300, b = 48, c = 0\nS/ε\na1\nb1\nρ\nδ\nR2\nSNR=20\nS=2\n297.6± 1.6\n46.1± 3.1\n1.1±0.5\n3.5±0.8\n0.91±0.05\n4\n299.1± 1.2\n46.7± 2.7\n0.7±0.4\n3.2±0.7\n0.92±0.04\n6\n299.7± 1.2\n46.7± 2.7\n0.7±0.4\n3.2±0.7\n0.91±0.04\nε = 10−1\n303.1± 2.4\n44.0± 5.5\n1.4±1.0\n4.5±1.9\n0.80±0.16\n10−2\n300.4± 8.0\n42.2± 6.3\n2.2±1.2\n5.4±2.0\n0.74±0.18\n10−3\n298.0± 1.6\n46.3± 2.9\n1.0±0.5\n3.3±0.8\n0.92±0.05\n−4\n10\n300.1± 1.3\n46.6± 2.9\n0.7±0.4\n3.3±0.8\n0.90±0.05\n−5\n10\n302.6± 3.3\n43.5± 4.7\n1.4±0.8\n4.9±1.6\n0.74±0.14\n−6\n10\n298.7± 2.0\n45.4± 2.9\n1.0±0.5\n3.4±0.9\n0.90±0.07\nSNR=10\nS=2\n297.4± 3.5\n41.2± 7.2\n2.2±1.5\n5.8±1.8\n0.75±0.15\n4\n301.7± 3.5\n41.3± 7.4\n2.1±1.4\n5.9±1.8\n0.72±0.17\n6\n304.3± 4.2\n40.1± 8.3\n2.5±1.7\n6.6±2.2\n0.67±0.19\nε = 10−1\n314.2±22.7\n25.8±18.8\n8.2±4.6\n13.4±5.5\n0.33±0.26\n10−2\n288.5±10.4\n34.1±12.1\n5.1±3.2\n8.7±3.7\n0.58±0.26\n10−3\n298.7± 3.2\n41.6± 6.7\n2.0±1.3\n5.6±1.6\n0.76±0.14\n−4\n10\n306.9± 5.9\n37.9±10.2\n3.2±2.2\n7.4±2.7\n0.60±0.22\n−5\n10\n293.5±10.7\n34.7± 9.9\n4.2±2.5\n7.6±3.0\n0.61±0.23\n10−6\n301.8± 3.3\n41.3± 6.8\n2.0±1.4\n5.5±1.7\n0.75±0.15\n\nof IMF4 for ε = 10−5 and 10−6 are comparable to those of IMF3 indicates mode\nmixing. The S stoppage criteria of S = 4 and 6 is likely to be stable.\nNote that b1 (the first derivative of frequency) in the case of time-dependent\nfrequency for the SNR=10 is always estimated smaller because of the noise effects\nand lower SNR.\nAlthoug we presented the results of the linear regression with fitting range\n−0.015 sec ≤ t ≤ 0.015 sec for the most part, they are the same in all essentials as\nthose of the quadratic regression and/or with fitting range −0.01 sec ≤ t ≤ 0.01 sec.\nFor further tables of all results, refer to [Takahashi et al. (2013)].\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\n16\n\nws-aada˙HT˙v3.3\n\nHirotaka Takahashi, Ken-ichi Oohara et al.\n\n2\nIMF3\n\nInstantaneous Amplitudes\n\nf = (300 + 48 t001) Hz\nXF = 0, σ = 2.0\n\n1.5\n\n1\n\n0.5\n\n0\nsignal\n-4\nε = 10-5\nε = 10\n-6\nε = 10\nS=4\nS=6\n\nIMF4\n1.5\n\n1\n\n0.5\n\n0\n-0.04\n\n-0.02\n\n0\nt (sec)\n\n0.02\n\n0.04\n\nFig. 8. Instantaneous amplitudes of IMF3 and IMF4 calculated with various stoppage criteria for\nthe SNR=10 signal of the time-dependent frequency.\n\nTable 5. Relative CPU time required by calculation of EEMD with each parameter set. Values are\nshown in units of the CPU time for XF 0 and S = 4.\nStoppage Criterion S = 2 S = 4 S = 6 ε = 10−3 ε = 10−4 ε = 10−5 ε = 10−6\nXF 0\n0.6\n1.0\n1.4\n0.8\n2.4\n8.6\n35.0\nXF 1\n1.6\n2.9\n4.1\n2.1\n6.9\n23.4\n76.7\nXF 2\n5.5\n10.6\n14.7\n4.4\n12.8\n39.4\n118.6\n\n5. Summary\nWe investigated the possibility of the application of the HHT to the search for\ngravitational waves. Since EMD and EEMD are an empirical method, there are\nsome parameters to be chosen. In this paper, we proposed and demonstrated a\nmethod to look for optimal values of these parameters.\nWe found that the most important parameter is the stoppage criterion ε or S for\nEMD and EEMD. The strict criterion is generally adequate. However, it sometimes\ncauses mode mixing and always requires long CPU time, as shown in Table 5.\nSelection of extrema finder XF affects required CPU time considerably, while it\ndoes not affect calculated IFs so much. CPU time with XF 1 is twice or more as\nlong as that with XF 0, and XF 2 requires still longer CPU time.\nThe dependence of the accuracy of the IFs on σe , the magnitude of the Gaussian\nnoise to be added to each trial of the EEMD, is weak. The best value of σe is\ndetermined by the amplitude of the signal rather than by the noise level.\nAs a result, EEMD with the following optimal parameter ranges may be promising: extrema finder of XF 0; the stoppage criterion of S = 2–4, or ε = 10−4 ; the\nstandard deviation of the Gaussian noise σe = 1.0–3.0.\nWe used a time series data that combined Gaussian noise with a sine-Gaussian\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\nws-aada˙HT˙v3.3\n\nOn Investigating EMD Parameters to Search for Gravitational Waves\n\n17\n\nsignal, but the time series data from the detectors of gravitational waves have many\nnon-Gaussian and nonstationary noise. Therefore, the parameter ranges discussed\nin this paper cannot be used in a straightforward manner in the search for real\ngravitational waves. However, using the ‘playground data’ method (which usually\nuses 10% of the real data to fix the search parameters and to estimate the noise\nbackground), we can determine the optimal values of these parameters using our\nproposed method.\nBased on this research, we will investigate the possibility of constructing an alert system using the HHT for the search for gravitational waves\n[Kaneyama et al. (2013)]. This alert system will be discussed elsewhere.\nAcknowledgments\nThe authors would like to thank Alexander Stroeer for many discussions about topics related to the research presented here. This work was supported in part by JSPS\nKAKENHI, a Grant-in-Aid for Scientific Research (No. 23540293; K. Oohara and\nH. Takahashi) and a Grant-in-Aid for Young Scientists (No. 23740207; H. Takahashi). This work was also supported in part by a Grant-in-Aid for Scientific Research on Innovative Areas (No. 24103005; K. Oohara and H. Takahashi) from the\nMinistry of Education, Culture, Sports, Science and Technology of Japan.\nAppendix A. Algorithms to identify the local extrema\nIn EMD sifting, we need to identify local extrema. Here we review the details of the\nalgorithms that we used.\nWe assume here that the time series data h(t) is produced by sampling a continuous signal at a discrete time, t = tj for j = 0, 1, · · · , N − 1. Thus, the value of\nh(t) is given by hj = h(tj ).\nA.1. Extrema finder 0 (XF 0) : EMD classic\nWe extract local maxima using the following simple algorithm:\n(1) If hj−1 < hj and hj > hj+1 , then hj is a local maximum at t = tj .\n(2) If hj−1 < hj = hj+1 and hj+1 > hj+2 , we take the point t = (tj + tj+1 )/2,\nh = hj + (hj − hj−1 )/2 as a local maximum.\nThe regions where hj−1 = hj = hj+1 are ignored in searching local maxima. Then\nwe calculate upper envelope by interpolating the extracted local maxima (b\ntp , b\nhp ),\n1 ≤ p ≤ NU , where NU is the number of the local maxima. In general, however,\nt0 , b\nh0 ), where b\nt0 = t0\nt0 < b\nt1 and tN > b\ntNU . Thus we add an interpolation point (b\nand b\nh0 is calculated by a quadratic interpolation using (b\ntk , b\nhk ), k = 1, 2 and 3.\nAn interpolation point (b\ntNU +1 = tN , b\nhNU +1 ) is also added similarly. Then upper\nenvelope U (t) is calculated by a cubic spline interpolation with (b\ntp , b\nhp ), 0 ≤ p ≤\nNU + 1.\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\n18\n\nws-aada˙HT˙v3.3\n\nHirotaka Takahashi, Ken-ichi Oohara et al.\n\nA similar procedure is followed to extract the local minima and calculate lower\nenvelope L(t).\nA.2. Extrema finder 1 (XF 1) : EMD TRUMAX1\nWhen we calculate upper and lower envelope, U (t) and L(t) as described above\n(EMD Classic), the time series data h(t) sometimes crosses U (t) or L(t). That is,\nthere may be points where h(tj ) > U (tj ) or h(tj ) < L(tj ). This is because we did\nnot identify the local extrema exactly. Thus, we make the following revision: We\nextract candidates of local maxima (b\ntp , b\nhp ) and minima (e\ntq , e\nhq ) using the similar\nalgorithm to EMD Classic, but the step (2) in EMD Classic is modified as\n\n(2)’ If hj−1 < hj = hj+1 and hj+1 > hj+2 , we take the point t = tj , h = hj as a\ncandidate of a local maximum.\n\nSince each point of local extrema b\ntp or e\ntq is equal to one of the sample, or observed,\npoints tj of the time series data h(t), we calculate a cubic spline function of h(t)\nwith 3 to 7 interpolation points near t = tj . It is a piecewise cubic polynomial as\nH(t) = ak ∆t3 + bk ∆t2 + ck ∆t + b\nhk\n\nfor b\ntk−1 ≤ t ≤ b\ntk ,\n\n(A.1)\n\nwhere max(j − 3, 0) ≤ k ≤ min(j + 3, N − 1) and ∆t = t − b\ntk . Then we take the\npoint where H ′ (t) = 0 and H ′′ (t) < 0 as ‘true’ local maximum near b\ntp . Note that\n′\nmeans the derivative with respect to t. Such a point is certainly found in the\nregion between tj−1 and tj (= tp ) or between tj and tj+1 . Similarly the point where\nH ′ (t) = 0 and H ′′ (t) > 0 is taken as ‘true’ local minimum near e\ntq .\nConnecting these ‘true’ local extrema by a cubic spline, we obtain the upper\nand lower envelope U (t) and L(t).\nA.3. Extrema finder 2 (XF 2) : EMD TRUMAX2\nEven if we calculate the envelope using EMD TRUMAX1, we sometimes found that\nthe time series data h(t) still crosses the upper envelope U (t) or the lower envelope\nL(t). Thus we replace the position of local maxima and minima as follows:\n(1) Extract the revised maxima (b\ntp , b\nhp ) through the same procedure as EMD TRUMAX1 and connect these points to calculate the revised candidate of upper\nenvelope Uc (t).\n(2) Calculate the difference ∆h(t) = h(t) − Uc (t). Note that ∆h(b\ntp ) = 0 and ∆h(t)\nbecomes positive if crossing of h(t) and Uc (t) takes place.\n(3) Under the procedure similar to EMD TRUMAX1, calculate a cubic spline funce\ntion ∆H(t) near (b\ntp , b\nhp ) and identify the local maxima (e\nt∆\np , ∆hp ) of ∆h(t),\n′\n′′\nwhere ∆H (t) = 0 and ∆H (t) < 0.\ntp given at step (1) to e\nt∆\n(4) Move the local maximum points b\np obtained at step\n∆\nb\nb\n(3) and hp = hp (of step (1)) + ∆H(b\ntp ), which can be considered to be the\n.\ninterpolation value of h(t) at t = b\nt∆\np\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\nws-aada˙HT˙v3.3\n\nOn Investigating EMD Parameters to Search for Gravitational Waves\n\n19\n\n(5) Connecting these local maxima to obtain the new upper envelope U (t).\nA similar procedure is followed to obtain the new lower envelope L(t).\nReferences\nAbbott B. P. et al. (2009). LIGO: the Laser Interferometer Gravitational-Wave Observatory. Rep. Prog. Phys., 72: 076901.\nAccadia T. et al. (2011). Calibration and sensitivity of the Virgo detector during its second\nscience run. Class. Quantum Grav., 28: 025005.\nCamp J. B., Cannizzo J. K. and Numata K. (2007). Application of the Hilbert-Huang\ntransform to the search for gravitational waves. Phys. Rev. D, 75: 061101(R).\nCamp J. B. et al. (2009). Search for gravitational waves with the Hilbert–Huang Transform.\nAdv. in Adap. Data Analy., 1 (4): 643–666.\nCohen, L. (2005). Time-Frequency Analysis. Prentice Hall, Englewood Cliffs, N. J.\nHuang, N. E., Long, S. R. and Shen, Z. (1996). The mechanism for frequency downshift\nin nonlinear wave evolution. Adv. Appl. Mech., 32: 59–111.\nHuang, N. E., Shen, Z., Long, S. R., Wu, M. C., Shih, H. H., Zheng,Q., Yen, N.-C., Tung, C.\nC. and Liu, H. H. (1998). The empirical mode decomposition and the Hilbert spectrum\nfor nonlinear and non-stationary time series analysis. Proc. R. Soc. London, Ser. A,\n454: 903–993.\nHuang, N. E., Shen, Z., and Long, S. R. (1999). A new view of nonlinear water waves —\nThe Hilbert spectrum. Annu. Rev. Fluid Mech., 31: 417–457.\nHuang, N. E., Wu, M. L., Long, S. R., Shen, S. S. , Qu, W. D., Gloersen, P. and Fan, K. L.\n(2003). A confidence limit for the position empirical mode decomposition and Hilbert\nspectral analysis. Proc. R. Soc. London, Ser. A, 459: 2317–2345.\nHuang, N. E. et al. (2005). Hilbert–Huang Transform and its Applications, World Scientific,\nSingapore.\nKaneyama, M, Oohara, K., Takahashi, H., Camp, J., B. (2013). Towards constructing an\nAlert System with the Hilbert-Huang Transform –Search for signals in noisy data–.\nsubmitted to ICIC Express Letters.\nNovak, V., Yang, A. CC., Lepicovsky, L., Goldberger, A. L., Lipsitz, L. A. and Peng, C. K.\n(2004). Multimodal pressure–flow method to assess dynamics of cerebral autoregulation\nin stroke and hypertension. Biomed. Eng. Online, 3: 39.\nSomiya, K. for the KAGRA Collaboration. (2012). Detector configuration of KAGRA –the\nJapanese cryogenic gravitational-wave detector. Class. Quantum Grav.,29: 124007.\nStroeer, A., Cannizzo, J., K. and Camp, J., B. (2009). Methods for detection and characterization of signals in noisy data with the Hilbert-Huang transform. Phys. Rev. D,\n79: 124022.\nStroeer, A., Blackburn, L. and Camp, J., B. (2011). Comparison of signals from gravitational wave detectors with instantaneous time–frequency maps. Class. Quantum Grav,\n28: 155001.\nTakahashi, H., Oohara, K., Kaneyama, M., Hiranuma, Y. and Camp, J., B. (2013).\nOn Investigating EMD Parameters to Search for Gravitational Waves – All Results –. http://astro1.sc.niigata-u.ac.jp/∼oohara/HHT/ws-aada-AllResults.pdf (accessed 2013.06.04)\nYang, J. N., Lei, Y., Lin, S. and Huang, N. E. (2004). Hilbert–Huang Based Approach for\nStructural Damage Detection. Journal of Engineering Mechanics American Society of\nCivil Engineers 130, 85.\nWu, Z. and Huang, N. E. (2005). Ensemble Empirical Mode Decomposition: A Noise\n\n\fApril 17, 2018 13:42 WSPC/INSTRUCTION FILE\n\n20\n\nws-aada˙HT˙v3.3\n\nHirotaka Takahashi, Ken-ichi Oohara et al.\n\nAssisted Data Analysis Method, COLA Tech. Rep. 193, Center for Ocean-LandAtmosphere Studies, Calverton, Md.\n\n\f"
        ],
        [
         "14",
         "14",
         "cs.CE",
         "Computational Engineering",
         "0604078v1.pdf",
         "The emergence of knowledge exchange:\nan agent-based model of a software market\nMaria Chli and Philippe De Wilde\n\narXiv:cs/0604078v1 [cs.MA] 20 Apr 2006\n\nAbstract\nWe investigate knowledge exchange among commercial organisations, the rationale behind it and its effects on the market.\nKnowledge exchange is known to be beneficial for industry, but in order to explain it, authors have used high level concepts like\nnetwork effects, reputation and trust. We attempt to formalise a plausible and elegant explanation of how and why companies\nadopt information exchange and why it benefits the market as a whole when this happens. This explanation is based on a multiagent model that simulates a market of software providers. Even though the model does not include any high-level concepts,\ninformation exchange naturally emerges during simulations as a successful profitable behaviour. The conclusions reached by this\nagent-based analysis are twofold: (1) A straightforward set of assumptions is enough to give rise to exchange in a software market.\n(2) Knowledge exchange is shown to increase the efficiency of the market.\nIndex Terms\nIntelligent agents, Multi-agent systems, Economics, Adaptive behaviour, Agent-based modelling\n\nThis work has been carried out as part of the project Digital Business Ecosystem, funded by the 6th Framework Programme of the European Commission.\nContract Number: IST - 2002 - 507953.\nThis work was carried out when M. Chli and P. De Wilde were at Imperial College London\n\n\f1\n\nThe emergence of knowledge exchange:\nan agent-based model of a software market\nI. I NTRODUCTION\nThe growth of the Internet as a medium of knowledge\nexchange has stimulated a lot of scientific interest originating from various disciplines. The willingness of individuals,\norganisations as well as commercial firms to share information via the Internet has been remarkable. In some sectors\nlike scientific research, the communication of newly acquired\nknowledge and expertise in a field is considered vital for their\nadvancement. On the other hand, in other sectors, the benefits\nof such exchanges may not be obvious. For instance, it might\neven be considered damaging for pharmaceutical companies to\nmake public any innovations generated by their Research and\nDevelopment (R&D) process. In spite of this view, exchange of\nintellectual property in some industries occurs quite frequently\nand in various different ways. These include the forming of\nstrategic partnerships, the participation in open source software\nprojects and the publication of scientific papers by research\nlabs that are part of commercial companies.\nWe study the knowledge exchange that occurs in the\nsoftware industry. In particular, we focus on analysing the\nrationale behind this exchange as well as its effect on the\nindustry. The complexity of software requirements is a characteristic that distinguishes the software market from others.\nHowever, the findings of this work might be relevant to other\nindustries as well. This effort fits within the framework of the\nDigital Business Ecosystem (DBE) project. The DBE project\nis an attempt to develop a distributed environment which will\ninterlink European Small and Medium Enterprises (SMEs) that\nare software providers and foster collaboration between them.\nOur broader interest lies in understanding the dynamics of\necosystems [11], [15], [43]. Furthermore, we are interested in\nanalysing the global system properties which emerge from the\ninteractions that occur in a market ecosystem. We have been\nusing techniques from agent based modelling to simulate the\nDBE environment. The main aspects of the DBE market are\ncaptured in a model where the SMEs are agents with bounded\nrationality. This model is then studied using simulations of\nvarious settings, and a number of observations are made. One\nof the most interesting observations is that exchanges between\nthe agents similar to the ones that happen in real-life arise in\nthe system. This behaviour emerges in the market even though\nthe model does not explicitly account for social issues of trust,\nnetwork effects or managerial strategies.\nThe paper is organised as follows. The following section\ngives an insight to the Digital Business Ecosystem project\nand the characteristic of the market that will be developed.\nIn section III we sketch the background of this work, namely\nwe review the types of exchanges that occur in markets, giving\nparticular attention to the software market. Section IV details\n\nthe model used for the investigation carried out. Section V\nanalyses the experiments performed and the results produced\nand section VI concludes.\nII. D IGITAL B USINESS E COSYSTEM\nIn this section we give a brief overview of the Digital\nBusiness Ecosystem project, highlighting its aims and motivation. The characteristics of the end-product are identified and\nspecial attention is given to the efficiency of the market that\nwill be formed.\nA. A DBE Economy\nIt is stated in [35] that virtual organisations make dynamic\ncoalitions of small groups possible. In this way the companies\ninvolved can provide more services and make more profits.\nMoreover, such coalitions can disband when they are no\nlonger effective. At present, coalition formation for virtual\norganisations is limited, with such organisations largely static.\nThe overall goal of the DBE project 1 [13] is to launch\na new technology paradigm for the creation of a digital\nbusiness ecosystem that will interlink SMEs and especially\nsoftware providers. The project is encompassed by the European Union’s initiative to become a leader in the field\nof software application development and to strengthen its\nSME industry. An open source distributed environment will\nsupport the spontaneous evolution, adaptation and composition\nof software components and services, allowing SMEs that are\nsolution and e-business service providers to cooperate in the\nproduction of components and applications adapted to local\nbusiness needs. This will allow small software providers in\nEurope to leverage new distribution channels providing niche\nservices in local ecosystems and extending their market reach\nthrough the DBE framework. Easy access and large availability\nof applications, adapted to local SMEs, will foster adoption\nof technology and local economic growth. It will change the\nway SMEs and EU software providers use and distribute their\nproducts and services.\nThe main objective of this work, which was carried out as\npart of the DBE project, was to study the properties of this new\ntype of market. It is clear that the interactions and exchanges\nbetween the SMEs within the Digital Business Ecosystem\nenvironment will have an effect on the dissemination of\ninformation and subsequently to the efficiency of the market.\nB. Market Efficiency\nWithin the environment of the DBE, business alliances,\nnetworks and supply chains require much less effort to be\n1 The\n\nweb page of the project can be found at www.digital-ecosystem.org\n\n\f2\n\nformed. This will promote cooperation and easier dissemination of information between the member SMEs. On the other\nhand, competition for a share of the market between SMEs will\nbecome more direct. It is to be hoped, that these factors will\nraise the levels of efficiency in the DBE market in comparison\nto a traditional market. While these aspects of the DBE are\nvery interesting and the subject of future research, this work\nstudies how market efficiency is affected by the exchange\nof information between SMEs. The experiments carried out\non our model, confirm that as the agents engage in more\ninformation exchanges between them, with time the market\nefficiency of the system rises.\nEfficient Markets Theory, as proposed by [19], is a field of\neconomics which seeks to explain the operation of an asset\nmarket. Specifically, it states that at any given time, the price\nof an asset reflects all available information [3], [12]. The\nefficient market hypothesis implies that it is not generally\npossible to make above-average returns in the stock market\nover the long term by trading lawfully, except through luck or\nby obtaining and trading on inside information.\nThe DBE environment is different from an asset market, so\nthe definition of efficiency needs to be modified, retaining the\nspirit of the efficient market hypothesis. In the model of the\nDBE used in this work, the market is driven by demand which\nis fixed and unaffected by the supplied DBE services. In this\ncase the market is efficient if, at any given time, the supply of\na service reflects all available information. This means that,\nthe services supplied are such that they satisfy the underlying\nmarket needs optimally. In other words, the SMEs are not\nconcentrating on catering for some needs while others are left\nunsatisfied. In an efficient DBE market, all the needs will be\nsatisfied evenly, assuming that there is equal demand for each\nof them. To draw a parallel between the traditional definition\nof an efficient asset market and the proposed definition for\nthe efficiency of the DBE market consider the following. In\nan inefficient asset market, a trading agent can earn excessive\nreturns by buying a particular stock which she believes to\nbe undervalued. Similarly, in an inefficient DBE market a\ncompany might make excessive profits by satisfying a need\nwhich it knows is not sufficiently satisfied. To invert the\nargument, in an efficient asset market, asset prices adjust instantaneously and in an unbiased fashion to publicly available\nnew information, so that no excess returns can be earned by\ntrading on that information. Similarly, in an efficient DBE\nmarket, the supply of services will adjust immediately to any\narising information about the underlying needs.\nCooperation, symbiosis [16], [27] as well as the efficiency\n[37], [40] of adaptive multi-agent systems has been studied in\nthe context of the simple games. In [40] no verifiable definition\nof efficiency is given, whereas in [37] the system is considered\nto be in an efficient market phase when all information that\ncan be used by the agents’ strategies is traded away, and\nno agent can accumulate more points than an agent making\nrandom guesses would. In the work presented in this paper,\nmarket efficiency, cooperation and competition are studied in\nthe context of a more realistic economic market.\n\nIII. BACKGROUND\nIn this section we list a number of ways in which exchange\nof knowledge between companies happens in a market and the\nrationale for each of them is briefly reviewed. As this work\nfocuses on SMEs that are software providers, we survey the\nkey characteristics of the software industry and the exchanges\nin this particular market.\nA. Exchange in economic markets\nIn an economic market there are many ways in which the\nfirms engage in exchanges between them. These include the\nforming of strategic partnerships, the participation in open\nsource software projects and the publication of scientific\npapers by research companies like HP Labs and Microsoft Research. In the paragraphs that follow we will briefly examine\nthe rationale behind these different forms of exchange.\nFor a strategic partnership to be formed, the partners must\nmutually benefit from the experience, expertise and talent that\nall the parties bring to the partnership. There usually is an\nimmediate worthy goal or objective that the partners concerned\nwish to achieve. For instance, they may wish to operate in\na new market, or to bring about a change of leadership in\nthe industry they operate in. Hagedoorn in [24] reports a\ndramatic rise especially in R&D partnerships, over the past\n40 years. These partnerships are mostly limited-time project\nbased collaborations as opposed to long-term alliances. The\nmain motives behind them are reported to be related to costcutting as well as risk minimisation whilst the partners attempt\nto enter new technological areas.\nRecent economics and management research has studied\nthe phenomenon of commercial firms contributing to open\nsource projects. The main motive indicated by these analyses\nis strategic [22], as set out in more detail in section III-B\nwhere the specifics of the software industry are analysed. This\nseems to be consistent with the fact that it is not the leaders\nin the industry who engage in open source development, but\nthe followers.\nAnother form of exchange, which at first might seem\ncounter-intuitive, is the publication of scientific papers containing the findings of the research commercial companies\nperform. It may be argued that it would be in the interest of\nthose companies, to keep their innovative work to themselves.\nAnother argument, however, is that by publicising their research they invite others to endorse it, add to it and in effect\nadvance it further. Then, they can use the knowledge acquired\nby this process to better their products.\nThe model of a software market that we propose as part\nof this work is simple in the sense that the agents/firms do\nnot have the ability to reason about complex situations. They\ncannot make decisions to operate in new markets, or form\npartnerships in order to change the leadership in the industry.\nThey cannot devise strategies to undercut their competitors.\nHowever, they operate in a capitalistic economy where the\nbest of them succeed whilst the worst perish. They are\nthus equipped with a simplistic mechanism of reinforcement\nlearning, i.e. being rewarded or punished for choices that prove\nto be good or bad respectively. When given the opportunity\n\n\f3\n\nto engage in exchange of services between them, they learn\nwith time under which circumstances this is beneficial to them\nand they proceed with it without ever being biased by external\nfactors towards exchanging.\n\nB. The software industry\nComplexity is a key characteristic of software which distinguishes the software industry from others. Typical software\nproducts carry a large number of features, with innumerable\n[2] interactions between them. For a program to be successful\nin the market, it is necessary that it has the right set of features\nto satisfy the customer base and that these features operate\nsuccessfully together.\nThe market of proprietary software providers/publishers is\ndominated by large companies, not SMEs. Microsoft Corporation holds the lion’s share in the software market with companies like Oracle, IBM, Hewlett-Packard and Sun following\nwith smaller shares2 .\nAt the same time, the open source3 movement has been quite\nsuccessful in developing relatively complex software products\nlike Linux, Apache or sendmail that are serious competitors of\nwell established proprietary software [38]. Networks of thousands of volunteers have contributed to these highly complex\nproducts. This appears, as it is pointed out in [2], to counter\nthe economic intuition that private agents, without property\nrights, will not invest sufficient effort in the development of\npublic goods because of free-rider externalities.\nLerner and Tirole in [33] justify the volunteers’ motivation\nfor contribution to the open source movement as an opportunity to ‘signal their quality’. In other words, the volunteers\nbelieve it will enhance their career prospects, as the names\nof the contributors are always listed in open source projects.\nOther individual motivations, like altruism or opportunity to\nexpress creativity are also mentioned.\nIt is important to point out that in recent years, open source\nprojects have not only received contributions by individuals.\nThere have been organised efforts by firms like Sun, IBM\nand others that have endorsed such projects. The survey [6]\nconducted among firms, as well as the account of [20] of\nSun Microsystems and [22] list strategic reasons behind the\nmotivation of firms to contribute to open source projects. These\nreasons include efforts to undercut rival products, gaining a\nwider tester base for their own products, initiating a gift economy culture between the firm and the open source developer\ncommunity (where the firm provides the software for free and\nthe community provides debugging or more source code in\nreturn) and giving out the software to clients in order to charge\nfor its maintenance and support.\n2 The information reflects the year 2002-2003 and was obtained from IBIS\nWorld, a strategic business information provider.\n\nhttp://www.ibisworld.com/snapshot/industry/default.asp?page=industry&industry id=1239\naccessed on 27/05/2005.\n3 In open source software, the source code for a program is made open\nand available for anyone to screen. There are different open source licenses\nwhich prescribe what one is allowed to do with the source code e.g.\nscreen it, interpret it, make changes etc. This is in contrast to proprietary\nsoftware licenses where the source code is protected by property rights against\nmodification.\n\nPrevious work in this area includes that of Johnson in [28]\nand Bessen in [2] who have used mathematical models to\nexplain the emergence of the open source initiative. Johnson focuses more on analysing the individual motives and\nestablishing the relationship between the size of the developer\nbase and whether the development goes on. On the other\nhand, Bessen concentrates on the firm motives for participation\nin open source initiatives. Bessen, models software as a bit\nstring, each bit being a certain feature of the software. In\nthis way the notion that the number of combinations of\nfeatures grows exponentially with the number of features is\ncaptured, depicting the complexity the software can have.\nIn his work, he compares open source development with\nproprietary, pre-packaged provision of software and concludes\nthat the two complement each other, recognising that they\nserve different groups of customers. The latter suits customers\nwith standard, non-complex software needs, while the former\nserves customers who have software development capabilities\nand who need more complex software products.\nBonaccorsi and Rossi in [5] have designed a multi-agent\nsystem simulation with which they explore the circumstances\nfor adoption of open source software. They also conclude that\nproprietary and open source software will coexist in the future.\nTheir model of the diffusion of the two competing streams of\nsoftware production takes into account issues like the effect of\nadvertising, network externalities and achievement of critical\nmass as in [34].\nThe stylised model presented in this work simulates a\nmarket in which the companies try to satisfy a set of underlying software needs with the services that they develop.\nThe companies follow simple, high-level rules imposed by\na capitalistic economy. Interestingly, exchanges between the\nagents similar to the ones that happen in real software markets,\narise in the system. This behaviour emerges in the system\neven though we have avoided modelling issues like social or\nstrategic motives of the contributors or network effects.\nIV. A N\n\nAGENT- BASED MODEL OF THE\n\nDBE\n\nA. Agent-based Modelling\nAgent-based modelling has been recently used in Economics research work to study models of markets, e.g. the\nSanta Fe artificial stock market [4], [32], and their characteristics [31], in Computing-Economics interdisciplinary work to\nstudy information economies of autonomous agents [14], [23],\n[29], [30], [39] and business processes [26], in Social Sciences\nto study emergent behaviour [17], issues of trust [18] and to\nperform syndromic behaviour surveillance [10] and in other\ndisciplines.\nMuch research in multi-agent systems explores how refinements to one agent’s reasoning can affect the performance\nof the system [8]. Significant effort has been directed towards\nformally defining emergence in agent-based systems. A strong\nemergent property is a property of the system that cannot\nbe found in the properties of the system’s parts or in the\ninteractions between the parts [1]. Additionally, in [42] the\nnotion of universality is studied: systems whose elements\ndiffer widely may have common emergent features.\n\n\f4\n\nAgent-based modelling according to [41] “is a method for\nstudying systems exhibiting the following two properties:\n1) the system is composed of interacting agents; and\n2) the system exhibits emergent properties, that is, properties arising from the interactions of the agents that\ncannot be deduced simply by aggregating the properties\nof the agents.”\nIn models like the one proposed below, where the interaction\nof the agents is determined by past experience and the agents\ncontinually adapt to that experience, mathematical analysis\nis typically very limited in its ability to derive the dynamic\nconsequences. In this case, agent-based modelling might be\nthe only practical method of analysis.\nWe follow a ‘bottom-up’ approach, after a brief overview\nof the methods used in section VII which follows, in sections\nIV-B and IV-C we describe the first principles of agent\nbehaviour and in section V we analyse the macro-properties\nemerging from the agent interactions.\nB. The setting\nIn this section, the model used for the simulation of the\nDBE environment is set out.\nSMEs are modelled as agents in a multi-agent system. The\nservices the SMEs provide are modelled as bit strings in the\nsame manner software services are modelled in [2], each bit\nsymbolising a feature of the service. Finally, the underlying\nmarket is modelled by a set of requests (market needs) which\nare exogenous and are generated randomly. A request is a bit\nstring of the same size as a service bit string.\nEach SME has a population (or portfolio) of services. This\npopulation is not static throughout the lifetime of the SME. If\na service is successful, the SME tends to add similar services\nto the portfolio while an unsuccessful service is usually\ndiscarded. The whole process is modelled quite elegantly by\na genetic algorithm (GA) within the portfolio which involves\nmutation and crossover with survival of the fittest. Through\nthis population each SME can choose which request it will\ntry to satisfy. The genetic algorithm represents the R&D\nbusinesses perform in order to improve their services. An\noverview of genetic algorithms is given in appendix VII.\nThe use of genetic algorithms is a natural and simple way\nto model R&D, with minimal assumptions. The GA captures\nthe following characteristics:\n1) trying to find a solution to a particular problem,\n2) using a population of possible solutions.\nAny other method that can capture the above two characteristics may be used in place of the GA.\nThe objective of an SME is to increase its fitness. Each\nSME maintains a portfolio of candidate services, only one of\nwhich will be submitted to the market. Each candidate service\nreceives a rating according to how profitable it would be for\nthe SME if it was submitted to the market. This calculation is\nperformed using the services submitted by all other SMEs in\nthe previous round. The rating of each candidate service within\nthe SME portfolio is used to: a) decide on which service to\nsubmit to the market and b) evolve the best services in the\n\nportfolio (with mutation and crossover) and eliminate the worst\nservices.\nThe fitness of a service measures how profitable it is to its\nowner. The profitability of a service depends on:\n1) how close the service is to the market needs (servicerequest similarity) and\n2) how many other services satisfy those needs (limited\ndemand).\nThe fitness of an SME equals the fitness of the service it\noffers.\nIn the section that follows we discuss the factors that affect\nthe fitness (or profitability) of a service.\n1) Service-Request Similarity and Limited Demand: Assume there are m SMEs in the market, each one offering\na single service. Consider a service S and a request R,\neach represented by a bit string of fixed length. Similarity is\nmeasured by the percentage of shared bit values between S and\nR, denoted by d(Ri , Sj ), 0 ≤ d ≤ 1. If the market requests are\nR1 , R2 , ..., Rn , services in the market are S1 (t), . . . , Sm (t),\nthe fitness of a service Sj (t) is\nUj (t) =\n\nn\nX\n\n(φ(Ri , Sj (t)) × ρi (t)),\n\n(1)\n\ni=0\n\nwhere\nφ(Ri , Sj (t)) = e−\n\n1−d(Ri,Sj (t))\nα2\n\n.\n\n(2)\n\nThe variable φ is used to parametrise the fitness landscape\n(make maxima more or less pronounced), α being a shape\nparameter. Figure 1 shows the relationship of φ with with the\nsimilarity d. The weight/discounting factor ρ is given by\n)\n(\n1\n.\n(3)\nρi (t) = min 1, P\nj=1 φ(Ri , Sj (t))\nThe variable ρ models the fact that the demand in the market\nis limited. When a request is saturated (i.e. too many services\ntry to satisfy it) then ρ < 1. Subsequently, the fitness of the\nservice is discounted. Otherwise, when ρ = 1 the fitness of\nthe service equals φ.\nThe fitness of an SME is equal to the fitness of the service\nit submits to the market.\n2) Satisfaction of Requests and Market Efficiency: An\nadditional useful measure is the degree to which a request\nis satisfied. This is a metric of how saturated it is, in terms\nof how many services try to satisfy it and how similar their\nfeatures are to those of the request. The degree of satisfaction\nQi (t) of a request Ri at round t is given by:\nQi (t) =\n\nm\nX\n\nφ(Ri , Sj (t)).\n\n(4)\n\nj=1\n\nThis measure is necessary for assessing the efficiency of the\nDBE market. As discussed in section II-B, in an efficient\nDBE market all the market requests will be equally saturated,\nassuming there is the same demand for all of them. Thus, we\ncalculate the standard deviation σ(t) of the satisfaction values\nof all the requests in the market at round t. The smaller it\n\n\f5\n\n1\n0.9\n0.8\n0.7\n0.6\n\nφ\n\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nd (percentage similarity between service and request)\n\nFig. 1. The relationship of φ with the service-request similarity d for a = 0.2.\nThe variable φ is used to parametrise the fitness landscape (make maxima\nmore or less pronounced).\n\nis, the more similar to each other the saturation levels of the\nrequests are.\nσ(t) = stdev{Q1 (t), . . . , Qn (t)}\n\n(5)\n\nThe mean of the saturation values will be constant due to the\ndemand in the model being fixed.\nC. Exchange of Services\nAs outlined in III-A exchange of services may encompass\nmany real-life situations that occur in a market. These include\nthe forming of strategic partnerships of companies, participation in free/open source projects and others. The setting\ndescribed here is a loose model of such situations which aims\nto identify the basic factors that lead to this general behaviour\nof exchanging.\nIn our model, the exchange involves selecting a set of\nservices from one SME’s portfolio and swapping them with\nthe corresponding set of services of the other SME’s portfolio.\nWhen a company chooses to swap a set of services, this means\nthat after the exchange has taken place it won’t have these\nservices in its portfolio any more. The services in a portfolio\nof a company are sorted according to their fitness (i.e. how\nprofitable they are to the SME that owns them). The model in\nits current state supports exchange of services that are in the\nsame rank, in the two portfolios, e.g. the 5th service in the\nportfolio of one SME with the 5th service in the portfolio of\nthe other4.\nAt each time tick, the SMEs need to decide whether they\nwant to exchange some of their services with one of the other\nSMEs. A statistical classification algorithm is used to model\nthe decision problems an individual agent faces. An overview\nof statistical classification is given in Appendix VII.\n4 Experiments have shown that the rank of the services being exchanged is\nnot of much significance, assuming that services of the same rank are being\nexchanged, but we plan to investigate this further in the future.\n\n1) Exchange decisions: Every SME has a classifier system\nwhich it uses to decide on whether they want to exchange\nsome of their services with one of the other SMEs. The rules\nof the classifier are shown in table I below. The objective of\nan SME at all times is to increase its fitness.\nThe rules’ condition part refers to the rank of the SME\nin the market with respect to the rank of its colleagues. The\naction part examines the potential partner’s rank and prompts\nthe SMEs either to engage in an exchange with a specific\ntype of partner or abstain from exchanging. For simplicity, the\nSMEs are clustered in three5 groups according to their rank.\nTherefore we have upper, middle and lower ranked SMEs. For\nan exchange to take place both parties need to agree.\nWe experiment both with settings in which the rank is based\non the fitness of each company and others where the rank is\nnot linked to SME performance in any way. For example,\nin experiments where rank is based on SME performance,\nthe SME with the highest fitness will have rank = 1,\nwhilst the SME with the lowest fitness will have rank =\nnumber of SM Es. On the other hand, in experiments where\nrank is unrelated to performance in the market the rank of\nan SME may be its id number. In section V we analyse these\nexperiments and present the effect the different meanings rank\nmay take have on the learning that occurs.\nif\nif\nif\nif\nif\nif\nif\nif\nif\n..\n.\n\nmy rank = lower\nmy rank = lower\nmy rank = lower\nmy rank = lower\nmy rank = middle\nmy rank = middle\nmy rank = middle\nmy rank = middle\nmy rank = upper\n\nthen\nthen\nthen\nthen\nthen\nthen\nthen\nthen\nthen\n..\n.\n\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\ns9\n..\n.\n\nexchange with lower cluster,\nexchange with middle cluster,\nexchange with upper cluster,\ndo not exchange,\nexchange with lower cluster,\nexchange with middle cluster,\nexchange with upper cluster,\ndo not exchange,\nexchange with lower cluster,\n\nTABLE I\nA\n\nFEW EXAMPLE RULES OF THE CLASSIFIER WHICH AN\n\nSME USES TO\n\nDECIDE ON WHAT TYPE OF PARTNER TO CHOOSE FOR AN EXCHANGE .\n\nThe classifier system operates as follows [31]. First, it\nexamines the if part of each rule to determine and shortlist\nthe rules whose conditions are satisfied at a given time t. It\nthen assigns a score b to the shortlisted rules, sk being the\nstrength of the k th rule:\nbk (t) = sk (t) + ε, where ε ≃ N (0, σ).\n\n(6)\n\nThe rule with the highest score b becomes the active rule.\nAfter the active rule has been executed and has generated\npayoff ω during the previous round t − 1, the classifier system\nupdates its strength s:\nsk = sk (t − 1) − csk (t − 1) + cω(t − 1), where c ∈ [0, 1]. (7)\nIn other words, ∆sk (t) = c[ω(t− 1)− sk (t− 1)]. Therefore,\nas long as the payoff in round t− 1 is greater than the strength\n5 Experiments have been carried out which showed that model behaviour\ndoesn’t vary significantly with cluster size. Three is the optimal number of\nclusters with respect to having a model which is realistic enough while taking\na reasonable amount of time to execute and giving us the ability to present\nthe results in an efficient and clear way.\n\n\f6\n\nof the rule on that round, the strength will increase. If the\nselection of the rule led to a small payoff being generated,\nthe strength of the rule will decrease, making it less likely to\nbe activated in the future. The strength of each rule converges\nto some weighted average of the rewards ω generated by the\nenvironment in response to that specific rule.\nIn our implementation of the model all the rules have initial\nstrength 0. The rule strengths are adjusted as the simulation\ngoes on. The strength of each rule that is activated is updated\nat every round using the following payoff from the external\nenvironment: ω(t) = Uj (t) − Uj (t − 1). In other words,\nthe payoff is the difference in the fitness of the company\nbetween the current and the previous round. The payoff may\nbe negative, zero, or positive according to the change in fitness.\n2) Exchange decisions resolution: Once the companies that\nhave decided to participate in an exchange have selected the\ntype of partner they prefer, they are teamed up accordingly.\nFor instance, an SME in the cluster of middle ranked SMEs,\nwho has decided to exchange with a high fitness company\nwill be coupled with a high ranked company who wants to\nexchange with a middle ranked one. If a suitable partner is\nnot found the exchange does not happen. The strength of the\nrule that was activated in that case will still be updated even\nif the transaction was not carried out. This reflects the effect\nchoosing a partner who is unwilling to collaborate has on the\nfitness of the company.\nD. Discussion\nThe model outlined above is simple in that it has captured\nthe main aspects of a digital business ecosystem. It is the\nmodel of a market in which the companies try to satisfy a set\nof underlying requests. They do so by producing and making\navailable services that are as close as possible to the specified\nrequests. Each company has its own R&D portfolio of services\nthat it evolves. At each round the companies go to the market\nwith what they believe is the best service in their portfolio. In\naddition, the companies have an option to exchange services\nwith partners that they select themselves.\nThe simplicity of the model is also inherent in the behaviour\nof the agents. The agents have to find which is the best service\nto make available, based on the services that were submitted\nto the market during the previous round. Also, they need to\ndecide whether and with whom to exchange their services\nbased on their rank in the market. These are all abstractions\nfrom reality. We do not assume any network effects in the\nmarket. Also, there are no indicators about value of the brand\nof a company.\nV. A NALYSIS\n\nOF THE MODEL\n\nIn this section the experiments carried out using the model\nof the DBE are described. The analysis focuses on two main\nfindings:\n1) The companies discover themselves that under certain\ncircumstances it is beneficial to them to exchange services between them.\n2) Allowing exchange to take place in the market, makes\nfor greater market efficiency levels.\n\nIt is important at this point to stress that the choice to\nexchange services is not a practice that is imposed by the\nmodel mechanism. Instead, it is a feature that emerges from\nthe classifiers as it is a gainful practice for the companies\nunder certain circumstances.\nThe model behaviour is quite general and has been observed\nfor a very wide range of parameters and initial conditions. The\ngraphs and figures shown below come from randomly selected\nruns of the simulation, unless it is stated otherwise.\nA. Service Exchange\n1) Exchange Decision: As described in section IV-C each\nagent/company uses a classifier to decide whether or not to\nexchange some of its services. The decision is based on the\ncompany’s rank in the market. Figures 2(a) and 2(b) show the\naverage strength of the rules of all the companies’ classifiers\nat the end of a simulation which lasted for 10 000 iterations.\nThe companies are ranked according to their fitness. The fittest\ncompany will have rank 1 whilst the least fit company will\nhave rank equal to the number of companies in the market. To\nmake for less time consuming simulations and more readable\ngraphs the companies are grouped into three clusters according\nto their rank; so they are divided into lower, mid and upper\nranked SMEs. Figure 2(a) was generated from a run of the\nsimulation where the DBE market consisted of 21 SMEs,\neach having 20 services in its portfolio. Each service had\n10 features. There were 4 software requests in the market,\ngenerated randomly. The run of the simulation which produced\nfigure 2(b) had largely similar parameters, the difference being\nthat there were 30 services in the SMEs’ portfolios and there\nwere 5 requests in the market.\nThe strongest of the rules at each situation is the one which\nis more likely to be activated. In other words, it is shown in\nfigures 2(a) and 2(b) that if a company belongs to the mid\nor lower cluster it is likely that it will choose to participate\nin an exchange (preferably with a upper ranked company)\nwhile if it belongs to the upper ranked cluster it will avoid\nengaging in any exchange activities. The graphs show that in\nthe less successful, lower ranked SMEs the classifier rules that\ncorrespond to exchange actions have higher strengths than\nthe rule that leads SMEs not to exchange. The opposite holds\nfor higher ranked SMEs, i.e. the rule that corresponds to a not\nexchange action has higher strength than the exchange\nrules. For mid-ranked SMEs, a rule prompting the firm to\nexchange is the stronger of all, but exchanging is not always\na profitable practice; the rule that leads the SME to avoid\nexchanging is often stronger than some exchange rules.\nThe generality in the behaviour of the model is confirmed by\nfigure 2(c). A wide range of parameters and initial conditions\nwere varied in a total of 200 experiments, keeping the number\nof SMEs in the market constant (21). Figure 2(c) shows the\naverage values of the SME classifiers’ strengths over those\n200 experiments. The general trend which emerges is that the\naverage performing (mid cluster) and worst performing (lower\ncluster) SMEs learn that it is to their advantage to exchange\nservices with others while the top performers (upper cluster)\nlearn to avoid exchanging .\n\n\f7\n\nRun 1\n\nRun 2\n\nAverage Rule Strength\n\n6\n\n10\n\nAverage Rule Strength\n\n8\n\nAverage Rule Strenghts over 200 experiments\n\n15\nAction: Exchange with upper cluster\nAction: Exchange with mid cluster\nAction: Exchange with lower cluster\nAction: Do not Exchange\n\n4\n2\n0\n−2\n−4\n−6\n\n10\nAction: Exchange with upper cluster\nAction: Exchange with mid cluster\nAction: Exchange with lower cluster\nAction: Do not Exchange\n\n8\n6\n\nAverage Rule Strength\n\n10\n\n5\n\n0\n\n−10\n\n4\n2\n0\n−2\n−4\n\n−5\n\n−6\n\n−10\n\n−10\n\n−8\n\nAction: Exchange with upper cluster\nAction: Exchange with mid cluster\nAction: Exchange with lower cluster\nAction: Do not Exchange\n\n−8\nif my rank is upper\n\nif my rank is mid\n\nif my rank is lower\n\nif my rank is upper\n\nif my rank is mid\n\nRule\n\nif my rank is lower\n\nif my rank is upper\n\nif my rank is mid\n\nRule\n\n(a) Run 1\n\n(b) Run 2\n\nRun with Rank based on SME Fitness Growth Rates\n\n10\nAction: Exchange\nAction: Do not Exchange\n\n8\n\nAction: Exchange\nAction: Do not Exchange\n\n6\n\nAverage Rule Strength\n\nAverage Rule Strength\n\n6\n4\n2\n0\n−2\n−4\n−6\n\n4\n2\n0\n−2\n−4\n−6\n\n−8\n−10\n\n(c) Average over 200 Experiments\n\nRun with Rank based on the 20−moving average of the SME fitness\n\n10\n8\n\n−8\nif my rank is upper\n\nif my rank is mid\n\n−10\n\nif my rank is lower\n\nif my rank is upper\n\nRule\n\nif my rank is lower\n\n(e) Rank based on Fitness Moving Average\n\nRun with Rank based on the SME Id which is static\n\nRun with Rank based on the SME Id which is changing\n\n10\n\n10\nAction: Exchange\nAction: Do not Exchange\n\n8\n\nAction: Exchange\nAction: Do not Exchange\n\n6\n\nAverage Rule Strength\n\n6\n\nAverage Rule Strength\n\nif my rank is mid\n\nRule\n\n(d) Rank based on Fitness Growth Rate\n\n8\n\nif my rank is lower\n\nRule\n\n4\n2\n0\n−2\n−4\n−6\n\n4\n2\n0\n−2\n−4\n−6\n\n−8\n\n−8\n\n−10\n\n−10\n\nif my rank is upper\n\nif my rank is mid\n\nif my rank is lower\n\nRule\n\n(f) Rank based on SME Id which is random\nand static throughout the simulation\n\nif my rank is upper\n\nif my rank is mid\n\nif my rank is lower\n\nRule\n\n(g) Rank based on SME Id which is random\nand constantly changing throughout the simulation\n\nFig. 2. Average Exchange Rule Strength The graphs show the strength values of each rule at the end of a simulation averaged out over all SMEs’ classifiers.\nThe SMEs decide whether to participate in an exchange of services according to their rank. The classifier each SME has is as follows:\nexchange with lower cluster,\ns1\nif\nmy rank = lower\nthen\nif\nmy rank = lower\nthen\nexchange with middle cluster,\ns2\nif\nmy rank = lower\nthen\nexchange with upper cluster,\ns3\nif\nmy rank = lower\nthen\ndo not exchange,\ns4\nif my rank = middle\nthen\nexchange with lower cluster,\ns5\n..\n..\n..\n.\n.\n.\nFor figures 2(a)-2(e) the rank of the SMEs is based on measures related to their fitness, while figures 2(f) and 2(g) were created for settings in which the\nSME rank was unrelated to fitness. The graphs show in settings where the rank is associated with some fitness measure the SMEs that are further down in\nthe rank learn that is beneficial to them to participate in an exchange.\n2(a) Run 1 parameters: 21 SMEs, each having 20 services in its portfolio. Each service had 10 features. There were 4 software requests in the market. The\nrank was based on the fitness value of the SME.\n2(b) Run 2 parameters: 21 SMEs, each having 30 services in its portfolio. Each service had 10 features. There were 5 software requests in the market. The\nrank was based on the fitness value of the SME.\n2(c) Average values over 200 experiments This figure confirms the generality of the behaviour of the model. A wide range of parameters and initial conditions\nwere varied in a total of 200 experiments, keeping the number of SMEs in the market constant (21). The rank was based on the fitness value of the SME.\n2(d) and 2(e) Average Exchange Rule Strength based on SME performance measures. The SMEs decide whether to participate in an exchange of services\naccording to their performance. In 2(d) the performance measure deciding the rank of the SMEs is their fitness growth rate, while in 2(e) it is the 20-moving\naverage of the SME fitness. When the ranking of the SMEs is performance related information exchange emerges as a gainful strategy.\n2(f) and 2(g) Average Exchange Rule Strength not based on SME performance measures. In 2(f) the SMEs decide whether to participate in an exchange\nof services according to their unique id. In 2(g) the ranking of the SMEs is random and constantly changes. In both cases, the ranking is unrelated to SME\n\n\f8\n\n∆Uj (t) = Uj (t) − Uj (t − 1),\n\nµ=\n\n1\nN\n\nUj (T ).\n\n6\n5\n\nSME0\n\n4\n\nSME1\nSME2\n\n3\n\nSME3\n\n2\n\nSME4\nSME5\n\n1\n0\n1\n\n201\n\n401\n\n601\n\n801\n\n1001\n\n1201\n\n1401\n\n1601\n\n1801\n\n2001\n\ntime\n\n(8)\n\nrather than fitness itself. The graphs produced are similar in\npattern to those in figure 2(c). These strengths imply that\nthe rules are significant and learning has taken place in the\nsystem. Similar results, shown in figure 2(e), were produced\nwhen SMEs were ranked according to the N-moving average\nof their fitness, given by\nt\nX\n\nFitness of SMEs\n\nFitness\n\nTo understand better the behaviour of the system we performed experiments with different rankings of the SMEs.\nAmongst the ranking methods we tested were variants of\nthe fitness ranking, as well as rankings unrelated to SME\nperformance altogether. The results seem to indicate that\ninformation exchange emerges as long as the ranking is in\nsome way related to SME performance. We show in figure 2(d)\nthe rule strengths in the case the SMEs were ranked according\nto fitness growth rates\n\n(9)\n\nT =t−N\n\nOn the other hand, in figure 2(f) a typical case of a ranking\nthat is unrelated to SME fitness is shown. In that particular\ncase we gave the SMEs an arbitrary ranking that remained\nfixed throughout the simulation. The rule strengths indicate\nthat no rule is significantly more important than any other one\nimplying that the rules are not relevant and no learning has\noccurred. We also tried a completely random and constantly\nchanging SME ranking which produced similar results, shown\nin figure 2(g).\n2) Choice of Exchange Partner: An interesting result which\narose from the experiments is the choice of potential partners\nfor the companies who decide to exchange. In all three situations (if my rank is upper, if my rank is mid\nand if my rank is lower) the strength of the rules that\nprompt SMEs to exchange reveal a decreasing preference from\nleft to right between upper, mid and lower ranked partners.\nThat result is entirely intuitive and confirms the validity of\nthe model.\nA result that might not be so obvious is the fact that the\nlower ranked SMEs benefit from exchanging even between\nthemselves. This is reflected in the fairly high strength of the\nrelevant rule and it is better illustrated in figure 3.\nThe experiment that yielded figure 3 is as follows. To make\nfor a more intelligible graph, there are only six SMEs in\nthe market and two distinct requests. Every 400 rounds the\nunderlying requests in the market change. Every 200 rounds\n(but not when the requests change), the lower ranked SMEs\nexchanged services between them. As the purpose of this\nexperiment was to verify the finding that exchange among\nlower ranked SMEs is beneficial, the exchange was done\ndeliberately and not using the classifier. As shown in figure 3,\nin round 200 the exchange does not upset the equilibrium too\nmuch as the SMEs have more or less the same fitness. In round\n600 the exchange drives the lower ranked SMEs up, whilst\ndamaging the fitness of the others in the market. In round\n1000 the exchange not only drives the under-performers up\nbut also causes one of them, SM E1 to join the upper cluster.\n\nFig. 3. This is an experiment that illustrates that exchange among lower\nranked SMEs is beneficial to them. Every 400 rounds the underlying requests\nin the market change. Every 200 rounds (but not when the requests change),\nthe lower ranked SMEs exchanged services between them. In most instances\nthe exchange drives the under-performers up, in terms of fitness.\n\nThe experiment described above illustrated that exchanges\nbetween low-ranked SMEs can be highly beneficial. This is\nbecause the fusion of their portfolios might yield services that\nenable them to operate in a new market segment, in other\nwords it may lead them to satisfy another request which was\npreviously not catered for. This can cause their rank in the\nmarket to improve and even bring about a change of leadership\nin the industry.\nB. Market Efficiency\nAs discussed in section II-B, the increased flow of information within the DBE, will make it easier for the participating\ncompanies to find the right trading partners. Consequently, it\nwill make for greater market efficiency levels in comparison\nto a conventional market (e.g. the software industry). An\ninteresting observation which emerged from the analysis of the\nsimulations carried out is that allowing the SMEs to exchange\nservices between them, increases the efficiency further.\nA DBE market is considered efficient when all the requests\nare equally saturated. In an efficient DBE market, the supply\nof services will adjust immediately to any arising information\nabout the underlying requests. In other words, there is no\nexcess profit to be gained by an SME choosing to satisfy\nanother request than the ones it currently does. As mentioned\nin section IV-B.2, the degree of satisfaction of a request R is\ngiven by equation 4. In order to assess the level of efficiency\nin the market we need to calculate the standard deviation σ(t)\nof the satisfaction values of all the requests in the market,\nas given by equation 5. The smaller it is, the more similar\nto each other the saturation levels of the requests are. It\nis important to mention at this point that the mean of the\nsaturation levels remains constant, because in the model we\nassume equal demand for all of them, and it is equal to\nnumber of services in the DBE\n.\nnumber of requests\nFigure 4 shows the standard deviation σ(t) of the saturation\nvalues Qi (t) of all the requests {R1 , . . . , R4 } in the market,\nfor two different runs of the DBE simulation. Both runs had\nbeen initialised with the same parameters, for one of them\nexchange between the SMEs was not permitted, whereas for\nthe other one the SMEs were free to exchange services with\neach other according to the procedure detailed in section\n\n\f9\n\nIV-C. In order to train the classifiers used for the exchange\ndecisions, every 500 rounds all SMEs’ portfolios were reset to\nthe services they had at round 0. To make comparison easier,\nthe resetting of the portfolios was also done during the run\nwhere exchange was not allowed. In effect, in this experiment,\n‘history’ repeats itself every 500 rounds. This is the reason\nspikes occur in the graph every 500 rounds. When exchange is\npermitted, the SMEs are given the chance to exchange services\nwith each other at rounds 250, 750, 1250, 1750, etc. The graph\nshows a period of 5000 rounds, when the classifiers have been\nsufficiently trained.\nMarket efficiency: with and without exchange\nWithout Exchange\n2.5\n2\nLevels\n\nSt. Dev. of the Requests' Saturation\n\nWith Exchange\n3\n\n1.5\n1\n0.5\n0\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\n\n3500\n\n4000\n\n4500\n\nTime\n\nFig. 4. Market Efficiency: We assess the level of market efficiency by plotting\nthe standard deviation of the saturation degrees of the requests in the DBE\nMarket. The smaller the standard deviation, the greater the market efficiency.\nThe graph contrasts these data for a situation in which the SMEs are allowed\nto exchange services with each other and for a situation where exchange is\nnot allowed. The standard deviation of the saturation degrees of the requests\nis significantly smaller when exchange is allowed, indicating a more efficient\nmarket. For classifier training purposes every 500 rounds all SMEs’ portfolios\nwere reset to the services they had at round 0. In the case where service\nexchanges are allowed, these happen in the middle of each cycle, i.e. at rounds\n250, 750, 1250, 1750, etc.\n\nIt is evident from the graph, that when exchange of services\nbetween SMEs is allowed, the standard deviation of the\nrequests saturation values is considerably smaller. In other\nwords, the requests in the market are more evenly satisfied.\nThis result is quite invariant to initial conditions and parameters of the simulation. So in the system described, not\nonly will SMEs adopt information exchange as beneficial to\ntheir individual progress, but it will also result in a global\nimprovement to the efficiency of the market. Again this is\nin agreement with what is observed in real economies where\nopen standards, publication of innovations and dissemination\nof ideas lead to highly efficient markets.\nVI. C ONCLUDING\n\nREMARKS\n\nThe aim of this work has been to study the rationale\nas well as the effect of knowledge exchange in economic\nmarkets. We focus especially on the software industry, our\nfindings, however, to some extent apply to other industries\nas well. Sharing of information between commercial firms\nis considered controversial. Although it is acknowledged that\nwhen two companies join forces to develop an innovative\nproduct they can both benefit, sharing trade secrets is not\nundertaken lightly. Our main aim has been to formalise a\nplausible and elegant explanation of how and why companies\nadopt information exchange and why it benefits the market as\na whole when this happens.\n\nAn agent based model of a Digital Business Ecosystem\nmarket has been implemented to assist us in understanding the\ndynamics of the market mechanisms. Firms are modelled as\nagents with minimal reasoning capabilities. We investigated\nthe properties that emerge from the agent interactions that\noccur in the market. Specifically, we examined two key\ncharacteristics that we observed in the simulations carried\nout. Namely, the fact that the agents discover themselves\nthat under certain circumstances it is beneficial for them to\nexchange services and that allowing exchange to take place in\nthe market, makes for greater market efficiency levels.\nThe technologic infrastructure of the DBE will facilitate\nthe dissemination of knowledge among the member SMEs,\nincreasing the volume and the speed of the information flowing\nin the market. As a result, it is expected that it will allow for\ngreater market efficiency levels in comparison to a conventional market. Admittedly, it is difficult to compare the market\nefficiency of two different markets. However, an interesting\nresult arose when we performed simulations of the DBE\ncontrasting settings in which exchanges among SMEs were\npermitted with settings where exchanges were not permitted.\nExchanges among SMEs within the DBE further increase\nthe efficiency of the market, which is in agreement with the\ncommon intuition that exchanging information is ultimately\nbeneficial for the entire market.\nThe second and most important conclusion that emerged\nfrom the DBE simulation is that exchanges between the agents\nsimilar to the ones that happen in real-life arise naturally in\nour system. At regular time intervals, the SMEs were given the\nchance to decide whether they wanted to choose a partner and\nswap some of their services. The decision was taken using\nclassifiers, which were separate for each agent. The agents\nwere not pre-programmed or biased in any way to engage in\nexchanges. The SMEs, on their own, discovered in which cases\nexchanging is beneficial for them and what type of partner is\nthe best. Exchange is a practice that emerges, and is not forced\nupon the agents.\nThis work does not directly advocate knowledge exchange\nas a means of increasing profitability of software companies.\nKnowledge exchange, is indeed an already existing phenomenon in industry as explained in section III-A. The results\npresented merely serve as a demonstration of a parsimonious\nset of assumptions that give rise to exchange in a software\nmarket. In other words, we identify the substance of this\nphenomenon, ridding it from unnecessary assumptions, like\nnetwork effects, social issues of trust, or managerial strategies\nand show the minimal set of assumptions that allow it to\nemerge.\nVII. M ETHODOLOGY: E VOLUTIONARY A LGORITHMS\nIn order to model evolution in populations as well as\nlearning we have used several evolutionary algorithms in our\nmodel. In this section we give a brief overview of these\nalgorithms.\nEvolutionary algorithms [7] ‘is an umbrella term employed\nto describe computer-based problem solving systems which\nuse computational models of some of the known mechanisms\n\n\f10\n\nof evolution as key elements in their design and implementation.’ A variety of evolutionary algorithms have been proposed\nby several researchers. The major ones are: genetic algorithms,\nevolutionary programming, evolution strategies, classifier systems and genetic programming. They all share a common\nconcept of simulating the evolution of objects/structures using\nthe processes of selection, mutation and reproduction. The\nprocesses depend on the performance/fitness of the individuals\nunder consideration as defined by their environment and\nquantified by a fitness function.\nMore precisely, evolutionary algorithms maintain a population of structures, that evolve according to rules of selection\nand other operators, that are referred to as “search operators”\n(or genetic operators), such as recombination and mutation.\nEach individual in the population receives a measure of its\nfitness in the environment. Reproduction focuses attention on\nhigh fitness individuals, thus exploiting the available fitness\ninformation. Recombination and mutation perturb those individuals, providing general heuristics for exploration. Although\nsimplistic, these algorithms are sufficiently complex to provide\nrobust and powerful adaptive search mechanisms.\nA genetic algorithm (GA) [21] is a model of machine\nlearning inspired by the mechanisms of genetics, which has\nbeen applied to optimisation. It operates with an initial population containing a number of trial solutions. Each member\nof the population is evaluated (to yield a fitness) and a new\ngeneration is created from the better of them. The process is\ncontinued through a number of generations with the aim that\nthe population should evolve to contain an acceptable solution.\nIn [36] it is stated that GAs are particularly suitable for solving\ncomplex optimization problems and hence for applications that\nrequire adaptive problem-solving strategies. In order to make\ngenetic algorithms reach an optimal solution faster, parallel\nimplementations of GAs are often used [9].\nGenetic algorithms are used for a number of different application areas. An example of this would be multidimensional\noptimisation problems in which the character string of the\nchromosome can be used to encode the values for the different\nparameters being optimised.\nIn practice, therefore, we can implement this genetic model\nof computation by having arrays of bits or characters to represent the chromosomes. Simple bit manipulation operations\nallow the implementation of crossover, mutation as well as\nother operations. Crossover involves combining strings to swap\nvalues, e.g. 101001 + 111111 → 101111. Mutation involves\nspontaneous alteration of characters in a string, e.g. 000101 →\n100101. Although a substantial amount of research has been\nperformed on variable-length strings and other structures, the\nmajority of work with genetic algorithms is focused on fixedlength character strings.\nStatistical classification is a type of supervised learning\nalgorithm which takes a feature representation of an object or\nconcept and maps it to a classification label. A classification\nalgorithm is designed to learn, or in other words, to approximate the behaviour of a function which maps a vector of\nfeatures [X1 , X2 , ..., Xn ] into one of several classes by looking\nat several input-output examples of the function.\nAn instance of a classification algorithm is called a classi-\n\nfier. Learning Classifier Systems [25] are a machine learning technique which combines evolutionary computing and\nreinforcement learning to produce adaptive systems. It is a\nminimal form of modelling learning in the sense that it is\nnot necessary to make assumptions about the way the agents\nperform their reasoning. In addition to that, the absence of any\nassumptions or biases in the learning process leads to results\nthat can be generalised. A classifier consists of a set of rules,\nwhich have a condition C (if part) an action A (then part)\nand a strength measure s. An example of a classifier system\nis shown in table II.\nif\nif\nif\nif\n.\n..\n\nC1\nC2\nC3\n...\n\nthen\nthen\nthen\nthen\n.\n..\n\nA1 ,\nA2 ,\nA3 ,\n...\n\ns1\ns2\ns3\n...\n.\n..\n\nTABLE II\nA N EXAMPLE OF\n\nA CLASSIFIER SYSTEM .\n\nIn the model described in detail in section IV-B, genetic\nalgorithms and classification algorithms have been used to\nmodel evolution of populations of solutions and learning.\nR EFERENCES\n[1] Yaneer Bar-Yam. A mathematical theory of strong emergence using\nmultiscale variety. Complexity, 9(6):15–24, 2004.\n[2] James Bessen. Open source software: Free provision of complex public\ngoods, 2002. Unpublished working paper, Research on Innovation.\n[3] Z. Bodie, A. Kane, and A. Marcus. Investments, 5th Edition, chapter\n12: Market Efficiency. McGraw-Hill and Irwin, 2002.\n[4] Eric Bonabeau, Marco Dorigo, and Guy Theraulaz. Swarm intelligence:\nfrom natural to artificial systems. Oxford University Press, Inc., New\nYork, NY, USA, 1999.\n[5] Andrea Bonaccorsi and Cristina Rossi. Why Open Source software can\nsucceed. Research Policy, 32(7):1243–1258, 2003.\n[6] Andrea Bonaccorsi and Cristina Rossi. Altruistic individuals, selfish\nfirms? The structure of motivation in Open Source software. First\nMonday, 9(1), 2004.\n[7] P.B. (ed.) Brazdil. Editorial, Machine Learning: Proceedings of the\nEuropean Conference on Machine Learning. Springer, New York, NY,\nUSA, 1993.\n[8] T. Brenner. Local Industrial Clusters, Existence, Emergence and\nEvolution. Studies in Global Competition. Routledge, London, 2004.\n[9] E. Cantu-Paz. A survey of parallel genetic algorithms. Calculateurs\nParalleles, Reseaux et Systems Repartis, 10(2):141–147, 1998.\n[10] K.M. Carley, D.B. Fridsma, E. Casman, A. Yahja, N. Altman, L.-C.\nChen, B. Kaminsky, and D. Nave. Biowar: Scalable agent-based model\nof bioattacks. IEEE Transactions on Systems, Man and Cybernetics,\nPart A, 36(2):252–265, 2006.\n[11] M. Chli, P. De Wilde, J. Goossenaerts, V. Abramov, N. Szirbik, L. Correia, P. Mariano, and R. Ribeiro. Stability of multi-agent systems. In\nProceedings of the 2003 IEEE International Conference on Systems,\nMan, and Cybernetics, pages 551–556, 2003.\n[12] A. Damodaran. Investment Valuation, 2nd Edition, chapter 6: Market\nEfficiency - Theory and Models. Wiley, 2001.\n[13] DBE. Annex I - Description of Work, Digital Business Ecosystem.\nTechnical report, 2002.\n[14] P. De Wilde. Fuzzy utility and equilibria. IEEE Transactions on Systems,\nMan and Cybernetics, Part B, 34(4):1774–1785, 2004.\n[15] P. De Wilde, M. Chli, L. Correia, R. Ribeiro, P. Mariano, V. Abramov,\nand J. Goossenaerts. Adapting populations of agents. Lecture Notes in\nArtificial Intelligence, 2636:110–124, 2003.\n[16] T. Eguchi, K. Hirasawa, J. Hu, and N. Ota. Aa study of evolutionary\nmultiagent models based on symbiosis. IEEE Transactions on Systems,\nMan and Cybernetics, Part B, 36(1):179–193, 2006.\n\n\f11\n\n[17] Joshua M. Epstein. Modeling civil violence: An agent-based computational approach. Proceedings of the National Academy of Sciences,\nU.S.A., 99(10, Supplement 3):7243–7250, 2002.\n[18] R. Falcone and C. Castelfranchi. The human in the loop of a delegated\nagent: the theory of adjustable social autonomy. IEEE Transactions on\nSystems, Man and Cybernetics, Part A, 31(5):406–418, 2001.\n[19] E. F. Fama. Efficient capital markets: A review of theory and empirical\nwork. Journal of finance, 25:383–417, 1970.\n[20] Richard P. Gabriel and Ron Goldman. Open source: Beyond the\nfairytales, 2002. [Online; accessed 25-May-2005].\n[21] David E. Goldberg. Genetic Algorithms in Search, Optimization and\nMachine Learning. Addison-Wesley Longman Publishing Co., Inc.,\nBoston, MA, USA, 1989.\n[22] Simon Grand, Georg von Krogh, Dorothy Leonard, and Walter Swap.\nResource allocation beyond firm boundaries: A multi-level model for\nopen source innovation. Long Range Planning, 37(6):591–610, 2004.\n[23] Nathan Griffiths and Michael Luck. Coalition formation through\nmotivation and trust. In Proceedings of the 2nd international joint\nconference on Autonomous agents and multiagent systems, pages 17–\n24, New York, NY, USA, 2003. ACM Press.\n[24] John Hagedoorn. Inter-firm R&D partnerships: an overview of major\ntrends and patterns since 1960. Research Policy, 31(4):477–492, 2002.\n[25] J. H. Holland. Adaptation. Progress in Theoretical Biology, 4:263–293,\n1976.\n[26] Chun-Che Huang. Using intelligent agents to manage fuzzy business\nprocesses. IEEE Transactions on Systems, Man and Cybernetics, Part\nA, 31(6):508–523, 2001.\n[27] Z. Jing, E. Billard, and S. Lakshmivarahan. Learning in multilevel games\nwith incomplete information. ii. IEEE Transactions on Systems, Man\nand Cybernetics, Part B, 29(3):340–349, 1999.\n[28] Justin Pappas Johnson. Open source software: Private provision of\na public good. Journal of Economics and Management Strategy,\n11(4):637–662, 2002.\n[29] J. O. Kephart. Software agents and the route to information economy.\nProceedings of the National Academy of Sciences, U.S.A., 99(10, Supplement 3):7207–7213, 2002.\n[30] Jeffrey O. Kephart, James E. Hanson, and Jakka Sairamesh. Price and\nniche wars in a free-market economy of software agents. Artificial. Life,\n4(1):1–23, 1997.\n[31] Alan P. Kirman and Nicolaas J. Vriend. Evolving market structure:\nAn ACE model of price dispersion and loyalty. Journal of Economic\nDynamics and Control, 25(3):459–502, 2001.\n[32] Blake LeBaron.\nBuilding the Santa Fe Artificial Stock\nMarket.\nWorking Paper, June 2002.\nAvailable at\nhttp://www.econ.iastate.edu/tesfatsi/blake.sfisum.pdf.\n[33] Josh Lerner and Jean Tirole. Some simple economics of open source.\nJournal of Industrial Economics, 50:197–234, June 2002.\n[34] Christoph H. Loch and Bernardo A. Huberman.\nA punctuatedequilibrium model of technology diffusion. Management Science,\n45(2):160–177, 1999.\n[35] M. Luck, P. McBurney, and C. Preist. Agent Technology: Enabling\nNext Generation Computing (A Roadmap for Agent Based Computing).\nAgentLink, 2003.\n[36] J. L. Ribeiro Filho, P. C. Treleaven, and C. Alippi. Genetic-algorithm\nprogramming environments. Computer, 27(6):28–43, 1994.\n[37] R. Savit, R. Manuca, and R. Riolo. Adaptive competition, market\nefficiency and phase transitions. Physical Review Letters, 82(10):2203–\n2206, 1999.\n[38] Klaus Schmidt and Monika Schnitzer. Public Subsidies for Open\nSource? Some Economic Policy Issues of the Software Market. Technical Report 3793, C.E.P.R. Discussion Papers, February 2003. Available\nat http://ideas.repec.org/p/cpr/ceprdp/3793.html.\n[39] Kwang Mong Sim and Eric Wong. Toward market-driven agents for\nelectronic auction. IEEE Transactions on Systems, Man and Cybernetics,\nPart A, 31(6):474–484, 2001.\n[40] M. Sysi-Aho, A. Chakraborti, and K. Kaski. Searching for good\nstrategies in adaptive minority games. Physical Review E, 69(3):36125–\n1–36125–7, 2004.\n[41] Leigh Tesfatsion. Handbook of Computational Economics, Vol. 2: AgentBased Computational Economics, chapter 1, Agent-Based Computational Economics: A Constructive Approach to Economic Theory. NorthHolland, 2005. To appear.\n[42] H. Van Dyke Parunak, Sven Brueckner, and Robert Savit. Universality\nin multi-agent systems. In AAMAS ’04: Proceedings of the 3rd\nInternational Joint Conference on Autonomous Agents and Multiagent\nSystems, pages 930–937, Washington, DC, USA, 2004. IEEE Computer\nSociety.\n\n[43] T. Yamasaki and T. Ushio. An application of a computational ecology\nmodel to a routing method in computer networks. IEEE Transactions\non Systems, Man and Cybernetics, Part B, 32(1):99–106, 2002.\n\n\f"
        ],
        [
         "15",
         "15",
         "cs.CE",
         "Computational Engineering",
         "1307.7757v4.pdf",
         "Household Electricity Consumption Data Cleansing\nGuoming Tang\n\nKui Wu\n\nJian Pei\n\nUniversity of Victoria, Victoria,\nBC, Canada\n\nUniversity of Victoria, Victoria,\nBC, Canada\n\nSimon Fraser University,\nBurnaby, BC, China\n\nguoming@uvic.ca\n\nwkui@uvic.ca\n\nJiuyang Tang\nNational University of Defense\nTechnology, China\n\narXiv:1307.7757v4 [cs.CE] 19 May 2014\n\njiuyang_tang@nudt.edu.cn\nABSTRACT\nLoad curve data in power systems refers to users’ electrical energy\nconsumption data periodically collected with meters. It has become\none of the most important assets for modern power systems. Many\noperational decisions are made based on the information discovered in the data. Load curve data, however, usually suffers from\ncorruptions caused by various factors, such as data transmission errors or malfunctioning meters. To solve the problem, tremendous\nresearch efforts have been made on load curve data cleansing. Most\nexisting approaches apply outlier detection methods from the supply side (i.e., electricity service providers), which may only have\naggregated load data. In this paper, we propose to seek aid from\nthe demand side (i.e., electricity service users). With the help of\nreadily available knowledge on consumers’ appliances, we present\na new appliance-driven approach to load curve data cleansing. This\napproach utilizes data generation rules and a Sequential Local Optimization Algorithm (SLOA) to solve the Corrupted Data Identification Problem (CDIP). We evaluate the performance of SLOA\nwith real-world trace data and synthetic data. The results indicate\nthat, comparing to existing load data cleansing methods, such as Bspline smoothing, our approach has an overall better performance\nand can effectively identify consecutive corrupted data. Experimental results also demonstrate that our method is robust in various\ntests. Our method provides a highly feasible and reliable solution\nto an emerging industry application.\n\n1.\n\nINTRODUCTION\n\nElectricity usage data, on the one hand, plays an important role\nin big data applications, and on the other hand, has been severely\nunder explored. A recent news article appeared in Forbes [22] said,\n“But for the most part, utilities have yet to realize the potential of\nthe flood of new data that has begun flowing to them from the power\ngrid, . . . , And in some cases, they may not welcome it.” Yet, existing power grid is facing challenges related to efficiency, reliability,\nenvironmental impact, and sustainability. For instance, the low efficiency of current electric grid could lead to 8% of electric energy\n\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot made or distributed for profit or commercial advantage and that copies\nbear this notice and the full citation on the first page. To copy otherwise, to\nrepublish, to post on servers or to redistribute to lists, requires prior specific\npermission and/or a fee.\nCopyright 20XX ACM X-XXXXX-XX-X/XX/XX ...$15.00.\n\njpei@cs.sfu.ca\nJingsheng Lei\n\nShanghai University of Electric\nPower, China\n\njshlei@shiep.edu.cn\n\nloss along its transmission lines, and the maximum generation capacity is in use only 5% of the time [13].\nThe emerging smart grid technology aspires to revolutionize traditional power grid with state-of-the-art information technologies\nin sensing, control, communications, data mining, and machine\nlearning [6, 13]. Worldwide, significant research and development\nefforts and substantial investment are being committed to the necessary infrastructure to enable intelligent control of power systems,\nby installing advanced metering systems and establishing data communication networks throughout the grid. Consequently, power\nnetworks and data communication networks are envisioned to harmonize together to achieve highly efficient, flexible, and reliable\npower systems.\nAmong the various types of data transmitted over the smart grid,\nload curve data, which refers to the electric energy consumption periodically recorded by meters at points of interest across the power\ngrid, has become the critical assets for utility companies to make\nright decisions on energy generation, billing, and smart grid operations. Load curve data, which “is beginning to give us a view of\nwhat the customer is actually experiencing, something that we’ve\nnever ever seen before” [22], is precious user behavior data, and is\nan important type of big data.\nLoad curve data collected and reported from smart meters at endusers’ premises is especially important for both energy supply and\nenergy demand sides. On the demand side, it has direct impact on\ncustomers’ energy bills and their trust on the still nascent smart grid\ntechnology. On the energy supply side, inaccurate load data may\nlead to large profit losses and wrong business decisions. In 2012,\n126.8 million residential customers in the US used over 1, 374 billion kWh, which counts to over 33% of the total electric energy in\nthe US [11]. The importance of this huge amount of energy and its\nfinancial implication cannot be over emphasized.\nNevertheless, it is unavoidable that load curves contain corrupted data and missing data, caused by various factors, such as\nmalfunctioning meters, data packet losses in wireless networks, unexpected interruption or shutdown in electricity use, and unscheduled maintenance [5]. Due to the huge volume of load curve data, it\nis hard for utilities to manually identify corrupted load curve data.\nUnfortunately, problems caused by corrupted data are usually realized only after it is too late, such as after a customer receiving a\nsuspicious yet hard to rebut high energy bill.\nAs a concrete example, according to the news reports [19, 23],\nsome customers in the province of British Columbia, Canada, were\nbaffled by energy bills that are more than double what they were\ncharged before the smart meter installation. While the problem\ncould be identified by common sense and certain agreement might\nbe reached by good faith negotiations [19, 23], fixing the ques-\n\n\fState\n\nAggregated Power Range\n\n2~4\n\nA2\n\n10~12\n\n12 10 4\n\nA3\n\n30~32\n\nA1+A2\n\n12~16\n\nA1+A3\n\n32~36\n\nA2+A3\n\n40~44\n\nA1+A2\n+A3\n\n42~48\n\n2\n\n4\n\nA2\n10\n\n12\n\nSmart Meter\n\nA3\n30\n\n32\n\nHidden\nCorrupted Data\n\n44 42 40 36\n\nA1\n\n32 30\n\nA1\n\n16\n\n2\n\n0\n\nIndividual Power Range\n\n48\n\ntionable bill is another head-scratching and embarrassing issue to\nthe utility. As a response to customer complaints, the utility normally took remedy actions, such as replacing the smart meters or\ntaking back the smart meters for lab testing [23]. Such a remedy,\nhowever, can hardly be effective. According to CBC News [24],\n“Government estimates indicate there are about 60,000 smart meter holdouts (in the province).” Overall, the users and the utility\ncompany have the well-aligned interest and should work together\nto tackle this critical problem plaguing the electric power industry.\nTechniques of load data cleansing have been proposed to deal\nwith load data corruption problem recently [5]. Most existing load\ndata cleansing methods are designed for the supply side (i.e., electricity service providers), to help the utility companies find the corrupted data and protect their profits. From the supply side, the collected load data is usually aggregated data, i.e., the energy consumption of a billing unit such as a house or a commercial building. When performing data cleansing on the supply side, due to\nthe difficulty of obtaining extra knowledge behind the aggregated\nload data, most existing approaches apply outlier detection methods, i.e., the data that deviates remarkably from the regular pattern is identified as corrupted data. Various assumptions about the\ndata generation mechanism are required for outlier detection, but\ndue to limited information, those assumptions are usually based on\nempirical knowledge or statistic features of the data. Such outlier\ndetection methods are oblivious of appliances’ various energy consumption models and may not be accurate or fair to customers. We\ncall these methods appliance-oblivious. Such methods suffer from\na few important deficiencies.\nFor example, the regression-based outlier detection methods find\nstatistical patterns of load data and claim the data significantly deviating from the patterns as corrupted data. Nevertheless, such\nresulted outliers are not necessarily corrupted data. In addition,\nwithout the knowledge of appliances’ energy consumption models,\nsome “hidden\" corrupted data is hard to detect. To be specific, the\nenergy consumption of a group of appliances in a house or a building is a stochastic process. The stochastic feature makes it hard\nto establish a fixed pattern. Turning on/off any high-power appliance may lead to a steep change in load curve. Using applianceoblivious data cleansing methods, the data generated under such a\ncondition is likely to be captured as outliers.\nAs another example, appliance-oblivious methods cannot deal\nwith “hidden” corrupted data. Fig. 1 shows an example of\nthree appliances, A1 , A2 , and A3 , which have power ranges of\n[2, 4], [10, 12] and [30, 32], respectively. The load data within some\nranges such as (4, 10), (16, 30), and (36, 40) cannot be generated\nby any combination of the three appliances. Nevertheless, such data\nmay not be identified by existing outlier detection as corrupted data.\nRecently, with the emergence of fine-grained in-house energy\nmonitoring systems, customers now have the capability to monitor their own energy usage more closely and more accurately [27].\nAs such, users may possess more knowledge behind the data, e.g.,\nthe decomposition of total energy consumption according to main\nappliances. Even if in-house energy monitoring system is not available, users should know the rated power of appliances’, which are\neasily accessible from the appliances’ manual, technical specification or public websites, such as [12]. This knowledge presents new\nopportunities to perform load data cleansing on the customer side\ndirectly or on the supply side with auxiliary information from customers. This new angle of tackling the corrupted load data problem\ncan greatly improve the quality of load data.\nIn this paper, we tackle the practical problem of corrupted load\ncurve data identification in the industry context by developing an\nappliance-driven approach. Specifically, we make the following\n\nFigure 1: An example showing hidden corrupted data generated with three appliances\n\ncontributions:\n• First, we define a new criterion in identification of corrupted\nload data. The criterion is aware of domain knowledge, including the power range of appliances and the physical laws\nbehind valid load data.\n• Second, we formally formulate the Corrupted Data Identification Problem (CDIP) and establish an optimization model\nto solve the problem. Furthermore, we introduce a new concept, called virtual appliance, in the objective function to\nhelp record corrupted data. Our empirical study in a proofof-concept electricity usage test environment shows that a\nsolution to the optimization problem is capable of precisely\nidentifying the corrupted data, even without obtaining the exact on/off states of appliances. This nice feature indicates\nthat our method is both effective and robust.\n• We develop a sequential local optimization algorithm\n(SLOA) to approach CDIP efficiently. SLOA focuses on\nsolving CDIP in a smaller time window, and considers the\ncorrelation between consecutive small windows. While we\nshow that the original CDIP problem is NP-complete, our\nSLOA method offers an efficient heuristic solution and can\nachieve a very high detection precision. As an extra benefit,\nby applying the sequential optimization algorithm, we can\neasily handle consecutive corrupted data.\nThe rest of the paper is organized as follows. In Section 2, we\nreview the related work. In Section 3, we formally formulate the\ncorrupted data identification problem (CDIP). To solve CDIP, we\ndevelop an optimization model in Section 4. As CDIP is proven to\nbe NP-complete, we develop SLOA to find an approximate solution\nin Section 5. We evaluate the performance of our method with\nreal-world trace data and synthetic data in Section 6 and Section 7,\nrespectively, and test the robustness of SLOA in Section 8. The\npaper is concluded in Section 9.\n\n2.\n\nRELATED WORK\n\nMost related literature treats the corrupted data the same as outliers in load pattern and focuses on outlier detection. A broad spectrum of techniques for outlier detection in load data have been developed, which include regression-based time series analysis, uni-\n\n\fvariate statistical methods, and data mining techniques. We review\nthem briefly here.\n\n2.1\n\nRegression-Based Time Series Analysis\n\nRegression-based time series analysis is the most widely used\napproach for outlier detection in load data [1, 5, 20, 21]. Mateos\nand Giannakis [21] developed a nonparametric regression method\nthat approximates the regression function via `0 -norm regularization. Chen et al. [5] proposed a nonparametric regression method\nbased on B-spline and kernel smoothing to identify corrupted data.\nAbraham and Chuang [1] analyzed residual patterns from some regression models of time series and used the patterns to construct\noutlier indicators. They also proposed a four-step procedure for\nmodeling time series in the presence of outliers. Greta et al. [20]\nconsidered the estimation and detection of outliers in time series\ngenerated by a Gaussian auto-regression moving average (ARMA)\nprocess, and showed that the estimation of additive outliers is related to the estimation of missing observations. ARMA is also utilized in [15, 2, 1, 26] as a fundamental model to identify outliers.\n\n2.2\n\nUnivariate Statistical Methods\n\nUnivariate statistical methods are another type of techniques for\noutlier detection. Univariate statistical methods deal with outliers\nin load data by processing load data as one-dimensional real values [14, 8, 16, 9]. Most univariate methods for outlier detection\nassume an underlying a prior distribution of data. The outlier detection problem is then translated to finding those observations that\nlie in the so-called outlier region of the assumed distribution, which\nis defined by a confidence coefficient value [9]. Since the statistical\nmethods are susceptible to the number of exemplars, a simple but\neffective method named Boxplot or IQR is proposed in [28] to deal\nwith small-sized exemplars.\n\n2.3\n\nData Mining Techniques\n\nIn addition to the above methods, data mining techniques are also\napplied to identify outliers, such as k-nearest neighbor [25, 17], kmeans, k-medoids [4], and DBSCAN [18]. As a type of clustering\nmethods, they group data with similar features, and identify data\nitems that do not strongly belong to any cluster or far from other\nclusters as outliers. Recently, Aggarwal [3] provided a thorough\nsurvey on outlier detection.\nNevertheless, all the above methods do not consider the special\nphysical laws behind the load data. Regression-based methods assume that the data follows a certain pattern, which can be modeled\nby a function governed by a set of parameters; univariate methods\nassume that the data is sampled from a certain known distribution;\nclustering methods assume that the data is well structured as clusters and the corrupted data deviates significantly from the normal\nstructure. Obviously, the underlying assumptions in the existing\nmethods are quite general and do not capture the specific features\nof load data well. Our paper fills the gap and differs from the existing literature by offering a completely new angle to address the\nload data corruption problem.\n\n3.\n\nTHE CORRUPTED DATA IDENTIFICATION PROBLEM\n\nIn this section, we present a formal problem definition. Before\nthat, we first describe an energy consumption model and discuss\nthe generation rules of load data.\n\n3.1\n\nEnergy Consumption Model\n\nLoad data is time series data that records users’ energy consumption. It is collected by smart meters periodically at a certain sam-\n\npling frequency. Without loss of generality, we assume that the time\nis slotted, with each timeslot equal to the sampling interval time. In\nthe rest of the paper, we thus use the terms “time”, “timeslot” and\n“sampling interval” interchangeably.\nWe denote the load data from timeslot t = 1 to timeslot t = n\nin a column vector as\nY ≡ [y1 , y2 , · · · , yn ]T ,\n\n(1)\n\nwhere each value yi in the vector represents the aggregated energy\nconsumption of all appliances in a property, say a house at timeslot\ni. The energy consumption at a time instant depends on the appliances’ on-off states and their individual power level.\nWe assume that a house includes m appliances in total, and the\npower of the k-th appliance is pk (watts). At any time instant, if\nwe record the power level of each individual appliance, we can\ndefine an m dimensional column power vector to capture energy\nconsumption of the house:\nP ≡ [p1 , p2 , · · · , pm ]T .\n\n(2)\n\nNote that the power level of an appliance normally does not remain at a fixed value but changes in a certain range. For this reason,\nwe define two m dimensional column vectors, denoted as Pl and\nPu , respectively:\nPl = [l1 , l2 , · · · , lm ]T\nT\n\nPu = [u1 , u2 , · · · , um ] ,\n\n(3)\n(4)\n\nwhere li and ui represent the lower and upper bounds of the power\nlevel of the i-th appliance, respectively. A power vector P is called\nvalid if, for each value pi in P , li ≤ pi ≤ ui .\nAt any instant, the state of an appliance may be either on or off.\nWe use an n × m 0-1 state matrix, S = [Sij ]n×m , to record\nthe states of the m appliances from time t = 1 to t = n, where\nSi,k = 1 indicates that the k-th appliance is on at time i, and 0\notherwise. In addition, we call the i-th row of S a state vector at\ntime i, denoted by:\nSi ≡ [Si,1 , Si,2 , · · · , Si,m ].\n\n3.2\n\n(5)\n\nGeneration Rules of Load Data\n\nWe have the following observations. First, a valid load data element yi (in watt-hours) should be equal to the inner product of the\nstate vector and the power vector at t = i, multiplied by the sampling interval time. This is a basic physical law for load curve data\ngeneration. Second, since the sampling interval is typically small\n(e.g., 10 seconds1 ), we assume that the probability that an appliance\nhas more than one on-off switch events during a timeslot is negligible. In addition, the total number of on-off state switches of all\nappliances during a timeslot should be small. This feature is called\nthe temporal sparsity of on-off state switching events. Intuitively,\nthis feature means that in normal operation it is unlikely that the\nhousehold turns on-off many appliances in a short time. Based on\nthe above observations, we can define the generation rules of load\ndata.\nD EFINITION 1 (G ENERATION RULES ). Assume that the initial state of appliances is S0 . We claim that each valid load data,\nyi , must satisfy the following rules:\n\u001a\n\nSi · Pl /f ≤ yi ≤ Si · Pu /f\nkSi − Si−1 k1 ≤ δ,\n\n(6)\n\n1\nResidential smart meters can support sampling rate as high as 1\nsample per second [29].\n\n\fwhere f is data sampling frequency, 1 ≤ i ≤ n, and δ is the upper\nbound on the total number of on-off state switches for m appliances\nduring a sampling interval.\nNote that the energy consumption value (watt-hours) is calculated with power value (watt) multiplied by time 1/f (hour). To\nkeep our discussion simple, we assume that a valid initial state vector S0 is given at this moment. We will relax this assumption later\nand show that the impact of an inaccurate initial state vector quickly\nbecomes negligible as long as the system runs for just a little while\n(Section 8).\n\n3.3\n\n5.\n\nProblem Definition\n\nBased on the above generation rules, corrupted data is the values\nthat break any of the rules.\nD EFINITION 2 (CDIP). The corrupted data identification\nproblem (CDIP) is, given load data Y = {y1 , y2 , · · · , yn }, power\nbound vectors Pl , Pu , and a sampling frequency f , find corrupted\ndata items that violate any of the generation rules, i.e., C ≡ {yi :\nyi violates (Equation 6), for 1 ≤ i ≤ n}.\n\n4.\n\nsolution is also the sparsest solution (i.e., resulting in the minimal\nnumber of non-zero values of vi ) [10]. In addition, a larger vi value\nmeans that a corrupted yi is farther away from a valid range. In this\nsense, the value of vi can be also regarded as the corrupted degree\nof yi .\nThe problem can be proven NP-complete (refer to Appendix A).\nBy investigating the special structure of the problem, however, we\ncan develop an effective heuristic algorithm introduced in the next\nsection.\n\nAN IMPORTANT\nSOLVING CDIP\n\nSTEP\n\nTOWARDS\n\nTo solve CDIP, a naïve idea is to find all the solutions satisfying the constraints in (Equation 6), by brute-force, exhaustive\nsearch for all possible appliance states. This method is very timeconsuming. Even for a small data set it is very costly to find the\nanswer. Since the generation rules can be considered as constraints\nin an optimization problem, we will show how the problem can be\ntransformed to an optimization problem, which sheds light on a fast\nsolution to an approximate problem.\n\nIn this section, we propose a Sequential Local Optimization Algorithm (SLOA) and develop a quantitative strategy to estimate the\nminimum local window size.\n\n5.1\n\nStep 1. Consider a small time window with size of w, 1 ≤ w < n,\nwhich starts from time k to time k + w − 1. Given the state\nvector at time k − 1, i.e., Sk−1 , we consider the following\noptimization problem:\nminimize\nSi ,vi\n\nsubject to\n\nsubject to\n\n(Si · Pl + vi ) /f ≤ yi ≤ (Si · Pu + vi ) /f\n(8)\n\n1≤i≤n\n1≤j≤m\nTo understand the rationale behind the formulation of Equation (8), it is worthwhile to point out that vi ∈ V will come into\nplay whenever Si cannot satisfy the generation rules, i.e., the virtual appliance is “turned on” when the load data yi is corrupted.\nThus, vi essentially makes a record to the corrupted data. After\nobtaining the final solution to Equation (8), the vi variables with\nnon-zero values indicate the corrupted data, i.e.,\nC = {yi : vi 6= 0 for 1 ≤ i ≤ n}.\n\n(9)\n\nWe try to minimize `1 -norm, because it is proven that for most large\nunder-determined systems of linear equations the minimal `1 -norm\n\n(Si · Pl + vi ) /f ≤ yi ≤ (Si · Pu + vi ) /f\nkSi − Si−1 k1 ≤ δ\n\n(10)\nBy setting w \u001c n, we can significantly reduce the search\nspace. Actually, we can show that the computational comw\nplexity\nthe above\n\u0001 tomsolve\n\u0001\n\u0001 problem is O(M ), where M =\nm\nm\n+ 1 + · · · + δ (refer to Appendix B). Since m is\n0\nthe total number of appliances and δ \u001c m, the problem can\nbe solved quickly using tools such as CVX 2.0 with a Gurobi\nengine [7].\n\nkV k1\nkSi − Si−1 k1 ≤ δ\nSi,j ∈ {0, 1}\n\n|vi |\n\ni=k\n\n1≤j≤m\n\n(7)\n\nBy introducing the virtual appliance, we can develop the following optimization model to solve CDIP:\nSi ,vi\n\nk+w−1\nX\n\nSi,j ∈ {0, 1}\nk ≤i≤k+w−1\n\nwhere vi ∈ (−∞, +∞) denotes the virtual power at time t = i.\n\nminimize\n\nSLOA\n\nThe temporal sparsity of corrupted load data suggests that we\ncan perform optimization in a smaller, local time window. By considering the correlation between consecutive timeslots, we design\na Sequential Local Optimization Algorithm (SLOA). Without loss\nof generality, we take a load data from time t = 1 to t = n as an\nexample to show the major steps of SLOA.\n\nD EFINITION 3 (V IRTUAL A PPLIANCE ). Besides the real appliances, we introduce a virtual appliance into the system. Its associated power is called virtual power, and we record the values of\nvirtual power from time t = 1 to t = n in a virtual power vector\nV ≡ [v1 , v2 , · · · , vn ]T ,\n\nSEQUENTIAL LOCAL OPTIMIZATION\nALGORITHM\n\nStep 2. For the k-th time window that starts from time k, we use\nthe following strategy to handle consecutive corrupted data:\nif the data point at time k is identified to be corrupted, i.e.,\nvk 6= 0, recover the current state vector from the previous\none, i.e., set Sk = Sk−1 .\nStep 3. Repeat Step 1) and Step 2) from k = 1 to k = n, and\nsolve problems in form of Equation (10) sequentially. After\nn iterations, we can get a sequential solution v1 , v2 , · · · , vn .\nThus, the corrupted data set is C = {yi : vi 6= 0, 1 ≤ i ≤\nn}, in which vi is the corrupted degree of load data yi .\nAlgorithm 1 shows the pseudo code of SLOA.\n\n5.2\n\nEstimation of Minimum Local Window\nSize\n\nClearly, one key question in SLOA is to determine a suitable size\nof the local window. In principle, we want the size to be as small\n\n\fAlgorithm 1 Sequential Local Optimization Algorithm\nInput: Load data {y1 , y2 , · · · , yn }, power bounds Pl , Pu , initial\nstate S0 , sampling frequency f , local time window size w.\nOutput: Corrupted data set C, corrupted degree vi , 1 ≤ i ≤ n\n1: v0 = 0\n2: C = ∅\n3: for k = 1 : n do\n4:\nSolve Problem (Equation 10), and obtain vi and Si where\nk ≤i≤k+w−1\n5:\nif vk 6= 0 then\n6:\nC = C ∪ {yk }\n7:\nSk = Sk−1\n8:\nend if\n9: end for\n10: return C, {v1 , v2 , · · · , vn }\n\nas possible to speed up the calculation, but a size too small may\nresult in a poor solution largely deviating from the global optimal\none. For example, in the extreme case of w = 1, SLOA becomes\na simple greedy search algorithm, where the final solution may not\nbe good. On the other hand, if w = n, the problem becomes the\nsame as Equation (8), which is hard to solve. What is the minimum\nlocal window size that leads to a nearly global optimal solution?\nSince it is hard to obtain a strict proof that the local optimal\nsolutions together lead to the global optimal solution, we use the\nfollowing heuristics to estimate the minimum local window size:\nwithin the local window, it should be possible that one state vector\ncan be transited to any other state vectors in the vector space. In\nother words, within the local window, we should cover all possible\nstate vectors in the search. This heuristics sheds light on finding the\nminimum local window size, as formulated below.\nD EFINITION 4 (OVERLAP I NDEX ). Consider m appliances\ndenoted by a set {R1 , R2 , · · · , Rm }, where Ri ≡ [li , ui ] ⊂ < and\nrepresents the i-th appliance’s power range. The overlap index of\nm appliances is defined as:\nPm R pmax\ni=1\n\nO ≡ R Pmax\n\npmin\n\nI (Ri ∩ {x})dx\n\nI((∪m\ni=1 Ri )\nPmin\n\n∩ {x})dx\n\n(11)\n\nwhere pmax and pmin stand for the maximum and minimum power\nof all appliances, respectively, and I(x) is an indicator function\n\u001a\n1 , x 6= ∅\nI(x) =\n(12)\n0,x=∅\n\na timeslot, in order to get the nearly global optimal solution to\nEquation (8)\nvia\u0007Equation (10), the minimum local window size\n\u0006 m\n, 1}.\nw = max{ δ·O\nP ROOF. We prove the following condition holds: within the local window, we can cover all possible state vectors in the search.\nFirst, the value of w relates to upper bound δ(≤ m) on the total\nnumber of on-off switches in one timeslot. It is obvious that from\ntime t = i to t = i + 1, the state vector Si can only change to\nanother state vector Si+1 , with kSi+1 − Si k1 ≤ δ. If δ = m, then\nwithin one step, a state vector is allowed to change to any other\nstate vector. On the other hand, if δ = 1, within one step, a state\nvector can only change one value in the vector.\n\u0006 \u0007 In other words,\nfrom one state vector, it requires at least m\ntimeslots to reach\nδ\n\u0006 \u0007\nany other state vector in the state vector space, i.e., w ≥ m\n.\nδ\nSecond, the overlap index O can reduce the value of w. Based on\nthe meaning of O, m appliances with overlap index O are equivalent to [m/O] appliances without\n\u0006 m \u0007 overlapped power. Replace m\n. Considering w ≥ 1, we conwith m/O, we can get w ≥ δ·O\nclude:\nl m m\nw ≥ max{\n, 1}.\n(13)\nδ·O\n\u0006 m \u0007\nSince we want w to be as small as possible, w = max{ δ·O , 1}.\n2\nPlease note that, although the minimum local window size obtained above is an estimation, it works effectively in our experiments with real-world data as well as with synthetic data.\n\n5.3\n\nAlgorithm Analysis\n\nGiven n load values, m appliances, and the upper bound δ(≤\nm) on the total number of on-off state switches in a timeslot, the\ncomputational complexity of\nproblem\n\u0001 themoriginal\n\u0001\n\u0001 (Equation 8) is\nO(M n ), where M = m\n+ 1 + ··· + m\n. Using SLOA,\n0\nδ\nsolving the optimization problem (Equation 10) for n times results\nin the time complexity of O(n · M w ), where w ∈ Z + and w \u001c n\n(refer to Appendix B). Please note that the appliance number m is\na constant value and w is also a small constant.\nObviously, the larger the value of w, the higher the computational complexity. Fortunately, the overlap index of appliances in a\nhouse or a building is usually high, as observed in our real-world\nexperiment testbed. This fact allows us to select a small local window size following Lemma 1. Therefore, in the application scenarios, SLOA can approach the NP-complete problem heuristically\nand efficiently. We will show that this algorithm indeed can provide\na good solution with abundant experimental results in the following\nsections.\n\nRP\nNote that the denominator P max I((∪m\ni=1 Ri ) ∩ {x})dx inmin\ncludes all valid power values, i.e., the ones that can be covered by\nat least one appliance’s power range. We can see that the overlap\nindex represents the number of appliances whose power range covers a valid power value, averaged over the whole power range of all\nappliances. In particular, O = 1 indicates that no pair of appliances\nhave overlapped power, and O = m means that all appliances have\nthe same power range. Intuitively, when O is large, there is a good\nchance of finding multiple local optimal solutions to Equation (10),\nsince there are multiple equivalent choices to turn on/off appliances\nin each iteration.\nWith the heuristics in estimating the minimum local window\nsize, we have the following result.\n\nWe evaluate our method with real-world trace data from a realworld energy monitoring platform. We monitored the appliances’\nenergy consumption of a typical laboratory and a lounge room on\nthe fifth floor of the Engineering/Computer Science Building at the\nUniversity of Victoria (UVic) for two months. The real-time power\nof laptops, desktops and some household appliances was recorded.\nEach appliance’s power level was measured every 10 seconds and\nthe measurement results were transmitted with ZigBee radio to a\nserver that stores the data. The monitored appliances and their regular power2 are shown in Fig. 2.\n\nL EMMA 1. Given the overlap index O of m appliances and\nthe upper bound δ on the total number of on-off state switches in\n\n2\nAn appliance’s regular power is an approximate range around the\nrated power where this appliance works.\n\n6.\n\nEXPERIMENTAL\nREAL DATA\n\nEVALUATION\n\nON\n\n\f5\n\n6\n\n7\n\n4\n\n3\n\ncision is the ratio of the number of correctly detected corrupted\nvalues over the total number of detected values; recall is the ratio\nof the number of correctly detected values over the number of prelabeled corrupted values; and the F-measure is a harmonic mean of\nprecision and recall, i.e.,\n\n9\n\n8\n\n1\n\n2\n\n11\n12\n\nAppliances List\nID\n1~4\n5~7\n8\n9\n10\n11\n12\n13\n\n13\n\n10\n\nName\nDesktop (type 1)\nDesktop (type 2)\nLaptop\nPrinter\nRefrigerator\nDrinking fountain\nMicrowave\nCoffee maker\n\nRegular Power(Watt)\n[80, 150]\n[40, 80]\n[30, 60]\n[800, 1000]\n[100, 300]\n[100, 500]\n[800, 1200]\n[500, 800]\n\nFigure 2: Energy monitoring platform and appliances’ power\nranges\n\n0.015\n\nLoad / kWh\n\n0.01\n\n0.005\n\n0\n\n0\n\n0.864\n\n1.728\n\n2.592\n\n3.456\n\n4.32\n\n5.184\n\n6.048\n\nTime / 10s\n\n4\n\nx 10\n\n0.012\n\n0.01\n\nLoad / kWh\n\n2 · P recision · Recall\n.\n(14)\nP recision + Recall\nFor comparison, we implement and test an appliance-oblivious\ndata cleansing method, B-spline smoothing, which is introduced\nin [5] to identify corrupted load data. In the B-spline smoothing\nmethod, we set the confidence coefficient α = 0.05, which results\nin a confidence interval of 95%. We treat the degree of freedom\n(df ) as a variable, whose value is trained when smoothing the load\ncurve data. For our method, the overlap index is obtained as O ≈ 2,\nand the upper bound of on-off switching events of appliances within\nthe sampling interval is set to 2, i.e., δ = 2. According to Equation (13), the local window size, i.e., the value of w in Algorithm 1,\nis set to 3. Since the value of local window size is an estimation,\nin order to obtain more comprehensive performance evaluation for\nour method, we also vary the local window size in a range.\nTable 1 summarizes some of the results from the two methods.\nFurthermore, Fig. 4 and Fig. 5 illustrate one of the outcomes from\nour appliance-driven method and the B-spline smoothing method,\nrespectively. From the results, we have the following interesting\nobservations.\nF-measure =\n\n0.008\n\n• Comparing to B-spline smoothing, our method performs\nmuch better in Precision, but worse in Recall. This shows\nthat our method can identify the corrupted data more accurately, even though our output does not cover the completed\nset of all corrupted data. In addition, our appliance-driven\nmethod achieves a higher F-measure. F-measure reflects a\nbalanced mean between precision and recall, indicating that\nour method has overall better performance.\n\n0.006\n\n0.004\n\n0.002\n\n0\n\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\n5000\n\n6000\n\n7000\n\n8000\n\nTime / 10s\n\nFigure 3: One-week and one-day load data collected via the\nenergy monitoring platform.\n\nWe test the data day by day over the two-month period. Fig. 3\ndemonstrates one-week and one-day load data collected by our\nplatform. For clear illustration, we only show one-day data as an\nexample. Note that even in a lab setting like ours, there indeed exists some apparent corrupted data, indicated by the dashed red dots\nin Fig. 33 .\nIn order to introduce more corrupted data, we asked three students to distort the load data with “falsification”, i.e., they were\nasked to arbitrarily modify the aggregated load data within the\nrange of [0, ∞). These changed data together with the original\ncorrupted ones were labeled and used as the ground-truth to verify\nthe performance of our method.\nSince the existing appliance-oblivious load data cleansing methods, such as B-spline smoothing, detect outliers and consider outliers as corrupted data, we use the terms “outliers” and “corrupted\ndata” interchangeably hereafter. For outlier detection, four statistical results can be obtained: (1) true positive (T P ), the number\nof points that are identified correctly as outliers; (2) false positive\n(F P ), the number of points that are normal but are identified as\noutliers; (3) true negative (T N ), the number of points that are normal and are not identified as outliers; (4) false negative (F N ), the\nnumber of points that are outliers but are not identified. Using\nT P, F P, T N and F N , we evaluate the following three broadlyused performance metrics: precision, recall, and F-measure. Pre3\n\nThe corrupted data mainly comes from some incorrect power values from the laptop that occasionally reports impossible values\nsuch as hundreds of Watts.\n\n• The performance of our method remains roughly the same\nwhen the local window size is beyond the minimum value\nestimated using Lemma 1. Further increase of the local window size does not bring clear performance gain but with a\ncost in longer running time. This suggests that our previous\nestimation on the minimum local window size for SLOA is\nappropriate.\n\n7.\n\nEVALUATION ON SYNTHETIC DATA\n\nTo thoroughly test our method, we evaluate its performance using large-scale synthetic data that simulates a large number of appliances and much diverse energy patterns. With different synthetic\ndatasets, we can also test the robustness of our method.\n\n7.1\n\nLoad Data Generation via Monte Carlo\nSimulation\n\nThere is no standard model for the load curve data of a house,\nsince the data actually results from a complex process related to\nhuman activities. We thus use the Monte Carlo simulation to generate the load data using the following method:\n• Given the lowest appliance power (Pmin ) and the highest appliance power (Pmax ), the lower bound of an appliance (pl )\nis a random variable uniformly distributed between Pmin\nand Pmax . The upper bound of the appliance (pu ) is determined by a parameter called power range ratio (r) and is calculated by pu = min{pl +random([0, rpl ]), Pmax }, where\nrandom([0, rpl ]) returns a random number uniformly distributed in the range [0, rpl ].\n\n\fTable 1: Results of corrupted data identification on real data: our appliance-driven approach vs. B-spline smoothing\nappliance-driven approach\nw=1\nw=2\nw=3\nw=5\n89.29% 95.83% 85.29% 84.38%\n50.00% 46.00% 58.00% 54.00%\n64.10% 62.16% 69.05% 65.85%\n\nP recision\nRecall\nF − measure\n\ndf = 128\n48.68%\n72.55%\n58.27%\n\nB-spline Smoothing\ndf = 188 df = 258\n50.00%\n51.39%\n74.51%\n82.35%\n59.84%\n60.16%\n\ndf = 388\n47.44%\n72.55%\n57.36%\n\nm = 13, δ = 2\nLoad / kWh\n\n0.015\n\nCorrupted Data\nNormal Data\nEstimated Bounds\n\n0.01\n\n0.005\n\n0\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\n5000\n\n6000\n\n7000\n\n8000\n\nCorrupted Degree\n\n0.4\n\nCorrupted\nNormal\n\n0.3\n0.2\n0.1\n0\n−0.1\n0\n\n1000\n\n2000\n\n3000\n\n4000\n\n5000\n\n6000\n\n7000\n\n8000\n\nTime / 10 Seconds\n\nFigure 4: Result of corrupted data identification on real data with our appliance-driven method (w = 1, δ = 2); Estimated bounds\ndenote the upper and lower power bounds based on current state vector; Corrupted degree indicates the value of the virtual appliance\n(Section 4.)\n\n• At a given sampling frequency, each appliance reports its\ncurrent power value, which is a random number uniformly\ndistributed between the appliance’s lower power bound and\nupper power bound. It reports 0 if its state is off.\n\nTable 2: Parameter settings for load data generation and corruption\nParameter\nNumber of Appliances (m)\nSampling Frequency(f )\nTotal Time Span\nLowest Appliance Power(Pmin )\nHighest Appliance Power(Pmax )\nPower Range Ratio(r)\nInitial State(S)\nPoisson Parameter(λ)\nExponential Parameter(µ)\nCorrupted Data Range\n\n• In a sampling interval, the number of total on-off switch\nevents follows a Poisson distribution4 with parameter λ.\n• At the end of each sampling interval, the load data of the\nhouse is recorded as the aggregated power value of all appliances (i.e., the sum of all appliances’ load values).\nTo introduce some corrupted data and test the effectiveness of\nour method, we “corrupt\" some data values by replacing them with\nrandom values uniformly distributed between [0, M ax], where\nM ax is a given large constant. The time interval of introducing\ncorrupted data is assumed to follow an exponential distribution with\nthe mean value of µ.\n\n7.2\n\nCorrupted Data Identification on LargeScale Appliances\n\nThe parameters used to generate the synthetic data and the corrupted data are listed in Table 2.\nWe treat the bound on the total number of on-off switches in a\nsampling interval δ as a variable. To speed up the processing, we\nset the local window size to 1. The small local window size may\nnot lead to the best performance of SLOA. However, as shown in\nour experimental results, even with this setting, our method already\n4\nPoisson distribution is a good model for situations where the total\nnumber of items is large and the probability that each individual\nitem changes its state is small. It has been broadly adopted to simulate events related to human behavior, such as the number of telephone calls in a telephone system and the number of cars on high\nway.\n\nSetting\n50\n1/6Hz\n3600s\n50w\n2000w\n15%\n[0, 0, · · · , 0]T\n5\n30\n[0, 50kW ]\n\nperforms better than B-spline smoothing. For the B-spline smoothing method, the degree of freedom (df ) is set as a variable and is\ntrained when smoothing the synthetic data.\nThe performance results of our method and the B-spline smoothing method are summarized in Table 3. Fig. 6 and Fig. 7 illustrate\none of the outcomes from our method and the B-spline smoothing\nmethod, respectively.\nFrom the results, we can see that the our method works effectively on large-scale synthetic data. In particular, we find that the\nprecision of our method increases with increase of δ, and can even\nreach 100%. This result indicates that our method can provide excellent correct identification when δ is large enough. Regarding\nthe overall performance in view of F-measure, our method works\nbetter with a smaller δ value and outperforms B-spline smoothing\nclearly.\n\n7.3\n\nIdentification of Consecutive Corrupted\nData\n\nIn practice, we often meet the situation that all data within a\n\n\f0.010\n0.005\n0.000\n\nLoad / kWh\n\nRaw Data\nFitted Curve\n95% C.I.\n\n0\n\n2000\n\n4000\n\n6000\n\n8000\n\nTime / 10 Seconds\n\nFigure 5: Result of corrupted data identification on real data with the B-spline smoothing method (df = 258)\n\nTable 3: Results of corrupted data identification on synthetic data: appliance-driven method vs. B-spline smoothing method\nP recision\nRecall\nF − measure\n\nappliance-driven Method\nδ=4\nδ=5\nδ=6\n93.94% 93.94%\n100%\n81.58% 81.58% 63.16%\n87.32% 87.32% 77.42%\n\ndf = 140\n78.57%\n86.84%\n82.50%\n\nB-spline Smoothing\ndf = 160 df = 180\n86.49%\n84.61%\n84.21%\n86.84%\n85.33%\n85.71%\n\ndf = 200\n84.21%\n84.21%\n84.21%\n\nm = 50, δ = 5\n0.14\n\n0.02\n0\n0\n\n600\n\n1200\n\n1800\n\n2400\n\n3000\n\n3600\n\nCorrupted Degree\n\n0.4\n\nCorrupted\nNormal\n\n0.2\n\nRaw Data\nFitted Curve\n95% C.I.\n\n0.10\n\n0.04\n\nLoad / kWh\n\n0.06\n\n0.05\n\n0.08\n\n0.00\n\nLoad / kWh\n\n0.1\n\n0.15\n\nCorrupted Data\nNormal Data\nEstimated Bounds\n\n0.12\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\n\n3500\n\nTime / s\n0\n\n−0.2\n\n−0.4\n0\n\n600\n\n1200\n\n1800\n\n2400\n\n3000\n\n3600\n\nTime / s\n\nFigure 6: Result of corrupted data identification on synthetic\ndata with our appliance-driven method (w = 1, δ = 5)\n\nsmall time interval are corrupted or lost. Consecutive corrupted\ndata poses a big challenge to regression-based methods, as will be\nillustrated in this subsection.\nTo introduce consecutive corrupted data, we replace the load data\nin a small time window as 0s, as shown in the upper part of Fig. 8.\nWe then use our method and the B-spline smoothing method to\ntest the data. Fig. 8 and Fig. 9 illustrate one outcome from our\nappliance-driven approach and the B-spline smoothing method, respectively.\nFrom the results, we can see that our method does much better than B-spline smoothing for consecutive corrupted data identification. With δ = 5, our method can correctly identify all the\ncorrupted data. On the other hand, even though we regulate the\nparameters for B-spline smoothing, it almost failed every time to\nidentify even half of the corrupted data.\nAn interesting phenomenon can be found around the consecutive\ncorrupted data in Fig. 9. There is an apparent trend with B-spline\nsmoothing to fit the corrupted data. This is mainly because the B-\n\nFigure 7: Result of corrupted data identification on synthetic\ndata with B-spline smoothing method(df = 160)\n\nspline smoothing method tries to fit the curve pattern and reduce the\ntotal bias error with global optimization, indicating that it cannot\ndeal with consecutive corrupted data well.\n\n8.\n\nROBUSTNESS TESTING\n\nOne may question whether the performance of SLOA relies on\na correct initial state vector, accurate information regarding appliances power ranges, and an accurate estimation on appliances’ onoff states. all of such information may be hard to obtain in practice.\nTo answer this question, we test the robustness of SLOA. We use\nthe synthetic data created using the same parameters in Table 2. We\nfirst disclose the test results and then explain the reasons.\n\n8.1\n\nImpact of the Initial State\n\nFor this test, we change the initial state of an appliance to a random 0-1 value, and perform multiple tests. Fig. 10 shows one of\nthe outcomes. We find that, even with an incorrect initial state, our\nmethod can always recover to correct load data after a few steps.\nThis result indicates that our SLOA method is robust against inaccurate initial power state setting.\n\n8.2\n\nImpact of Power Ranges\n\n\fTable 4: Robustness tests with incorrect power ranges of appliances\nWiden (5%)\nδ=3\nδ=4\n93.55% 87.50%\n76.32% 55.26%\n84.06% 67.74%\n\nP recision\nRecall\nF − measure\n\nWiden (10%)\nδ=3\nδ=4\n91.30% 94.12%\n55.26% 42.11%\n68.84% 58.18%\n\nShift& Widen (5%)\nδ=3\nδ=4\n67.39%\n90.91%\n81.58%\n78.95%\n73.81%\n84.51%\n\nShift& Widen (10%)\nδ=3\nδ=4\n71.11%\n84.62%\n84.21%\n57.89%\n77.11%\n68.75%\n\nm = 50, δ = 5\n\nm = 50, δ = 5\n\nRecovery Window\nReal Data\nEstimated Bounds\n\n0.14\n\nCorrupted Data\nNormal Data\nEstimated Bounds\n\n0.1\n\n20\n\n0.08\n\nLoad / kWh\n\nLoad / kWh\n\n0.12\n\n25\n\n0.06\n0.04\n0.02\n0\n0\n\n600\n\n1200\n\n1800\n\n2400\n\n3000\n\n15\n\n10\n\n3600\n5\n\nCorrupted Degree\n\n0.15\n\n0\n\nCorrupted\nNormal\n\n0.1\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\nTime / s\n\n0.05\n0\n−0.05\n−0.1\n−0.15\n−0.2\n0\n\n600\n\n1200\n\n1800\n\n2400\n\n3000\n\n3600\n\nFigure 10: Fast recovery of estimated load starting from a random initial state\n\nTime / s\n\nm = 50, δ = 5\n35\n\nFigure 8: Identification of consecutive corrupted data with our\nappliance-driven method\n\n25\n\nr\n\ne\n\n|| S − S ||\n\n30\n\n20\n\n15\n\n10\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\n\n3500\n\n0.05\n\n0.10\n\nTime / s\n\nFigure 11: Difference between estimated state Se and real state\nSr\n\n0.00\n\nLoad / kWh\n\n0.15\n\nRaw Data\nFitted Curve\n95% C.I.\n\n0\n\n500\n\n1000\n\n1500\n\n2000\n\n2500\n\n3000\n\n3500\n\nTime / s\n\nFigure 9: Identification of consecutive corrupted data with Bspline smoothing method (df = 100)\n\nIn practice, we may not precisely know the power ranges of appliances. Based on this consideration, we run extra simulations to\ntest the robustness of our method when the power range information of appliances is inaccurate. We carry out two kinds of tests as\nfollow.\n• Widen power range: each appliance’s power range is\nwidened by 5% or 10%, respectively, with the center power\nvalue, i.e., (upper bound − lower bound)/2, unchanged.\n• Shift& widen power range: each appliance’s lower power\nbound is increased by 5% or 10%, and upper bound is increased by 5% or 10%, respectively. Note that the above\noperations will shift the center power value as well as widen\nthe power range.\nWe do not consider the situation where the appliances’ power\nranges are narrowed, since intuitively a user can always widen an\nappliance’s power range if she/he is not sure about the right values.\nThe test results are summarized in Table 4. The results clearly\nindicate that, with inaccurate or even wrong power ranges of appliances, our method can still manage to identify corrupted data with\nhigh precision.\n\n8.3\n\nImpact of State Vector\n\nWe have seen that our method can give correct bounds for energy\nconsumption most of the time. Accordingly, we might infer that the\nestimated states of the appliances should be the same with the real\nsituation, or at least quite close.\nIn order to verify this conjecture, we calculate the difference\n(one-norm distance) between the estimated state Se and the corresponding real state Sr at each time instance. Fig. 11 shows the\nresult.\nTo our surprise, the estimated states are not close to the real\nstates, and actually deviate remarkably from their real states. We\ncan see that in Fig. 11, the mean distance between Se and Sr is\naround 25, indicating that nearly half of the appliances are not estimated with the correct states. This shows that the solution to the\nCDIP problem is not unique but multiple, and our method can provide the right load data without need to always find the right states\nof appliances.\n\n8.4\n\nWhy Is SLOA Robust?\n\nIn real life, a lot of appliances are with similar or overlapped\npower range. In this sense, we indeed do not need to know the\nexact state for similar appliances as long as we can give a good\napproximation for their total consumption. In addition, due to the\ntemporal sparsity of on-off switch events in the short sampling interval and the fact that only some appliances are on at any time\ninstant, the negative impact of inaccurate power range estimation\non one appliance can be offset by the negative impact of incorrect\nstate estimation of another appliance. The offsetting is enforced au-\n\n\ftomatically by the optimization objective function that minimizes\nthe gap between the actual load data and the estimated value.\n\n9.\n\nCONCLUSION AND FUTURE WORK\n\nTo answer the industrial call for improving quality of load data,\nwe developed a new appliance-driven approach for corrupted data\nidentification that particularly takes advantage of information available on the demand side. Our appliance-driven approach considers\nthe operating ranges of appliances that are readily available from\nusers’ manual, technical specification, or public websites [12]. It\nidentifies corrupted data by solving a carefully-designed optimization problem. To solve the problem efficiently, we developed a\nsequential local optimization algorithm (SLOA) that practically approach the original NP-complete problem approximately by solving an optimization problem in polynomial time.\nWe evaluated our method using both real trace data from a realworld energy monitoring system and large-scale synthetic data.\nTest results indicate that our method can precisely capture corrupted data. In addition, SLOA is robust under various test scenarios, and its performance is resilient to inaccurate power range\ninformation or inaccurate power state estimation.\nOur method greatly augments the arsenal of existing load data\ncleansing tools to minimize human effort in identifying corrupted\ndata. Yet, we ignore the privacy issues in this study. Even if the\ncustomers and the utility companies have an aligned common goal\nfor accurate load data, some customers may be reluctant to collaborate due to privacy concerns. Our future research is to enhance our\nappliance-driven approach by developing privacy-preserving load\ndata cleansing methods. In addition, how to replacing aberrant values and missing values is out of the focus of this paper, because this\nissue is relevant to utilities’ internal rules and thus requires human\ninteraction. Considering various policies and methods for load data\nimputation will be our future work.\n\n10.\n\nREFERENCES\n\n[1] B. Abraham and A. Chuang. Outlier detection and time series\nmodeling. Technometrics, 31(2):241–248, 1989.\n[2] B. Abraham and N. Yatawara. A score test for detection of time\nseries outliers. Journal of time series analysis, 9(2):109–119, 1988.\n[3] C. C. Aggarwal. Outlier Analysis. Springer, 2013.\n[4] R. J. Bolton, D. J. Hand, et al. Unsupervised profiling methods for\nfraud detection. Credit Scoring and Credit Control VII, pages\n235–255, 2001.\n[5] J. Chen, W. Li, A. Lau, J. Cao, and K. Wang. Automated load curve\ndata cleansing in power systems. IEEE Transactions on Smart Grid,\n1(2):213–221, 2010.\n[6] S.-y. Chen, S.-f. Song, L. Li, and J. Shen. Survey on smart grid\ntechnology. Power System Technology, 33(8):1–7, 2009.\n[7] CVX. Matlab software for disciplined convex programming.\ncvxr.com/cvx, accessed in July 2013.\n[8] H. David. Robust estimation in the presence of outliers. Robustness\nin statistics, 1:61–74, 1979.\n[9] L. Davies and U. Gather. The identification of multiple outliers.\nJournal of the American Statistical Association, 88(423):782–792,\n1993.\n[10] D. L. Donoho. For most large underdetermined systems of linear\nequations the minimal l1 norm solution is also the sparsest solution.\nCommunications on pure and applied mathematics, 59(6):797–829,\n2006.\n[11] EIA. Electric power annual 2012. http://www.eia.gov/\nelectricity/annual/?src=Electricity-f4, accessed\nin Jan. 2014.\n\n[12] EPA. A tool of home product finder by energy star.\nhttp://www.energystar.gov/productfinder/, 2013.\n[13] H. Farhangi. The path of the smart grid. Power and Energy\nMagazine, IEEE, 8(1):18–28, 2010.\n[14] T. S. Ferguson. On the rejection of outliers. In Proceedings of the\nFourth Berkeley Symposium on Mathematical Statistics and\nProbability, volume 1, pages 253–287, 1961.\n[15] A. J. Fox. Outliers in time series. Journal of the Royal Statistical\nSociety. Series B (Methodological), pages 350–363, 1972.\n[16] U. Gather. Testing for multisource contamination in location/scale\nfamilies. Communications in Statistics-Theory and Methods,\n18(1):1–34, 1989.\n[17] E. M. Knox and R. T. Ng. Algorithms for mining distance-based\noutliers in large datasets. In Proceedings of the International\nConference on Very Large Data Bases, 1998.\n[18] H.-P. Kriegel and M. Pfeifle. Density-based clustering of uncertain\ndata. In Proceedings of the eleventh ACM SIGKDD international\nconference on Knowledge discovery in data mining, pages 672–677.\nACM, 2005.\n[19] J. Lang. A bad case of smart meter regret. http://www.\ncloverdalereporter.com/news/189923521.html,\naccessed in Jan. 2014.\n[20] G. M. Ljung. On outlier detection in time series. Journal of the Royal\nStatistical Society. Series B (Methodological), pages 559–567, 1993.\n[21] G. Mateos and G. B. Giannakis. Robust nonparametric regression via\nsparsity control with application to load curve data cleansing. IEEE\nTransactions on Signal Processing, 60(4):1571–1584, 2012.\n[22] J. McMahon. Big data from smart grid tells utilities more than they\nwant to know. Forbes, September 2013.\n[23] S. Moneo. Bc hydro pulls smart meters for testing. http://www.\ntheglobeandmail.com/news/british-columbia/\nbc-hydro-pulls-smart-meters-for-testing/\narticle534384/, accessed in Jan. 2014.\n[24] C. News. BC Hydro outlines smart meter refusal cost. September 13,\n2013.\n[25] S. Ramaswamy, R. Rastogi, and K. Shim. Efficient algorithms for\nmining outliers from large data sets. In ACM SIGMOD Record, pages\n427–438. ACM, 2000.\n[26] W. Schmid. The multiple outlier problem in time series analysis.\nAustralian Journal of Statistics, 28(3):400–413, 1986.\n[27] J. Stragier, L. Hauttekeete, and L. De Marez. Introducing smart grids\nin residential contexts: Consumers’ perception of smart household\nappliances. In Innovative Technologies for an Efficient and Reliable\nElectricity Supply (CITRES), 2010 IEEE Conference on, pages\n135–142, 2010.\n[28] J. W. Tukey. Exploratory data analysis. Reading, MA, 231, 1977.\n[29] M. Weiss, A. Helfenstein, F. Mattern, and T. Staake. Leveraging\nsmart meter data to recognize home appliances. In 2012 IEEE\nInternational Conference on Pervasive Computing and\nCommunications (PerCom), pages 190–197, 2012.\n\nAppendix A: Proof of NP-Completeness of\nCDIP\n10.1 Preparation\nFirst, we introduce a tree structure T = (M, N ), which is a complete\nM -ary tree with height of N , i.e., every internal node has exactly M children and all leaves have the same depth of N . Furthermore, each edge (i, j)\nof T has a non-negative cost c(i, j), which will be defined later.\nAssume that load data {y1 , y2 , · · · , yn } is generated by m appliances\nwith the initial state vector S0 . Also assume that the upper bound on the\ntotal number of on-off switches within a sampling interval is δ(< m). We\ncan build the following tree:\nStep 1. Set S0 as the root of the tree.\n\n\f1\n\nStep 2. Set the children of root as all possible states that can be transited from\nS0 , with the constraint that the total number of on-off switches is no\nlarger than δ. Therefore, we can add M children to the root, where\n0 + C1 + · · · + Cδ ;\nM = Cm\nm\nm\n\nStep 2. For each leaf node of the current tree, add its children as all the\nother nodes of G. Since G is a complete graph, we can add\n|V | − 1 children to each leaf node, where |V | is the number of\nnodes in G.\nStep 3. Repeat Step 2 for |V | times. At the end, we build T = (M, N ),\nwhere M = |V | − 1 and N = |V |;\nStep 4. Set the cost of edge (i, j) in T , c(i, j), as follows:\n(a) Initialization: c(i, j) = c0 (i, j), where c0 (i, j) is the edge\ncost in G.\n(b) For each edge (i, j) of T where j is a non-leaf node, if j\nhas appeared in the path from the root (including the root)\nto i, i.e., j is an ancestor of i in the tree already, replace\nc(i, j) = ∞.\n(c) For each edge (i, j) of T where j is a leaf node, if j is not\nthe same as the root node, replace c(i, j) = ∞.\n\nStep 3. For each node of the tree, set its children as all possible states (M\nstates) that can be transited from it;\n\nTo help understand the construction of T with G, Fig. 12 show an\nexample with three nodes in G.\n\nc12\n\n1\nc12\n2\n\n∞\n\nc13\nc23\n\n1\n∞\n\n3\n2\n\n∞\n\n3\n\nc13\n\n1\n\nc13\n\n2\n\n3\n\nc23\n\n∞\n\n3\n\n1\n\n∞\n\n∞\n\n2\n\n2\n\nc23\n\n2\n∞\n\nc12\n\n3\n\n∞\n\n1\n\n3\n\nFigure 12: An example showing the construction of T with G\n\nStep 4. Repeat Step 3) from t = 1 to n. At the end, we obtain T = (M, N ),\nwhere N = n;\nStep 5. Set the cost of edge (i, j) as c(i, j) = |v|, where v is obtained by\nsolving the optimization problem: \u0001\n\u0001\nminimize |v|, subject to PlT Sj − v /f ≤ yi ≤ PuT Sj + v /f .\nThus, we can translate CDIP into the problem of finding the minimumcost path in T (M, N ) from the root to a leaf. Equivalently, we need to answer the following question: given a constant k, is there a path in T (M, N )\nfrom the root to a leaf with total cost no larger than k? In the following, we\ncall a path from the root to a leaf in the tree as a full path.\nWith the notation above, CDIP can be re-formulated as\nCDIP = {hT, c, ki :T = (M, N ),\nc is the cost function ,\n+\n\nk ∈ < , and\nT has a full path with cost ≤ k}.\nWe next reduce a well-known NP-complete problem, the Traveling\nSalesperson Problem (TSP) to CDIP. TSP can be formulated as\nT SP = { G, c0 , k :G = (V, E) is a complete graph ,\n0\n\nc is the cost function ,\n\n• Thirdly, we show that\nG, c0 , k ∈ T SP ⇔ hT, c, ki ∈ CDIP.\n– (⇒)\nG has a Hamiltonian cycle with cost ≤ k.\n⇒ there exists a full path in treeT with cost ≤ k.\n(Note that there will be no internal node along the path\noccurring more than once, otherwise the cost will\nbe infinite with operation in Step 4.)\n– (⇐)\nT has a full path with cost ≤ k.\n⇒ there exists a traverse instance in its corresponding\ngraph G with cost ≤ k.\n(Note that based on the tree construction procedure,\nonly the full paths starting and ending at the same\nnode can have a cost no larger than k, because other\n\nk ∈ < , and\n\npaths have a cost of infinity.)\n\nG has a Hamiltonian cycle with cost ≤ k}.\n\n⇒ so G has a Hamiltonian cycle with cost ≤ k.\n\n+\n\n10.2\n\n• Secondly, it is easy to see that F takes O(N 2 ) running time.\n\nProof\n\nWe complete the proof in two steps: firstly we show that CDIP is NP;\nthen, we prove that CDIP is NP-hard by showing T SP ≤P CDIP , i.e.,\nthere exists a reduction from TSP to CDIP.\na. CDIP is NP\n• Certificate: A path of T .\n• Algorithm:\n– Check that the path is full, i.e., the path starts from the root and\nends at a leaf.\n– Sum up the edge costs along the path and check if it is no larger\nthan k.\n• Polynomial Time: We need N steps to check the fullness of path and\nobtain the total cost.\nb. CDIP is NP-hard\n• Firstly, we develop an algorithm F : hG, c0 , ki → hT, c, ki, i.e., G\nand c0 in TSP can be transferred to T and c in CDIP as follow:\nStep 1. Choose any node of G as the root of T ;\n\nWith step a. and step b., we prove that CDIP is NP-complete.\n\nAppendix B: Computational Complexity\nFor problem (Equation 8), the second constraint means that during a sampling interval, there are at most\nof m appliances\nthat can change\n\u0001 δ out\nm\u0001\nm\u0001\ntheir states. This results in m\n+\n+\n·\n·\n·\n+\nfeasible\nsolutions.\n0\n1\nδ\nTherefore, \u0001the total\u0001 number of possible\nstate sequences is M n , where\n\u0001\nM = m\n+ m\n+ ··· + m\n. Thus, the computational complexity\n0\n1\nδ\nof problem (Equation 8) is O(M n ), which is exponential.\nAs to problem (Equation 10), the whole search space is split into n local windows of size w, and the optimization \u0001is confined\nwithin the \u0001local\nm\u0001\nwindow. Given Si−1 , there are at most m\n+\n+\n··· + m\nin0\n1\nδ\nstances. We have to traverse all the instances to find the one that minimizes\nvi in each step of a local optimization. Therefore, the computational complexity to\u0001find a \u0001local optimal \u0001solution with w steps is O(M w ), where\nM = m\n+ m\n+ ··· + m\n.\n0\n1\nδ\nHence, after applying SLOA in each of the n local windows, the total\ncomputational complexity to obtain the final solution is O(n · M w ). Considering that the number of appliance m is a constant value and w is also a\nsmall constant, SLOA cuts down the computational complexity of the original problem from exponential to polynomial.\n\n\f"
        ],
        [
         "16",
         "16",
         "cs.CE",
         "Computational Engineering",
         "1402.7324v1.pdf",
         "Министерство образования и науки Российской Федерации\nФедеральное агентство по образованию\nГосударственное образовательное учреждение\nвысшего профессионального образования\n«Московский государственный университет печати»\n__________________________________________________________________________________\n\nНИКУЛЬЧЕВ Е. В.\n\nГЕОМЕТРИЧЕСКИЙ ПОДХОД\nК МОДЕЛИРОВАНИЮ НЕЛИНЕЙНЫХ\nСИСТЕМ ПО ЭКСПЕРИМЕНТАЛЬНЫМ\nДАННЫМ\n\nМонография\n\nМосква\n2007\n1\n\n\fУДК 519.711.3\nББК 22.18\nН65\n\nРецензенты:\nзав. каф. компьютерных технологий и систем Санкт-Петербургского гос. ун-та,\nд-р физ.-мат. наук, профессор Е. И. Веремей,\nзав. каф. прикладной математики и информатики Астраханского гос. ун-та,\nд-р физ.-мат. наук Ю. Ю. Тарасевич\n\nН65\n\nНикульчев Е. В. Геометрический подход к моделированию\nнелинейных\nсистем\nпо\nэкспериментальным\nданным:\nмонография.— М.: МГУП, 2007.— 162 с.\nISBN 978–5–8122–0926–1\nВ монографии изложен геометрический метод моделирования\nнелинейных динамических систем по экспериментальным данным. Основой\nметода является качественный подход к анализу нелинейных моделей и\nпостроение групп симметрий аттракторов динамических систем с управлениями.\nПриведено теоретическое обоснование, включая теоремы о центральном\nмногообразии, определяющие условия существования рассматриваемого класса\nмоделей в локальной области с учетом групповых свойств; алгоритмы оценки\nинвариантных характеристик, методы построения идентифицируемых моделей и\nописание результатов, полученных при использовании метода для\nмоделирования управляемых технических процессов. Включены два\nприложения, являющиеся развитием предложенного подхода: выявление групп\nсимметрий по фазовым портретам динамических систем и методика построения\nнейросетевых прогнозирующих моделей.\nМонография рассчитана на аспирантов и специалистов в области\nприкладной математики и математического моделирования систем.\n\nРабота выполнена при финансовой поддержке\nпо проекту МД-601.2007.8\nГранта Президента РФ для молодых докторов наук\nISBN 978–5–8122–0926–1\n© Никульчев Е. В., 2007\n© Московский государственный\nуниверситет печати, 2007\n\n2\n\n\fОГЛАВЛЕНИЕ\n\nПРЕДИСЛОВИЕ ....................................................................................................... 5\nГЛАВА 1. ТЕОРЕТИЧЕСКИЕ ОСНОВЫ ГЕОМЕТРИЧЕСКОГО\nПОДХОДА К МОДЕЛИРОВАНИЮ СИСТЕМ ............................................. 14\n1.1. Математические модели нелинейных систем ................................. 14\n1.2. Качественное исследование динамических систем ........................ 20\n1.3. Топологическая классификация грубых состояний равновесия ... 23\nГЛАВА 2. АППАРАТ ГРУПП СИММЕТРИЙ ДЛЯ ИССЛЕДОВАНИЯ\nМОДЕЛЕЙ УПРАВЛЯЕМЫХ СИСТЕМ........................................................ 29\n2.1. Групповой анализ нелинейных систем с управлением .......................... 29\n2.2. Методика группового анализа систем с управлением ........................... 32\n2.2.1. Применение классификации систем, допускающих группы\nсимметрий для анализа решений............................................................. 32\n2.2.2. Методика исследования групповых систем ................................. 36\n2.2.3. Методика исследования симметрий по состоянию ..................... 38\n2.3. Аппарат непрерывных симметрий дискретных моделей....................... 46\n2.4. Группы симметрий фазового пространства ............................................ 51\nГЛАВА 3. РЕДУКЦИЯ НА ЦЕНТРАЛЬНОЕ МНОГООБРАЗИЕ СИСТЕМ,\nДОПУСКАЮЩИХ ГРУППЫ СИММЕТРИЙ ............................................... 56\n3.1.1. Использование групповых свойств для построения эквивалентных\nотображений ...................................................................................................... 56\n3.2. Метод моделирования систем, редуцированных на инвариантное\nмногообразие в локальной области ................................................................. 60\n3.2.1. Исследования инвариантного многообразия ............................... 60\n3.2.2. Метод редукции систем на центральное многообразие.............. 64\n3.3. Обобщение теоремы о центральном многообразии для систем,\nдопускающих группы симметрий.................................................................... 68\nГЛАВА 4. МОДЕЛИРОВАНИЕ СИСТЕМ С НЕЛИНЕЙНОЙ\nДИНАМИКОЙ ПО ЭКСПЕРИМЕНТАЛЬНЫМ ДАННЫМ ....................... 75\n4.1. Оценка показателей Ляпунова по временному ряду .............................. 75\n4.1.1. Методика использования свойств показателей Ляпунова для\nмоделирования........................................................................................... 75\n4.1.2. Методы расчета показателей Ляпунова........................................ 82\n4.1.3. Разработка алгоритмов оценки показателей Ляпунова по\nвременному ряду ....................................................................................... 84\n4.2. Разработка алгоритмов оценки инвариантных характеристик ............. 87\n4.3. Модифицированный метод реконструкции аттракторов для систем,\nдопускающих группы симметрий.................................................................... 90\n3\n\n\f4.4. Метод моделирования нелинейных систем по экспериментальным\nданным................................................................................................................ 94\nГЛАВА 5. ТЕХНОЛОГИЯ ПОВЫШЕНИЯ КАЧЕСТВА\nФУНКЦИОНИРОВАНИЯ УПРАВЛЯЕМЫХ ТЕХНИЧЕСКИХ СИСТЕМ\n........................................................................................................................... 105\n5.1. Технология обеспечения повышения качества функционирования\nуправляемых систем........................................................................................ 105\n5.2. Методика применения прогнозирующих моделей ............................... 107\n5.3. Управление системой теплообмена с вязкой средой ........................... 114\n5.4. Управление процессом охлаждения алюминиевых слитков............... 122\nЗАКЛЮЧЕНИЕ ..................................................................................................... 126\nСПИСОК ИСПОЛЬЗУЕМЫХ ИСТОЧНИКОВ............................................................ 127\nПРИЛОЖЕНИЯ.\nБОРИСОВ Ю. Ю. МЕТОДИКА ПОСТРОЕНИЯ НЕЙРОСЕТЕВЫХ\nПРОГНОЗИРУЮЩИХ МОДЕЛЕЙ НА ОСНОВЕ АНАЛИЗА\nСВОЙСТВ АТТРАКТОРОВ ..................................................................................... 144\n\nКОЗЛОВ О. В. ВЫЯВЛЕНИЕ СИММЕТРИЙ РЕКОНСТРУИРОВАННЫХ\nФАЗОВЫХ ТРАЕКТОРИЙ ДИНАМИЧЕСКИХ СИСТЕМ ............................................ 155\n\n4\n\n\fПредисловие\nСовременное развитие техники и технологий определяет высокие\nтребования к системам управления. Методы совершенствования качества и\nнадежности функционирования управляемых систем в значительной\nстепени определяются используемыми математическими методами и\nмоделями. Это особенно проявляется в работающих промышленных\nобъектах, динамическое поведение контролируемых параметров которых,\nнесмотря на управление, носит нелинейный характер. Построение\nмоделей,\nадекватных\nдинамическому\nповедению,\nопределяет\nпроектирование качественных и надежных систем автоматического\nуправления. Ниже проводится краткий аналитический обзор подходов к\nпостроению\nматематических\nмоделей\nнелинейных\nявлений,\nиспользованию качественной теории нелинейных систем [117, 143],\nобъектом исследования которой является фазовые портреты. Проведен\nтакже анализ современных методов нелинейной динамики, связанный с\nзадачами исследования хаотических систем.\nПодходы к моделированию управляемых технических систем. При\nрешении задач управления техническими системами одной из важных\nзадач является формализация эволюционного поведения объекта\nуправления, процесс моделирования также включает математическое\nописание целей управления, требований к качеству и надежности\nфункционирования. На основании построенных моделей требуется\nсформировать алгоритмы управления. К сожалению, современное\nсостояние\nпромышленности\n(несмотря\nна\nусовершенствование\nтехнологий) таково, что проектировщики оперируют только с базовыми\nлинейными моделями, возлагая решение задач регулирования\nнелинейными явлениями на промышленные контроллеры. В результате\nимеют место реально функционирующие управляемые системы,\nработающие не только не оптимально, но, даже и в режиме ручной\nподрегулировки управляющих процессов. Таким образом, для реальных,\nуже функционирующих систем повышение требований к качеству и\nнадежности не может быть в полной мере обеспечено классическими\nмоделями и средствами управления. Сложность стоящих задач\nопределяется конструкциями промышленных устройств и протекающими\nнелинейными процессами такими как, процессы теплообмена, течения\nвязких жидкостей, волновые явления, химические реакции и др.\nМожно выделить три классических подхода к моделированию\nнелинейных явлений в технических системах. Первый заключается в\nиспользовании стохастических моделей, при этом считается, что\nнелинейные колебания являются реализацией случайного процесса,\nхарактеристики которого требуется найти. В этом случае, очевидно, не\nмогут быть строго обоснованы предположения и допущения, которые не\nносят явно вероятностный характер, а наблюдаемое поведение присуще\nцелому классу нелинейных объектов. Оценка адекватности при\n5\n\n\fпостроении стохастических моделей заключается в проверке соответствия\nгипотез первоначальным предположениям. Второй подход — построение\nмоделей исходя из физических свойств всех протекающих в технических\nсистемах явлений и вывод эволюционных уравнений. Несмотря на\nкажущуюся очевидность такого решения, оно является практически\nнереальным для технических систем, т. к. закрытость и структурная\nсложность конструкций определяет громоздкость и большую\nнаукоемкость такого физического моделирования, тем более что\nполучение необходимых для управления эволюционных уравнений из\nполученных моделей является отдельной сложной задачей. Например,\nуравнение неравномерного нагрева в заданных условиях технологической\nконструкции явно нетривиальная задача, при этом из решения требуется\nполучить зависимость температуры нагрева от изменения параметров\nкакого-либо управляющего механизма.\nПеречисленные сложности моделирования реальных функционирующих систем определяют третий, наиболее ориентированный на\nширокое использование подход к моделированию, заключающийся в\nвыборе вида математической модели в виде эволюционного уравнения и\nпоследующей идентификации параметров, либо непараметрической\nидентификации модели. Модель считается адекватной, если оценка\nзаданного критерия адекватности, вычисленная как зависимость невязки\nмодели от экспериментальных данных находится в допустимых пределах.\nВ работе рассматриваются модели с управлением в виде системы\nдифференциальных уравнений с сосредоточенными параметрами с\nнезависимыми функциями (управлениями) или их дискретные аналоги,\nописываемые\nконечно-разностными\nуравнениями.\nНеобходимость\nрассмотрения непрерывных объектов в дискретном времени определяется,\nс одной стороны, широким применением в практике автоматического\nуправления цифровых управляющих устройств, с другой — конечноразностные аналоги дифференциальных систем дают возможность\nполучения вычислительно-надежных и эффективных методов управления.\nВыбор класса объектов для моделирования обусловлен практической\nзначимостью и разработанностью методов и средств управления для этих\nмоделей. Например, моделирование распределенных систем с\nиспользованием разработанного аппарата представляется возможным, но,\nсколько-нибудь общие, инструментальные и методические средства\nсинтеза управления для этого класса систем отсутствуют.\nТаким образом, метод моделирования, изложенный в настоящей\nмонографии, состоит в построении, на основании исследования\nэкспериментальных данных, уравнений, решение которых имеет\nадекватное динамическое поведение, моделируя при этом качественную\nдинамическую сложность изучаемого процесса. Важным свойством\nфункционирующих систем является устойчивость. Следовательно,\nтребуется рассмотрение моделей на бесконечном интервале времени, т. е.\nдинамических систем.\n6\n\n\fМоделирование динамической системы состоит из выделения трех\nкомпонентов: 1) определение фазового пространства в условиях\nограничений; 2) выбор дискретности или непрерывности времени и\n3) закона эволюции, т. е. отображение любой заданной точки в фазовом\nпространстве и любого значения времени в однозначно определенное\nсостояние системы. При этом для дискретного времени ищется закон\nэволюции в виде\nx(t + 1) = ψ( x, t , u ),\n\ny = h( x(t )),\nдля непрерывного времени:\nx = f ( x, u , t ),\n\nt ∈ Z,\n\ny = h ( x(t )),\nt ∈ R1 ,\nгде x — вектор состояний; y — измеряемые процессы (далее, если это не\nопределено особенностями задачи будем предполагать, что наблюдению\nдоступны все состояния); t — время; f , ψ, h, h — в определенном смысле\nнепрерывные и гладкие вектор-функции.\nИспользование качественной теории нелинейных систем в теории\nуправления традиционно ограничено исследованиями устойчивости по\nЛяпунову. Применению методов функций Ляпунова для синтеза\nуправлений посвящены работы Н. Н. Красовского, В. И. Зубова, их\nпоследователей и учеников. Н. Н. Красовский установил связь метода\nфункций Ляпунова с методом динамического программирования Беллмана\nи показал, что принципу оптимальности удовлетворяют только те\nоптимизирующие функции, которые являются функциями Ляпунова для\nзамкнутой системы. Найденные по этим законам управления оптимальны и\nобеспечивают устойчивость движения. В соответствии с теорией\nВ. И. Зубова, управления строятся из условия реализации наибольшей\nскорости убывания функции Ляпунова.\nВ настоящее время происходит развитие геометрической теории\nуправления, обусловленное переходом методов моделирования систем от\nпостроения пространства состояний к многообразиям. Развитию этого\nнаправления посвящены геометрические исследования А. Г. Бутковского\n[33], Г. В. Кондратьева [74], А. П. Крищенко [78], В. И. Елкина [50],\nА. А. Аграчеева [1], Ю. Л. Сачкова [1], В. И. Краснощекова [77],\nК. Г. Гареева [38], а также в работах по развитию аппарата групп\nсимметрий управляемых систем Ю. Н. Павловского [109], Г. Н. Яковенко\n[150] и др.\nОднако про геометрическую теорию управления сложно сказать как\nо сформировавшемся научном направлении со своей терминологией и\nсистемой обозначений. Характерно, что первый учебник на русском\nязыке [1] вышел в 2006 году.\nМетоды качественной теории нелинейных систем. Традиционно,\nисследование управляемых систем происходит во временной области.\n\n7\n\n\fРазвитие геометрических принципов управления, позволивших получение\nрешения важных задач в терминах симметрий, а также необходимость\nисследования для нелинейных систем качественного поведения\nопределяют переход к многообразию и методам нелинейной динамики.\nВ основе нелинейной динамики гладких систем лежат работы\nА. Пуанкаре, А. М. Ляпунова, Ж. Адамара. На раннем этапе вклад в\nразвитие внесли Д. Биркхгоф, Е. Хопф, С. Катуни. Уже к тридцатым годам\nпрошлого века сформировалась математическая теория колебаний\nдвумерных систем [3, 4, 5, 23, 28, 113]. А. А. Андроновым и\nЛ. С. Понтрягиным определено понятие грубых, структурно-устойчивых\nсистем, А. А. Андроновым, Е. А. Леонтович рассмотрены основные\nбифуркации предельных циклов, А. А. Андроновым, Л. С. Понтрягиным\nисследованы полные топологические инварианты для грубых систем,\nрезультаты обобщены Е. А. Леонтович и А. Г. Майером. В качественной\nтеории выделяют основные типы фазовых портретов: состояние\nравновесия (которое соответствует стационарному состоянию во\nвременной области), предельный цикл (соответствует автоколебаниям),\nинвариантный тор с квазипериодической траекторией (соответствует\nмодуляциям) [3, 53, 63]. Важным классом траекторий являются\nтраектории, устойчивые по Пуассону. При этом движение стремится к\nсвоему начальному положению, однако для малой фиксированной\nокрестности исходного положения последовательность соответствующих\nвремён возвращения может быть неограниченна, то есть движение\nоказывается непредсказуемым. Все типы движения, соответствующие\nнепереходному поведению, по классификации Биркгофа [28],\nследующие — стационарные, периодические, квазипериодические, почти\nпериодические и устойчивые по Пуассону траектории. Причем, Марковым\nустановлено, что устойчивая по Пуассону траектория\nявляется\nравномерно устойчивой по Ляпунову, то она должна быть почти\nпериодической. Динамика гладких систем тесно связана с эргодической\nтеорией, основанной на геометрической теории и комбинаторном подходе\nА. Н. Колмогорова. Следует упомянуть работы Д. В. Аносова, А. Б. Катка\nдля построения гладких систем. Для сложных траекторий создана\nгиперболическая теория [129].\nС. Смейлом [128] и Л. П. Шильниковым [144] в 70-м году\nодновременно были опубликованы статьи, в которых показано, что\nсистемы со сложным поведением орбит могут быть структурноустойчивыми. Таким образом, портреты с гомоклинической траекторией\nПуанкаре обладают бесконечным множеством сосуществующих\nпериодических траекторий и континуумом устойчивых по Пуассону\nтраекторий. В большинстве случаев пространство параметров может быть\nразбито на две области — с простым или сложным поведением. Одним из\nосновных признаков сложного поведения является наличие в системе\nгомоклинической траектории Пуанкаре.\n8\n\n\fРазвитие\nнелинейной\nдинамики\nопределилось\nоткрытием\nхаотических систем, модели которых имеют простой вид. В середине\nсемидесятых начались исследования модели [85]:\nx = −σ( x − y ),\ny = rx − y − xz ,\nz = −bz + xy,\nхаотическое поведение решений которой, численно определил Э. Лоренц в\n1962 году [206]. Система Лоренца построена как галеркинское\nприближение задачи о плоском слое жидкости. Модель Лоренца явилась\nфактическим доказательством существования хаоса. В системе существует\nтак называемый странный аттрактор: поведение траекторий неустойчиво\nпри малых гладких возмущениях системы. Для аттракторов лоренцева\nтипа (с единственным положением равновесия — седло) характерно, что\nпри помощи конечного числа бифуркаций к ним можно перейти от систем\nс тривиальной динамикой [26, 27, 40].\nС появлением многочисленных и тщательных исследований системы\nЛоренца [17, 26, 62, 65, 139, 225, 230, 238, 242 и др.], динамический хаос\nстал общепризнанным. Однако позже было строго доказано, что более\nблизкая к физике является математическая модель Чуа [166, 167], в\nкоторой также имеет место динамический хаос [20].\nОбъектом анализа теории динамических систем является\nгамильтонова динамика, которая в части анализа интегрируемых систем\nпривела к КАМ-теории (А. Н. Колмогоров, В. И. Арнольд, Ю. Мозер [72,\n11, 12]). Согласно теории многие качественные особенности сохраняются\nпод действием возмущений, а также возникают в типичных ситуациях,\nнапример, в окрестности эллиптической точки.\nВ общем, можно сделать вывод, что имеются теоретические\nпредпосылки для использования моделей нелинейной динамики для задач\nмоделирования технических объектов и имеется возможность\nиспользования методов управления для хаотических объектов.\nУправление системами с нелинейным динамическим поведением.\nДинамическая природа хаотических режимов и их чувствительности по\nотношению к малым возмущениям определяют возможности по\nэффективному управлению. Целью внешнего воздействия может быть\nреализация в системе периодического режима вместо хаоса или попадание\nв заданную область фазового пространства. Этот подход, предложенный\nгруппой американских исследователей из университета штата\nМериленд [213], стал основой для решения прикладных задач.\nУспешные примеры управления хаосом реализованы в механических\nсистемах, электронных устройствах, лазерах. В обзоре [80] приведен\nпример расчета управления космического аппарата при полете на Луну.\nПредлагается с помощью малых контролируемых воздействий решить\nзадачу с существенной экономией топлива.\n9\n\n\fДругое направление применения идей и методов нелинейной\nдинамики связано с проблемой обработки сигналов [9]. Предложены\nметодики, позволяющие выяснить, произведен ли сигнал динамической\nсистемой, а также получить информацию о свойствах и характеристиках\nэтой системы. Таким образом, аппарат нелинейной динамики\nпревращается в инструмент исследования, позволяющий сделать\nзаключение или предположение о структуре объекта, сконструировать его\nдинамическую модель и т. д. Разработку методов и алгоритмов анализа\nсигналов можно считать важным направлением нелинейной динамики,\nнепосредственно связанным с возможными приложениями.\nСуществуют\nпримеры\nуспешного\nиспользования\nметодов\nнелинейной динамики для анализа и обработки сигналов, конструирования\nмоделей, а также методик управления хаосом применительно к проблемам\nмедицины и биологии [7, 92].\nВ радиотехнике и электронике разработаны генераторы\nшумоподобных колебаний, функционирующие в режиме динамического\nхаоса [8, 15, 19, 79, 202, 211]. Например, генераторы с запаздывающей\nсвязью, на лампе бегущей волны и на лампе обратной волны,\nпредложенные академиком Гинзбургом. Одно из возможных приложений\nхаоса состоит в использовании генерируемых динамическими системами\nхаотических сигналов в целях коммуникации.\nОснованные на хаотической природе сигналов создаются и широко\nиспользуются новые методы кодирования информации, например, с целью\nсделать ее труднодоступной для перехвата.\nРезультаты, полученные в нелинейной динамике, используются при\nсжатии и хранении информации. Интересным примером такого рода может\nслужить предложенная в Институте радиотехники и электроники РАН\nсхема кодирования с использованием одномерных отображений.\nСуществуют\nработы\nпо\nаналитическому\nпроектированию\nавтоматических регуляторов. Например, работы А. А. Колесникова [70],\nН. А. Магницкого и С. В. Сидорова [87, 88] и др. [14, 87, 159, 173, 193].\nСозданию и классификации методов управления посвящены исследования\nпредставителей школы Саратовского государственного университета\n(В. С. Анищенко и др.) [8, 15 и др.].\nШирокое распространение получили методы по проектированию\nсистем управления, основанные на нечеткой логике и принципах\nадаптивности (Л. М. Пекора, Дж. Х. Пенг, К. Танака и др.) [216, 217, 218,\n224, 234].\nДля класса задач управления хаотическими системами имеются\nрезультаты по оценке времени управления хаосом [137].\nПостроение инвариантных характеристик по наблюдаемым данным.\nЗначительный интерес для практического применения представляют\nметоды вычисления инвариантных характеристик и реконструкции\nаттрактора по временному ряду, полученному в ходе экспериментального\n10\n\n\fисследования нелинейных систем (Ф. Такенс, Д. Рюэль, Н. Паккард,\nА. Вольф, Г. Г. Малинецкий, В. С. Анищенко, и др.) [6, 26, 29, 90, 91, 214,\n233, 239]. Практическое применение и классификации методов\nреконструкции инвариантных характеристик изложены в четвертой главе.\nАнализ публикаций по использованию инвариантных характеристик\nпоказывает, что доведено до практического применения в основном\nмоделирование на основе нейросетевых и нечетких моделей [76]. В\nобласти управляемых систем, такие модели ограничивают методы\nуправления адаптивным подходом, что не позволяет в полной мере\nиспользовать нелинейную теорию динамических систем.\nРеконструкция систем по экспериментальным данным. Проблема\nопределения вида динамической системы по ее одномерной реализации\nотносится к классу некорректных задач. В отличие от задачи анализа\nданная проблема неоднозначна, т. к. существует бесконечное множество\nдинамических систем различного вида и различной сложности, способных\nвоспроизвести имеющийся сигнал с заданной степенью точности. К настоящему моменту разработаны лишь общие рекомендации для случаев,\nкогда исходная система не является слишком сложной [7].\nМетод глобальной реконструкции уравнений динамической системы\nпо ее одномерной реализации был предложен в [169, 170]. Алгоритм\nсостоит в следующем. По одномерной реализации процесса в некоторой\nсистеме, которая считается «черным ящиком», восстанавливается фазовый\nпортрет, по теореме Такенса, топологически эквивалентный аттрактору\nисходной системы [232]. По априорно заданным уравнениям, находится\nметодом наименьших квадратов набор неизвестных коэффициентов.\nВ настоящее время имеется значительное количество работ,\nразвивающих и совершенствующих предложенный метод [9, 90, 152, 153,\n154, 161, 180, 221, 226 и др.]. Например, в работах Р. Брауна и др. [159] для\nвоссоздания динамических уравнений по экспериментальному временному\nряду с широкополосным сплошным спектром использовалась\nдополнительная информация о динамических и статистических свойствах\nисходной системы, содержащаяся в реализации. При получении уравнений\nучитывались значения показателей Ляпунова [108] и плотности\nвероятности, рассчитанные по исходному временному ряду. Однако\nрезультирующие эволюционные уравнения имели очень громоздкий вид,\nнеудобный для применения. В работе [160] для записи модельных\nуравнений использовались скрытые переменные. В [159] описывается\nметод синхронизации модели с исходными данными. В ряде работ\nО. Л. Аносова\nпредложен\nалгоритм\nвосстановления\nскалярного\nдифференциального уравнения для систем с задержкой.\nОднако особенностью многих работ является то, что предлагаемые\nметоды проиллюстрированы на примерах простых маломерных модельных\nсистем, когда заранее известно, каким должен быть результат глобальной\nреконструкции. При этом не показаны существенные преимущества,\nдаваемые каким-либо усовершенствованным методом по сравнению с [169].\n11\n\n\fОписываемые в доступных публикациях алгоритмы тестируются на ряде\nизвестных модельных систем, имеющих малую размерность и достаточно\nпростой вид правых частей [169]. Например, в обзоре [7] авторами\nаргументация в пользу новых сложных алгоритмов представляется\nнеубедительной, т. к. работоспособность методов не продемонстрирована\nна примере сложных временных рядов, генерируемых реальными\n«черными ящиками».\nК настоящему времени существует незначительное количество\nпубликаций, в которых описывается применение данных методик к\nсигналам, порожденным реальными системами, об операторе эволюции\nкоторых ничего не известно. Все это, с одной стороны, создает\nпредпосылки для разработки методологических основ моделирования\nнелинейных явлений по экспериментальным данным, с другой —\nнерешенной остается задача создания математических методов и моделей,\nпозволяющих описывать качественное динамическое поведение реальных\nтехнических систем с заранее неизвестными структурами моделей.\nСложность задачи состоит в необходимости работать с\nзашумленными данными при обработке экспериментальных временных\nрядов. С одной стороны, более желательным является использование\nметода последовательного дифференцирования для восстановления\nфазовой траектории, поскольку при этом можно получить модель,\nсодержащую в общем случае приблизительно в п раз меньше\nкоэффициентов при различных нелинейностях, чем при использовании\nметода задержки. Но дифференцирование неизбежно будет приводить к\nусилению шумовой компоненты в производных высокого порядка. Без\nпредварительной фильтрации зависимость от времени уже второй\nпроизводной может оказаться шумоподобным процессом. Кроме того,\nтрадиционные методы вложения [91, 123] имеют очевидные недостатки\nпри анализе существенно неоднородных реализаций, т. е. сигналов, в\nкоторых участки с быстрым движением чередуются с участками медленных\nдвижений.\nПроизвольный выбор нелинейностей, как правило, не позволяет\nосуществить удачную реконструкцию динамических уравнений для\nреальных систем. В частности, в работе [181] указывается на наличие трех\nтипичных случаев:\n1. Восстановленные уравнения локально описывают фазовую\nтраекторию исходной системы. При этом реконструированная модель\nнеустойчива в том смысле, что решение полученных уравнений\nвоспроизводит исследуемый сигнал только в течение короткого\nпромежутка времени.\n2. Имеет место плохая локальная предсказуемость фазовой\nтраектории, однако наблюдается визуальное сходство фазовых портретов.\nРешение восстановленных уравнений устойчиво по Пуассону. В этом\nслучае аттрактор реконструированной модели имеет метрические\nхарактеристики, близкие к характеристикам исходного аттрактора.\n12\n\n\f3. Имеет место хорошая локальная предсказуемость фазовой\nтраектории с любой ее точки при значениях времени, превышающих\nхарактерное время корреляции. Фазовый портрет реконструированной\nмодели идентичен исходному, а сама система является устойчивой по\nПуассону.\nАлгоритм глобальной реконструкции в последнее время стал\nиспользоваться не только для получения математической модели, но и для\nклассификации динамических режимов [200]. Метод классификации\nпредполагает переход из фазового пространства исходной динамической\nсистемы в пространство коэффициентов восстановленных уравнений.\nОсобенность данного подхода состоит в том, что применение к\nэкспериментальным данным алгоритма реконструкции не нацелено на\nполучение модели, способной воспроизводить исходный режим.\nКоэффициенты в получаемых уравнениях являются количественными\nхарактеристиками, несущими информацию о линейных и нелинейных\nкорреляциях в исходном сигнале. В результате аппроксимации\nкоэффициентов по участкам реализации динамической системы\nисследователь получает множество точек в пространстве коэффициентов.\nРазным классам динамических систем отвечают непересекающиеся в\nданном пространстве области, что и дало основание использовать\nописанную методику в целях классификации.\nМонография имеет следующую структуру по главам:\nглава 1 — определение вида и класса динамических моделей;\nописание системы определений и обозначений, используемых в работе,\nнекоторые необходимые для последовательного изложения положения из\nкачественной теории нелинейных систем;\nглава 2 — изложение основ теории групп симметрий систем с\nуправлениями и разработка теоретических основ группового анализа\nдискретных систем;\nглава 3 — построение теории о центральном многообразии,\nопределяющей условия существования выбранного класса моделей в\nлокальной области с учетом групповых свойств;\nглава 4 — алгоритмы оценки инвариантных характеристик, метода\nпостроения моделей и методики идентификации их параметров;\nглава 5 — разработка технологии повышения качества функционирования управляемых технических систем и описание результатов,\nполученных при внедрении в промышленность.\n\n13\n\n\fГЛАВА 1. ТЕОРЕТИЧЕСКИЕ ОСНОВЫ ГЕОМЕТРИЧЕСКОГО\nПОДХОДА К МОДЕЛИРОВАНИЮ СИСТЕМ\nВ главе определен объект исследования, применительно к которому\nсформулированы понятия качественной теории систем; проведен анализ и\nобобщены положения геометрического подхода к моделированию фазовых\nтраекторий систем с управлением; сформулированы классы решаемых\nзадач и ограничения применимости метода.\n\n1.1. Математические модели нелинейных систем\nПроведем выбор системы допущений к формальному описанию\nнелинейных моделей, реконструируемых по экспериментальным данным с\nучетом требуемых свойств и ограничений, связанный с классом\nуправляемых систем.\nПредположим,\nчто\nданные,\nполученные\nв\nрезультате\nфункционирования управляемых систем, являются выходными процессами\nнелинейной\nаффинной\nдинамической\nсистемы,\nопределяемой\nпреобразованием D × U → Tx X :\n(1.1)\nx = f ( x(t ), t ) + h ( x(t ) ) u (t ),\nгде x —вектор состояний системы из R n ; u(t) — p-мерный вектор\nуправлений из множества допустимых управлений U ⊆ R p (p ≤ n);\nмногообразие X, обычно отождествляемое с R n , либо с R n+1 , в последнем\nслучае многообразие включает независимую переменную t (время);\nx ∈ Tx X ; TxX — касательное пространство к X в точке x, определяемое\nдопустимыми управлениями; f (⋅) = ( f1 , ...., f n ) , h(⋅) = (h1 , ...., hn ) — Cr гладкие (r ≥ 1) вектор-функции, определенные в некоторой области\nкоторая\nрассматривается\nкак\nфазовое\nпространство\nD ⊆ Rn ,\n(ограниченное, неограниченное или совпадающее с евклидовым\nпространством n ).\nНа правую часть уравнения управляемой системы наложены\nограничения:\n1. x\nf ( x ) — гладкое векторное поле на X при любом фиксированном\nu ∈ U;\n2. ( x, u )\nf ( x ) + h(u ) — непрерывное отображение x∈ Х, u ∈ U;\n3. ( x, u ) ∂ [ f ( x) + h(u ) ] / ∂x — непрерывное отображение в любых\nлокальных координатах на X при x∈ Х, u ∈ U.\nДопустимые управления — измеримые локально ограниченные\nотображения\nu :t\nu (t ) ∈ U .\n14\n\n\fПусть допущением является то, что в зависимости от выбора\nуправляющих сигналов, система (1.1) может быть преобразована к\nэквивалентной автономной, т. е. при наличии управления параметризует\nавтономную систему. Таким образом, вспомогательным объектом\nявляются автономные системы обыкновенных дифференциальных\nуравнений, записанные в виде\n(1.2)\nx = f (x).\nДифференцируемое отображение ϕ : τ → D , где τ — интервал на оси t,\nназывается решением х = ϕ (t) системы (1.1), если\n(1.3)\nϕ(t ) = f (ϕ(t )) для любого t ∈τ.\nПо предположению, условия теоремы Коши выполняются, следовательно;\nдля любых значений x0 ∈ D и t0 ∈ R1 существует единственное решение ϕ,\nудовлетворяющее начальному условию\nx0 = ϕ(t0 ) .\n(1.4)\nРешение определено на некотором интервале (t+, t–), содержащем значение\nt = t0. Вообще, граничные точки t+ и t– могут принимать как конечные, так\nи бесконечные значения. Решения системы (1.2) обладают следующими\nсвойствами:\n1. Если х = ϕ (t) — решение системы (1.2), то очевидно, что х. = ϕ (t + C)\nтакже является решением, определенным на интервале (t+– С, t–– С).\n2. Решения х = ϕ (t) и х = ϕ (t + C) можно рассматривать как решения,\nсоответствующие одной начальной точке х0, но различным начальным\nмоментам времени t0.\n3. Решение, удовлетворяющее условию (1.4), можно записать в виде\nх. = ϕ (t – t0, x0), где ϕ(0, х0) = х0.\n4. Если х1 = ϕ (t1 – t0, х0), то ϕ (t – t0, х0) = ϕ (t – t1, х1). Обозначая t1 – t0\nчерез новое t1, a t – t2 — через t2, получаем так называемое групповое\nсвойство решений:\n(1.5)\nϕ(t2 , ϕ(t1 , x0 )) = ϕ(t1 + t2 , x0 ).\nИзвестно, что решение х1 = ϕ (t1–t0, х0) задачи Коши (1.3) для Cr гладкой системы (1.1) является гладким (C r ) относительно времени и\n∂ϕ\n— удовленачальных данных х0. Первая производная η(t − t0 , x0 ) ≡\n∂x0\nтворяет так называемому уравнению в вариациях η = f '(ϕ(t − t0 , x0 ))η с\nначальным условием: η(0, x0 ) = I (I — единичная матрица). Уравнение в\nвариациях — это линейная неавтономная система, полученная\nформальным дифференцированием выражения (1.2).\nВ связи с тем, что теория управления, с одной стороны, изучает\nдинамические модели, с другой — многие вопросы и результаты\nкачественной теории не являются общепринятыми и однозначно\nопределенными. Приведем основные результаты качественной теории\n15\n\n\fнелинейных систем в соответствии с [4, 28], а также с их современными\nэквивалентными представлениями в [143].\nРассмотрим траекторию Λ, отличную от состояния равновесия,\nсоответствует такому решению ϕ(t ) системы (1.2), что ϕ(t1 ) = ϕ(t2 ) при\nt1 ≠ t2 . Тогда ϕ(t ) определено для всех значений t и периодично, a Λ —\nгладкая замкнутая кривая. Если τ — наименьший период ϕ(t ) , тo\nпараметрическое уравнение траектории Λ принимает вид x = ϕ(t ) , где\nt0 ≤ t ≤ t0 + τ , причем в данном интервале различные значения t\nсоответствуют различным точкам траектории Λ. Траекторию Λ,\nназывают\nсоответствующую\nпериодическому\nрешению\nϕ(t ) ,\nпериодической.\nЛюбая другая траектория, не являющаяся ни положением\nравновесия, ни периодической траекторией, есть незамкнутая кривая [4].\nОтсюда следует, что незамкнутая траектория не имеет точек\nсамопересечения.\nЛюбые два решения, которые отличаются лишь выбором начального\nмомента времени t0, соответствуют одной и той же траектории. И\nнаоборот: два различных решения, отвечающие одной и той же\nтраектории, совпадают с точностью до сдвига по времени t → t + C . Таким\nобразом, решения, отвечающие одной и той же периодической траектории,\nпериодичны и имеют равные периоды [5]. Как известно, любая траектория,\nлежащая в ограниченной области, является целой, т. е. для нее определено\nрешение для t ∈ (−∞, + ∞) .\nДля траекторий, отличных от положений равновесия, можно задать\nположительное направление движения, совпадающее с направлением\nвозрастания t. В каждой точке такой траектории направление определятся\nпри помощи соответствующего касательного вектора. Вместе с\nсистемой (1.1) в нелинейной динамике принято рассматривать\nсоответствующую «обращенную во времени» систему\n(1.5)\nx = − f ( x) .\nЗдесь векторное поле системы получается из векторного поля системы\n(1.1) при изменении направления каждого касательного вектора на\nпротивоположное. При этом решение x = ϕ(t) системы (1.1) соответствует\nрешению x = ϕ(–t) системы (1.5) и наоборот. Очевидно, что системы (1.1) и\n(1.5) имеют одни и те же фазовые кривые с точностью до замены времени\nt → −t .\nВажным свойством является возможность при рассмотрении\nфазовых портретов перемасштабирование времени. Для системы (1.1),\nтраектории вида x = ϕ(t − t0 , x0 ) , проходящей через точку x0 при t = t0, при\nпараметризации в соответствии с правилом\n\ndt =\n\ndt\nf (ϕ(t − t0 , x0 ))\n16\n\n\fили\nt\n\ndt = t0 + ∫\nt0\n\nимеется траектория системы\n\ndθ\nf (ϕ(θ − t0 , x0 ))\n\nx = f ( x) F ( x) .\n\n(1.6)\n\n1\nЗдесь Cr -гладкая функция F ( x ) : D\nне обращается в нуль в D.\nСледовательно, системы (1.1) и (1.6) имеют одни и те же фазовые кривые с\nточностью до замены времени. Траектории обеих систем имеют\nодинаковое направление при F(x) > 0 и противоположное при F(x) < 0.\nС точки зрения динамики, целые траектории или траектории,\nкоторые можно определить, по крайней мере, для всех положительных t на\nбесконечном промежутке времени, представляют особый интерес.\nСистемы, решения которых могут быть определены на бесконечном\nпромежутке времени, были названы Биркгофом [28] динамическими.\nОбщепринято определение динамической системы состоит из ее трех\nкомпонентов:\n1) фазового пространства D;\n2) типа времени t: непрерывного ( t ∈ R1 ) или дискретного ( t ∈ Z );\n3) закона эволюции, т. е. отображение заданной точки x ∈ D и\nлюбого t в однозначно определенное состояние\nϕ(t , x) ∈ D ,\nудовлетворяющее теоретико-групповым свойствам:\n\n1. ϕ(0, x) = x.\n2. ϕ(t1 , ϕ(t2 , x)) = ϕ(t1 + t2 , x)) .\n3. ϕ(t , x ) непрерывно по ( x, t ) .\nЕсли переменная t непрерывна, приведенные условия определяют\nнепрерывную динамическую систему, являющейся с точки зрения\nгеометрии потоком, т. е. однопараметрической группой гомеоморфизмов\nфазового пространства D. Фиксируя x и изменяя t от —∞ до +∞, получаем\nориентируемую кривую, называемую фазовой траекторией. Принята\nследующая классификация фазовых траекторий [4]:\n− состояния равновесия;\n− периодические траектории;\n− незамкнутые траектории.\nЗаметим, что в качестве фазового пространства D, как правило,\nиспользуют область в R n , либо на n-мерным торе\nS1 × S1 × ... × S1 ,\nk раз\n\nгладкой поверхности или многообразии. При этом соответствие\nмежду гладким потоком и векторным полем путем определения поля\nскоростей определяется соотношением\n17\n\n\fd ϕ(t , x)\n.\ndt\nt =0\nДля дискретной динамической системы\nx(k + 1) = ψ ( x(k )) ,\nf ( x) =\n\nПоследовательность {x( k )}+∞\nназывается\nk = −∞\nСуществует три типа траекторий:\n\n(1.7)\nтраекторией\n\nточки\n\nx0.\n\n1. Точка x(0). Эта точка является неподвижной точкой гомеоморфизма\nψ ( x ) , т. е. отображается при помощи ψ ( x ) в себя.\n2. Цикл ( x(0), ..., x(k − 1)) , где x(i ) = ψ( x(0)), i = 0, k − 1 ; x0 = ψ k ( x0 ) ; xi ≠ x j\nпри i ≠ j . Здесь число k является периодом; каждая точка xi —\nпериодической точкой с периодом k.\n3. Бесконечная в обе стороны траектория, т. е. последовательность\n{xk }+∞\nk =−∞ , где xi ≠ x j при i ≠ j . Как и в случае потоков, такую траекторию\nназывают незамкнутой.\nВ соответствии с качественной теорией нелинейных систем [3–5,\n143] введем некоторые понятия.\nМножество А называется инвариантным относительно динамической\nсистемы, если A = ϕ(t , A) для любого t. В этом выражении ϕ(t , A) обозначает множество\n\n∪ ϕ(t , A) . Из данного определения следует, что если\n\nx∈ A\n\nx ∈ A , то траектория ϕ(t , A) лежит в множестве А.\n\nТочка x0 называется блуждающей, если существует открытая окрестность U(x0) и такое положительное значение Т, что\nU ( x0 ) ∩ ϕ(t , U ( x0 )) = ∅ при t > T.\n(1.8)\nПрименяя к (1.8) преобразование ϕ(−t , ⋅) , получаем, что\nϕ(−t , U ( x0 )) ∩ U ( x0 ) = ∅ при t > T.\n(1.9)\nСледовательно, определение блуждающей точки является симметричным\nотносительно обращения времени [47].\nМножество блуждающих является открытым и инвариантным.\nОткрытость множества следует из того, что вместе с x0 любая точка в\nокрестности U ( x0 ) является блуждающей. Инвариантность множества\nследует из того, что если точка x0 — блуждающая, то точка ϕ(t0 , x0 ) также\nбудет блуждающей при любом значении t0.\nНапример, для динамической системы\nx = f ( x, θ),\n\nθ = 1,\nзаданной в R\nимеет место (1.8), поскольку θ(t ) = θ0 + t монотонно\nвозрастает с увеличением t. Следовательно, каждая точка фазового\nпространства является блуждающей. Состояния равновесия, и точки,\nn+1\n\n18\n\n\fпринадлежащие периодическим траекториям, являются неблуждающими.\nВсе точки двояко-асимптотических траекторий, которые при t → ±∞\nстремятся к состояниям равновесия и периодическим траекториям, также\nнеблуждающие. Такая двояко-асимптотическая траектория незамкнута и\nназывается гомоклинической. Точки, принадлежащие устойчивым по\nПуассону траекториям, также неблуждающие.\nТочка x0 называется положительно устойчивой по Пуассону, если для\nпроизвольной окрестности U(x0) и любого Т > 0 существует такое значение\nt > Т, что\nϕ(t , x0 ) ⊂ U ( x0 ).\nВ этих условиях если для любого T > 0 существует такое значение t, что\nt < —Т, то x0 — отрицательно устойчивая по Пуассону точка. Если точка\nодновременно и положительно, и отрицательно устойчива по Пуассону, то\nона называется устойчивой по Пуассону.\nЗаметим, что если точка x0 положительно (отрицательно) устойчива\nпо Пуассону, то любая точка на траектории ϕ(t , x0 ) также является положительно (отрицательно) устойчивой по Пуассону.\nТаким образом, вводятся [143] Р+-траектории (положительно\nустойчивая по Пуассону), Р–-траектории (отрицательно устойчивая по\nПуассону) и просто Р-траектории (устойчивая по Пуассону).\nНепосредственно из (1.8) следует, что Р+, Р– и Р-траектории состоят из\nнеблуждающих точек.\nОчевидно, что состояния равновесия и периодические траектории\nявляются замкнутыми Р-траекториями. Если Р+ (Р–, Р)-траектория\nнезамкнута, то ее замыкание Σ содержит континуум незамкнутых Ртраекторий [28]). Р-траектория последовательно пересекает любую εокрестность U ε ( x0 ) бесконечное число раз. Для потоков множество\nзначений, для которых Р-траектория пересекает окрестность U ε ( x0 ) ,\nсостоит из бесконечного множества временных интервалов I n (ε) и tn (ε) —\nодно из значений из I n (ε) .\nВеличина\nτn (ε) = tn +1 (ε) − tn (ε)\n\nназывается временем возвращения Пуанкаре. Здесь последовательность\n{tn (ε)}+∞\n−∞ такая, что t n (ε ) < t n +1 (ε) и ϕ(t n (ε), x0 ) ⊂ U ε ( x0 ). В литературе по\nобыкновенным\nдифференциальным\nуравнениям\nнетривиальным\nвозвращением часто называют наличие непериодических рекуррентных\nточек, что является примером сложного асимптотического поведения.\nТаким образом для незамкнутой Р-траектории возможны 2 случая.\n1. Рекуррентная последовательность, т. е. {τn (ε)} ограничена для\nлюбого конечного ε , т. е. существует число K (ε) , что τn (ε) < K (ε) при\nлюбом п.\n19\n\n\f2. Последовательность {τn (ε)} неограничена при любом достаточно\nмалом ε.\nВ первом случае, все траектории, лежащие в ее замыкании Σ, также\nрекуррентные, а само замыкание — минимальное множество, т. е.\nмножество непусто, инвариантно, замкнуто и не содержит собственных\nподмножеств, обладающих указанными тремя свойствами. Основное\nсвойство такой траектории состоит в том, что она возвращается в εокрестность точки x0 в промежуток времени, не превышающий K(ε).\nОднако в отличие от периодических траекторий, время возвращения\nкоторых фиксировано, времена возвращения рекуррентной траектории не\nограничены.\nВо втором случае замыкание Σ незамкнутой Р-траектории\nназывается квазиминимальным множеством. В данном случае в Σ всегда\nсуществуют другие инвариантные замкнутые подмножества, которые\nмогут быть состояниями равновесия, периодическими траекториями,\nинвариантными торами и т. д. В этом случае времена возвращения\nПуанкаре могут быть сколь угодно велики.\nПо определению, аттрактор Atr — это замкнутое инвариантное\nмножество, имеющее такую окрестность (поглощающую область) U(Atr),\nчто траектория ϕ(t , x) произвольной точки x, принадлежащей U(Atr),\nудовлетворяет условию\nρ(ϕ(t , x), Atr ) → 0 при t → +∞ ,\n\nгде\n\n(1.10)\n\nρ( x, Atr ) = inf || x − x0 || .\nx0∈Atr\n\nПримерами аттракторов являются состояния равновесия, устойчивые\nпериодические траектории и устойчивые инвариантные торы, содержащие\nквазипериодические траектории.\nДля странных аттракторов хаотических систем, являющиеся инвариантными замкнутыми множествами, состоящими только из\nнеустойчивых траекторий выполняется условие квазиминимальности.\n1.2. Качественное исследование динамических систем\n\nВ данном разделе проведем анализ и обоснованный выбор методов\nкачественной теории нелинейной динамики.\nИсследование\nмоделей\nсистем,\nпредставленных\nв\nвиде\nуравнений (1.1) включает поиск решение, т. е. интегрирование системы.\nЭта цель достижима только для линейных систем с постоянными коэффициентами и для некоторых очень специальных уравнений, которые можно\nпроинтегрировать в квадратурах. Поэтому для многих задач уместно\nиспользование нелинейной динамики, которая исследует качественные\n20\n\n\fсвойства: устойчивость, количество состояний равновесия, существование\nпериодических траекторий и т. д.\nКачественное исследование включает два этапа:\n− определение возможных типов траекторий, имеющих различное\nповедение и «формы»;\n− описание для каждой группы топологически схожих траекторий.\nПервый этап заключается в определении траектории движения при\n( t → +∞ ) и при (t → −∞) . При этом делается положение, что траектория L,\nзадаваемая уравнением x = ϕ(t ) , остается в некоторой ограниченной\nобласти фазового пространства при t ≥ t0 (t ≤ t0 ) .\nДля полноты изложения имеет смысл введение следующих понятий.\nТочку x* называют ω-предельной точкой траектории L, если для\nпоследовательности {tk }, где tk → +∞ ,\nlim ϕ(tk ) = x*.\nk →∞\n\n*\n\nТочку x называют α-предельной, если tk → −∞ при k → ∞ .\nОбозначим множество всех ω-предельных точек, принадлежащих\nтраектории L, через Ω L , а множество α-предельных точек — через AL.\nСостояние равновесия является единственной предельной точкой самого\nсебя. В случае если траектория L — периодическая, все ее точки являются\nα- и ω-предельными: L = Ω L = Α L . Если L — незамкнутая устойчивая по\nПуассону траектория, то множества ΩL и AL совпадают с ее замыканием L .\nМножество L является либо минимальным (если L — рекуррентная\nтраектория), либо квазиминимальным множеством, если времена\nвозвращения Пуанкаре траектории L не ограничены. Все состояния\nравновесия, а также периодические и устойчивые по Пуассону траектории\nсамопределъны [66].\nСтруктура множеств ΩL и AL подробно исследована для двумерных\nсистем [4, 5] на плоскости. Пуанкаре и Бендиксон [13] установили, что\nмножество ΩL может быть только одного из трех приведенных ниже\nтопологических типов:\n(1) состояние равновесия;\n(2) периодическая траектория;\n(3) контур, образованный состояниями равновесия и траекториями,\nстремящимся к данным состояниям равновесия при t → ±∞ .\nИспользуя указанную общую классификацию, можно перечислить\nвсе типы положительных полутраекторий систем на плоскости:\n− состояния равновесия;\n− периодические траектории;\n− полутраектории, стремящиеся к состоянию равновесия;\n− полутраектории, стремящиеся к периодической траектории;\n− полутраектории, стремящиеся к предельному множеству\nтипа (3).\n21\n\n\fДля многомерных систем классификация значительно сложнее, т. к.\nкроме состояний равновесия и периодических траекторий, предельные\nмножества могут быть минимальными или квазиминимальными\nмножествами различных топологических типов — таких, например, как\nстранные аттракторы, которые могут быть гладкими или негладкими\nмногообразиями или фрактальными множествами с локальной структурой\nпрямого произведения диска на канторово множество, и даже еще более\nэкзотическими множествами [143].\nВ качественной теории динамических систем [10] для построения\nфазовых портретов используется понятие топологической эквивалентности.\nКак известно [18], две системы называются топологически\nэквивалентными, если существует гомеоморфизм соответствующих\nфазовых пространств, отображающий траектории одной системы в\nтраектории второй. Следовательно, состояния равновесия, а также периодические и незамкнутые траектории одной системы соответственно отображаются в состояния равновесия, периодические и незамкнутые траектории другой системы.\nПонятие топологической эквивалентности двух динамических\nсистем определяет методы разбиения фазового пространства на области\nсуществования траекторий различных топологических типов. Такие\nструктуры должны быть инвариантны относительно всех возможных\nгомеоморфизмов фазового пространства.\nПусть N — ограниченная область фазового пространства, а Н =\n{hi} — множество гомеоморфизмов в N. Вводя метрику\ndist(h1 , h2 ) = sup || h1 x − h2 x || ,\nx∈N\n\nполучим, что траекторию L( L ⊂ N ) можно назвать особой, если для\nдостаточно малого значения ε < 0 для всех гомеоморфизмов hi,\nпереводящих траектории в траектории и удовлетворяющих условию\ndist (hi , I ) < ε , где I — тождественное отображение, выполняется условие\nhiL = L.\nВсе изолированные состояния равновесия и периодически\nтраектории являются особыми траекториями. Незамкнутые траектории\nтакже могут быть особыми. Например, все траектории двумерной системы,\nстремящиеся к седловым состояниям равновесия при t → +∞ при t → −∞ ,\nявляются особыми. Такие траектории называют сепаратрисами [99].\nБудем\nиспользовать\nследующее\nпонятие\nтопологической\nэквивалентности [143]. Две траектории L1 и L2 эквивалентны, если любого\nε > 0 существуют такие переводящие траектории в траектории\nгомеоморфизмы h1 , h2 ,..., hm ( ε ) , что\nL2 = hm ( ε ) ⋅ ⋅ ⋅ h1 L1 ,\nгде dist( hk , I ) < ε ( k = 1, m(ε)).\n\n22\n\n\fЗаметим, что метод исследования, изложенный в классической\nмонографии [5] не применим для общего случая многомерных систем.\nМножество особых траекторий в трехмерной системе может быть\nбесконечным или континуальным. Таким образом, решение задач\nнахождения полного топологического инварианта не является реальным.\n1.3. Топологическая классификация грубых состояний\nравновесия\n\nДля построения методов моделирования приведем сведения о\nпостроении топологически эквивалентных линейных системах и\nклассификации грубых состояний нелинейных систем. Приведенные\nисследования соответствуют [143] и приводятся для обоснованности\nразработанных методов моделирования.\nПоскольку разработанный геометрический метод моделирования\nиспользует модели вида\n(1.11)\nx = Ax + g ( x) ,\nрассмотрим соответствующую линеаризованную систему\n(1.12)\nx = Ax .\nВ соответствии с качественной нелинейной теорией опишем технику\nоценки топологической эквивалентности системы поведения системы\n(1.11) поведению линеаризованной системы (1.12).\nКак известно, две n-мерные системы\nx = F1 ( x) и x = F2 ( x) ,\nопределенные в областях D1 и D2, соответственно, топологически\nэквивалентны в подобластях U1 ⊆ D1 и U 2 ⊆ D2 , если существует\nгомеоморфизм\nη : U1 → U 2 ,\nпод действием которого, траектория (полутраектория, отрезок траектории)\nпервой системы отображается в траекторию (полутраекторию, отрезок\nтраектории) второй системы, с сохранении ориентации (направления\nдвижения).\nЗаметим, что эквивалентности нелинейной системы (1.11) и\nлинеаризованной (1.12) в состоянии равновесия не имеет смысла, если\nесть хотя бы один характеристический показатель на мнимой оси. Таким\nобразом, вопрос о топологической эквивалентности в окрестности\nнегрубого состояния равновесия не имеет смысла.\nПриведем два важных примера из [143], которые иллюстрируют это\nутверждение и демонстрируют основные типы траекторий для случая\nсистем на плоскости.\nРассмотрим нелинейную систему\nx = ωy + g1 ( x, y ),\n(1.13)\ny = ωx + g 2 ( x, y ),\n23\n\n\fгде функции g1 и g2, а также их первые производные обращаются в нуль в\nначале координат. Состояние равновесия имеет пару чисто мнимых\nпоказателей λ1,2 = ±iω ( ω > 0 ).\nОбщее решение соответствующей линеаризованной системы имеет\nвид:\nx = x0 cos(ωt ) − y0 sin(ωt ),\ny = y0 cos(ωt ) − x0 sin(ωt ),\nфазовые траектории которой являются замкнутыми кривыми, в центре\nкоторых лежит начало координат (рис. 1.1). Такое состояние равновесия\nназывается центром.\n\nРис. 1.1. Состояние типа «центр»\nДля нелинейной системы фазовый портрет может значительно\nотличаться от заданного. Например, если g1 = − x( x 2 + y2 ) , g 2 = − y ( x 2 + y2 ) ,\nто общее решение уравнения (1.13) в полярных координатах имеет вид:\n1\nr2 =\n, ϕ = ωt + ϕ0 .\n2t + r0−2\nВ данном случае все фазовые траектории принадлежат к типу\n«седло» (см. рис. 1.2).\n\nРис. 1.2. Пример фазового портрета нелинейной системы.\nТаким образом, в любой малой окрестности такого состояния\nравновесия невозможно отыскать гомеоморфизм, при помощи которого\nтраектории данной системы отображаются в траектории линеаризованной\n24\n\n\fсистемы, в связи с тем, что гомеоморфизм отображает замкнутые кривые в\nзамкнутые кривые. Заметим, что в случае систем с управлением,\nрассматривая семейство систем с независимой управляющей функцией,\nпреобразование может быть найдено в заданном классе.\nРассмотрим систему вида\nx = g1 ( x, y ),\n(1.14)\ny = λy + g 2 ( x, y ).\nПри условии, что один характеристический показатель λ1 равен нулю, а\nвторой отрицательный. В (1.14) предполагается, что функции g1 и g2\nвместе со своими первыми производными обращаются в нуль в начале\nкоординат.\nПо решению линеаризованной\nx = x0 , y = e −λt y0 .\nПостроим фазовый портрет, показанный на рис. 1.3. Ось Oх целиком\nсостоит из состояний равновесия линеаризованной системы, каждое из\nкоторых притягивает только пару траекторий. Очевидно, что нелинейная\nсистема может содержать континуум состояний равновесия только при\nочень специальном выборе функций g1 и g2. Следовательно, между\nисходной и линеаризованной системами топологической эквивалентности\nне существует.\nНа рис. 1.4 показан фазовый портрет для случая, когда g1 = x2 и\ng2 = 0. Из рисунков видно, что два локальных фазовых портрета (так\nназываемый «седло-узел») не могут быть топологически эквивалентными.\n\nРис. 1.3. Пример фазовых траекторий линеаризованной системы.\n\nРис. 1.4. Негрубая точка типа «седло-узел».\n25\n\n\fЗадача исследования локальной топологической эквивалентности\nгрубых состояний равновесия сформулирована в теореме ГробманаХартмана [191], которую можно изобразить следующей коммутативной\nдиаграммой:\nf:\nU1 → U 2\nh\n\n↓\n\n↓h .\n\nDf 0 : V1 → V2\nЗдесь f непрерывное дифференцируемое отображение открытого\nмножества U ⊂ R n в R n ; U1, U2, V1, V2 — окрестности гиперболической\nнеподвижной точки O ∈ U .\nТаким образом, существуют окрестности U1 и U2, в которых\nисходная и линеаризованная системы топологически эквивалентны.\nИзложим результаты о топологической эквивалентности линейных\nсистем. Топологический тип грубого состояния равновесия определяется\nдвумя числами (k, n — k), где k — количество характеристических\nпоказателей, лежащих слева от мнимой оси, а (n — k) — справа от нее.\nИзвестно [18], что линейные системы с состояниями равновесия\nодно типа топологически эквивалентны. Использование этого результата\nR n можно\nконструктивно в том смысле, что гомеоморфизм η : R n\nпостроить в явном виде.\nНапример, для двух линейных систем, первая из которых в начале\nкоординат имеет фокус:\nx = − x + y,\n(1.15)\ny = − x − y,\nа вторая — узел\nx = − x,\n(1.16)\n1\ny = − y.\n3\nЭти системы топологически эквивалентны, так как гомеоморфизм\n( x, y )\n( x cos( τ) − y 3 sin( τ), y 3 cos(τ) − x sin( τ)),\nгде τ( x, y ) = − ln( x 2 + y 6 ) / 2 , отображает траектории системы (1.16) в\nтраектории системы (1.15).\nСледовательно, п-мерная система может иметь только (n + 1)\nразличных топологических типов грубых состояний равновесия. В\nчастности, любая система с грубым состоянием равновесия типа (k, n — k)\nлокально топологически эквивалентна системе\n(1.17)\nx = Ak x ,\nгде\n0 ⎞\n⎛ −I\nAk = ⎜ k\n⎟;\n⎝ 0 In−k ⎠\nIi — единичная матрица размерности i.\n\n26\n\n\fВводя обозначения\n\n⎛s⎞\nx = ⎜ ⎟,\n⎝z⎠\nгде s ∈ k , z ∈ n − k , система (1.17) преобразуется в вид:\ns = −z\nz = s,\nс общим решением\ns (t ) = e − I k t s0 ,\n\n(1.18)\n\n(1.19)\nz (t ) = e I n−k t z0 .\nРассмотрим k = п. При t → +∞ все траектории системы (1.19) стремятся к\nсостоянию равновесия, т. е. любая траектория из достаточно малой\nокрестности состояния равновесия типа (n, 0) нелинейной системы также\nстремится к состоянию равновесия. Такое состояние равновесия принято\nназывать устойчивым топологическим узлом, или стоком. В условиях,\nкогда траектория, начинающаяся в малой окрестности состояния\nравновесия типа (0, п), стремится к нему при t → −∞ , и выходящая за\nокрестность при t → +∞ , состояние равновесия называется неустойчивым\nтопологическим узлом, или источником.\nОстальные грубые состояния равновесия являются топологическими\nседлами. Из теоремы Гробмана-Хартмана следует, что топологическое\nседло исходной нелинейной системы имеет локально устойчивое и\n*\nлокально неустойчивое многообразия Wloc\nи Wloc0 размерности k и (п — k),\nсоответственно. Таким образом, если h — локальный гомеоморфизм, под\nдействием которого траектории линеаризованной системы отображаются\nна траектории нелинейной системы, то образы hE * и hE 0 устойчивого и\nнеустойчивого инвариантных подпространств линеаризованной системы\nкак раз являются устойчивыми и неустойчивыми многообразиями.\nАналогично линейному случаю, положительная полутраектория,\n*\nвыходящая из любой точки многообразия Wloc\n, лежит в нем полностью и\nстремится к состоянию равновесия О при t → +∞ . Подобным образом и\nотрицательная полутраектория, начинающаяся в любой точке\nмногообразия Wloc0 , лежит в нем полностью и стремится к состоянию\n*\n*\nравновесия О при t → −∞ . Траектории точек вне Wloc\nпокидают\n∪ Wloc\n*\nлюбую окрестность седла при t → ±∞ . Многообразия Wloc\nи Wloc0 являются\nинвариантными, т. е. включают в себя все траектории (до тех пор, пока\nтраектории остаются в некоторой окрестности топологического седла).\nОчевидно, что если две системы Х1 и Х2 топологически\nэквивалентны, то при помощи гомеоморфизма, осуществляющего\nтопологическую эквивалентность, состояния равновесия системы Х1\nотображаются на состояния равновесия системы Х2. Если О1 — состояние\nравновесия системы, a O2 — образ О1 относительно гомеоморфизма, то\n\n27\n\n\fтраектория, асимптотическая к О1 при t → +∞ (или, соответственно,\nt → −∞ ), отображается в траекторию, асимптотическую к O1 при t → +∞\n( t → −∞ ). Следовательно, устойчивые (неустойчивые) многообразия\nлокально топологически эквивалентных седел имеют равные размерности.\nТаким образом, для цели исследования важно, что в соответствии с [143]\nдва грубых состояния равновесия локально топологически эквивалентны\nтогда и только тогда, когда они принадлежат одному и тому же\nтопологическому типу.\nТопологический подход позволил решить задачу классификации\nгрубых состояний равновесия. Отметим, что метод не позволяет\nопределить гладкость инвариантных многообразий, поэтому этот вопрос\nтребует дополнительных исследований.\nВ заключении приведем результат о экспоненциальной\nустойчивости узла. В [143] показано, что при малом δ > 0 для любого x0\n( || x0 ||< δ ) траектория x(t) системы (1.11) с начальной точкой x0 при любых\nзначениях t ≥ 0 удовлетворяет неравенству\n|| x(t ) ||≤ Ce(max Re λi +ε )t || y0 || ,\nгде ε( δ ) > 0 — константа; C > 0 — множитель, зависящий от выбора базиса\nв Rn .\n\n\fединственности решений, т. е. через начальную точку\nпринадлежащую области, проходит единственное решение\nx = ϕ ( t , t0 , x0 ) .\nПусть для вектор-функции f выполняется тождество:\nϕ ( t0 , t0 , x0 ) = x0 ,\nиз которого следует равенство\n\ndet\n\n∂ϕi ( t , t0 , x0 )\n\n(t0, x0),\n(2.2)\n\n= 1,\n\n∂x0k\n\nt =t0\n\nа на некотором интервале t ∈ [t0 , t1 ] в некоторой окрестности начальной\nточки (t0, x0) справедливо условие:\n\ndet\n\n∂ϕi ( t , t0 , x0 )\n∂x0k\n\n≠ 0.\n\nИнфинитезимальная образующая однопараметрической группы\nпреобразований,\nпредставляющая\nв\nданном\nслучае\nоператор\nдифференцирования по t, имеет вид [57]:\nn\n∂\n∂\n(2.3)\nX 0 = + ∑ f k (t, x ) k .\n∂t k =1\n∂x\nОператор (2.3) группы удовлетворяет системе уравнений Ли:\n⎧ dt\n⎪⎪ da = 1,\n⎨\n⎪ dx = f ( t , x ) ,\n⎪⎩ da\n\nt ( 0 ) = t0 ,\nx ( 0 ) = x0 .\n\nЗдесь a — независимая переменная (групповой параметр). [Примеры\nиспользования см. 60, 77].\nГруппу сдвига пространства R n+1 (t , x) вдоль решений системы (2.1)\nобразуют преобразования вида\nt = t0 + a ,\n(2.4)\nx = ϕ ( t0 + a, t0 , x0 ) .\nс операцией коммутирования операторов (скобки Ли), причем, единицей\nгруппы является тождественное преобразование. Функция ϕ(⋅) определена\nобщим решением (2.2) системы (2.1). Таким образом, группа (2.4) есть\nоднопараметрическое семейство преобразований, переносящих каждую\nточку (t0, x0) вдоль проходящего через эту точку решения в текущую точку\n(t, x). Из (2.4) следует, что групповой параметр a совпадает с интервалом\na = t – t0 изменения независимой в системе (2.1) переменной t.\nГруппа сдвигов является практической значимой в рамках\nрассматриваемого в работе класса динамических систем с нелинейными\n30\n\n\fколебаниями (например, группами Ли являются группы изометрий на\nплоскости). Известно [260–263], что группами являются преобразования\nтрансляции на компактных коммутативных группах и линейные потоки на\nторе. Как уже отмечалось, колебательные системы, допускают\nпреобразования сдвига, а для хаотических систем со странным\nаттрактором можно считать, что имеет место так называемое слабое\nнарушение симметрии [134].\nДля автономного случая ( ∂f ∂t = 0 ):\n\nx = f ( x) , x ∈\n\nn\n\n,\n\n(2.5)\n\nобщее решение (2.2) имеет вид [105]:\n\nx = ϕ ( t − t0 , x0 ) ,\nсоответствующая группа сдвигов (2.4) —\n\nt = t0 + a ,\n\nx = ϕ ( a, x0 ) .\n\nВ автономном случае последняя группа есть прямое произведение групп,\nодна из которых преобразует переменные t, другая — переменную x\nнезависимо друг от друга. Следовательно, множество преобразований\nx = ϕ(a, x0) само по себе является группой, порожденной оператором\nn\n∂\n∂\nX 0 = + ∑ fk (t, x ) k .\n∂t k =1\n∂x\n\nВ частности, для линейных систем\n\nx = A (t ) x , x ∈\n\nn\n\nс общим решением (2.2)\n\nx = Ф ( t , t0 ) x0 ,\nгруппа сдвигов определяется следующими преобразованиями вдоль\nрешений:\nt = t0 + a,\nx = Φ (t0 + a, t0 )x0 .\nЗдесь Ф ( t , t0 ) — переходная матрица системы. Для стационарной системы\n(A = const) общее решение\nA t −t\nx = e ( 0 ) x0\nопределяет группу:\nt = t0 + a,\n\nx = e Aa x0 .\nТаким образом, определены группы симметрий, определяющие\nосновные преобразования решений колебательных систем.\n31\n\n\f2.2. Методика группового анализа систем с управлением\n2.2.1. Применение классификации систем, допускающих группы\nсимметрий для анализа решений\n\nРассматривается техника вычисления первых интегралов и\nклассификация групп, предложенная Г. Н. Яковенко [148–151, 95, 81].\nДанная классификация определяет методы построения алгебр Ли и\nметодики группового анализа.\nРассмотрим динамическую систему с управлением:\n\nx = f ( t , x, u ) ,\n\nx ∈ R n , u ∈U ⊂\n\nr\n\n,\n\n(2.6)\n\nв D–области пространства R n (t , x) , где x = (x1, ..., xn) — вектор состояний\nдинамической системы; u = (u1, ..., ur) —управление, u ∈U ⊂ r ; t —\nвремя; считаем выполненные допущения по гладкости вектор-функции f,\nсформулированные в п. 1.2.\nПроцедура группового анализа системы (2.6) в соответствии с\nклассом систем состоит из двух этапов: выделение базисных операторов и\nпополнение этой системы операторов.\nГ. Н. Яковенко [148] выделены следующие классы систем,\nдопускающих группы симметрий.\n1. B-система, определяется базисом операторов, получающихся из\nоператора полного дифференцирования по времени t для системы (2.6):\nn\n∂\n∂\nX ( u ) = + ∑ f i ( t , x, u ) i\n∂t i=1\n∂x\n\n(2.7)\n\nпри различных допустимых значений управлений u ∈U ,\nXj =\n\nn\n∂\n∂\n+ ∑ f i j ( t , x, u ) i ,\n∂t i=1\n∂x\n\nj = 1, p,\n\n(2.8)\n\nлинейно несвязанны [109]. Здесь обозначено где fi j (t , x, u ) = fi (t , x, u j ) .\nПодстановка в (2.7) любого другого допустимого управления приводит\nоператору, который линейно связанно выражается через операторы\nX0, X1, …, Xp:\np\n\nX ( u ) = ∑ f i ( t , x, u ) X j .\n\n(2.9)\n\ni =1\n\n2. F-система, определяется полная B-системой операторов (2.8), и\nоператоров, являющихся коммутаторами [⋅, ⋅] операторов (2.8),\nвычисленных в процессе пополнения после выделения базиса (2.8) [105]:\nn\n∂\n(2.10)\nX k = ∑ f ki ( t , x ) i , k = p + 1, m,\n∂x\ni =1\n32\n\n\fпри этом матрица коэффициентов операторов X0, X1, …, Xp, Xp+1, …, Xm\nимеет вид:\n1 … 1\n0 … 0\n\nΘ=\n\nf 01 …\n\nf p1\n\nf p1+1 …\n\nf m1\n\nf 0n …\n\nf pn\n\nf pn+11 …\n\nf mn\n\n.\n\nОтметим, что согласно [105] для полной системы выполняется\n\nrank Θ = m + 1 .\n\n(2.11)\n\nОписанная процедура вычисления F-системы в точке t, x —\nкорректна и содержит конечное число шагов: из построения и из условия\n(2.11) следует p ≤ m ≤ n. При разных же значениях t, x разные управления u\nмогут создавать B-систему (2.8), и к операторам (2.10) могут приводить\nразные последовательности вычисления коммутаторов, вследствие чего, в\nчастности, возможна потеря непрерывности у элементов f ki ( t , x )\nматрицы Θ .\nЗаметим, что можно выделить класс регулярных в D-области\nсистем [41], определяемый условиями: (1) один и тот же набор постоянных\nдопустимых управлений u0, …, up выделяет B-систему (2.8); (2) одна и та\nже\nпоследовательность\nкоммутаторов\nприводит\nк\nF-системе\nX0, X1, …, Xp, Xp+1, …, Xm, содержащей одно и то же количество m\nоператоров.\n3. Ft-система получается, как результат\n\n∂\n, X 0 , X 1 , ..., X m , X m+1 , ..., X M\n∂t\nпополнения системы операторов ∂ ∂t , X 0 , X 1 , ..., X m , где X 0 , X 1 , ..., X m —\nоператоры F-системы.\nВ рамках группового анализа введем для (2.6) понятия решения и\nпервого интеграла [51, 109].\nРешением системы (2.6) является пара вектор-функций x(t), u(t)\n(u(t) — допустимое управление), которая при подстановке ее в систему\n(2.6) приводит к тождеству.\nКак известно, первым интегралом системы с управлением (2.6)\nназывается функция w(t, x), которая на любом решении x(t), u(t) системы\n(2.6) сохраняет постоянное значение [37]\nw ( t , x ( t ) ) = w ( t0 , x0 ) = const .\n\nПри этом стационарным первым интегралом системы с управлением\n(2.6) называется первый интеграл w(x), не зависящий от времени t.\nИнтегральным базисом первых интегралов (стационарных первых\nинтегралов) является такой набор функционально независимых первых\n33\n\n\fинтегралов w1 (t , x), ..., wQ (t , x) ( w1 ( x), ..., wq ( x)) , что для любого другого\nпервого интеграла w(t, x) (w(x)) справедливо с некоторой функцией\nF (⋅, ..., ⋅) равенство\n\nw(t , x) = F ( w1 (t , x), ..., wQ (t , x)) ( w( x) = F ( w1 ( x), ..., wq ( x))).\nПостроение функций w(t, x), являющихся первым интегралом\nсистемы (2.6) определяется решением полной системы [41, 105]\n(2.12)\nX 0 w = 0, X 1w = 0, ..., X m w = 0 ,\nгде X 0 , X 1 , ..., X m — F-система (2.8), (2.10), соответствующая регулярной\nсистеме (2.6). Интегральный базис первых интегралов состоит из n – m\nфункций.\nОтсюда следует, что нетривиальные первые интегралы, т. е. те, для\nкоторых выполнено условие\n⎛ ∂w ( t , x ) ⎞\n⎜\n⎟ ≠ 0.\n∑\n∂xi ⎠\ni =1 ⎝\nотсутствуют, если количество уравнений в полной системе (2.12) равно\nn+1, где n — размерность регулярной системы (2.6).\nСтационарный первый интеграл системы (2.6) строится как решение\nполной системы [150]\n∂w\n= 0; X 0 w = 0, ..., X p w = 0, X M = 0 ,\n(2.13)\n∂t\nгде ∂w ∂t , X 0 , ..., X M — Ft-система: результат пополнения системы\n( X 0 , ..., X m — F-система). Интегральный базис\n∂w ∂t , X 0 , ..., X m\nстационарных первых интегралов содержит n – M функций.\nТак как система (2.12) входит в систему (2.13), любое решение\nсистемы (2.13) является первым интегралом. Первое уравнение ∂w ∂t = 0 в\n(2.13) гарантирует независимость решений от t. Количество n – M\nфункционально независимых первых интегралов следует из полноты\nсистемы (2.13) [106].\nИзвестно, что [149], что если для F-системы нетривиальные первые\nинтегралы отсутствуют, то система управляема.\nВ целях практического анализа рассмотрим систему с периодической\nтраекторией (x, y, u — скаляры) [148]:\nx = sin u + x cos u ,\n2\n\nn\n\ny = cos u ,\n\nu ∈ U = [ 0, 2π ) .\n\nСистеме соответствует оператор (2.7)\n∂\n∂\n∂\nX ( u ) = + ( sin u + x cos u ) + cos u .\n∂t\n∂x\n∂y\nВ условиях, когда множество U состоит, по крайней мере, из двух разных\nуправлений, возможны два альтернативных варианта.\nI. Множеству U принадлежат три разных управления u0, u1, u2.\n34\n\n\fДля определенности с учетом X(u) предполагаем\n0 ≤ u0 < u1 < u2 < 2π.\nПодстановка u0, u1, u2 приводит к операторам:\n∂\n∂\n∂\nX 0 = X ( u0 ) = + ( sin u0 + x cos u0 ) + cos u0 ,\n∂t\n∂x\n∂y\n∂\n∂\n∂\nX 1 = X ( u1 ) = + ( sin u1 + x cos u1 ) + cos u1 ,\n∂t\n∂x\n∂y\n∂\n∂\n∂\nX 2 = X ( u2 ) = + ( sin u2 + x cos u2 ) + cos u2 .\n∂t\n∂x\n∂y\nВычисления для матрицы Θ , составленной из коэффициентов\nоператоров X0, X1, X2, дают результат\n\ndet Θ = 4sin\n\nu −u\nu −u\nu2 − u1\nsin 1 0 sin 0 2 .\n2\n2\n2\n\nПри условии det Θ ≠ 0 операторы X0, X1, X2 — линейно несвязанны, а так\nкак их количество совпадает с размерностью пространства R3 (t , x, y) , то\nпри любом другом управлении u ∈ U оператор X(u) линейно связанно\nвыразится через операторы Xi, i = 0, 1, 2. Через них выразится и каждый\nкоммутатор [Xi, Xj], i, j = 0, 1, 2. Таким образом, если множество U\nсодержит не менее трех управлений, то система является регулярной во\nвсем пространстве R3 (t , x, y) , а система операторов (2.60) — B-система и\nF-система. Первые интегралы в этом случае отсутствуют. В D-области\nвозможна локальная управляемость.\nII. Пусть множество U допустимых значений состоит из двух\nуправлений u0, u1, и соответствующие операторы X0, X1 линейно\nнесвязанны — являются B-системой.\nКоммутатор:\n∂\n[ X 0 , X1 ] = sin ( u0 − u1 ) .\n∂x\nДля матрицы Θ1, составленной из коэффициентов операторов X0, X1 и\n[X0, X1], выполняется\n\ndet Θ1 = 2sin 2\n\nu0 − u1\nu +u\nu −u\nsin 0 1 cos 0 1 .\n2\n2\n2\n\nСправедливо неравенство\n\nu0 − u1\n≠ 0,\n2\nпоэтому возможны только два случая, приводящие к вырожденности\nматрицы Θ1 и к тому, B-система X0, X1 есть F-система.\na) Случай\nu +u\nsin 0 1 = 0 ,\n2\nsin\n\n35\n\n\fреализуемый множеством\n\nU = {u0 = α ≠ 0; u1 = 2π − α} .\nСистеме соответствует B-система, являющаяся F-системой:\n∂\n∂\n∂\nX 0 = + ( sin α + x cos α ) + cos α ,\n∂t\n∂x\n∂y\n∂\n∂\n∂\nX 1 = + ( − sin α + x cos α ) + cos α .\n∂t\n∂x\n∂y\nРешив систему X0w = 0, X1w = 0, приходим к гарантированному первому\nинтегралу системы\n\nw = y − t cos α .\nb) Случай\n\ncos\n\nu0 − u1\n=0\n2\n\nB-система реализуется множеством\nπ\nπ π\nπ⎫\n⎧\nU = ⎨u0 = β − ; u1 = β + ; ≤ β < 3 ⎬ .\n2\n2 2\n2⎭\n⎩\n\nСистеме соответствуют B- и F-системы\n∂\n∂\n∂\nX 0 = − ( cos β − x sin β ) + sin β ,\n∂t\n∂x\n∂y\n∂\n∂\n∂\nX 1 = + ( cos β − x sin β ) − sin β .\n∂t\n∂x\n∂y\nРешение полной системы X0w = 0, X1w = 0, приводит к стационарному\nпервому интегралу\nw = ( cos β − x sin β ) e − y .\nСледовательно, управляемость в любой D-области отсутствует.\n2.2.2. Методика исследования групповых систем\n\nДля практического использования имеет значение класс систем,\nвосходящий к результатам С. Ли [203]. Для этого класса, несмотря на\nфункциональную мощность множества допустимых управлений u(t),\nпреобразование сдвигов вдоль решений принадлежит конечнопараметрической группе. Такой класс называется групповыми системами и\nопределяется следующим образом [51, 148].\nВ групповой системе\nr\n\nxk = ∑ f kl ( x ) ul ( t ) ,\n\nk = 1, n, u ∈ U ⊂\n\nl =1\n\n36\n\n.\n\n(2.14)\n\n\fдля функций f kl ( x) выполнены условия\n\nrank f kl ( x ) = min {n, r} ,\n\n(2.15)\n\n⎧ r l l\n⎫\nl\nl\n⎨∑ c f k ( x ) = 0, c = const ⎬ , ⇒ c = 0, l = 1, r ,\n⎩ l =1\n⎭\n\n{\n\n}\n\nr\n\n⎡⎣ X i , X j ⎤⎦ = ∑ Сijk X k , Сijk = const , i, j , k = l = 1, r ,\n\n(2.16)\n\nl =1\n\nгде обозначено\nn\n\nX l = ∑ f kl ( x )\nk =1\n\n∂\n, l = 1, r ;\n∂x k\n\n[Xi, Xj] — коммутатор операторов. Следовательно, операторы\n\nr\n\n∑u\nk =1\n\nk\n\nXk ,\n\nuk = const, есть алгебра Ли с базисом Xk и структурными постоянными Cijk .\nЕсли в (2.14) выполнено n > r, то полная система уравнений Xlw = 0,\nl = 1, r , имеет n–r функционально независимых решений w1(x), …,\nwn–r(x) — первых интегралов системы (2.14) [30]. При переходе на\nконкретную\nинвариантную\nповерхность\nw1(x) = c1, …, wn–r(x) = cn–r\nколичество уравнений уменьшается до r. Далее предполагается n ≤ r.\nКаждой групповой системе (2.14) ставится в соответствие r–\nпараметрическая группа сдвигов вдоль решений системы\nxk = g k ( x10 , ..., xn 0 , u1 , ..., ur ) , k = 1, n,\n(2.17)\nуравнения, которой по базису соответствующей алгебры Ли можно\nвычислить, например, следующим способом. Каждому оператору Xl\nсистема уравнений dxi dv j = fi j ( x ) , x(0) = x0 ставится в соответствие\nрешение x = g j ( x10 , ..., xn0 , v j ) — однопараметрическую группу преобразований; r-параметрическая группа (2.17) — суперпозиция этих групп,\nуравнения которой вычисляются по (2.14) при ul(t) = const. Группе (2.17)\nсоответствует алгебра Ли с базисом (2.68) и структурными постоянными\nCijk , определенными в (2.16).\nСопоставим паре (u(t), t) преобразование пространства R n — сдвиги\nвдоль решений x(t) групповой системы (2.14), в которую подставлены\nфункции u(t): из точек x(0) = x0 в точки x(t). Преобразование x0 ↔ x(t)\nпринадлежит группе (2.17), т. е. каждой паре (u(t), t) соответствует такой\nнабор параметров v1, …, vr, что преобразование (2.17) и преобразование\nx0 ↔ x(t) совпадают.\nL-система [148] есть групповая система\nr\n\nxk = ∑ f kl ( x ) ul ( t ) ,\n\nk = 1, n, u ∈ U ⊂\n\nl =1\n\nпри r = n.\n37\n\n(2.18)\n\n\fПринадлежность к L-системам инвариантна по отношению к\nx = ϕ( x)\nнеособенному преобразованию\nпеременных состояния.\nДействительно, в переменных x система (2.18) имеет такую же структуру\nr\n\nx k = ∑ fl k ( x ) u l ( t ) ,\n\nk = 1, n ,\n\n(2.19)\n\nl =1\n\nгде обозначено\nfl\n\nФункции fl\n\nk\n\n( x)\n\nk\n\nn\n\n( x) = ∑\ni =1\n\n∂f k ( x )\n∂x\n\ni\n\nfl i ( x )\n\n.\nx→ x\n\nудовлетворяют условиям (2.15)–(2.16), причем с теми же\n\nпостоянными Cijk в (2.16) , что в переменных x. Если системы (2.18), (2.19),\nсвязанные\nнеособенным\nпреобразованием\nсчитать\nx ↔ x,\nэквивалентными, то каждому классу эквивалентности соответствуют\nпостоянные Cijk :\nr\n⎧ k\n⎫\nk\nl\n=\nx\nf\nx\nu\nt\n,\n(\n)\n(\n)\n∑\nl\n⎪\n⎪\nl =1\n⎪⎪\n⎪⎪\n⎨ x↔x\n⎬.\n⎪\n⎪\nr\nk\nk\nl\n⎪ x = ∑ fl ( x ) u ( t ) ,⎪\n⎪⎩\n⎪⎭\nl =1\n\n(2.20)\n\nПриведенное соответствие является взаимно однозначным: по\nструктурным постоянным Cijk в некоторых переменных вычисляется базис\nX1, …, Xn, алгебры Ли, по операторам Xl определяется представитель\nкласса эквивалентности (2.20) с возможностью заменой переменных\nперейти к другому представителю.\n2.2.3. Методика исследования симметрий по состоянию\n\nДля исследования моделей нелинейных систем разработаем\nметодику группового анализа систем, допускающих симметрии фазовых\nтраекторий\nв\nD-области,\nт. е.\nоднопараметрическую\nгруппу\nпреобразований (τ — групповой параметр):\n\nx = x ( t , x, τ ) ,\n\n(2.21)\n\nс инфинитезимальным оператором фазовой траектории\nn\n\nY = ∑ εi ( t , x )\nk =1\n\n∂x j ( t , x, τ )\n∂\n, εj =\n.\n∂xk\n∂τ\nτ=0\n\n(2.22)\n\nЗдесь переменные t и u преобразуются тождественно; преобразования\nгруппы не зависят от управления u.\n\n38\n\n\fТаким образом, неособенное преобразование x = x(t , x) является\nпреобразованием симметрии D-области фазового портрета (2.21), если\nзамена переменных в (2.21) приводит к системе\ndx\n= f ( t , x, u ) ,\n(2.23)\ndt\nс такими же функциями f в правой части, что и у исходной системы (2.21).\nТо есть группу симметрий, допускаемая системой (2.6), образуют\nоднопараметрические преобразования вида (2.21), при этом преобразуются\nтолько состояния x, принадлежащие D-области фазовой траектории.\nСледуя определению, группу симметрий фазовых траекторий можно\nхарактеризовать следующим переводом решений в решения:\ngraph {x(t), u(t)} → graph {x(t), u(t)},\nт. е. группа (2.21) является группой симметрий для каждой системы\nобыкновенных дифференциальных уравнений, получающейся подстановкой в (2.23) конкретного значения допустимого управления u(t).\nДля группы симметрий системы обыкновенных дифференциальных\nуравнений определяются из условий коммутирования\n(2.24)\n∀u ( t ) , ⎡⎣ X ( u ( t ) ) , Y ⎤⎦ = 0 .\nЗаметим, что условие (2.24) можно получить независимо, если\nпродифференцировать (2.22) при замене переменных (2.21) по групповому\nпараметру τ, положить затем τ = 0 и раскрыть полную производную\nd ε (t, x )\n= X ( u ( t ) ) εi ( t , x ) .\ndt\nВ связи с тем, что в операторе (2.24) отсутствует дифференцирование по\nвремени t, поэтому формула, полученная после раскрытия коммутатора в\n(2.24), будет содержать только значения вектор-функции u(t) и не\nсодержать значений производных от управляющей функции. То есть\nусловие (2.24) имеет вид:\n∀u, ⎡⎣ X ( u ) , Y ⎤⎦ = 0 .\n(2.25)\nЗдесь использован оператор (2.7) с постоянными управлениями u.\nВыполнение условий (2.24), (2.25) имеет место для регулярных\nсистем [148], а также при выполнении условий\n\n[ X k , Y ] = 0,\n\nk = 0, m ,\n\n(2.26)\n\nYf j = 0,\n\nj = 0, p ,\n\n(2.27)\n\nгде Xk — операторы (2.8), (2.10) F-системы, f j — функции в линейной\nсвязи (2.9), Y — оператор (2.22) группы (2.21).\nСовокупность операторов симметрий по состоянию (2.22) является\nалгеброй Ли [83]: если операторы Y1 и Y2 удовлетворяют условию (2.24)\n(или (2.25)), то операторы aY1 + bY2; (a, b = const) и [Y1, Y2] также\nудовлетворяют (2.24) ((2.25)).\n39\n\n\fОбозначим алгебру Ли A0, при этом множество операторов Y\nсимметрий фазовых траекторий, удовлетворяет одному из эквивалентных\nусловий (2.24), (2.25).\nВ отличие от систем обыкновенных дифференциальных уравнений в\nнормальном виде, у которых множество операторов симметрий всегда\nимеет функциональную мощность [38], у системы с управлением (2.23)\nвозможны ситуации:\n1) Алгебра A0 состоит из единственного оператора Y = 0, а группа\nсимметрий — из тождественного преобразования.\n2) Множество операторов симметрий не вмещается ни в одну\nконечномерную алгебру Ли.\nОсобое положение занимают системы с пустым интегральным\nбазисом первых интегралов. Предположение об отсутствии у регулярной\nсистемы (2.23) нетривиальных первых интегралов можно записать в виде\n[148]:\n\n{∀u ∈U , X ( u ( t ) ) = 0} ⇒ {w ≡ const} ,\nгде X(u(t)) — семейство операторов.\nОтметим, что если регулярная система (2.6) не имеет нетривиальных\nпервых интегралов, тогда алгебра A0 операторов Y симметрий по\nсостоянию конечномерна, и ее размерность q не превосходит размерности\nn пространства состояний системы (2.23): 0 ≤ q ≤ n [148].\nКонечномерность алгебры Ли с базисом Yk, k = 1, q , и\nпринадлежность коммутаторов [Yi, Yj] базисных операторов к этой алгебре,\nв частности, означают, что для [Yi, Yj] справедливы равенства\nq\n\n⎡⎣Yi , Y j ⎤⎦ = ∑ Cijk Yk , i, j = 1, q ,\nk =1\n\nгде числа Cijk — структурные постоянные q-мерной алгебры Ли: алгебры\nA0. По базисным операторам Yk, k = 1, q , алгебры Ли вычисляется qпараметрическая группа [105]:\nx = x ( t , x, τ ) ,\n\nτ∈\n\nq\n\n.\n\n(2.28)\n\nкоторой принадлежат все однопараметрические группы симметрий по\nсостоянию системы (2.23). Числа Cijk являются структурными\nпостоянными и для группы (2.28).\nРазмерность q алгебры A0 и соответствующих базисных операторов\nYk, k = 1, q , определяются характеристиками систем (2.26), (2.27).\nРегулярные системы (2.23) с нетривиальной алгеброй A0; (dim A0 ≥ 1)\nобладают свойством декомпозиции системы (2.6).\nСогласно [149], регулярная система (2.23) допускает qпараметрическую группу симметрий по состоянию (2.28) (q ≤ n) с\n40\n\n\fоператорами Yk, k = 1, q , в том и только в том случае, если неособенным\nпреобразованием x ↔ y, z системе (2.23) можно придать вид\n\ny = fi (t , y, u ), i = 1, n − q,\nq\n\nzk = ∑ flk ( z ) fl (t , y, u ), k = 1, q,\n\n(2.29)\n\nl =1\n\nпричем для квадратной матрицы || flk ( z ) || необходимо выполнение\ndet f lk ( z ) ≠ 0 ,\nn\n\n⎡⎣ Z i , Z j ⎤⎦ = ∑ Cijk Z k ,\n\ni, j = 1, q ,\n\nk =1\n\nгде\n\n∂\n, l = 1, q .\n∂\nz\nk =1\nk\nk\nЧисла Cij совпадают со структурными постоянными группы симметрий\n(2.28) и соответствующей алгебры A0.\nПриведение системы (2.23) к указанному виду (2.29) эквивалентно\nпроцессу приведения преобразованием x ↔ y, z базисных операторов Yj,\nk = 1, q , алгебры A0 к виду:\nq\n\nZ l = ∑ flk ( z )\n\n∂\n, l = 1, q, z ∈ q .\nk\n∂z\nk =1\nТаким образом, методику группового анализа нелинейных\nуправляемых систем, допускающих симметрии локальных областей\nфазовых траекторий можно представить в виде алгоритма, состоящего из\nследующих шагов.\n1. Вычисление инфинитезимального оператора (2.7).\n2. Вычисление операторов (2.8) B-системы.\n3. Составление матрицы из коэффициентов операторов Xi;\nвычисление определителя матрицы.\n4. Дополнение базиса операторов до F-системы.\n5. Проверка конечнопараметричности группы (2.28).\n6. Вычисления коэффициентов оператора симметрий:\n∂\n∂\nY = θ ( t , x , y ) + µ ( t , x, y ) ,\n∂x\n∂y\nсоответствующего (2.25).\n7. Построение эквивалентной системы.\n8. Вычисление решений эквивалентной системы.\n9. Построение группы симметрий фазовых траекторий 2.28.\n10. Анализ управляемости и возможности построения моделей по\nнаблюдаемым частным решениям.\nq\n\nYl = ∑ εlk ( z )\n\n41\n\n\fРассмотрим 2 примера, иллюстрирующие описанную технику.\nПример 1. Рассмотрим систему\nx = ux + v + by 2 ,\n(2.30)\ny = u,\nгде x, y — переменные состояния, u, v — управления, b = const.\nСистеме соответствует оператор (2.7):\n∂\n∂\n∂\nX ( u , v ) = + ( ux + v + by 2 ) + u\n∂t\n∂x\n∂y\nи операторы (2.8) B-системы\n∂\n∂\nX 0 = X ( 0, 0 ) = + ( by 2 ) ,\n∂t\n∂x\n∂\n∂ ∂\nX 1 = X (1, 0 ) = + ( x + by 2 ) + ,\n∂t\n∂x ∂y\n∂\n∂\nX 2 = X ( 0,1) = + (1 + by 2 ) .\n∂t\n∂x\nОпределитель матрицы, составленной из коэффициентов операторов Xi,\nпри любых значениях параметров a и b равен 1.\nУсловие (2.9) выполняется в случае:\nf 0 = 1 − u − v,\nf1 = u ,\nf 2 = v.\nЗаметим, что B-система Xi, является и F-системой, т. к. операторы Xi,\nлинейно выражается любой оператор X(u, v), а также каждый оператор в\nпространстве переменных t, x, y, в том числе коммутаторы операторов Xi,.\nНетривиальные первые интегралы у системы (2.30) отсутствуют, система\n(2.30) допускает конечномерную алгебру A0 симметрий (конечнопараметрическую группу (2.28)).\nДля вычисления коэффициентов θ(t, x, y), µ(t, x, y) оператора\nсимметрий (2.22):\n∂\n∂\nY = θ ( t , x , y ) + µ ( t , x, y ) ,\n∂x\n∂y\nпреобразуем его к эквивалентной системе операторов\n∂\n∂\nX 0* = X 0\n= + by 2 ,\n∂t\n∂x\n∂\n∂\nX 1* = X 1 − X 0 = x + ,\n∂x ∂y\n∂\nX 2* = X 2 − X 0 = .\n∂x\nИспользуем полученные уравнения, а также тот факт, что (2.26), (2.27) для\nкоэффициентов выполняются тождественно, т. е. условий на функции\n\n42\n\n\fθ(t, x, y), µ(t, x, y) не накладывают. Одно из уравнений [ X 2* , Y ] = 0 системы\n(2.26) имеет вид:\n∂θ ∂ ∂µ ∂\n+\n= 0,\n∂x ∂x ∂x ∂y\nчто эквивалентно системе относительно коэффициентов оператора\n∂µ\n∂θ\n= 0,\n= 0.\n∂x\n∂x\nС учетом этого уравнение [ X 2* , Y ] = 0 системы (2.26) принимает вид:\n∂θ ∂ ∂µ ∂\n∂\n+\n− θ = 0,\n∂x ∂x ∂y ∂y\n∂x\nили\n∂µ\n∂θ\n= 0,\n= 0,\n∂x\n∂x\nи результату для коэффициентов оператора: θ = ν(t)ey, µ(t).\nСледовательно, оставшееся уравнение [ X 2* , Y ] = 0 системы (2.26)\nпринимает вид:\n∂\n∂\n∂\nµ (t ) + ν (t ) e y\n− 2 {byµ ( t )} = 0 ,\n∂x\n∂y\n∂x\nчто эквивалентно системе\nµ ( t ) − 2 {byµ ( t )} = 0,\nν (t ) e y = 0 ,\nкоторая после «расщепления» по переменным t, x, y и очевидных\nсокращений приводит к требованию выполнения равенств:\nµ ( t ) = 0,\nbµ ( t ) = 0, ν ( t ) = 0 .\n(2.31)\nОпределим группу симметрий фазовых траекторий системы (2.29), к\nкоторым сводится соответствующей заменой переменных система (2.30).\nИз (2.31) следует ν = C1 = const, µ = 0, а с учетом θ = νey: θ = С1ey,\nµ = 0, т. е. алгебра симметрий A0 одномерна с базисным оператором\n\nY1 = e y\n\n∂\n.\n∂x\n\nВычисление по полученному оператору, однопараметрической\nгруппы симметрий (2.28), дает результат:\nx = x + τe y ,\ny = y.\nЗамена переменных x, y ↔ z = xey, y приводит систему к\nиерархическому виду (2.29):\ny = u,\nz = (v + by 2 )e − y .\nТаким образом, замена x, y ↔ z = xey, y вычисляется при переводе\nоператора (2.30) к выпрямленному виду Z = ∂ ∂z .\n\n43\n\n\fПример 2. Для системы\nx = ux + v + ax 2 ,\ny = u,\n(a = Const) оператор (2.7) имеет вид\n∂\n∂\n∂\nX ( u , v ) = + ( ux + v + ax 2 ) + u .\n∂t\n∂x\n∂y\nОператоры (2.8) B-системы\n∂\n∂\nX 0 = X ( 0, 0 ) = + ax 2\n,\n∂t\n∂x\n∂\n∂\n∂\nX 1 = X (1, 0 ) = + x + ax 2\n+ ,\n∂t\n∂x ∂y\n∂\n∂\nX 2 = X ( 0,1) = + 1 + ax 2\n.\n∂t\n∂x\nУсловие (2.9) выполняется в случае:\nf 0 = 1 − u − v,\n\n(2.32)\n\n( )\n\n(\n\n)\n\n(\n\n)\n\nf1 = u ,\nf 2 = v.\nВидно, что B-система Xi, является и F-системой, т. к. операторы Xi,\nлинейно выражается любой оператор X(u, v), а также каждый оператор в\nпространстве переменных t, x, y, в том числе коммутаторы операторов Xi,.\nНетривиальные первые интегралы у системы (2.32) отсутствуют, система\nдопускает конечномерную алгебру A0 симметрий.\nДля вычисления коэффициентов θ(t, x, y), µ(t, x, y) оператора\nсимметрий преобразуем к эквивалентной системе операторов\n∂\n∂\nX 0* = X 0\n= + ax 2 ,\n∂t\n∂x\n∂\n∂\nX 1* = X 1 − X 0 = x + ,\n∂x ∂y\n∂\nX 2* = X 2 − X 0 = .\n∂x\nИз полученных уравнений следует, что (2.26), (2.27) для коэффициентов\nвыполняются тождественно, т. е. условий на функции θ(t, x, y), µ(t, x, y) не\nнакладывают.\nПервое из уравнений [ X 2* , Y ] = 0 системы (2.26) имеет вид:\n∂θ ∂ ∂µ ∂\n+\n= 0,\n∂x ∂x ∂x ∂y\nчто эквивалентно системе относительно коэффициентов оператора\n∂µ\n∂θ\n= 0,\n= 0.\n∂x\n∂x\n\n44\n\n\fС учетом этого уравнение [ X 2* , Y ] = 0 системы (2.26) принимает вид:\n∂θ ∂ ∂µ ∂\n∂\n+\n− θ = 0,\n∂x ∂x ∂y ∂y\n∂x\nили\n∂µ\n∂θ\n= 0,\n= 0,\n∂x\n∂x\nи результату для коэффициентов оператора: θ = ν(t)ey, µ(t).\nСледовательно, оставшееся уравнение [ X 2* , Y ] = 0 системы (2.26)\nпринимает вид:\n∂\n∂\n∂\nµ (t ) + ν (t ) e y\n− 2axν ( t ) e y\n= 0,\n∂x\n∂y\n∂x\nили\nµ ( t ) − 2axν ( t ) e y = 0,\nν (t ) e y = 0 ,\nкоторая после очевидных преобразований приводит к требованию\nвыполнения равенств:\nµ ( t ) = 0,\nav ( t ) = 0, ν ( t ) = 0 .\n(2.33)\nОпределим группу симметрий фазовых траекторий системы (2.32), к\nкоторым сводится соответствующей заменой переменных система (2.33).\nПолучим группы симметрий фазовых портретов.\nИз (2.32) следует, что ν = 0, µ = С2,:\nθ = νe y = 0, µ = C2 ,\nт. е. алгебра симметрий A0 одномерна с базисным оператором\n∂\nY2 = .\n∂y\nСоответствующая однопараметрическая группа симметрий фазовых\nтраекторий:\nx = x,\ny = y + τ2 .\nГрупповые системы, несмотря на ярко выраженную групповую\nструктуру, могут допускать группу симметрий по состоянию, состоящую\nтолько из тождественного преобразования. Это имеет место для случая,\nкогда количество параметров в группе симметрий по состоянию совпадает\nс размерностью пространства состояний (q = n) [148].\nВ [148] доказано, что регулярная n-мерная система с управлением\n(2.23) допускает n-параметрическую группу симметрий по состоянию\n(2.28) в том и только в том случае, если (2.23) является L-системой (2.14)\nпри r = n.\nСледовательно, выбором базисных операторов Yl в алгебре\nсимметрии A0 или операторов Xl, определяющих L-систему, можно\nдобиться совпадения структурных постоянных Cijk у группы симметрий\n(2.28) и у группы сдвигов вдоль решений (2.17). То есть, произвольной\n45\n\n\fгрупповой системе (2.14) можно поставить в соответствие (неоднозначно)\nL-систему, а, следовательно, и группу симметрий.\nПусть для (2.14) выполняется\nn > r.\nn\nТогда пространство состояний R ( x ) расслаивается на инвариантные\nповерхности системы (2.14):\n\nw1 ( x ) = c1 , ..., wn−r ( x ) = cn−r.\nСуществует неособенная замена переменных\nx1, ..., xn ↔ z1, ..., zr, w1, ..., wn – r,\nкоторая ставит в соответствие групповой системе (2.14) L-систему:\nr\n\nzk = ∑ f lk ( z ) ul ,\n\nk = 1, r ,\n\nl =1\n\nопределяющую поведение групповой системы (2.14)\nинвариантной поверхности (2.36) .\nПусть для групповой системы (2.14) выполняется\nn > r.\nТогда добавлением к (2.14) уравнений\nr\n\nxn+i = ∑ f l ,n+i ( x ) ul ,\n\nна\n\nкаждой\n\ni = 1, r − n.\n\nl =1\n\nможно добиться того, что расширенная система (2.14), является Lсистемой с теми же структурными постоянными Cijk , что и в (2.16) [148].\n2.3. Аппарат непрерывных симметрий дискретных моделей\n\nРаздел посвящен разработке основ аппарата групп симметрий\nдискретных систем с управлением [274, 273].\nВ математической физике определено новое геометрическое\nнаправление, связанное с приложением групп Ли к конечно-разностным\nуравнениям, сеткам, разностным функционалам. Результаты группового\nанализа дискретных структур [45, 183] показывают, что наличие\nсимметрии у разностных моделей так же, как и в классическом случае\nинвариантности дифференциальных уравнений, приводит к понижению\nпорядка и интегрируемости обыкновенных разностных уравнений, к\nналичию инвариантных решений у уравнений в частных разностных\nпроизводных, к существованию разностных законов сохранения у\nинвариантных вариационных задач. При построении разностных моделей,\nсохраняется непрерывная симметрия исходных дифференциальных\nуравнений. Вместе с тем, наличие дискретных геометрических моделей\nимеет практическую значимость применительно к исследованию сечений\nПуанкаре.\n46\n\n\fНелокальность объектов, необходимых для описания разностных\nуравнений, сеток, разностных функционалов с необходимостью требует\nпривлечения бесконечномерных пространств или пространств последовательностей [45].\nРассмотрим пространство Z последовательностей (t, x1, x2, ..., u), где\nu — независимая переменная, x — зависимая переменная, x1, x2, …, —\nдифференциальные переменные; Под z будем подразумевать любое\nконечное число координат вектора (t, x1, x2, ...), под zl — i-ю его\nкоординату.\nВ пространстве Z зададим отображение D (дифференцирование),\nдействующее по правилу:\nD(t) = 1, D(x1) = x2, …, D(xs–1) = xs, s = 1, 2, ...\nПусть А — пространство аналитических функций F(z) от конечного\nчисла переменных z (при этом разные функции F(z), входящие в А, могут\nзависеть от разного набора переменных из (t, x1, x2, ...), но сам набор\nконечен\nвсегда).\nОтождествляя\nD\nс\nдействием\nлинейного\nдифференциального оператора первого порядка\n∂\n∂\n∂\n(2.34)\n+ ... + xs −1\n+ ... ,\nD = + x1\n∂t\n∂x2\n∂xs\nтем самым распространим дифференцирование D на функцию из A, при\nэтом D(F(z))∈A.\nРассмотрим последовательности формальных степенных рядов от\nпараметра a:\n∞\n\nf ( z , a ) = ∑ Aki ( z )a k , i = 1, 2, ...,\ni\n\n(2.35)\n\nk =0\n\nгде Aki ( z ) ∈ A , причем Ai0 = z i ; z i — i-я координата вектора из Z.\nПространство последовательностей формальных степенных рядов\n(2.35)\n\n(f\n\n1\n\n( z , a ), f 2 ( z , a ), ..., f s ( z , a ), ...)\n\nобозначим через Z . Последовательности (t, x1, x2, ...) являются частным\nслучаем последовательности рядов (2.35): Z ⊂ Z .\nВ Z по определению зададим операции сложения, умножения на\nчисло и произведения формальных рядов, совпадающими с\nсоответствующими операциями для сходящихся рядов:\n⎛ ∞\n⎞ ⎛ ∞\n⎞ ∞\nα ⎜ ∑ Aki a k ⎟ + β ⎜ ∑ Bki a k ⎟ = ∑ ( αAki + β Bki ) a k ,\n⎝ k =0\n⎠ ⎝ k =0\n⎠ k =0\n∞\n∞\n⎛\n⎞ ∞ ⎛ ∞\n⎞ k\nk2\ni k1\ni\ni\ni\nA\na\nB\na\nA\nB\n(2.36)\n=\n⎜ ∑ k1 k2 ⎟ a ,\n∑ k1 ⎜ k∑=0 k2 ⎟ ∑\n=\n+\n=\n0\nk1 = 0\nk\nk\nk\nk\n⎝ 2\n⎠\n⎝1 2\n⎠\ni = 1, 2, ..., α, β = const ,\n47\n\n\fа также операции дифференцирования рядов (2.35):\n∞\n⎛ ∞ i\nk ⎞\nD ⎜ ∑ Ak ( z )a ⎟ = ∑ D ( Aki ( z ) )a k ,\n⎝ k =0\n⎠ k =0\n∞\n∞\n∂ ⎛\ni\nk ⎞\nAk ( z )a ⎟ = ∑ kAki ( z )a k −1 ,\n∑\n⎜\n∂a ⎝ k =0\n⎠ k =1\n\n(2.37)\n\n∂ ⎛ ∞ i\n⎞\n= A1i ( z ), i = 1, 2, ... .\nAk ( z )a k ⎟\n∑\n⎜\n∂a ⎝ k = 0\n⎠ a =0\n\nФигурирующие выше равенства степенных рядов означают совпадение\nкоэффициентов рядов при соответствующих степенях а. Например, ряд\n(2.35) равен 0, если все Aki = 0, k = 0, 1, 2, ....\nВ Z рассмотрим преобразования, определяемые рядами (2.35):\nz i* = f i ( z, a), i = q, 2, ...,\n(2.38)\ni\ni*\nПереводящие последовательность z в z .\nВведенные операции над рядами (2.35) позволяют рассматривать\nстепени от рядов вида (2.35), мономы, полиномы, и аналитические\nфункции (или формальные степенные ряды) от конечного числа\nпеременных z i . Тем самым определена суперпозиция преобразований вида\n(2.38)\nz\n\ni**\n\n∞\n\n∞\n\n= f ( z , b) = ∑ A ( z )b = ∑ Aki ( f i ( z * , b) ) b k ,\ni\n\n*\n\nk =0\n\ni\nk\n\n*\n\nk\n\ni = 1, 2, ....\n\nk =0\n\nТакая суперпозиция, вообще говоря, выводит однопараметрические\nряды (2.35) из Z . Рассмотрим только ряды (2.35), и соответствующие\nпреобразования (2.38), структура коэффициентов которых обеспечивает\nзамкнутость в Z преобразований (2.38):\nz\n\ni**\n\n∞\n\n= f ( z , b) = f ( z , (a + b)) = ∑ Aki ( z )(a + b) k ,\ni\n\ni\n\n*\n\ni = 1, 2, .... (2.39)\n\nk =0\n\nСвойство (2.39) формальных рядов (2.35) означает, что преобразования\n(2.36) образуют формальную однопараметрическую группу в Z .\nСвойство (2.39) эквивалентно экспоненциальному представлению\nстепенных рядов [124]:\n∞\nas (s) i\ni*\ni\naX\ni\nz = f ( z , b) = e ( z ) ≡ ∑ X ( z ), i = 1, 2, ...,\n(2.40)\ns\n!\ns =0\nгде X — инфинитезимальный оператор группы,\n∂\n(2.41)\nX = ξi ( z )\n,\n∂xi\n∂f i ( z , a )\nξ ( z) =\n, i = 1, 2, ..., ξi ( z ) ∈ A.\n∂a a =0\ni\n\nПредставление (2.40) эквивалентно определению формальной группы\n(2.39). Для этого рассмотрим суперпозицию преобразований вида\n48\n\n\fbl ( l ) ⎛ ∞ a s ( s ) i ⎞\nz = e (e (z )) = ∑ X ⎜ ∑ X (z ) ⎟ =\nl =0 l !\n⎝ s =0 s !\n⎠\ni*\n\nbX\n\naX\n\ni\n\n∞\n\n⎛ ∞ a s bl ( l = s ) i ⎞ ∞ ( a + b )\nk\na +b X\n= ∑⎜ ∑\nX\nX ( ) ( z i ) = e( ) ( z i ).\n(z ) ⎟ = ∑\nk = 0 ⎝ s + l = k s !l !\n⎠ k =0 k !\nПредставление (2.40) эквивалентно (2.39). Кроме того, очевидно, что\nk\n\n∞\n\nf i ( z, a )\n\na =0\n\n= z i , f i ( f i ( z, a ) , ( −a ) ) = z i .\n\nТаким образом, для формальных однопараметрических групп\nсправедливо экспоненциальное представление через инфинитезимальный\nоператор так же, как и для классических локальных групп Ли\nпреобразований.\nЗаметим, что экспоненциальное представление (2.40) дает\nрекуррентную цепочку для коэффициентов формальной группы (2.38)\n1\nAki ( z ) = X Aki −1 ( z ) , k = 1, 2, ..., i = 1, 2, ...,\n(2.42)\nk\nа также удобную для вычисления коэффициентов формальных рядов (2.38)\nформулу\n1 k\nAki ( z ) = X ( ) z i , k = 1, 2, ..., i = 1, 2, ....\nk!\nВ теории формальных групп показывается [45], что касательное\nвекторное поле\n∂f i ( z , a)\ni\nξ ( z) =\n∂a a = 0\nсвязано с формальными рядами (2.35) с помощью уравнений Ли:\n\n(\n\n)\n\n( )\n\n∂f i\n= ξi ( f ),\n∂a\nf i ( z, a)\n= z i , i = 1, 2,...,\n\n(2.43)\n\na =0\n\nт. е. последовательность формальных рядов (f 1, f 2, ...), образующих группу\nс касательным полем ξi ( z ) , удовлетворяет системе (2.43), и, обратно, для\nлюбой последовательности функций (ξ1 ( z ), ξ2 ( z ), ...), ξi ( z ) ∈ A , решение\nсистемы (2.43) образует формальную однопараметрическую группу.\nТаким образом, на формальные однопараметрические группы\nраспространяется такая же связь группы и оператора, какая присутствует в\nклассических локальных группах Ли. В случае, когда формальные\nстепенные\nряды\nсходятся\nи\nобразуют\nдостаточно\nгладкие\nдифференцируемые функции, мы имеем дело с группами Ли точечных или\nкасательных преобразований. Таким образом, точечные и контактные\nпреобразования составляют часть формальных групп, но не исчерпывают\nих целиком. Дополнительным классом к группам точечных и касательных\n49\n\n\fпреобразований являются группы «высших симметрии» или группы Ли–\nБеклунда [58].\nДля формальных групп, так же, как и для групп Ли точечных и\nкасательных преобразований, можно определить понятие инварианта и\nинвариантного многообразия.\nКак известно, локально-аналитическая функция F(z) от конечного\nчисла переменных называется инвариантом формальной группы, если\nF(z*) = F(z) для любых преобразований группы (2.38).\nДля того чтобы F ( z ) ∈ A была инвариантом, необходимо и\nдостаточно, чтобы\nX F ( z) = 0 ,\nгде X —оператор группы (2.41).\nМногообразие, заданное в Z с помощью функции ϕ( z ) ∈ A :\n\nϕ( z ) = 0\nинвариантно, если для всех решений и всех преобразований формальной\nгруппы справедливо\n\nϕ( z* ) = 0 .\nКритерий инвариантности многообразия также записывается с помощью оператора группы [45]:\nX ϕ( z ) ϕ( z )=0 = 0.\n\nСреди формальных групп, преобразования которых описываются\nформальными степенными рядами (2.35), особое место занимают точечные\nи контактные группы, а также высшие симметрии. Если первые два класса\nпреобразований можно рассматривать в конечномерной части Z , то любая\nнетривиальная высшая симметрия может быть реализована лишь во всем\nбесконечномерном пространстве Z .\nРассмотрим группу преобразований (2.38) в пространстве Z формальных рядов с инфинитезимальным оператором, совпадающим с\nоператором (полного) дифференцирования:\n∂\n∂\n∂\nD = + x1\n+ ... + xs −1\n+ ...\n∂t\n∂x2\n∂xs\nДля простоты рассмотрим случай одной независимой переменной х и\nодной зависимой переменной и.\nПреобразования (2.38) этой группы в соответствии с\nэкспоненциальным представлением (2.40) определены действием\nоператора Ta ≡ eaD :\n∞\nas (s) i\ni*\naD\ni\nz = e ( z ) ≡ ∑ D ( z ).\ns =0 s !\n*\nТочка z ∈ Z имеет следующие координаты:\n50\n\n\ft * = Ta ( x) = t + a\n...\nas\nx = Ta ( xk ) = ∑ xk + s ,\ns =0 s !\n...\nпреобразования, рассмотренные на поверхности x = x(t), представляют\nсобой разложение в формальные ряды Тейлора функции x = x(t) в точке\n(х + а), поэтому группа преобразований с оператором полного\nдифференцирования была названа группой тейлоровского сдвига или\nгруппой Тейлора [124].\nВ теории высших симметрии или групп Ли–Беклунда [58] среди\nпреобразований вида (2.38), составляющих группу, выделяются\nпреобразования, которые сохраняют определение и геометрический смысл\nпроизводных (x1, …) в Z .\nГруппа Тейлора сохраняет инвариантность системы и поэтому\nявляется группой высших симметрий.\nБолее того, группа Тейлора является нетривиальной группой высших\nсимметрии, т. е. не является продолжением в Z группы точечных или\nкасательных преобразований. Это следует из того, что последовательность\nуравнений Ли для определения конечных преобразований группы Тейлора\nпо оператору группы D не имеет решения в конечномерной части Z .\nТаким образом, разложения в формальные ряды Тейлора образуют\nоднопараметрическую группу лишь в бесконечномерном пространстве Z ,\nт. е. для динамических систем.\n∞\n\n*\nk\n\n2.4. Группы симметрий фазового пространства\n\nРассмотрим модель дискретной динамической системы:\nx(t + 1) = ψ( x(t ), u (t )),\n\n(2.44)\n\nгде x(t) — n-мерный вектор состояний системы из многообразия X, которое\nобычно отождествляется с n ; u(t) — r-мерный вектор управлений из\nмножества допустимых управлений U ⊆ r ; x (t + 1) ∈ Tx X ; TxX —\nкасательное пространство к X в точке x, определяемое допустимыми\nуправлениями; t — дискретное время. Функции ψ удовлетворяют\nпринятым в п.1.2 допущениям о гладкости.\nДля практического применения также будем рассматривать\nлинеаризованную вдоль программной траектории систему\n∆x(t + 1) = A(t )∆x(t ) + B(t )∆u (t ),\n(2.45)\nгде A(t ) = Dx F ( x (t ), u (t )) — матрица Якоби, которая определяет свойства\n— матрица,\nустойчивости по Ляпунову;\nB (t ) = Du F ( x (t ), u (t ))\n51\n\n\fопределяющая вынужденные колебания, вызванные управляющими\nпроцессами; D — оператор дифференцирования.\nВведем следующие определения [274].\nВ динамической системе имеет место преобразование симметрии,\nесли нелинейное динамическое уравнение сохраняет свою структуру при\nлинейных преобразованиях g: x → x′ = g(x) в пространстве состояний.\nФормально, уравнение динамической системы (2.44), допускает\nгруппу симметрий G, если отображение ψ коммутирует по всем\nгрупповым операциям:\nψ ( g ( x ) , u ) = g ( ψ ( x, u ) ) , ∀g ∈ G, ∀x ∈ X ,\nили, другими словами, группа G делает функцию ψ (x, u) инвариантной по\nпервому аргументу. Группа G может состоять из базовых преобразований\nсимметрии физического интервала — преобразование сдвига, вращение и\nдр. [8], которые обычно являются однопараметрическими.\nПредположим, что преобразования, переводящие решения системы\n(2.44) в программную траекторию принадлежат некоторой подгруппе Gx\nгруппы G\ng ( x ) = x , ∀g ∈ G x .\nПри линеаризации структура симметрий эволюционного уравнения\n(2.44) не исчезает, а заменяется связанной динамической симметрией. Это\nсвязано с понятием алгебры Ли, представляющей собой в определенном\nсмысле локальную линеаризацию группы Ли.\nРассмотрим стационарные линейные системы, т. e. матрицы А и В\nявно не зависят от времени.\nИспользуя определение, и то, что преобразования симметрии\nлинейные, для линейной аппроксимации при произвольных g ∈ Gx [275]:\nx + g ( A∆x) = g ( x ) + g ( A∆x) = g ( x + A∆x) =\n= g (ψ ( x , u ) + A∆x) = g (ψ ( x + ∆x, u )) =\n= ψ ( g ( x + ∆x), u ) = ψ ( g ( x ) + g (∆x)), u ) =\n= ψ ( x + g (∆x), u ) = ψ ( x , u ) + Ag (∆x) =\n= x + Ag (∆x),\nт. е. при преобразованиях g уравнение сохраняет свою структуру.\nСогласно [275], определим, что группа L называется полной группы\nсимметрий линеаризованного уравнения (2.45) автономной системы\n(∆u(t) = 0):\ng(A∆x) = Ag(∆x); ∀g ∈ L.\nТаким образом, группа L описывает динамическую симметрию\nсистемы, линеаризованную около программной траектории x , включает\nвсе преобразования g ∈ Gx : Gx ⊆ L . В практических приложениях можно\nпринять, что L обычно совпадет с Gx [278]. Это обосновывается тем, что\n\n52\n\n\fна основе экспериментального исследования невозможно определить\nгруппу Gx , которая исчерпывает динамические симметрии системы или\nгруппа L содержит «скрытые» симметрии [31]. Однако можно показать\n[277], что множество ограничений при определении управляющих\nпараметров может быть получено на основе произвольной\nоднопараметрической подгруппы L ' ⊂ L.\nВ касательном пространстве TxX, под действием преобразований g из\nпроизвольной подгруппы L′ полной динамической группы симметрии L,\nобразуется группа, которая может быть заданна в виде оператора T в\nматричном представлении:\nn\n\n( g ( x ) )i = (T ( g ) x )i = ∑ Tij ( g ) x j ,\nj =1\n\n∀x ∈ Tx X .\n\n(2.46)\n\nНестационарные модели могут быть рассмотрены как совокупность\nстационарных систем, каждая из которых определенна на некотором\nвременном интервале. При этом подходе определение группы симметрий L\nможно дать аналогично стационарным системам. Полагая, что траектории\nx (1) , x ( 2 ) , ..., x ( τ ) имеет период τ, и симметрия точки x ( t ) в\nпрограммной траектории описывается группой Gx (t ) ⊆ G , можно записать\n\n(\n\n)\n\ng ( x ( t + 1) ) = g F ( x ( t ) , u ) =\n\n(\n\n)\n\n= F g ( x ( t ) , u ) = F ( x ( t ) , u ) = x ( t + 1)\nдля каждого g ∈ Gx ( t ) .\nСвойства симметрии всех точек в программной траектории те же,\nчто и у группы симметрии Gx , которая может быть однозначно определена\nдля произвольной точки x ( t ) , Gx = Gx ( t ) . Матрица динамики\nлинеаризованной системы будет иметь блочно-диагональный вид:\n⎡ J k1 ( t )\n⎤\n⎢\n⎥\nJ k (t ) = ⎢\n⎥.\n⎢\nJ kq ( t ) ⎥⎦\n⎣\nгде матрица Jk определенна для каждого k-го интервала стационарности.\nПриведенные\nниже\nрезультаты\nлегко\nобобщаются\nдля\nнестационарных динамических систем в контексте такого определения.\nКак известно, для линейных систем управления можно подобрать\nтакой базис, при котором матрица динамики дифференциальных\nуравнений записывается в каноническом виде: диагональном,\nканонической жордановой форме, форме Фробениуса и т. п. В этом смысле\nимеет значение получение формы описания преобразований в групповых\nкоординатах, тем более что для групп преобразований вид\nинфинитезимальной образующей зависит от выбранного базиса\nоператоров.\n53\n\n\fПо определению алгебра Ли, отвечающая группе Ли — пространство\nлевоинвариантных векторных полей на группе Ли с операцией [⋅, ⋅] —\nскобкой Ли (коммутатором векторных полей).\nИзвестно, что группа является группой симметрий уравнения, если\nона переводит любое решения этого уравнения в некоторое решение того\nже уравнения. Необходимым и достаточным условием этого является\nвыполнение равенства:\n[ A, T ] = 0.\nили\nT ( g ) A = AT ( g ), ∀g ∈ L ' ⊆ L,\nгде T определяется соотношением (2.46).\nГруппа T может быть определена базисным набором преобразований\n(все преобразования симметрии могут быть выражены через базовые) в\nвиде\nT = p1T 1 ⊕ p2T 2 ⊕ ... ⊕ pqT q\n(2.47)\nс n = p1d1 + p2 d 2 + ... + pq d q ; где pr — число эквивалентных представлений\nTr в декомпозиции (2.46), и q — общее число инфинитезимальных\nобразующих в базисе.\nАналогично (2.47) может быть разложено само касательное\nпространство TxX на сумму инвариантных подпространств LrLα' , таких, что\nT ( g ) x ∈ LrLα' , ∀x ∈ LrLα' и ∀g ∈ L ':\n\nTx X = L1L ' ⊕ L2L ' ⊕ ... ⊕ LqL ' ,\n\n(2.48)\n\nLrL ' = LrL1' ⊕ LrL1' ⊕ ... ⊕ LrpL r' ,\n\n(2.49)\n\nгде\nи α = 1, pr — индексы возможных инвариантных подпространств, которые\nвписываются в ту же группу эквивалентного Tr. Это имеете место, и в\nслучае, если декомпозиция (2.48) уникальная, а декомпозиция (2.49) —\nнет, а так же при pr = 1. Обозначим базисные векторы eirα , i = 1, d r , в\nкаждом инвариантном подпространстве LrLα' и выбирем те базисные\nвекторы, которые соответствуют Tr, то есть —\ndr\n\nT ( g ) e = ∑ Tijr ( g ) eirα ,\nrα\ni\n\n∀g ∈ L '.\n\nj =1\n\nДля однопараметрических преобразований Tr, универсальное\nусловие ортогональности между базисными векторами eirα может быть\nустановлено в соответствии с [175]:\n( eirβ ⋅ esjα ) = δr ,s δi, j ( eirβ ⋅ eirα ) .\nКроме того, для pr > 1, декомпозиция (2.49) может всегда выполнена\nтаким образом, чтобы ( eirβ ⋅ eirα ) = δα ,β . Это требование оставляет некоторую\nсвободу на выборе инвариантного подпространств LrLα' . Набор базисных\n54\n\n\fвекторов {eirα } , ( r = 1, q, α = 1, pr , i = 1, d r ) — ортонормальный. Определим\nортогональную матрицу P:\n⎡ ( er1 )T ⎤\n⎡ P1r ⎤\n⎢ i\n⎥\n⎢ ⎥\nr\nr\nP = ⎢ ⎥ , Pi = ⎢\n⎥.\n⎢ rp T ⎥\n⎢ Pd1 ⎥\n⎣ r⎦\n⎢⎣( ei r ) ⎥⎦\nСогласно известной теореме [30] элементы произвольной матрицы (в\nконкретном случае матрицы A) инвариантны относительно любого\nгруппового преобразования\nT(g)A = AT(g), ∀g∈L′⊆L,\nесли удовлетворяет условию:\n( eirβ ⋅ Aesjα ) = δr ,s δi, j ( eirβ ⋅ Aeirα ) ,\n\n⎡ P1 ⎤\n⎢ ⎥\nP = ⎢ ⎥,\n⎢⎣ P q ⎥⎦\n\nгде скалярное произведение\n( Λ r )αβ ≡ ( eirα ⋅ Aeirβ )\n\n(2.50)\n\nне зависит от индекса i = 1, d r (но зависит от декомпозиции (2.49)).\nВ результате в групповых координатах матрицу Якоби можно\nпредставить в блочно диагональном виде:\n⎡ A1\n⎤\n⎢\n⎥\nA = PAP −1 = ⎢\n⎥,\n⎢\nA q ⎥⎦\n⎣\nгде каждый блок A r сам является блочно диагональным\n⎡ Λ1\n⎤\n⎢\n⎥\nAr = ⎢\n⎥,\n⎢\nΛ r ⎥⎦\n⎣\nи состоит из dr идентичных pr×pr блоков Λ r с матричными элементами,\nопределемыми скалярным произведением (2.50).\n\n55\n\n\fГЛАВА 3. РЕДУКЦИЯ НА ЦЕНТРАЛЬНОЕ\nМНОГООБРАЗИЕ СИСТЕМ, ДОПУСКАЮЩИХ ГРУППЫ\nСИММЕТРИЙ\nРассмотрено поведение фазовых портретов на инвариантном торе;\nсформулированы и доказаны теоремы для редукции систем, допускающих\nгруппы симметрий, на устойчивое и неустойчивое многообразие;\nсформулировано обобщение теоремы о центральном многообразии на\nоснове однопараметрических преобразований.\nГлава содержит описание результатов, посвященных обоснованию\nвыбора вида модели и фазового пространства, на котором будет\nпроисходить разработка методов моделирования и параметрической\nидентификации по экспериментальным данным.\n3.1.1. Использование групповых свойств для построения\nэквивалентных отображений\nРассмотрим вопросы исследования глобальной структуры орбит\nдинамической системы, которые не зависят от выбора системы координат,\nт. е. допускают преобразования симметрии. С глобальной точки зрения\nзамена координат представляет собой некоторый диффеоморфизм (в\nслучае гладкой структуры) или гомеоморфизм (в топологической ситуации) фазовых пространств. Таким образом, можно ввести естественные\nотношения эквивалентности между динамическими системами, связанные\nс различными классами замен координат, и интерпретировать проблему\nописания структуры орбит как задачу классификации динамических\nсистем с точностью до этих отношений эквивалентности.\nРассмотрим дискретный случай при условии существования m эквивалентности, определяемой диффеоморфизмом h. Это означает [66],\nчто f1 : M → M совпадает с f 2 : N → N c точностью до некоторой m замены координат. Это отношение является естественным отношением\nэквивалентности для динамики со структурной точки зрения, и в свете\nрассматриваемой задачи. Пусть р — периодическая точка периода п для f1.\nОчевидно, для всякого отображения f2, m -эквивалентного f1, выполнено\nf2(h(p))n = h(f1(p))n = h(p), так что точка q = h(p) является периодической\nточкой для f2 того же периода n. Кроме того, если m ≥ 1, то для всякой, не\nобязательно периодической, точки х и для каждого п выполнено равенство:\n\nDx f1n = Dg hx h n−1 Dhx f 2 h n−1 Dx h .\nУчитывая свойства эквивалентности траекторий на торе [143],\nможно показать, что всякая периодическая точка р отображения f1, спектр\nкоторой не содержит единицы, определяет некоторые 1 -модули. Так как\n56\n\n\fтакие периодические орбиты отделены друг от друга, их спектры могут\nбыть возмущены независимо, по крайней мере, для любой конечной\nсовокупности точек. Таким образом, модули, полученные из различных\nпериодических орбит, в определенном смысле независимы.\nС другой стороны, по крайней мере, в некоторых случаях, локально\nспектр\nявляется\nполным\nинвариантом\nотносительно\nгладкой\nсопряженности. Различные подходы к проблеме локальной гладкой\nсопряженности показаны в [66].\nДля глобальной структуры орбит существуют способы построения\nконструкции независимых модулей, связанных с периодическими\nорбитами. В [29] показано, что в случае бесконечно большого числа\nпериодических орбит, как у растягивающего отображения и\nгиперболического автоморфизма тора, имеется бесконечно много\n1\n-эквивалентности. Для этих двух случаев,\nинвариантов локальной\nкоторые представляют собой самые простые примеры гиперболических\nсистем [126], спектры периодических орбит образуют полную систему\nинвариантов для 1 - и ∞ -эквивалентностей в окрестности указанных\n1\nотображений\nсоответственно.\nТакже\nизвестен\n-сопряжения\nотображений тора [18]. Удовлетворительное описание множества\nвозможных значений для собственных значений всех периодических точек\nостается открытой проблемой.\nРазнообразные модули дают существенную, хотя и не полную\nинформацию о гладкой эквивалентности в окрестности вращения Ra.\nКаждое вещественно аналитическое сжатие сохраняет определенную\nединственным образом аффинную структуру. Для рассматриваемого\nдискретного отображения ψ, две структуры, определенные вблизи концов\nотрезка, встречаются в середине. Функции перехода между двумя\nструктурами в любой фундаментальной области [a, ψ (а)] порождают\nбесконечномерное пространство модулей ψ [66]. Практически это можно\nинтерпретировать так, что существуют такие замены координат, что ψ\nстанет аффинным отображением из [0, ψ-1(0)] в [0, а] и из [ψ(а), 1] в\n[ψ2(а), 1]. Координаты определяются единственным образом с точностью\nдо двух множителей, по одному на каждый конец. Тогда отображение\n[ψ(а), 1] → [ψ2(а), 1], может быть нормализовано, так что ψ a :[0, 1] → [0, 1] :\n\nψa =\n\nψ (a + t (ψ(a ) − a )) − ψ (a )\n,\nψ 2 (a) − ψ(a)\n\nкоторый может быть расширен на всю действительную прямую по\nформуле\n\nψ a (T + k ) = ψ a (T ) + k ,\n\nk∈ .\n\nТаким образом, два диффеоморфизма ψ1 и ψ2, для которых\nψ1(0) = ψ2(0) = 0 являются эквивалентными, если ψ 2 (T ) = ψ1 (T + s ) − ψ1 ( s )\n57\n\n\fдля некоторого s из [0, 1]. Построенное выше отображение перехода\nзависит от множителей, определяющих линеаризацию ψ в окрестности\nконцов отрезка, а также от выбора базовой точки а. Очевидно, что\nизменение линеаризации не меняет отображение перехода, если а\nизменено соответствующим образом. Изменение базовой точки приводит к\nзамене отображения перехода на эквивалентное.\nПонятие гладкой эквивалентности в соответствии с [66] может быть\nлегко перенесено на случай непрерывного времени. Эквивалентность\nпотоков есть сопряженность потоков как дифференцируемых действий\nгруппы R вещественных чисел. Структура орбит потоков, в отличие от\nслучая систем с дискретным временем, имеет два различных аспекта: (1)\nотносительное поведение точек на различных орбитах и (2) эволюция\nданного начального условия вдоль орбиты с течением времени. Имеется\nестественный способ изменять данный поток, сохраняя первый аспект\nструктуры его орбит, а именно вовсе не изменять его орбит.\nДля двух потоков ϕt и εt дадим определение замены времени с\nиспользованием векторные полей инфинитезимальной образующей\nгруппы симметрий:\nd ϕt\nξ=\ndt\n\nt =0\n\nd εt\nи η=\ndt\n\n.\nt =0\n\nИз единственности решений дифференциальных уравнений следует,\nчто нули векторного поля являются неподвижными точками\nсоответствующего потока. Таким образом, мы получаем, что ξ(х) = 0,\nтогда и только тогда, когда η(х) = 0. Кроме того, если х не является\nнеподвижной точкой, то касательные векторы к кривым { ϕt (x)} и { εt (x)},\nне обращаются в нуль и имеют одно и то же направление.\nЕстественно попытаться описать все замены времени данного потока\nпо модулю тривиальных замен. Эта проблема по существу эквивалентна\nпроблеме описания пространства всех достаточно гладких положительных\nфункций с точностью до прибавления функций, которые представляют\nсобой производные других гладких функций по направлению потока. В\n[275] предложено решение проблемы с помощью естественных модулей, а\nименно периодов периодических орбит. Это верно, например, для\nспециальных потоков, соответствующих гиперболическому автоморфизму\nтора.\nОпределение m -модуля может использовано для потоков двумя\nспособами,\nпредполагающими\nсохранение\nзначений\nлибо\nна\nэквивалентных потоках, либо на орбитально эквивалентных потоках.\nЗаметим,\nразличие\nмежду\nэквивалентностью\nи\nорбитальной\nэквивалентностью потоков, состоит в том, что в обоих случаях образ\nпериодической орбиты потока есть периодическая орбита его образа.\nОднако даже из 0 -эквивалентности потоков следует, что период такой\n58\n\n\fорбиты сохраняется, в то время как при орбитальной эквивалентности\nможет произойти изменение этого периода. Например, типичная замена\nвремени изменяет период данной орбиты, и такие изменения могут\nпроизводиться независимо для различных орбит. Таким образом, периоды\nпериодических орбит являются модулями даже для 0 -эквивалентности\nпотоков, и в случаях типа надстройки гиперболического автоморфизма\nтора F имеется бесконечно много таких модулей. Взаимоотношения между\nэтими модулями и модулями, соответствующими собственным значениям\nлинеаризации этого отображения, далеко не тривиальны.\nИмеет смысл при определении эквивалентности рассматривать\nсохранение структурной устойчивости для потоков. Один из способов\nопределения базируется на эквивалентность всех возмущений [16]. Не\nявляясь полностью вырожденным, это требование редко выполняется;\nнапример, при наличии периодических орбит их периоды являются\nмодулями в таком смысле. Будем считать, что из локальной\nтопологической эквивалентности следует сохранение структурной\nустойчивости, при этом преобразующий гомеоморфизм может быть\nдостаточно близким к тождественному для малых возмущений.\nДля всех приведенных понятиях этого раздела компактность\nсоответствующих фазовых пространств несущественна. Кроме того,\nможно естественным образом модифицировать эти определения для\nслучаев, когда для некоторых точек динамическая система определена\nтолько на конечном отрезке времени, как это имеет место, например, в\nокрестности\nгиперболической\nнеподвижной\nточки\nлинейного\nотображения. Такое обобщение ведет к понятиям локальной и\nполулокальной структурной устойчивости.\nДля произвольного сжимающего отображения фазовое пространство\nможет не иметь гладкой структуры, так что приведенные понятия прямо не\nприменимы. Однако сжимающее отображение, определенное в маленьком\nдиске в евклидовом пространстве, структурно устойчиво, так же как и\nгиперболическое линейное отображение в окрестности неподвижной\nточки [66].\nОтображения поворота не являются структурно устойчивыми.\nПоскольку топологическая сопряженность сохраняет периодические\nорбиты, а преобразование поворота на иррациональный угол не может\nбыть сопряжено с преобразованием поворота, для которого\nсоответствующее число рационально. Но так как и множество\nрациональных чисел, и множество иррациональных чисел плотны, то\nсреди произвольно малых возмущений поворота на рациональный угол\nможно найти поворот на иррациональный угол, и наоборот. Аналогичное\nутверждение верно и для сдвига на торе.\n\n59\n\n\f3.2. Метод моделирования систем, редуцированных\nна инвариантное многообразие в локальной области\n3.2.1. Исследования инвариантного многообразия\n\nСогласно теореме Гробмана-Хартмана [135], в окрестности грубого\nсостояния равновесия система\nx = Ax + f ( x)\n(3.1)\nгде\nf (0) = 0, f '(0)=0,\nтопологически эквивалентна линеаризованной системе\ny = Ay .\n\n(3.2)\n\nИзвестно, что в некоторой малой окрестности положения равновесия\nО существует гладкая замена переменных\ny = x + ϕ( x) ,\n(3.3)\nгде ϕ(0) = 0 и ϕ′(0) = 0, позволяющая привести систему (3.1) к виду (3.2).\nПри гладкой замене переменных сохраняются собственные значения (λ1,\n..., λn) матрицы А; более того, при замене переменных типа (3.3), т. е.\nлокально близкой к тождественной, сохраняется и сама матрица А. При\nприведении исходной нелинейной системы к линейному виду возникает\nряд трудностей, главная из которых обусловлена наличием резонансов.\nПо определению, множество { λ1, ..., λn} собственных значений (λ1, ...,\nλn) матрицы А называется резонансным, если существует линейная\nзависимость (резонанс)\nn\n\nρk = (m, λ ) = ∑ m j λ j ,\nj =1\n\nгде т = (m1, ..., тп) — строка таких неотрицательных целых чисел, что\nn\n\nпорядок резонанса | m |= ∑ m j ≥ 2 .\nj =1\n\nВажной характеристикой колебательных систем являются\nхарактеристические показатели или показатели Ляпунова, связанные с\nсобственными значениями матрицы линеаризованного уравнения вдоль\nпериодической траектории (мультипликаторами):\n1\nλ k = ln ρk .\nτ\nДля практического применения важно, что показатели Ляпунова,\nявляясь инвариантами, могут быть вычислены на основании\nэкспериментально полученного временного ряда [92]. Автором в\nсоавторстве с Воловичем М. Е. [35] разработаны методики по расчету\nпоказателей Ляпунова по временным рядам [248, 248, 250], которые\nприменены для различных классов задач [252, 267, 271, 273, 277, 278, 279].\n\n60\n\n\fВ окрестности положения равновесия C∞ -систему можно привести к\nодной из двух нормальной форм: либо к нормальной форме Пуанкаре\ny = Ay ,\nлибо к нормальной форме Дюлака\ny = A( y ) + R( y ) ,\nгде R(y) — полином конечного порядка, составленный из резонансных\nмономов.\nС точки зрения нелинейной динамики значительный интерес\nпредставляет линеаризация вблизи седла, поскольку седло может иметь\nдвояко-асимптотические траектории, принадлежащие как устойчивому,\nтак и не устойчивому многообразию. Такие траектории называются\nгомоклиническими петлями. В случае состояния «седло-фокус», из одной\nгомоклинической петли может возникнуть бесконечное множество\nпериодических траекторий. В локальной области можно ограничиться\nрассмотрением линеаризованной системы, однако при рассмотрении\nглобальных\nбифуркаций\nтребуется\nрассмотрение\nконечнопараметрического семейства систем [143].\nРассмотрим семейство динамических систем\nx = f ( x, u ) ,\n(3.4)\nгде x ∈ R n ; u ∈ U ⊆ R p ; f — Cr -гладкая относительно всех своих\nаргументов функция, определенная на некоторой области D × U , где\nD ⊆ n . Положим, что при u = u 0 семейство (3.4) имеет экспоненциально\nустойчивую\nнеподвижную\nточку\nO0 ( x = x0 ) .\nT. е.\nкорни\nхарактеристического уравнения\ndet | A0 − λI |= 0\nсоответствующей линеаризованной системы\nx = A0 x\n\nлежат слева от мнимой оси. Здесь A0 = ( ∂f ( x0 , u 0 ) ∂x ) .\n\nПоскольку det | A0 |≠ 0 , то, согласно теореме о неявной функции,\nсуществует такое малое δ > 0, что при | u − u 0 |< δ система (3.4) имеет\nсостояние равновесия Ou ( x = x(u )) , близкое к точке O0. При этом\nнеподвижная точка Оu остается устойчивой для всех | u − u 0 |< δ0 ≤ δ , так\nкак корни характеристического уравнения\ndet | A(u ) − λI |= 0 ,\nгде A(u ) = ( ∂f ( x(u ), u ) ∂x ) , непрерывно зависят от u. Рассмотрим\nпроизвольно выбранное управление u1, удовлетворяющую условию\n| u1 − u 0 |< δ0 . Повторяя рассуждения, можно найти новую окрестность\n| u − u1 |< δ1 , в которой система (3.4) имеет устойчивое состояние\nравновесия, и так далее. В итоге в пространстве параметров можно\nпостроить максимальное открытое множество ϒ , которое называется\n\n61\n\n\fобластью устойчивости состояния равновесия Оu. При этом может\nоказаться, что область устойчивости имеет разветвленную структуру.\nГраница Г области устойчивости ϒ соответствует случаю, когда\nнесколько характеристических показателей положения равновесия Оu\n(скажем λ1 ,..., λ m ) лежат на мнимой оси, тогда как остальные собственные\nзначения λ m+1 ,..., λ n расположены в открытой левой полуплоскости. Таким\nобразом, при фиксированном значении параметра на границе система в\nокресности негрубого состояния равновесия принимает вид\ny = By + f1 ( x, y ),\n(3.5)\nx = Ax + ψ 0 ( x, y ),\nгде x ∈\n\nm\n\n; y∈\n\nn −m\n\n; spectr B = {λ m+1 ,..., λ n } (здесь Re λ j < 0, j = m + 1, n );\n\nspectr A = {λ1 ,..., λ m } (здесь Re λ j = 0, j = 1, m ); ψ 0 и f1 — Cr -гладкие\nфункции, которые вместе с их первыми производными обращаются в нуль\nв начале координат.\nДля описания поведения траекторий вблизи точки Оu недостаточно\nанализа только линеаризованной системы, необходимо также учитывать\nнелинейные члены. Ляпунов назвал такие случаи критическими и получил\nдля них ряд условий устойчивости положения равновесия.\nВ теории локальных бифуркаций, фундаментальную роль в которой\nиграет теорема о центральном многообразии Тураева, включает в себя\nрешение задач потери устойчивости, а также поведение при переходе через\nграницу Г области устойчивости.\nДля решения указанных задач необходимо исследовать систему с\nуправлением u:\ny = By + f1 ( x, y, u ),\n(3.6)\nx = Ax + ψ 0 ( x, y, u ),\nгде u принимает значения, близкие к некоторой критической величине\nпараметра u* (далее полагаем, что u* = 0).\nТеорема о центральном многообразии Тураева [143] формулируется\nследующим образом.\nПусть в системе (3.6) f1, ψ 0 ∈ Cr , где 1 ≤ r< ∞. Тогда существует\nокрестность ϒ состояния равновесия О, которая при всех достаточно\nмалых u содержит Cr -гладкое инвариантное центральное многообразие\nWC, задаваемое уравнением\ny = φ( x, u ) ,\n(3.7)\nгде\n∂φ\nφ(0, 0) = 0,\n(0, 0) = 0.\n∂x\nВсе траектории, не покидающие окрестности ϒ, лежат в центральном\nмногообразии [143].\n\n62\n\n\fСуществование центрального многообразия позволяет свести\nрешение задач, связанных с критическими случаями, к исследованию mмерной системы\nx = Bx + ψ 0 ( x, φ( x, u ), u ) .\n(3.8)\nРазмерность последней системы равна количеству характеристических\nпоказателей, лежащих в критический момент времени на мнимой оси, и не\nзависит от размерности исходной системы (dim = n), которая может быть\nнеограниченно большой. Редукция произвольной системы (3.8) большой\nразмерности к системе (3.6) меньшей размерности значительно упращает\nисследование и определяет идентифицируемость таких систем.\nЦентральное инвариантное многообразие обладает лишь конечной\nгладкостью, так что даже в случае, когда исходная система является\nаналитической, соответствующая редуцированная система теряет\nаналитическую структуру. Следовательно, результаты, полученные при\nисследовании аналитических систем малой размерности, не могут быть\nнепосредственно применены при изучении критических случаев.\nНеобходимо рассмотреть инвариантные характеристики, связанные с\nнеединственностью центрального многообразия.\nДля применения теории центрального многообразия необходимо\nрассматривать другой геометрический объект — сильно устойчивое инвариантное слоение. Его существование позволяет локально привести\nсистему (путем Cr −1 -гладкой замены переменных) к наиболее простой и\nудобной треугольной форме\ny = ( B + F1 (u , y, u )) y,\n(3.9)\nx = Ax + Ψ 0 ( x, u ),\nгде Ψ0(x, u) ≡ ψ 0 (x, φ(x, u), u); F1 ∈ Cr −1 ; F1(0, 0, 0) = 0. Таким образом,\nповедение «критических» переменных х в малой окрестности негрубого\nсостояния равновесия не зависит от других переменных и одинаково с\nповедением на центральном многообразии. Для переменных у имеет место\nэкспоненциальное сжатие (поскольку спектр матрицы B лежит строго слева от мнимой оси).\nАналог теоремы о центральном многообразии верен и в общем\nслучае, то есть для состояний равновесия, некоторые из\nхарактеристических показателей которых лежат справа от мнимой оси.\nТаким образом, в этом случае качественное исследование локальных\nбифуркаций также можно свести к изучению системы меньшей\nразмерности. Аналогичен результат при изучении поведения решений на\nгранице области устойчивости периодических траекторий, но в условиях\nсуществования границы областей устойчивости двух различных\nтипов [143]:\n1) бифурцирующая периодическая траектория существует, когда\nпараметр лежит на границе;\n2) периодическая траектория не существует на границе.\n63\n\n\fГЛАВА 2. АППАРАТ ГРУПП СИММЕТРИЙ ДЛЯ\nИССЛЕДОВАНИЯ МОДЕЛЕЙ УПРАВЛЯЕМЫХ СИСТЕМ\nВ главе изложены вопросы применения аппарата групп симметрий\nдля анализа систем с управлением; разработан аппарат непрерывных групп\nсимметрий дискретных моделей; определены основные результаты для\nанализа фазовых портретов систем с нелинейной динамикой.\n2.1. Групповой анализ нелинейных систем с управлением\nИспользование аппарата групп симметрий для моделирования\nдинамики управляемых систем по экспериментальным данным является\nоригинальным подходом, в связи, с чем необходимо проведение\nисследований в рамках группового анализа с целью его эффективного\nиспользования в аппарате геометрического моделирования.\nРазвитие дифференциальной геометрии и теории групп Ли [49, 101,\n103, 124, 134, 138] позволило выделить в отдельное научное направление\nтеорию групп преобразований решений дифференциальных уравнений —\nтеорию групп симметрий. Софус Ли [203], классифицируя преобразования,\nсвязывающие частные решения дифференциальных уравнений, получил,\nчто преобразования образуют группу (единичный элемент —\nтождественное преобразование). Развитие методов группового анализа\nнелинейных динамических получило в работах Л. В. Овсянникова [105,\n106] Н. Х. Ибрагимова [58], П. Олвера [212], а также в работах по\nисследованию управляемых систем Ю. Н. Павловского [109], В. И. Елкина\n[50], Г. Н. Яковенко [150] и др. [см., напр. обзор 77]. Анализ результатов\nпозволил разработать геометрические основы исследования управляемых\nсистем, отраженных в публикациях автора [259–265, 268, 273, 274, 275,\n280].\nСистемы, в динамическом поведении которых наблюдаются\nпредельные циклы и аттракторы, допускают конечнопараметрические\nгруппы преобразований решений. Как правило, у таких систем множество\nпреобразований симметрии не вкладывается в конечнопараметрическую\nгруппу. Для систем с управлением множество преобразований сдвигов\nвдоль решений имеет функциональную мощность [149], а единственным\nпреобразованием симметрии является тождественное. Важными\nсвойствами систем, допускающих группы симметрий, является\nвозможность редукции к системам меньшей размерности [43, 50, 125] и\nвозможности интегрирования [52, 69, 82].\nПусть система\n\nx = f (t, x ) , x ∈\nв\n\nнекоторой\n\nобласти\n\nудовлетворяет\n29\n\nn\n\nусловиям\n\n(2.1)\nсуществования\n\nи\n\n\fДля состояний равновесия границ второго типа нет, в то время как\nпериодические траектории могут исчезать при достижении момента\nбифуркации. Практическое значение в рамках решаемой задачи имеют\nграницы первого типа. В момент бифуркации можно построить секущую к\nкритической периодической траектории (которая существует по\nпредположению) и изучить поведение траекторий соответствующего\nотображения Пуанкаре вблизи бифуркационной неподвижной точки.\nПосле этого теория центрального многообразия применяется так же, как в\nслучае состояний равновесия. Это позволит развить единый подход,\nдействительный как для состояния равновесия, так и для периодических\nтраекторий. Помимо динамических приложений теорем об инвариантных\nмногообразиях, полученные результаты можно использовать, например,\nпри разработке алгоритмов управления для проведения движения системы\nв окресности седла к специальной форме. Для этого используются сильно\nустойчивое и неустойчивое многообразия.\n3.2.2. Метод редукции систем на центральное многообразие\n\nРассмотрим n-мерную систему дифференциальных уравнений (3.5) в\nмалой окрестности негрубого состояния равновесия О. Пусть система\nзависит от допустимых управлений (3.6). Проведем построение\nцентральных многообразий в соответствии с [42].\nПусть система (3.6), где функции f и ψ вместе со своими первыми\nпроизводными по (х, у) непрерывны по u и f1 (0, 0, 0) = 0, ψ 0 (0, 0, 0) = 0.\n( f1 , ψ 0 )( x , y ) (0, 0, 0) = 0 , при любых малых u имеет m-мерное Cr -гладкое\n\nинвариантное локальное центральное многообразие WlocC : y = φ( x, u ) (здесь\nфункция φ вместе со всеми ее производными по х непрерывно зависит от\nu), которое касается пространства в точке О при u = 0,\nφ(0, 0) = 0, ∂φ(0, 0) /(∂x) = 0.\nВ этих условиях при всех u центральное многообразие содержит все\nтраектории, которые целиком лежат в малой окрестности точки О.\nВ случае, когда правая часть системы (3.6) гладко зависит от u,\nцентральное многообразие также гладко зависит от u. Таким образом, если\nфункции f и ψ являются Cr -гладкими относительно (х, у, u), то функция φ\n(график которой у = φ(х, u) представляет собой многообразие WlocC ) может\nбыть выбрана Cr -гладкой относительно (х, u). Этот результат получается\nформальным добавлением к системе (3.6) уравнения\nu = 0.\nЕсли рассматривать (х, u) в качестве нового состояния х, то вид\nрасширенной системы будет аналогичен системе (3.5). Применяя для\nданного случая теорему Тураева, получаем центральное многообразие,\n64\n\n\fкоторое Cr -гладко зависит от новой переменной х, то есть является Cr гладким относительно (х, u).\nЦентральное многообразие WС не обязательно является C∞ -гладким,\nдаже если система имеет гладкость C∞ . Если при любом значении r можно\nприменить теорему о центральном многообразии, то выполнено\nследующее утверждение [143]. Если исходная система является C∞ гладкой, то при любом конечном значении r существует окрестность ϒr\nначала координат, где многообразие WlocC является Cr -гладким. Указанные\nокрестности могут сжиматься до нуля, однако при изменении u состояние\nравновесия О может сохраняться, но характеристические показатели точки\nО, лежащие на мнимой оси при u = 0, могут сместиться при u ≠ 0, скажем,\nвлево. Эти показатели соответствуют ведущим собственным значениям\nсоответствующей линеаризованной системы. Следовательно, при\nненулевом u центральное многообразие будет совпадать с некоторым\nведущим многообразием, которое имеет лишь конечную гладкость.\nПри u = 0 выполняется следующее достаточное условие гладкости\n[143]. Если при u = 0 каждая траектория на центральном многообразии\nC∞ -гладкой системы стремится к состоянию равновесия О либо при\nt → +∞ , либо при t → −∞ , то центральное многообразие является C∞ гладким.\nСледствие теоремы о центральном многообразии заключается в том,\nчто при исследовании локальных бифуркаций негрубого состояния\nравновесия О (то есть при изучении множества траекторий, никогда не\nвыходящих за пределы малой окрестности точки О, и зависимости этого\nмножества от u) систему можно ограничить на центральное многообразие\nWC и исследовать систему (3.8). Здесь есть неопределенность, вызванная\nтем, что центральное многообразие определяется системой неоднозначно.\nПоэтому редуцирование исследований на центральное многообразие\nтребует доказательства и разработки специализированной методики.\nНа основании теории центрального многообразия, для любых двух\nцентральных многообразий у = φ1(x) и у = φ2(x) при каждом х0 — таком,\nчто для некоторого у0 точка (х0, у0) ∈ Ν, — функция φ1 вместе со всеми\nсвоими производными совпадает c φ2(x):\n\nd k φ1\ndx k\n\nx = x0\n\nd k φ1\n=\ndx k\n\n,\n\nk = 1, r.\n\nx = x0\n\nПрименяя приведенный результат к точке О, получаем, что все\nпроизводные функции φ, график которой задает центральное\nмногообразие, определены однозначно в начале координат. Следовательно,\nнесмотря на то, что центральное многообразие не единственно, группа\nсимметрий, порожденная разложением Тейлора редуцированной системы,\nопределена однозначно [143].\n65\n\n\fДругим важным утверждением является теорема о гладкой\nсопряженности [143], согласно которой, для любых двух локальных\nцентральных многообразий WCl и WC2 существует Cr −1 -гладкая замена\nпеременных х, отображающая траектории первой редуцированной системы\nx = Ax + ψ 0 ( x, φ1 ( x, u ), u )\nна траектории второй редуцированной системы\nx = Ax + ψ 0 ( x, φ2 ( x, u ), u ) .\nПриведенная формулировка результата фактически утверждает, что\nв динамике на центральных многообразиях одной и той же системы нет\nсущественных качественных различий. То есть, система на центральном\nмногообразии\nпредставляет\nсобой\nдостаточно\nопределенный\nидентифицируемый объект. Разложение редуцированной системы в ряд\nТейлора может быть найдено различными способами. Инвариантность\nмногообразия у = φ(х) означает, в силу системы (3.5), что\n∂φ\n( Bx + ψ 0 ( x, φ( x, u ), u ) ) = Aφ( x) + f1 ( x, φ( x, u ), u )) .\n∂x\nРазлагая функции, входящие в данное уравнение, в формальный ряд по\nстепеням х, можно последовательно найти все коэффициенты ряда Тейлора функции φ. После этого можно определить по экспериментальным\nданным правой части редуцированной системы (3.8).\nДаллее использована теорема [143], в которой показано\nсуществование Cr −1 -гладкого преобразования координат ( C1 -близкое к\nтождественному вблизи начала координат), приводящее систему (3.1) к\nвиду\ny = ( B + F1 (u, y, u )) y,\n(3.10)\nx = Ax + Ψ 0 ( x, u ),\nгде Ψ0(x, u) ≡ ψ0(x, φ(x, u), u); Ψ 0 ∈ Cr −1 , F1 ∈ Cr ;\nF1(0, 0, 0) = 0, Ψ0 (0,0,0) = 0, Ψ'0 (0) = 0.\nЗдесь поверхность {у = 0} является инвариантным центральным\nмногообразием, т. е. распрямлением многообразия WlocC , представляющего\nсобой Cr -преобразование. Один порядок гладкости теряется, потому что\nна самом деле в теореме достигнуто гораздо большее: локальная эволюция\nпеременных х теперь полностью не зависит от у. Заметим, что,\nпреобразование координат является Cr −1 -гладким, функция Ψ0 является\nCr -гладкой, т. е. она совпадает с нелинейной частью ограничения (3.8)\nисходной системы на Cr -гладкое центральное многообразие. Таким\nобразом, для любой траектории системы (3.10) поведение переменных х\nтакое же, как на центральном многообразии, для переменных у при t → +∞\nимеет место экспоненциальное сжатие к у = 0. Последнее утверждение\n66\n\n\fможно проверить, на основе асимптотической экспоненциальной\nустойчивости состояний равновесия [143].\nЗаметим, что результат о гладкой сопряженности следует\nнепосредственно из [143]. Если система (3.10) имеет центральное\nмногообразие, отличное от {у = 0}, то редуцированная система задается\nвторым уравнением системы (3.10); то есть для системы в треугольной\nформе (3.10) ограничения на любые два центральные многообразия\nявляются тривиально сопряженными. Вследствие того, что преобразование\nкоординат, приводящее систему к этой форме, является Cr −1 -гладким, в\nслучае, когда система не приведена к такой форме, мы имеем Cr −1 сопряженность.\nГеометрическая интерпретация указанных свойств следующая.\nОчевидно, что если система приведена к треугольной форме (3.10), то\nобраз любой поверхности {х = const} при сдвиге на время t содержится в\nповерхности того же вида при любом t (до тех пор, пока траектории\nостаются в малой окрестности точки О). Следовательно, слоение малой\nокрестности состояния равновесия О на поверхности постоянного\nзначения х является инвариантным относительно системы (3.10). Замена\nкоординат, возвращающая систему (3.10) к исходному виду (3.5),\nотображает поверхности {х = const} в поверхности вида\nx = ξ + η( y, ξ) ,\n(3.11)\nгде ξ — координата х пересечения поверхности с центральным\nмногообразием; Cr −1 -гладкая функция η вместе с ее первыми\nпроизводными обращается в нуль в начале координат (заметим, что η ≡ 0\nна всем многообразии WC).\nПреобразование, отображающее поверхности {х = const} в\nповерхности, задаваемые уравнением (3.11), является диффеоморфизмом,\nт. о. уравнение (3.11) определяет слоение малой окрестности начала\nкоординат на поверхности, соответствующие фиксированному значению ξ.\nТо есть, для каждой точки (х, у) существует единственное ξ, при котором\nповерхность, соответствующая данному значению ξ, проходит через точку\n(х, у). Такая поверхность является слоем слоения: каждая точка в малой\nокрестности состояния равновесия О может принадлежать только одному\nслою. Поскольку слои параметризованы точками на многообразии WlocC ,\nцентральное многообразие является базой слоения. Слоение {х = const}\nинвариантно относительно системы (3.10), т. е. его образ (3.11) является\nинвариантным слоением системы (3.5). При произвольном значении t\nсдвиг на время t любого слоя лежит в одном слое того же слоения до тех\nпор, пока траектория остается в малой окрестности точки О. После\nраспрямления центрального многообразия приведение к треугольной\nформе (3.8) осуществляется просто преобразованием x ξ( x, y )\n(обратным к (3.11)): новой переменной х служит х-координата проекции\nточки вдоль слоев инвариантного слоения на центральное многообразие.\n67\n\n\fИнвариантность слоения означает, что эволюция новой координаты х = ξ\nне зависит от у. Таким образом, по существу, устанавлено существование\nслоения вида (3.11), трансверсального центральному многообразию и\nинвариантного относительно системы (3.5).\n3.3. Обобщение теоремы о центральном многообразии для\nсистем, допускающих группы симметрий\n\nРассмотрим общий случай, когда и справа от мнимой оси есть\nхарактеристические показатели состояния равновесия. Система вблизи\nточки О принимает вид\ny = By + f1 ( x, y, z ),\nz = Cz + f 2 ( x, y, z ),\n\n(3.12)\n\nx = Ax + ψ 0 ( x, y, z ),\n\nгде x ∈\n\nm\n\n, y∈\n\nk\n\n, z∈\n\nspectr A = {λ m+1 ,..., λ k } ,\n\nn−m−k\n\n; spectr B = {λ1 ,..., λ m } , Re λ j = 0, ( j = 1, m) ;\n\nRe λ j < 0,\n\n( j = m + 1, k ) ;\n\nspectr C = {λ k +1 ,..., λ n } ,\n\nRe λ j > 0, ( j = k + 1, n) ; r -функции f1, f2 и ψ 0 вместе со своими первыми\nпроизводными обращаются в нуль в начале координат. Правые части\nсистемы могут зависеть от управления u либо непрерывно (в таком случае\nгладкие многообразия, рассматриваемые ниже, непрерывно зависят от u),\nлибо гладко. В последнем случае u включается в число «центральных»\nпеременных х и, таким образом, исследуемые далее многообразия и\nслоения будут иметь гладкость по u, равную гладкости по х.\nДокажем действие теоремы о центральном многообразии для случая,\nкогда системы допускают группы симметрий.\nТеорема 3.1 (Обобщение теоремы Тураева о центральноустойчивом многообразии для систем, допускающие группы\nсимметрий [284]). В малой окрестности состояния равновесия О\nсуществует (т + k)-мерное инвариантное центрально-устойчивое\nмногообразие WlocsC : φsC ( x, y ) класса r , которое содержит точку О и\nкасается в этой точке подпространства {z = 0}. Многообразие WlocsC\nвключает в себя все траектории, остающиеся в малой окрестности\nточки О при всех положительных значениях времени. Хотя центральноустойчивое многообразие определено неоднозначно, для любых двух\nмногообразий W1sC и W2sC функции φ1sC и φ2sC определяют одну и ту же\nгруппу симметрий в точке О (и в каждой точке, траектория которой\nостается в малой окрестности точки О при всех значениях t ≥ 0), и,\nследовательно, могут быть идентифицированы на основании\nэкспериментального исследованиия.\n68\n\n\fДоказательство. Доказательство основы этой теоремы приводится в\n∞\n[143]. Заметим, что, если система является\n-гладкой, центральноустойчивое многообразие имеет, вообще говоря, только конечную\nгладкость: при любом конечном значении r существует окрестность ϒr\nточки О, в которой многообразие WsC является r -гладким.\nТребуется доказать, что система допускает группу симметрий, φ1sC и\n\nφ2sC определяют одну и ту же группу симметрий в точке О.\nНе нарушая общности, рассмотрим систему второго порядка. Пусть\nсистема,\nопределенная\nна\nмногообразии\nW1sC\nпорождает\nоднопараметрическую группу преобразований, порожденная оператором\n∂\n∂\nA = ξ1 ( x)\n,\n+ ξ2 ( x)\n∂x1\n∂x2\n\nс инфинитезимальным оператором\nX = η1 ( x)\n\n∂\n∂\n.\n+ ξ2 ( x)\n∂x1\n∂x2\n\nНеобходимо показать, что оператор\n∂\n∂\nA = ξ1 ( x ', τ)\n+ ξ 2 ( x ', τ)\n∂x1\n∂x2\nсистемы, определенной на W2sC является эквилентным (τ — групповой\nпараметр).\nЗапишем преобразование группы в виде рядов Ли (операторной\nэкспоненты):\nx '1 = e τX x1 ,\nx '2 = e τX x2 ,\nx1 = e τX x '1 ,\nx2 = e τX x '2 .\nПредставим оператор A в прежних координатах. Для этого вычислим:\n∂\n∂\n∂\n∂\n,\n+ Ax2\n= Ae−τX x '1\n+ Ae−τX x '2\nA = Ax1\nx = x '( x ) ∂x\nx = x '( x ) ∂x\nx = x '( x ) ∂x\nx = x '( x ) ∂x\n1\n2\n1\n2\nотсюда\nAe −τX x '1\n= ξ1 ( x),\n\n( )\n\n( )\n\n(\n\n(\n( Ae\n\n−τX\n\n)\nx' )\n2\n\n)\n\n(\n\n)\n\nx = x '( x )\n\nx = x '( x )\n\n= ξ 2 ( x).\n\nТак как ξ1,2 не зависят от τ, поскольку определяется оператором X, имеем\nd\nAe −τX x '1 = 0 ,\ndτ\n\n(\n\n)\n\n69\n\n\fследовательно\n∂A −τX\ne x '1 − AXe −τX x '1 + XAe−τX x '1 = 0 .\n∂τ\nАналогичная формула имеет место и для второй координаты, которые\nвместе образуют дифференциальные уравнения:\n∂A\n= AX − XA = [ A, X ]\n∂τ\nс начальным условием\nA( x ', τ) = A( x ') .\nτ=0\n\nРешение полученной задачи Коши можно получить, разложив оператор\nA( x ', τ) в ряд Тейлора по степеням τ:\n∂A\nτ2 ∂ 2 A\nA( x ', τ) = A( x ') + τ\n+\n+ ... ,\n∂τ τ=0 2! ∂τ2 τ=0\n\nтаким образом, имеем:\n∂A\n= [ A, X ] .\n∂τ τ=0\n\nАналогично\n∂2 A\n= [[ A, X ], X ] .\n∂τ2 τ=0\n\nОкончательно ряд примет вид, в форме Хаусдорфа:\nτ2\nA = A + τ[ A, X ] + [[ A, X ], X ] , ... .\n2!\nПо базовой теореме Тураева функции φ1sC и φ2sC определяют одинаковое\nразложение по ряду Тейлора, что позволяет сделать вывод о\nкоммутировании полей:\n[ A, X ] = 0 ,\n\nт. е.\nA = A.\nТаким образом, теорема применительно к система, допускающих группы\nсимметрий доказана.\nПри обращении времени t → –t матрицы А, В и С переходят в –А, –В\nи –С соответственно. Таким образом, часть спектра характеристических\nпоказателей, соответствующая переменным z, теперь находится слева от\nмнимой оси, а часть спектра, соответствующая переменным у — справа от\nнее. К системе, полученной из системы (3.12) путем обращения времени,\nснова можно применить теорему о центрально-устойчивом многообразии и\nполучить следующую теорему о центрально-неустойчивом многообразии.\n\n70\n\n\fТеорема 3.2 (Обобщение теоремы Тураева о центральнонеустойчивом многообразии для систем, допускающие группы\nсимметрий [284]). В малой окрестности состояния равновесия О\nr\nсуществует (п – k)-мерное\n-гладкое инвариантное многообразие\nusC\nusC\nWloc : y = φ ( x, z ) , содержащее точку О и касающееся в этой точке\nподпространства {у = 0}. Центрально-неустойчивое многообразие\nвключает в себя все траектории, остающиеся в малой окрестности\nточки О при всех отрицательных значениях времени. Для любых двух\nопределяют одну и ту же\nмногообразий W1usC и W2usC функции φ1usC и φusC\n2\nгруппу симметрий в точке О (так же, как и в каждой точке, траектория\nкоторой остается в малой окрестности точки О при всех значениях\nt ≤ 0), , и, следовательно, могут быть идентифицированы на основании\nэкспериментального исследования.\nКогда система ∞ -гладкая, центрально-неустойчивое многообразие\nимеет в общем случае только конечную гладкость, но если каждая\nтраектория многообразия WlocusC стремится к состоянию равновесия при\n\nt → – ∞, то многообразие WlocusC является ∞ -гладким.\nЛогика доказательства аналогична предыдущей теореме с учетом\nрассмотренных во второй главе особенностей группового анализа\nдискретных систем.\nПересечение центрально-устойчивого и центрального неустойчивого\nr\nмногообразий\nявляется\n-гладким\nm-мерным\nинвариантным\nC\nsC\nusC\nцентральным многообразием Wloc = Wloc ∩ Wloc , определяемым уравнением\nвида (y, z) = φC(x). Функция φC вместе со всеми производными однозначно\nопределена во всех точках множества N. В частности, разложение в ряд\nТейлора функции φC в точке О однозначно определяется системой.\nСистема\n(3.12),\nограниченная\nна\nцентрально-устойчивое\nмногообразие, имеет вид\ny = By + f1 ( x, y, φsC ( x, y )),\nx = Ax + ψ 0 ( x, y, φ sC ( x, y )),\n\n(3.13)\n\nаналогичный системе (3.5). Следовательно, в данном случае применима\nтеорема Тураева, а именно: для центральных многообразий все же\nвыполняется условие гладкой сопряженности. Таким образом, при\nизучении локальных бифуркаций систему также можно ограничить на\nцентральное многообразие. Более того, между динамикой на различных\nцентральных многообразиях одной системы нет значительной разницы.\nРаспрямление центрально-устойчивого и центрально-неустойчивого\nмногообразий, а также распрямление сильно устойчивого и сильно\nнеустойчивого инвариантных слоений на этих многообразиях приводит к\nследующей теореме, аналогичной теореме, являющейся следствием\nтеоремы Тураева [143].\n71\n\n\fТеорема 3.3 [284]. При помощи r −1 -гладкого преобразования систему (3.12) можно локально привести к виду\ny = ( A + F1 ( x, y, z ) ) y,\n\nz = ( C + F2 ( x, y, z ) ) z ,\n\n(3.14)\n\nx = Bx + Ψ 0 ( x) + Ψ1 ( x, y, z ) y + Ψ 2 ( x, y, z ) z ,\nr\nгде Ψ 0 —\n-гладкая функция, которая вместе со своей первой\nпроизводной обращается в нуль при х = 0; F1,2 являются r −1 -функциями,\nкоторые обращаются в нуль в начале координат; Ψ1,2 ∈ r−1 ;\nΨ1 ( x, y, 0) = Ψ 2 ( x, 0, z ) = 0 .\nПри этом Ψ i (⋅) определяются группой\nсимметрий.\nЗдесь локальное центрально-неустойчивое многообразие задается\nуравнением {у = 0}, локальное центрально-устойчивое многообразие —\n{z = 0}, а локальное центральное многообразие — {у = 0, z = 0}. Сильно\nустойчивое слоение состоит из поверхностей {х = const, z = 0}, а слои\nсильно неустойчивого слоения имеют вид {х = const, у = 0}.\nАналогичная теория строится для негрубых периодических траекторий. Изучение динамики в малой окрестности периодической траектории\nсводится к рассмотрению отображения Пуанкаре на секущей; точка О\nпересечения траектории с секущей является неподвижной точкой\nотображения Пуанкаре.\nПусть система имеет размерность (п + 1); таким образом, секущая nмерна. Пусть т мультипликаторов периодической траектории лежат на\nединичной окружности, k мультипликаторов лежат строго внутри единичной окружности, а остальные (п – т – k) мультипликаторов строго больше\n1 по абсолютной величине. Отображение Пуанкаре вблизи неподвижной\nточки О записывается в виде\n\ny = By + f1 ( x, y, z ),\nz = Cz + f 2 ( x, y, z ),\n\n(3.15)\n\nx = Ax + ψ 0 ( x, y, z ),\nгде\n\nx∈\n\nm\n\n, y∈\n\nk\n\n, z∈\n\nspectr B = {λ m+1 ,..., λ k } ,\n\nn −m−k\n\n; spectr A = {λ1 ,..., λ m } , | λ j |= 1,\n\n| λ j |< 1,\n\n( j = m + 1, k ) ;\n\n( j = 1, m) ;\n\nspectr B = {λ m+1 ,..., λ k } ,\n\n| λ j |> 1, ( j = m + 1, k ) ; f1, f2 и ψ0 — r -гладкие функции, которые вместе с\nих первыми производными обращаются в нуль в начале координат.\nПредполагаем, что правые части отображения (а также их производные)\nмогут непрерывно зависеть от управлений u. В этом случае многообразия\nи слоения, рассматриваемые ниже, будут непрерывно зависеть от u вместе\nсо всеми их производными.\nРаспрямлением инвариантных многообразий и инвариантных\nслоений получаем теорему, полностью аналогичную приведенной выше.\n72\n\n\fТеорему о центральном многообразии можно сформулировать\nследующим образом.\nТеорема 3.4 [284]. При помощи r −1 -гладкого преобразования систему (3.15) можно локально привести к виду\n\ny = ( A + F1 ( x, y, z ) ) y,\nz = ( C + F1 ( x, y, z ) ) z ,\n\n(3.16)\n\nx = Bx + Ψ 0 ( x) + Ψ1 ( x, y, z ) y + Ψ 2 ( x, y, z ) z,\nr\nгде Ψ 0 —\n-гладкая функция, которая вместе со своей первой\nпроизводной обращается в нуль при х = 0; F1,2 являются r −1 -функциями,\nкоторые обращаются в нуль в начале координат; Ψ1,2 ∈ r−1 ;\nΨ1 ( x, y, 0) = Ψ 2 ( x, 0, z ) = 0 . При этом Ψ i (⋅) определяются группой\nсимметрий.\nЗдесь локальное центрально-неустойчивое многообразие задается\nуравнением {у = 0}, локальное центрально-устойчивое многообразие —\n{z = 0}, а локальное центральное многообразие — {у = 0, z = 0}. Сильно\nустойчивое слоение состоит из поверхностей {х = const, z = 0}, а слои\nсильно неустойчивого слоения имеют вид {х = const, у = 0}.\nС учетом введенной во второй главе полной группы симметрий, из\nприведенных выше теорем можно показать, что функции Ψ i (⋅), (i = 0,1, 2)\nопределяются групповым анализом фазовых портетов, на основе\nконечнопараметрических преобразований графиков и соответствующей\nалгеброй Ли.\nПрименительно к системам, допускающих группы симметрий и\nпостроение их моделей, редуцированных на центральное многообразие\nрезультаты можно сформулировать следующим образом.\nТеорема 3.5 [275, 278]. В локальной области качественное\nдинамическое поведение систем с нелинейной динамикой может\nописываться редуцированной на центральное многообразие моделью\nx = Ax(t ) + Ψ 0 ( x) ,\n(3.17)\n\nгде Ψ 0 ( x, t ) — Cr -гладкая функция ( Ψ 0 ( x0 , t ) = Ψ ′0 ( x0 , t ) = 0, x0 = 0 ),\nкоторая определяется преобразованием симметрии, допускаемой\nреконструируемой по экспериментальным данным минимальной\nинвариантной системой.\nТеорема 3.7 [275, 278]. В локальной области качественное\nдинамическое поведение дискретных систем с нелинейной динамикой\nможет описываться редуцированной на центральное многообразие\nмоделью\nx(t + 1) = Bx(t ) + Ψ 0 ( x, t ),\n(3.18)\nгде x(t + 1) — n-мерный диффеоморфизм; Ψ 0 ( x, t ) — Cr -гладкая функция\n( Ψ 0 ( x0 , t ) = Ψ ′0 ( x0 , t ) = 0, x0 = 0 ), которая определяется преобразованием\n73\n\n\fсимметрии, допускаемой реконструируемой по экспериментальным\nданным минимальной инвариантной системой.\nТаким образом, в результате применения приведенных теорем,\nкачественное исследование в локальной области поведения нелинейной\nсистемы, допускающей группы симметрий, сводится к системе меньшей\nразмерности, редуцированной на центральное многообразие. На основании\nанализа модели можно сделать вывод о возможности ее параметрической\nидентификации на основании наблюдаемых процессов систем с\nнелинейной динамикой.\n\n\fГЛАВА 4. МОДЕЛИРОВАНИЕ СИСТЕМ С НЕЛИНЕЙНОЙ\nДИНАМИКОЙ ПО ЭКСПЕРИМЕНТАЛЬНЫМ ДАННЫМ\nПроведена\nклассификация\nметодов\nоценки\nхарактеристических\nпоказателей по временным рядам; изложен модифицированный метод\nвосстановления аттракторов нелинейных систем; разработан метод\nпостроения\nидентифицируемых\nмоделей;\nприводятся\nпримеры\nмоделирования реальных систем по экспериментальным данным.\n4.1. Оценка показателей Ляпунова по временному ряду\n\n4.1.1. Методика использования свойств показателей Ляпунова для\nмоделирования\n\nОценка показателей Ляпунова предлагается как обобщение\nподхода к исследованию устойчивости нелинейных систем на случай\nтраектории общего вида [91, 107]. При исследовании временных рядов\nвосстанавливаются\nлинеаризованные\nмодели\nнестационарной\nнеавтономной системы. Поэтому некоторые усредненные аналоги\nсобственных значений при исследовании позволяют получить ряд\nсущественных оценок [177, 250]. Как и в случае неподвижной точки, набор\nпоказателей Ляпунова не всегда полностью характеризует устойчивость\nсоответствующей траектории. Тем не менее, он несет существенную\nинформацию о системе.\nПусть задана непрерывная динамическая система\nx\u0005 = f ( x),\n\nx(0) = x0\nили ее дискретный аналог\nхk+1 = ψ(xk),\nx(0) = x0.\nИсследуем изменение х(t) (соответственно, хk), если начальным\nданным дать бесконечно малое приращение δx, т. е. рассмотреть\nбесконечно близкую траекторию х(t) + δх(t) или разность δx(t)\n(соответственно, δxk). Для широкого класса систем решение\nдифференцируемо по начальным данным для конечных значений t,\nпоэтому\nδx(t) = Ф(t)δx,\nгде Ф(t) — матрица производных решения по начальным данным:\n∂x (t )\nФ(t )ij = i .\n∂x0 j\n75\n\n\fДля линейных систем Ф(t) совпадает с нормированной фундаментальной\nматрицей.\nПо заданному начальному возмущению δx можно найти δx(t),\nрешая соответствующую линейную систему\nδdx/dt = Df(x(t))δx, δx(0) = δx0\nили δxk+1 = Dψ (xk)δxk, δx0 = δХ.\nВ силу линейности амплитуда решения несущественна, важен только\n«коэффициент прироста» решения за время t, поэтому от бесконечно\nмалых величин δ можно перейти к конечным и (можно, например, считать,\nчто δх = εs, а бесконечно малая амплитуда ε, входящая множителем, как в\nправую, так и в левую части уравнений, сокращается). Таким образом,\nисследование устойчивости приводит к линейным системам\nds/dt = A(t)s, A(t) = Df(x(t)), s(0) = S\nи\nsk+1 = Bksk, Bk = Dψ (xk), s0 = S.\nНачальное возмущение S(δx = εS) определяет направление, в\nкотором мы выбираем бесконечно близкую траекторию в точке X.\nЗаметим, что, строго говоря, векторы х и s принадлежат к разным\nпространствам: х принадлежит фазовому пространству динамической\nсистемы, а s — касательному пространству в точке х.\nДля заданных систем, определенных на R n , решение удобно\nвыразить через нормированную фундаментальную матрицу Ф, которая\nудовлетворяет уравнениям\ndФ/dt = А(t)Ф, Ф(0) = I\nи\nФk+1 = ВkФk, Ф0 = I.\nТогда s(t) = Ф(t)S (sk = ФkS). В общем случае для одних направлений S\nблизкие траектории будут экспоненциально удаляться, для других —\nэкспоненциально сближаться, для третьих расстояние остается примерно\nтем же или меняется медленнее, чем экспоненциально. Для неподвижной\nточки, когда A(t) = А = const (Bk = В = const), эти случаи соответствуют\nсобственным значениям с Reν > 0, Reν < 0, Reν = 0 (|µ| > 1, |µ| < 1, |µ| = 1),\nгде ν и µ — собственные значения соответственно А и В. Для того, чтобы\nохарактеризовать ситуацию в общем случае, А. М. Ляпунов ввел так\nназываемый характеристический показатель решения s(t):\nλ ( s ) = lim t −1 ln s (t ) .\nt →∞\n\nРассмотрим, чему будут равны значения λ(s) в случае неподвижной\nточки для системы dx/dt = f(x), x(t) = X = const. Предположим, что все\nсобственные значения νi, матрицы А вещественны, различны и\nпронумерованы в порядке убывания: ν1 > ν2 > … > νn. Обозначим\n76\n\n\fсоответствующие им собственные вектора через r(i), а r(i) = νir(i), ||r(i)|| = 1.\nТогда r(i) образуют базис в касательном пространстве в точке X, в общем\nслучае\nнеортогональный.\nЛюбое\nрешение\nлинейной\nсистемы\nDs/Dt = Df(x(t)) – s(t) можно представить как комбинацию базисных\nрешений si (t ) = e υit r ( i ) , отвечающих начальным данным si(0) = r(i). Если\ns = ∑ ci r (i ) ,то\ni\n\ns (t ) = ∑ ci si (t ) = ∑ ci eυit r (i ) .\ni\n\ni\n\nПусть j — номер, такой что с1 = с2 = ... = cj-1 = 0, сj ≠ 0. Тогда\nочевидно, что\n⎞\nυt ⎛\n(υ −υ ) t\ns (t ) = ∑ ci eυit r ( i ) = e j ⎜ c j r ( i ) + ∑ ci e i j r (i ) ⎟\ni= j\ni = j +1\n⎝\n⎠\n\nи\n1\nt →∞ t\n\n1\nt →∞ t\n\nλ (u ) = lim ln s (t ) = υi + lim ln c j r ( i ) +\n\n∑ ce\n\ni = j +1\n\ni\n\n(υi −υ j ) t ( i )\n\nr\n\n=υj .\n\nТаким образом, характеристический показатель может принимать\nтолько N различных значений {ν1,ν2,…,νn} в зависимости от начальных\nданных.\nВ общем случае у матрицы А могут быть кратные и комплексные\nсобственные значения. В этом случае соответствующих вещественных\nсобственных векторов может не быть, а будут минимальные инвариантные\nподпространства R j (для любого u ∈ \\ j , Au ∈ \\ j ) размерности\nm\n\ndj = Dim R (i = l , m), ∑ d s = n .\nj\n\ns =1\n\nДействительную матрицу А можно при помощи преобразований\nподобия А′ = САС-1 привести к блочно-диагональному виду. У такой\nматрицы k-мерное инвариантное пространство, но всего один собственный\nвектор.\nКаждому подпространству R j будут отвечать собственные\nзначения ν с одинаковыми действительными частями Re ν, и для всех\ns ∈ R j характеристический показатель будет равен именно этому\nзначению А = Reν. Размерность dj = dim R j называют кратностью\nсоответствующего показателя λ. В дальнейшем мы будем предполагать,\nчто в ряду показателей каждый из них встречается столько раз, какова его\nкратность, так что всего показателей ровно n штук, а упорядочены они по\nубыванию: λ1 < λ2 < … <λn. Если s имеет ненулевые проекции на\nнесколько R j , то λ(s) будет равен наибольшему среди показателей для\nэтих подпространств, т.е. показателю λi с наименьшим номером i. Заметим,\nчто кратность показателей может быть больше кратности собственных\nзначений матрицы А, поскольку показатели действительные, а\nсобственные значения, вообще говоря, комплексные.\n77\n\n\fВ данном случае также можно выбрать базис линейно независимых\nвекторов r(i), так чтобы каждый вектор принадлежал одному из R j (но не\nвсе из них могут быть собственными векторами А), а для решений с\nначальными данными si(0) = r(i)λ(si) = λi.\nРассмотрим случай линейных систем общего вида с переменными\nкоэффициентами:\n\nds/dt = A(t)s , s∈ R , lim t\nj\n\nt →∞\n\n−1\n\nt\n\n∫\n\nA(τ) d τ < ∞ .\n\n0\n\nВместо выделения инвариантных подпространств Ляпунов ввел\nпонятие нормальной системы линейно независимых решений: система\nрешений {s1(t), …, sn(t)} называется нормальной, если для любой другой\nлинейно независимой системы {s′i(t)} λ(s′i(t)) ≥ λ(si(t)). Сам набор\nпоказателей λ1, …, λn (среди них могут быть совпадающие, т. е. кратные\nпоказатели), не зависит от выбора нормальной системы решений и\nявляется характеристикой данной системы. Для любого решения s(t) ≠ 0\nпоказатель λ(s) может принимать только одно из этих значений.\nВ случае отображений вместо Reν будет использоваться |µ| для\nсобственных значений матрицы В. Аналогично для циклов систем ОДУ\nпоказатели Ляпунова равны логарифмам модулей множителей Флоке [6].\nСреди всего набора показателей Ляпунова важен старший λ1 [126].\nПоскольку для почти всех начальных данных S будет иметь ненулевую\nпроекцию на направление r(1), то типичной ситуацией будет λ(s) = λ1.\nЧтобы получить меньшее значение показателя λ(s) = λi необходим\nспециальный выбор начальных данных.\nКроме обычных показателей λ(s), характеризующих одно решение,\nт. е. растяжение или сжатие в одном направлении, используют показатели\nпорядка m > 1, характеризующие изменение n-мерных фазовых объемов\n(m ≤ n). Пусть s1(t),s2(t), … ,sm(t) — линейно независимые решения, a\nVol(s1(t),s2(t), … ,sm(t)) — объем образуемого ими n-мерного\nпараллелепипеда,\n( s1 , s1 ) ... ( s1 , sm )\n\nVol( s1 , ...., sm ) = det\n\n...\n...\n%\n.\n( sm , s1 ) ... ( sm , sm )\nТогда показателем Ляпунова порядка m называется\n1\nkm ( s1 , ...., sm ) = lim ln Vol( s1 , ...., sm ) .\nt →∞ t\nПодобно тому, как типичным значением λ(s) является λ1 типичным\nзначением km является λ1+λ2+ … +λn. Чтобы добиться иных значений,\nнеобходим специальный выбор начальных данных.\nСреди всех km выделяется показатель n-го порядка, для которого\nможно получить дополнительные результаты. Дело в том, что любой набор\nn линейно независимых решений образует фундаментальную матрицу Ф(t).\n78\n\n\fОбъем Vol(s1(t),s2(t), … ,sm(t)) = |W(t)|, где W(t) – определитель Вронского,\nдля которого d ln|W|/dt = tr A(t). Отсюда получаем, что\nt\n1\nkn = lim ∫ (trA(τ))d τ .\nt →∞ t\n0\nЕсли существует точный предел, то kn = <trA>, т. е. среднему по\nвремени от следа матрицы А.\nВ случае отображения фундаментальная матрица удовлетворяет\nуравнению Фk+1 = ВkФk , откуда Wk+1 = Wk × det Bk. Поэтому\n1 t\nln det Bk ,\n∑\nt →∞ t\nk =1\n\nkn = lim\n\nт. е. как правило kn = <ln|detB|>.\nВ соотношения для kn параметр s не входит, поэтому этот\nпоказатель можно рассматривать как характеристику всей линейной\nдинамической системы в целом. Согласно его значению динамические\nсистемы подразделяют на консервативные, сохраняющие n-мерные\nфазовые объемы (kn = 0) и диссипативные (kn < 0).\nДля нелинейной системы dx/dt = F(x) и ее траектории x(t) набор ЛП\nдля\nсистем\nбудет\nхарактеризовать\nустойчивость\nтраектории\nсоответствующей ей линейной системы. В этой связи выделяют класс так\nназываемых правильных по Ляпунову систем. Система ds/dt = A(t)s\nявляется правильной, если существует предел\nt\n1\nlim ∫ trA(τ)d τ = kn = λ1 + λ 2 + ... + λ n .\nt →∞ t\n0\nАналогично, дискретная система является правильной, если\nсуществует предел\n1 t\nlim ∑ ln det Bk = kn = λ1 + λ 2 + ... + λ n .\nt →∞ t\nk =1\nДля неправильных систем предел может не существовать или\nможет нарушаться равенство. Однако на практике неправильные по\nЛяпунову системы обычно не встречаются и соотношения эти всегда\nполагаются справедливыми. Для правильных систем доказано, что если\nλ1 < 0, то траектория х(t) асимптотически устойчива, если λ1 > 0 —\nнеустойчива.\nНабор показателей Ляпунова иногда называют спектром\nпоказателей Ляпунова соответствующей динамической системы.\nОтметим важное свойство показателей Ляпунова. Если уравнения\nдвижения динамической системы инвариантны относительно некоторого\nпреобразования,\nхарактеризующегося\nнепрерывно\nизменяющимся\nинвариантом, то у системы обычно есть нулевой показатель Ляпунова,\nсвязанный с этим преобразованием.\nПусть х(α,t) — однопараметрическое семейство траекторий,\nпричем ϕt(x) явно от α не зависит. Тогда x(α,t) = ϕt(x(α,0)), а\n∂x(α,t)/∂α = Dϕt(x(α,0)).∂x(α,0)/∂α = Ф(t).∂x(α,0)/∂α,\n79\n\n\fт. е. s(t) = ∂x(α,t)/∂α — решение линеаризованной системы для начальных\nданных S = ∂x(α,0)/∂α. Если справедливо соотношение\nc1 ≤\n\n∂x(α, t )\n≤ c2 ,\n∂α\n\nгде c1 и c2 не зависят от времени, то характеристический показатель для\nэтого решения будет равен нулю. Следовательно, в спектре показателей\nЛяпунова динамической системы должен быть нулевой показатель.\nЧастным случаем этой ситуации является сдвиг по времени для\nавтономных систем dх/dt = F(x). Если x(t) решение, то х(t + α) — тоже\nрешение. Так как ∂x/∂α = dх/dt = F(x), то условие существования нулевого\nпоказателя сводится просто к требованиям, чтобы на траектории не было\nнеподвижных точек. Таким образом, для аттракторов ОДУ, отличных от\nнеподвижных точек, должен быть по крайней мере один нулевой\nпоказатель Ляпунова.\nС точки зрения не линеаризованной, а исходной динамической\nсистемы показатели Ляпунова характеризуют скорость разбегания\nбесконечно близких траекторий, а показатели высших порядков —\nскорость изменения бесконечно малых фазовых объемов.\nДинамические системы, для которых n-мерный фазовый объем\nуменьшается, называются диссипативными. Если фазовый объем\nсохраняется, то такие системы носят название консервативных. У\nконсервативных систем всегда существует хотя бы один закон сохранения.\nНаличие\nзакона\nсохранения\nчасто\nвлечет\nсуществование\nсоответствующего ему нулевого показателя Ляпунова.\nДля диссипативных динамических систем сумма показателей\nЛяпунова всегда отрицательна, kn < 0.\nЕсли от системы дифференциальных уравнений Dх/Dt = F(x) с\nнабором показателей Ляпунова λ1, λ2, …, λn перейти к отображению\nxk+1 = f(xk), xk = x(t), xk+1 = x(t+τ) ≡ ϕτ(x(t)), то показателями Ляпунова для\nэтого отображения будут λ′i = λiτ. Аналогично, если от отображения\nперейти к некоторой его степени fm(x), то для нового отображения\nпоказатели будут в m раз больше, λi = mλi.\nЕсли от системы дифференциальных уравнений перейти к сечению\nПуанкаре и соответствующему отображению на единицу меньшей\nразмерности, то в спектре показателей для отображения не будет нулевого\nпоказателя, «отвечающего» за сдвиг вдоль траектории, остальными\nпоказателями будут λ′i = λi<τ>, где <τ> — среднее время возвращения на\nплоскость Пуанкаре.\nПри обращении времени (но для той же самой инвариантной\nмеры µi) «типичными» показателями Ляпунова будут –λ1, –λ2, … ,–λn.\nОднако при этом вместо аттрактора, к которому притягиваются траектории\nпри t→∞, нужно рассматривать неустойчивое множество — репеллер, к\nкоторому траектории притягиваются при t → –∞.\n80\n\n\fПо показателям Ляпунова можно многое сказать о динамической\nсистеме, о наблюдаемом режиме, о размерности аттрактора, если таковой\nимеется, и об энтропии динамической системы. Динамическому хаосу\nотвечает неустойчивость каждой отдельной траектории, т.е. наличие хотя\nбы одного положительного показателя Ляпунова. Для странного\nнехаотического аттрактора λ1 = 0.\nРегулярные периодические или квазипериодические режимы не\nимеют в спектре положительных показателей, а для k независимых частот\nимеют k (для ОДУ) или k–1 (для отображений) нулевых показателей.\nПоэтому для случая дифференциальных уравнений у цикла один нулевой\nпоказатель, у тора — два, у 3-тора — три и т.д. Для отображений у цикла\nнулевых показателей обычно нет, у тора — 1, у 3-тора — 2 и т. д. Эта\nзакономерность легко объяснима: когда аттрактором является «хорошее\nмножество», m-мерное многообразие, n-мерный фазовый объем должен\nсохраняться. Притяжение к аттрактору требует, чтобы фазовые объемы\nбольших размерностей сжимались. Это и отражено в ляпуновском спектре.\nТем не менее, количество независимых частот можно выяснить не\nвсегда, так как нулевые показатели могут быть связаны и с наличием\nсохраняющихся величин. Для диссипативных систем наличие законов\nсохранения, вообще говоря, нетипично, однако соответствующие примеры\nсуществуют.\nЗнание показателей Ляпунова позволяет оценить и фрактальную\nразмерность аттрактора\nk\n\nkk = ∑ λ i .\ni =1\n\nПусть k — такое число, что k1, k2, …, kk ≥ 0, а kk+1 < 0, т. е. k-мерный\nфазовый объем не уменьшается, а k+1-мерный — сокращается. Для\nаттракторов-многообразий фазовый объем, отвечающий размерности\nаттрактора, сохраняется. Для хаотических аттракторов обычно получается\nтак, что kk > 0, а kk+1 < 0, и целой размерности, обладающей таким\nсвойством, не существует. Однако можно попытаться найти подходящую\nдробную размерность. Для этого аппроксимируем зависимость kj = k(j)\nкусочно-линейной функцией k(j) = aj+b. Эта функция обязательно\nобратится в ноль в некоторой промежуточной точке j = dL, k < dL < k + 1.\nЭто значение и принимается за оценку размерности аттрактора, которая\nполучила название ляпуновская размерность [90]. Получим для нее\nвыражение. Как говорилось выше\nak + b = kk, a(k + 1) + b = kk–|λk+1|,\nт. е. а = –|λk+1|, b = kk + k|λk+1|, a для ляпуновской размерности получаем\nсоотношение —\n|λk+1|dL + kk + k|λk+ 1| = 0\nили\nk\ndL = k + k .\n\nλk +1\n\n81\n\n\fДоказано несколько теорем [195, 196, 210], согласно которым dL\nдает оценку сверху для хаусдорфовой размерности аттрактора, правда\nиногда вместо обычных показателей Ляпунова используют так называемые\n«глобальные показатели Ляпунова», которые больше или равны обычным.\nСоответствующая оценка размерности также будет больше.\nПусть у динамической системы j строго положительных\nпоказателей λi > 0. Для энтропии K1 существует строгая оценка\nj\n\nK1 ≤ ∑ λ i .\ni =1\n\nОднако на практике обычно считают, что выполняется точное равенство\nj\n\nK1 = ∑ λ i .\ni =1\n\nДля прочих энтропии Kq аналогичных оценок нет, но поскольку\nчаще всего все Кq при не слишком больших q близки, то приближенно\nможно пользоваться точным равенством.\n4.1.2. Методы расчета показателей Ляпунова\n\nПусть задана система xk+1 = f(xk). После линеаризации получаем\nлинейную систему sk+1 = Bksk, Тогда набор показателей Ляпунова будет\nзависеть от базовой траектории х(t), т.е. будет характеризовать\nтраекторию, а не аттрактор. Фундаментальное значение показателям\nЛяпунова придала мультипликативная эргодическая теорема, доказанная\nВ. И. Оселедцем [107]. Согласно ей, показатели Ляпунова совпадают почти\nдля всех траекторий по инвариантной мере µ. Этот факт использовался во\nмногих методах [196, 208, 219, 227].\nОбозначим x0 = х, и пусть Фk — фундаментальная матрица\nлинеаризованной системы. Мультипликативная эргодическая теорема\nпредполагает, что траектория хk может быть продолжена до бесконечности\nв обе стороны, т.е. при k → ∞, и при каждом хk существует Фk = Dϕk(x).\nЭто, вообще говоря, справедливо не для всех точек х. Поэтому будем\nпредполагать, что х принадлежит инвариантному множеству, которое\nявляется носителем некоторой инвариантной меры µ, т. е. преобразование f\nсохраняет меру µ. Матрицы Фk предполагаются невырожденными, а\nматрицы Вk = Df(xk) будем полагать ограниченными. В этих условиях: (1)\nдля почти всех х по мере µ существуют точные значения показателей\nЛяпунова всех порядков при t→±∞, т.е. линеаризованная система является\nправильной по Ляпунову; (2) значения показателей λi совпадают для почти\nвсех х по мере µ; (3) касательное пространство TхM(хk) в каждой точке хk\nраспадается на прямую сумму подпространств Ri(хk), так что если\nто\n\ns(0) ∈ Ri(х0),\n82\n\n\f1\nln u (t ) = ±λ i .\nt →∞ | t |\nПодпространства инвариантны в том смысле, что если xk+1 = f(xk), то\nRi(хk+1) = Вk Ri(хk).\nТаким образом, почти для всех точек х по мере µ линеаризованная\nсистема оказывается правильной по Ляпунову. Такие точки также\nназывают правильными. Согласно теореме, это свойство оказывается\nтипичным по инвариантной мере.\nПоказатели Ляпунова являются средними по мере от некоторого\nфункционала, зависящего от х. Предположим, что все показатели\nразличны, т.е. все подпространства Ri(хk) одномерны. Тогда в касательных\nпространствах можно выбрать базисы {R(i)(xk)}, причем так, что они тоже\nбудут инвариантными:\nBk r (i ) ( xk )\n(i )\n.\nr ( xk +1 ) =\nBk r (i ) ( xk )\nlim\n\nТакой базис называют базисом Оселедца.\nРассмотрим решение линеаризованной системы sk+1 = Bksk,\nначальные данные для которой S0 = R(i)(x0). Тогда, последовательно\nподставляя R(i)(xk+1)•||Bk R(i)(xk)|| вместо Bk R(i)(xk), получим\n\nuk = Bk −1Bk −2 ...B1B0 r ( i ) ( x0 ) = Bk −1Bk −2 ...B1r ( i ) ( x1 ) ⋅ B0 r (i ) ( x0 ) =\n= Bk −1r ( i ) ( xk −1 ) ⋅ Bk −2 r (i ) ( xk −2 ) ⋅ ..... ⋅ B1r (i ) ( x1 ) ⋅ B0 r ( i ) ( x0 ) ,\nоткуда\n1\n1 k −1\nln uk = ∑ ln B j r ( i ) ( x j ) .\nk\nk j =0\nПоскольку линеаризованная система является правильной почти\nдля всех х0 по мере µ,\n1 k −1\nln B j r (i ) ( x j ) = ln Df ( x j )r (i ) ( x j ) = ln Df ( x)r (i ) ( x)\n∑\nt →∞ k\nj =0\n\nλ i = lim\n\nµ\n\n.\n\nОднако инвариантные подпространства Wi = span{R(1)(x),…,R(i)(x)},\nчисленно найти удается при помощи алгоритма Бенеттина [91]. В нем\nполучаются другие вектора е(i)(х), такие что образуемые ими\nподпространства оказываются теми же самыми,\nspan{R(1)(x), …, R(i)(x)} = span{e(1)(x),…,e(i)(x)}.\nОни оказываются тесно связаны с другим вариантом мультипликативной\nэргодической теоремы, предложенным Д. Рюэллем [120].\nПеременные sk можно выразить через фундаментальную матрицу\nФk = Df k(x) и начальные данные S: sk = ФkS. Выражение, входящее в\nопределение показателя Ляпунова, можно переписать следующим образом:\n\n83\n\n\f(\n\n)\n\n1 2k\n1\n1\n.\nln(uk , uk ) =\nln (U , Φ k Φ*kU ) = ln (U , Φ*k Φ kU )\n2k\n2k\nКак показал Рюэлль, если преобразование f(x) сохраняет меру µ, а\nна Df(x) наложены те же условия, что и в теореме Оселедца, то для почти\nвсех х по мере µ существует предельная матрица\n\n1\n2k\n\nG∞ ( x) = lim ( Φ k Φ k ) .\n*\n\nk →∞\n\nЛогарифмы собственных значений этой матрицы λi = ln li — это\nнабор показателей Ляпунова, отвечающих точке х, (т.е. для почти всех х\nсобственные значения li совпадают. Собственные вектора g(i)(x) этой\nматрицы образуют ортонормированный базис в касательном пространстве\nк точке х. Их связь с векторами r(i)(x) довольно очевидна: они получаются,\nесли векторы {r(i)(x)} ортонормировать, начиная с последнего:\ng ( n ) ( x) = r ( n ) ( x),\n\ng ( n −1) ( x) = cn −1, n −1r ( n −1) ( x) + cn −1, n r ( n ) ( x),\n.............................,\nn\n\ng ( j ) ( x) = ∑ c j ,i r (i ) ( x), c j , j ≠ 0,\ni= j\n\n.............................,\nгде коэффициенты сi,j определяются из условия ортонормированности\nбазиса {g}. (Если ортонормировать начиная с первого, то начальным\nданным S = g(i) в общем случае будет отвечать показатель не λi, а λ1\nпоскольку будет ненулевая проекция на направление r(1)(x)).\nЕсли отображение f обратимо, то для обратного отображения f-1 и\nтраектории, продолженной в обратную сторону до бесконечности\n(xk-1 = f 1(xk), Sk-1 = Df-1(xk)sk), можно построить аналогичную матрицу\nG−∞ ( x) = lim ( Φ k Φ k )\n*\n\nk →∞\n\n1\n2k\n\n.\n\nЕе собственные значения l ′i = li-1= e− λ , а собственные вектора е(i)(х)\nполучаются ортогонализацией Грамма-Шмидта системы {r(i)(x)}, но\nначиная с первого.\ni\n\n4.1.3. Разработка алгоритмов оценки показателей Ляпунова\nпо временному ряду\n\nСуществующие в настоящее время алгоритмы оценки можно\nразделить на 2 класса: матричные методы [182, 226] и методы аналога [239,\n91, 7].\nМатричные методы. Алгоритмы, связанные с восстановлением в\nкаком-либо виде уравнений движения, аппроксимацией матрицы Df и\nрасчетом показателей называют матричными. Алгоритмы основаны на\n84\n\n\fпостроении локальных матриц Якоби (матрица Bk в системе sk+1= Bksk) для\nкаждой точки реконструированного аттрактора, после чего для\nнахождения показателей (можно попытаться оценить весь спектр)\nиспользуют численные методы, например метод Беннетина [162].\n1. Задаем n ортонормированных векторов v(i)0, i = 1,...,n,\nприсваиваем σ(i)0 = 0, t0 = 0, s(i)0=v(i)0, а также определяем шаг перенормировки ∆t.\n2. Находим xk+1 = f (xk), s(i)k = Df(xk).\n3. Ортогонализуем систему векторов s(i)k+1 и получаем s′ (i)k+1:\ns′(1)k+1= s(1)k+1,\ns′ (2)k+1= s(2)k+1+aij s(1)k+1, (s′(j)k+1,s(i)k+1) = 0,\n... ... ... ... ... ... ... ... ... ...,\n(i )\nk +1\n\ns'\n\n=s\n\n(i )\nk +1\n\nj −1\n\n+ ∑ a j ,i sk( i+)1 , a j , j ≠ 0, ( s '(k +j )1 , s '(ki+)1 ) = 0, i < j .\ni =1\n\n(i)\n\n4. Увеличиваем σ на логарифм нормы соответствующего вектора:\nδσ k +1 = ln u '(ki+) 1 , σ(i ) k +1 = σ(i ) k + δσ(i ) k +1 ; увеличиваем t: tk+1 = tk + ∆t. В\n(i )\n\nкачестве текущей оценки показателя Ляпунова можно использовать\nλ 'i (t ) = σ(i ) k +1 tk +1 .\nполучаем\n5.\nНормируем\nсистему\nвекторов\ns′ (i)k+1,\nортонормированный базис на следующем шаге vk( i+)1 = u '(ki+)1 u '(ki+)1 и заносим\nего снова в вектора s: s′(i)k+1= v(i)k+1.\n6. Повторяем пункты 2–5 заданное число раз.\n7. Получаем окончательную оценку показателя Ляпунова:\nОртогонализация осуществляется процедурой Грамма–Шмидта,\nкоторая в матричном виде называется QR-разложением на ортогональную\nматрицу Q, столбцы которой образуют ортонормированный базис {v(i)k}, и\nверхнюю треугольную матрицу R, тогда δσ(i)k = ln Ri,k. Поскольку,\nv(i)k → e(i)(xk) то с использованием векторов e(i)(xk) можно написать\nвыражение для показателей Ляпунова:\n1 N\nln ( e( i ) ( xk +1 ), Df ( xk )e( i ) ( xk ) ) .\n∑\nN →∞ N\nk =1\n\nλ i = lim\n\nВ описанном методе не обязательно вычислять все показатели.\nЕсли использовать m < n векторов s(i) (или v(i)), то будут получены m\nнаибольших показателей.\nМетоды аналога.\n1. Метод Волфа [239]. Первым шагом произвольная точка\nтраектории z0 (в реконструированном фазовом пространстве) принимается\nза начальную и ищется соседняя ближайшая к ней точка z00. Расстояние\nмежду этими двумя точками || L0 0 || .\nПри хаотической динамике со временем это расстояние растет. Если\n85\n\n\fследующее значение || L01 || > || L0 0 || , то оно отбрасывается и ищется новая\nточка z11, соседствующая с z1 и лежащая по возможности в том же\nнаправлении, что и z01. Для поиска точки, удовлетворяющей этому\n⎛ L0\nL1 ⎞\nусловию можно определить скалярное произведение S = ⎜ 1 , 1 ⎟ ,\n⎜ L01 L11 ⎟\n⎝\n⎠\nвеличина которого должна быть как можно ближе к единице.\nТак как Lij описывает поведение малого возмущения, его длина\nдолжна быть по возможности малой, чтобы линеаризованная вдоль\nтраектории система хорошо описывала эволюцию. С другой стороны она\nне должна быть настолько малой, чтобы стать сравнимой с уровнем\nшумов. Кроме того, необходимо чтобы z0 и z00 принадлежали разным\nтраекториям, иначе не будут получены положительные первые показатели\nЛяпунова λ1.\nЕсли эти условия выполняются, то старший показатель Ляпунова\nопределяется из выражения:\n⎛ Lj j +1 ⎞\nlog ⎜ j ⎟ ,\n∑\n⎜ L ⎟\nj =0\n⎝ j ⎠\nгде (M–1) число смен соседних траекторий.\nЕсли выбрать основание для логарифма равное двум, то первый\nпоказатель λ1 измеряется в единицах бит/шаг во времени.\nМетоды аналога не требуют смены траекторий. Наибольшую\nизвестность получили методы Канца [196] и Розенштейна [221].\n2. Метод Розенштейна прост для реализации и показывает\nхорошую скорость расчета, однако, результатом его работы является не\nчисленное значение λ1, а некоторая функция от времени:\n1\nln d j (i ) , d j (i ) = min x j − x ' j ,\ny (i, ∆t ) =\nxj\n∆t\nгде xj— рассматриваемая точка, а x’j — один из ее «соседей». Алгоритм\nоснован на связи dj и показателей Ляпунова: d j (i ) ≈ eλ (i∆t ) . Для оценки\nиспользуется ближайший сосед рассматриваемой точки. Старший\nпоказатель Ляпунова предлагается вычислить как угол наклона ее\nнаиболее линейного участка. Нахождение такого участка, оказывается\nнетривиальной задачей, а иногда такой участок и вовсе указать не удается.\n3. Метод Канца основан на соотношении d j (i ) ≈ eλ (i∆t ) и вычислении\nЛП по углу наклона наиболее линейного участка некоторой функции вида:\n1\nλ1 =\nt M − t0\n\nM −1\n\n1\n\n1\n\n⎛ 1\nS (ε 0 , j ) = ln ⎜\n⎜ ℵn\n⎝\n\n∑\n\nx ' j ∈ℵn\n\n⎞\nxj − x'j ⎟ .\n⎟\n⎠\n\nУсреднение берется по всем ближайшим соседям xj в окрестности\nточки равной ε.\n86\n\n\f4.2. Разработка алгоритмов оценки инвариантных\nхарактеристик\n\nДля практического использования, кроме приведенных выше\nхарактеристических показателей выбраны инвариантные характеристики,\nдля которых разработаны соответствующие расчетные алгоритмы.\nВыбор временного интервала. Используется методика, основанная на\nтеории информации и использовавшая первый минимум взаимной\nинформации для x(t) и x(t +τ). Для этого по временному ряду\nизготовляются гистограммы, аппроксимирующие распределение x(t) (оно\nже будет и для x(t + τ)) и совместное распределение x(t) и x(t + τ). Далее по\nпостроенным гистограммам рассчитываются энтропии и взаимная\nинформация.\np (τ)\nS = −∑ pij (τ)ln ij ,\npi p j\nij\nгде pi — вероятность нахождения точки в i–том интервале, а pij(τ) —\nсовместная вероятность, попадания x(t) в i-й интервал и попадания x(t+τ) в\nj-й.\nОценка корреляционной размерности по временному ряду. Для\nдинамической системы очень важным является исследование структуры\nаттракторов. Странные аттракторы нелинейных динамических систем\nимеют самоподобную структуру, поэтому для них удобно применять\nкачественное оценивание, идентификацию масштабных свойств, которые\nмогут быть измерены с помощью фрактальных размерностей. Это\nпозволяет оценить геометрическую структуру аттракторов и ввести меру\nдля числа степеней свободы динамической системы [29].\nКорреляционная размерность является частным случаем так\nназываемой генеральной размерности, для определения которой\nпространство вложения, имеющее размерность ED, разбивается на ячейки\nVi размером ε, i = 1, ..., m. Пусть вероятность того, что какая-то точка\nаттрактора находится в ячейке Vi есть pi. Тогда генеральная размерность\nопределяется:\n⎛ N\n⎞\nlog 2 ⎜ ∑ piq ⎟\n1\n⎝ i =1 ⎠ , для q = 0,\nDq = lim\nR →0 q − 1\nlog 2 (ε)\nlog(m)\n, для q →1\nD0 = lim\nε → 0 log(1/ ε )\nЗдесь информационная размерность D1 и информация I(ε) определяются\nсоотношениями:\nN\nI (ε )\n; I (ε) = −∑ pi log 2 ( pi ) .\nD1 = lim\nε→ 0 log (1/ ε )\ni =1\n2\n87\n\n\fДля q = 2 получается выражение для так называемой\nкорреляционной размерности, используемой для определения размерности\nобъектов, трудно поддающихся или не поддающихся аналитическому\nописанию (например, аттракторы, построенные методом задержек по\nопытным данным):\nlog 2 C (ε)\n,\nD2 = lim\nε→ 0\nlog 2 ε\nгде C(ε) —корреляционный интеграл:\n1 m\nC (ε) = lim 2 ∑ H (ε− || zi − z j ||) ;\nm→∞ m\ni , j =1\ni≠ j\n\nε — радиус сферы, для которого определяется число точек M(ε),\nоказавшихся внутри сферы; H — функция Хевисайда.\nПри условии достаточно большого времени наблюдения T\nзначениями\nlog 2 C (ε)\n1 m\nC (ε) ≈ 2 ∑ H (ε− || zi − z j ||) , D2 ≈\nm i , j =1\nlog 2 ε\nопределяется число точек M(ε) фазовой траектории, реконструированной\nиз временного ряда, оказавшихся внутри сферы радиусом ε.\nТаким образом, на основании приведенного анализа результатов\nможно сформировать алгоритмическое обеспечение, которое реализовано\nМ. В. Воловичем [248–250].\nПример вычислений. Рассмотрим временной ряд, порожденный\nсистемой:\n⎧ x\u0005 = − y − z\n⎪\n⎨ y\u0005 = x\n⎪\n2\n⎩ z\u0005 = 0.375 ⋅ ( y − y ) − 0.23 ⋅ z\n1. Расчет времени задержки иллюстрирует рис. 4.1. Расчетное\nзначение τ = 37.\n\nРис. 4.1. Расчет времени задержки\n88\n\n\f2. Оценка старшего показателя Ляпунова проводится\nиспользованием 3-х методов. Результаты сведены в табл. 4.1.\nМетод оценки\nМетод Вольфа\nМетод Канца\nМетод Розенштейна\n\nс\n\nТаблица 4.1\nЗначение старшего показателя Ляпунова\n0,008912\n0,009171\n0,009653\n\nОкончательный результат получен как усреднение по всем трем\nметодам\nλmax ≈ 0,009245.\n3. Оценка спектра показателей Ляпунова с использованием\nалгоритма, основанного на методе Беннетина, значение с первого по\nтретий показатели Ляпунова имеют соответствующее значения: 0,008675;\n0,000059; –0,028752.\n4. Вычисление энтропии системы показано на рис. 4.2.\n5. Оценка корреляционной размерности приведена на рис. 4.3.\n\nРис. 4.2. Результат вычисления энтропии\n\nРис. 4.3. Вычисление корреляционной размерности\n89\n\n\f4.3. Модифицированный метод реконструкции аттракторов\nдля систем, допускающих группы симметрий\n\nПервой работой по реконструкции странного аттрактора по\nвременным рядам была публикация результатов по гидродинамике [214]. В\nстатье\nпоказано,\nчто\nможно\nполучить\nудовлетворительную\nгеометрическую картину странного аттрактора небольшой размерности,\nесли вместо переменных x, входящих в уравнения динамической системы\ndx/dt = F(x), использовать m-мерные вектора, получаемые из элементов\nвременного ряда по тому же принципу, что и в задачах авторегрессии\nzi = {xi, xi+1, … ,xi+m–1}.\nВ том же году Ф. Такенс доложил о своей теореме,\nопубликованной годом позже [232]. Именно она лежит в основе всех\nалгоритмов анализа временных рядов методами нелинейной динамики\n[9, 108, 155, 198, 207, 215, 249 и др.].\nПусть Mk— k-мерное многообразие. Когда такое многообразие\nреализуется в виде поверхности Lk в n-мерном пространстве, которая не\nпересекается сама с собой, то говорят, что оно вложено в R n . Само\nвложение можно представить себе как дифференцируемую векторную\nфункцию F, определенную на Mk, для которой отображение Mk→Lk\nявляется взаимно однозначным и существует обратная дифференцируемая\nфункция F-1, отображающая Lk обратно в Мk. То есть Lk = F(Mk). Функция\nF-1 определена только на Lk, в противном случае она не будет однозначной.\nВыбирая разные F и n, можно получить различные представления одного и\nтого же многообразия.\nПусть на многообразии Mk (или на какой-либо поверхности Lk\nдиффеоморфной ему) определена векторная функция, нужное количество\nраз дифференцируемая и отображающая Mk в m-мерное евклидово\nпространство R n .\nПусть Mk — как минимум дважды дифференцируемое\nмногообразие, a g(x) — некоторая дважды дифференцируемая функция,\nотображающая Mk→ R n , для которой матрица производных ∂gi/∂xj имеет\nранг k. Последнее условие необходимо, чтобы при отображении не\nполучился объект меньшей размерности; скажем, плоскость не\nотображалась в одномерную кривую, т. е. ранг отображения должен быть\nравен k. Такое отображение будет давать погружение многообразия Mk в\nR n при условии, что m ≥ 2k + 1(теорема Уитни [136]). Погружение\nлокально аналогично вложению, но может содержать самопересечения, а\nпотому глобально невозможно определить обратное отображение.\nНапример, если в качестве многообразия рассматривать окружность, то на\nплоскости эллипс будет вложением, а восьмерка — только погружением.\nТочке пересечения восьмерки будут соответствовать две различные точки\n90\n\n\fокружности. Поэтому теоремы Уитни оказалось недостаточно для\nобоснования методов обработки временных рядов.\nПусть задана динамическая система ϕt(х) с фазовым\nпространством Р. Будем считать, что числа, образующие временной ряд,\nявляются значениями некоторой «наблюдаемой» — скалярной функции\nсостояния динамической системы х(t):\nxi = h(x(ti)) = h(ϕt(x0)).\nВ качестве многообразия М, фигурирующего в теореме Уитни,\nможет использоваться либо само фазовое пространство Р, либо какое-либо\nинвариантное многообразие Md из Р.\nПусть временной шаг между элементами временного ряда равен τ,\nа вектора x(ti) будем обозначать xi. Тогда\nxi = h(xi) ≡ Ф0(xi),\nxi+1 = h(xi+1) = h(ϕτ(xi)) ≡ Ф1(xi),\nxi+2 = h(xi+2) = h(ϕ2τ(xi)) ≡ Ф2(xi),\n... ... ... ... ... ... ...\nxi++m-1 = h(xi++m-1) = h(ϕ(m-1)τ(xi)) ≡ Фm-1(xi),\nxi+m = h(xi+m) = h(ϕmτ(xi)) ≡ Фm(xi)\nВсе компоненты вектора z связаны с одним и тем же состоянием\nдинамической системы xi. Следовательно, существует векторная функция,\nкоторую, следуя Такенсу, обозначим Λ, отображающая вектора xi ∈ Md в\nточки m-мерного евклидова пространства R m ,\nzi = Λ(xi), xi ∈ Md, zi ∈ R m .\nВ теореме предполагается, что Md, h и ϕt по крайней мере дважды\nдифференцируемы, а для всех неподвижных точек и циклов с периодами\nkτ, k < d, предполагается, что у них все собственные значения простые и не\nравны 1, а h(x) для них различны. Тогда теорема Такенса утверждает, что\nслучаем общего положения, т. е. типичным свойством отображения Λ\nбудет то, что при m ≥ 2d + 1 оно будет давать вложение Mk в R m . Образ\nМk в R m будем обозначать Ld: Ld = Λ(Md), и, согласно теореме, в типичном\nслучае у него не должно быть самопересечений.\n«Вложение» в данном случае будет означать, что:\n1. Функция Λ будет дифференцируема и будет иметь обратную\nдифференцируемую Λ-1, определенную на LD: Md = Λ-1(Ld)).\n2. Каждой\nтраектории\nдинамической\nсистемы\nбудет\nсоответствовать ее образ в z-пространстве. Причем для образов будут\nиметь место те же свойства, что и для исходных траекторий, в частности,\nчерез каждую точку Ld будет проходить одна и только одна z-траектория.\n3. На Ld можно определить динамическую систему.\nxi = Λ-1(zi), xi+1 = ϕτ(xi),\nzi+1 = Λ(xi+1) = Λ(ϕτ(Λ-1(zi))) ≡ Ψ(zi), zi∈Ld.\n91\n\n\fОтображение Ψ переводит Ld в Ld, а вне поверхности Ld — Ψ не\nопределено. Если оставить только последнюю компоненту этого\nсоотношения, получим другой вариант, который можно записать в виде\n«отображения с запаздыванием» или «нелинейной авторегрессии»\nxi = F (xi-1, … , xi-m).\nТаким образом, имеется два отображения:\nxi+1 = ϕτ(xi) ≡ Ф(xi), xi ∈ Md, Ф: Md → Md\nи\nzi+1 = Ψ(zi), zi ∈ Ld, Ψ: Ld → Ld.\nИх можно рассматривать как отображения, связанные невырожденной и\nобратимой заменой переменных z = Λ(х) или как различные представления\nодного и того же отображения. Следовательно, характеристики,\nинвариантные относительно невырожденной замены, у обеих систем\nдолжны совпадать. К ним относятся фрактальные размерности аттрактора,\nнабор обобщенных энтропии и все d показателей Ляпунова. Поэтому\nуказанные свойства можно пытаться определять по экспериментальным\nданным, не зная всех переменных динамической системы. Можно\nпытаться восстановить (аппроксимировать) и саму функцию Ф(z).\nТаким\nобразом,\nтеорема\nТакенса\nподводит\nстрогую\nматематическую основу под идеи нелинейной авторегрессии.\nПрактическая реализация идей реконструкции часто сталкивается с\nпроблемами. Возникают они из-за того, что длина обрабатываемого ряда\nвсегда ограничена, во-первых, возможностями хранения информации, вовторых, скоростью обработки, и, в-третьих, стационарностью\nисследуемого объекта — важно знать, в течение какого времени мы можем\nполагать, что исследуем одну и ту же динамическую систему (как только\nизменится ϕt(x), вектора z начнут строиться по-другому). Пусть имеется\nвременной ряд из N чисел, которые являются значениями некоторой\nнаблюдаемой, характеризующей одну и ту же динамическую систему.\nТогда реконструированные z-вектора дадут N–m точек на поверхности\nLd∈ R n , по которым надо будет судить о динамической системе Ψ и ее\nаттракторе. Объем информации, который можно извлечь из этого\nмножества точек, вообще говоря, зависит от свойств поверхности\n(насколько она искривлена, закручена и т. п.) и от свойств функции Ψ(z)\n(насколько велики ее производные). Так как точек конечное число, то\nсуществует некоторое характерное расстояние l между точкой и ее\nближайшим соседом. Меньшие масштабы будут неразрешимы для данного\nвременного ряда. Если на масштабах порядка l поверхность Ld сильно\nискривлена, а функция Ψ(z) сильно изменяется, то методы нелинейной\nдинамики будут, скорее всего, бесполезны. Эта же проблема в несколько\nином виде встречается, например, в задачах цифровой обработки сигналов\n(теорема Котельникова). Считается, что если временной интервал между\nотсчетами равен ∆t, то частоты больше чем 1/2∆t разрешить невозможно.\n92\n\n\fОднако в задачах реконструкции свойства Ld и Ψ(z) априорно неизвестны,\nпоэтому аналогичных оценок (скажем, кривизна или производная, не\nпревышающие ~l-1) сделать невозможно. Можно только разумно\nраспорядиться несколькими свободными параметрами. Чаще всего\nэто m и τ.\nСвойства Ld и Ψ(z) зависят от динамической системы ϕ,\nнаблюдаемой h, задержки τ и размерности векторов m («размерность\nвложения»).\nМетод восстановления аттрактора системы был модифицирован\nавтором применительно к системам, допускающих группы симметрий\n[275, 278, 279]. На основании теоремы Кинга-Стеварта [199] о вложениях,\nдля систем, допускающих симметрии, наблюдаемый выход был векторной,\nа не скалярной функцией состояния системы s ( t ) :\ny (t ) = W ( s (t )) ,\n\n(4.1)\nотображая пространство состояний идентифицируемой системы в mмерное евклидово пространство. Состояние системы может аналогично\nпредставлено вектором координат фазового пространства на основании\nсоответствующих временных интервалов задержек\nT\nx(t ) = ⎡ y ( t + T1 ) , ..., y t + Tne ⎤ ,\n⎣\n⎦\nгде размерность вложенного пространства n = m ne . Так как вид\nэволюционных уравнений неизвестен, то возможность построения\nфункции W можно установить на базе основных преобразований — групп\nсдвига, которые для систем с управлением обладают функциональной\nмощностью [276].\nЛинеаризуем выход (4.1) в окрестности неизменяемого во времени\nсостояния s , и, обозначая смещение ∆y ( t ) = G ( s ( t ) ) − G ( s ) , получим\n\n(\n\n)\n\n∆y ( t ) = C ( s ( t ) ) ,\n\n(4.2)\n\nгде матрица C — определена таким образом, что C = DsG ( s ) .\nДинамическая система или пара (A, C) должна быть наблюдаемой,\nт. е. в течение любого времени начальное состояние ∆s ( ti ) = ∆si может\nбыть определено из измерения управляющих возмущений ∆u ( t ) и\nвыхода ∆y ( t ) .\nГруппа симметрий T, в соответствии с определениями, введенными\nво второй главе, может быть определена базисным набором\nпреобразований в виде:\nT = p1T 1 ⊕ p2T 2 ⊕ ... ⊕ pqT q ,\n\n(4.3)\n\nгде n = p1d1 + p2 d 2 + ... + pq d q ; pr — число эквивалентных представлений\nTr в декомпозиции, и q — общее число инфинитезимальных образующих\nв базисе. Аналогично (4.3) может быть разложено само касательное\n\n93\n\n\fпространство TxX на сумму инвариантных подпространств LrLα' , таких, что\n\nT ( g ) x ∈ LrLα' , ∀x ∈ LrLα' и ∀g ∈ L ';\nTx X = L1L ' ⊕ L2L ' ⊕ ... ⊕ LqL ' ,\n\n(4.4)\n\nгде LrL ' = LrL1' ⊕ LrL1' ⊕ ... ⊕ LrpL r' ; α = 1, pr — индексы возможных инвариантных\nподпространств, которые вписываются в группу Tr.\nНа основании группового анализа систем, редуцированных на\nцентральное многообразие впервые получен следующий результат.\nТеорема 4.1 [274, 279]. Если в системе нет случайных вырождений,\nи группа T содержит не более одной копии каждого элемента\nдекомпозиции (4.4) представления группы симметрий, то для\nреконструкции динамических систем в окрестности состояния s(t)\nнеобходимо, чтобы число (m) измеряемых скалярных выходных сигналов\nyi (t ) , равнялось размерности конечнопараметрической алгебры A0,\nотвечающей группе симметрий графиков фазовых траекторий.\nСледствие [274, 279]. Для скалярного выходного сигнала\nнаблюдаемая колебательная система может быть реконструирована на\nцентральном\nинвариантном\nмногообразии\nна\nосновании\nоднопараметрической алгеброй Ли, а функция Ψ0 порождена групповой\nоперацией сдвига.\nДинамическая система или пара (A, C) называется наблюдаемой [99]\nесли, в течение любого времени, начальное состояние ∆s ( ti ) = ∆si может\nбыть определено из измерения управляющих возмущений ∆u ( t ) и выхода\n\n∆y ( t ) . Как известно, понятие наблюдаемости дуальное к понятию\nуправляемости — условие наблюдаемости для пары (A, C) эквивалент\nусловия управляемости для пары (A*, C*). Наличие группы симметрий\nнепосредственно подразумевает, что свойства симметрии матриц A и A* по\nсуществу идентичны (одинаковы структуры спектров собственных\nзначений, жордановых нормальных форм, и т. п.). В результате, известное\nранговое условие управляемости линейных систем, накладываемое на\nматрицу B должно также удовлетворяться и для матрицы C*. Иными\nсловами наблюдаемость линеаризованной системы является эквивалентом\nнаблюдаемости собственных векторов матрицы Якоби исходной системы.\n4.4. Метод моделирования нелинейных систем\nпо экспериментальным данным\n\nПолученные выше результаты позволили обосновать и обобщить\nпредложенные автором в работах [250–255] модели нелинейных систем,\nдопускающие группы симметрий, т. е. на основании временного ряда\n94\n\n\fидентифицировать параметры модели, редуцированной на центральное\nмногообразие\nx\u0005 = Ax + Ψ 0 ( x, t );\n(4.5)\ny = Cx.\nНа основе доказанных утверждений разработан оригинальный метод\nмоделирования по экспериментальным данным, представляющий собой\nреконструкцию систем, редуцированных на центральное многообразие.\nНаиболее эффективно практическое применение метода для повышения\nкачества функционирования существующих систем. Как правило, в этом\nслучае — наблюдаемые в результате эксперимента процессы являются\nгрубыми периодическими траекториями. Алгоритм метода можно\nпредставить в виде 4 этапов.\n1. Оценка инвариантных характеристик и реконструкция\nаттрактора. Включает оценку размерности минимального инерциального\nмножества; применение методов вычисления по временному ряду\nвыбранных характеристик и построение аттрактора системы с учетом\nсформулированной теоремы (3) третьей главы.\n2. Вычисление преобразований, допускаемых системой. При этом\nидентифицируются наблюдаемые по экспериментальным данным замены\nкоординат, переводящие одну область решений в другую, с учетом\nсуществования симметрии сдвига. Эта процедура возможна, в случае\nнаблюдаемости системы, описывающей переход от оператора (2.2) к\nгруппе (2.1), а также определяется линейностью алгебры Ли.\n3. Определение вида функции Ψ 0 ( x, t ) с учетом ее коммутирования с\nинфинитезимальным оператором, т. е. на основании теоремы Ли и\nформулы Хаусдорфа.\n4. Идентификация параметров системы в форме, предложенной в\nтеореме 1 или теореме 2 с помощью метода наименьших квадратов. Для\nприведенных в работе примеров использовался метод наименьших\nквадратов, реализованный Л. Льюнгом в System Identification Toolbox for\nMATLAB.\nНа основании полученных результатов сформулируем методику\nвычисления функций Ψ 0 ( x, t ) .\nСформировано и апробировано на большом количестве задач\nмоделирование тепловых процессов. Для этих процессов можно выделить\nхарактерные особенности на реконструированном фазовом портрете —\nчетко\nвидны\nпериодические\n(квазипериодические)\nтраектории,\nследовательно, имеет место симметрия сдвига. Учет симметрии сдвига в\nфункции Ψ 0 ( x, t ) , как правило, определяет хорошую адекватность модели\nвременному ряду. Даже выбор в качестве линейной функции времени\nΨ 0 ( x, t ) дает адекватность процесса около 50%, что является хорошей\nоценкой для восстановленных эволюционных уравнений нелинейных\nсистем.\n95\n\n\fТаким образом, для задач моделирования тепловых процессов\nсформированы следующие методологические рекомендации.\n1. Реконструкция аттрактора (или фазового портрета) системы.\n2. Выделение\nв\nфазовом\nпространстве\nобластей\nс\nпериодическими траекториями.\n3. Вычисление преобразования (автором использовались\nавтрегрессионные полиноминальные модели, представляющие собой\nгруппу Тейлора). С точки зрения системного анализа это\nпреобразование можно интерпретировать, как «внутреннее»\nпараметрическое управление автоколебательной системы.\n4. Если полученное преобразование имеет адекватность более\n75%, то делается допущение о достаточной согласованности\nпреобразования. Следовательно, выбранные локальные области\nявляются топологически эквивалентными.\n5. Построение системы вида (4.5) с учетом выполнения\nуравнений Ли.\nПри моделировании ситуации седло-фокус, а также других\nвозможных ситуаций, первым этапом предполагается, что система\nпериодическая, что, как показано во второй главе, как правило, имеет\nместо в системах с управлением в рассматриваемом классе технических\nзадач. И искомая функция Ψ 0 ( x, t ) представляет собой декомпозицию\nпреобразования сдвига и найденного преобразования. Для моделирования\nподобных явлении использовались группы преобразований, допускаемые\nизвестными системы исходя из физических явлений.\nПриведем примеры реконструкции уравнений по модельным\nпримерам и данным экспериментов реальных технических систем. Для\nтестирования использовались данные, полученные при компьютерном\nмоделировании известных систем в среде Simulink [272], так и данные,\nполученные в результате экспериментального исследования реальных\nсистем.\nПример 1. Рассмотрим временной ряд, порожденный линейной\nсистемой (модель приведена на рис. 4.4). Источником установившихся\nколебаний служит единичный сигнал. Сложность классического анализа\nэтой системы заключается в наличии двух чисто мнимых полюсов.\n\nРис. 4.4. Simulink-модель системы примера 1\n96\n\n\fСравнение исходного ряда с динамическим поведением\nидентифицированной модели приведено на рис. 4.5. По сути, полученная\nмодель представляет собой генератор колебаний с заданными\nпараметрами. Размерность полученной модели равна 4. В качестве\nфункции Ψ 0 ( x, t ) использовалась линейная функция.\n\nРис. 4.5. Сравнение исходного ряда с динамическим поведением\nидентифицированной модели\nПример 2. Пусть временной ряд порожден системой:\nx\u0005 = y\n\ny\u0005 = − x + x 2 − 0,05sin 2t.\nОтображение Пуанкаре со структурой резонансов этой системы\nприведено на рис. 4.6.\n\nРис. 4.6. Отображение Пуанкаре\n97\n\n\fДля идентификации (как «черный ящик») рассмотрим временной\nряд, порожденный Simulink-моделью (рис. 4.7) при начальных условиях\nx(0) = 0; y (0) = 0,042 . Динамика системы приведена на рис. 4.8.\n\nРис. 4.7. Simulink -модель системы\n\nРис. 4.8. Временной ряд, порожденный системой\nВ результате параметрической идентификации системы (4.1) с\nпомощью Identification Toolbox, в условиях предположении о симметрии\nсдвига получены следующие матрицы уравнения системы:\n⎡ 0,7707 −0,3206 0,542 −0,004127 −0,03038 ⎤\n⎢ 0,5014 0,8746\n−0,18\n0,05212 0,02569 ⎥\n⎢\n⎥\nA = ⎢ −0,3528 0, 2858 0,6789\n−0,5847\n0,1485 ⎥ .\n⎢\n⎥\n−\n0,2553\n0,\n2566\n0,\n4667\n0,7665\n0,00253\n⎢\n⎥\n⎢⎣ −0,1971 − 0,016 −0,00074 −0,0691 − 0,0922 ⎥⎦\nT\nΨ0 = ( 0,6757; −0,4970; 0,4360; 0,8414) ( 4t 2 + 3t ) ;\n\nC = ( 0, 4123; −0,1394; 0,1550; −0,0662 ) .\n\n98\n\n\fНа рис. 4.9 приведено сравнение динамики модели с исходным\nвременным рядом. Адекватность модели 84%, при этом качественное\nповедение обоих систем совпадает.\n\nРис. 4.9. Сравнение динамики модели и системы (сплошная линия —\nдинамика исходной системы, пунктирная — динамика модели)\nПример 3. Имеется два дискретных процесса, измеренные на выходе\nнелинейной системы (рис. 4.10). Процессы получены из системы:\nx1 ( t + 1) = 1.25 x1 ( t ) ⋅ (1 − x2 ) ;\n\nx2 ( t + 1) = 1.3x2 ( t ) ⋅ (1 − x1 ) + 0.1( x1 ( t ) − x2 ( t ) ) .\n\nВ результате вычислений — размерность пространства состояний 4,\nзадержка — 14. Для проверки существования аттрактора —\nреконструируем фазовый портрет системы (рис. 4.11). Видно, что система\nхаотическая. На рис. 4.12 представлено сравнение динамического\nповедения модели с исследуемых экспериментальных данных для момента\nвремени от 200 до 250. Высокая оценка адекватности (91% — для первого,\n80% — для второго процесса) объясняется тем, что пример был взят\nмодельный, и в нем отсутствуют шумы и ошибка измерений. Матрицы\nидентифицируемой системы:\n\n−0,1167 0,0013756 ⎞\n⎛ −0,9755 −0,18879\n⎜ −0,11943 0,0011553 0,96261\n−0,001488 ⎟⎟\n⎜\nA=\n;\n⎜ 0,18096 −0,95403 −0,005751 −0,2764 ⎟\n⎜\n⎟\n0,24917\n0,20189 ⎠\n⎝ 0,017716 −0,11469\n⎛ 0, 088151 ⎞\n⎜\n⎟\n0, 24325 ⎟ 2\n⎛14, 471 −1, 7559 0,87886 −0,17531 ⎞\nt ; C =⎜\nΨ (t ) = ⎜\n⎟.\n⎜ −0, 21885 ⎟\n⎝ −11,888 −5, 2264 1,8535 −0, 74647 ⎠\n⎜⎜\n⎟⎟\n⎝ −1, 7214 ⎠\n\n99\n\n\fа)\nб)\nРис. 4.10. Выходные процессы: а) y1(t); б) y2(t)\n\nРис. 4.11. Реконструированный аттрактор\n\nРис. 4.12. Оценка адекватности моделирования\n100\n\n\fПример 4. Одним из классических примеров хаотических систем\nявляется система Реслера:\nx\u00051 = −( x2 + x3 ),\nx\u00052 = x1 + 0.2 x2 ,\n\nx\u00053 = 0.2 + x3 ( x1 − 5.7),\nSimulink-модель которой приведена на рис. 4.13. Результат моделирования\nпоказан на рис. 4.14. На рис. 4.15а и 4.15б соответственно приведены\nаттракторы исходной и идентифицированной моделей.\nРезультаты восстановления аттрактора показывают, что для\nразработанного метода число наблюдаемых точек может быть сокращено,\nнапример для системы Реслера с 1000 до 200.\n\nРис. 4.13. Модель системы Реслера\n\nРис. 4.14. Сравнение динамики исследуемого ряда и построенной модели\n101\n\n\fа)\n\nб)\n\nРис. 4.15. Аттрактор системы Реслера: а) построенный\nпо реконструированной модели; б) построенный по исходной модели\nПример 5. Исходные данные получены с датчиков системы\nохлаждения алюминиевых сплавов: y1 — скорость охлаждения сплава,\ny2 — расход хладоносителя. На рис. 4.16 показан аттрактор системы.\nРезультаты сравнения динамики смоделированного процесса с реальными\nданными приведены на рис. 4.17.\n\nРис. 4.16. Восстановленный аттрактор системы охлаждения\nРеконструированное уравнение имеет вид (4.5) с соответствующими\nкоэффициентами:\n⎡0,7987 0,5871 − 0,1104 − 0,0795 -0,0100\n⎢−0,5605 0,6582 − 0,2665 − 0,3695 0,0204\n⎢\n⎢−0,1076 0,2538 0,9248 − 0,2668 − 0,0639\n⎢\n⎢−0,1254 0,2899 − 0,0054 0,7027 − 0,0487\nA = ⎢−0,0337 0,0967 0,2727 − 0,0898 0,7439\n⎢\n⎢−0,0186 0,0371 − 0,2327 0,2056 0,2983\n⎢−0,0062 0,0477 0,2632 − 0,0479 − 0,0213\n⎢\n⎢ 0,0110 0,0150 0,1896 0,1428 − 0,3417\n⎢⎢ 0,0537 − 0,0289 − 0,5188 0,0982 − 0,8413\n⎣\n\n102\n\n0,0100 − 0,0062 0,0283 − 0,0383 ⎤\n− 0,3091 − 0,1216 0,0319 0,0308 ⎥\n⎥\n− 0,3874 − 0,2682 − 0,3668 0,6859 ⎥\n⎥\n− 0,7902 0,0226 − 0,1279 0,6137 ⎥ ;\n− 0,6723 − 0,3121 − 0,0916 0,6098 ⎥\n⎥\n0,4822 0,0862 − 0,3811 − 0,2154 ⎥\n− 0,7668 0,0473 − 0,2913 − 0,1507⎥\n⎥\n− 0,0715 − 0,3949 0,5512 − 0,0141⎥\n1,5644 0,7244 − 0,1601 − 0,6049⎥⎥⎦\n\n\f0,7319 ⎤\n⎡ − 3,9240\n⎢ 1,7215 − 1,3336 ⎥\n⎥\n⎢\n⎢ 2,0934 − 1,6825 ⎥\n⎥\n⎢\n⎢ 2,4342 − 0,8467 ⎥ ⎡t 2 − 2t − 0,93;⎤\nB = ⎢ − 0,2892 − 0,8989 ⎥ ⎢\n⎥;\n⎥ ⎣sin(t − 10); ⎦\n⎢\n1,4839 ⎥\n⎢ 2,7220\n⎢ 0,4833\n1,0770 ⎥\n⎢\n⎥\n⎢ 1,0878 − 0,0228 ⎥\n⎢\n0,5654 ⎥⎥⎦\n⎣⎢ 2,8889\n⎡0,1857 0,0442 − 0,0082 − 0,0066 − 0,0005 − 0,0012 − 0,0010 0,0019 − 0,0022⎤\nC=⎢\n⎥.\n0,6124\n0,5277\n−\n8,6861\n9,1780\n−\n1,1570\n−\n2,1478\n−\n2,6497\n3,0497\n−\n0,3083\n⎣\n⎦\n\nРис. 4.17. Сравнение экспериментальных данных и построенной модели\nПример 6. В результате применения разработанного метода к\nмоделирования процесса добычи нефти, являющийся исследуемых\nпоказателем энергетической безопасности с 1996 по 2002 г. (по мес.)\nполучено: размерность пространства состояний n = 5 , задержка — 12.\nСравнение реальных данных по добычи нефти (сплошная линия) с\nдинамическим поведением идентифицированной системы (штриховая\nлиния) показан на рис. 4.18. Оценка адекватности модели составляет\n72,5%, т. е. ошибка прогнозирования — 27.4%. Для проверки оценки\nпостроена автокорреляционная функция (рис. 4.19).\n\n103\n\n\fРис. 4.18. Сравнение результатов моделирования с реальными данными\n\nРис. 4.19. Автокорреляционная функция\nПриведенные примеры идентификации определяют достоверность\nпредложенного метода и эффективность его применения для\nмоделирования различных систем.\n\n\fГЛАВА 5. ТЕХНОЛОГИЯ ПОВЫШЕНИЯ КАЧЕСТВА\nФУНКЦИОНИРОВАНИЯ УПРАВЛЯЕМЫХ ТЕХНИЧЕСКИХ\nСИСТЕМ\nНа основе использования созданных инвариантных геометрических\nметодов и моделей разработана технология, обеспечивающая повышение\nкачества функционирования управляемых технических систем с\nнелинейной динамикой; приведены результаты, полученные при\nвнедрении разработанных методов при моделировании и управлении\nпромышленных систем.\n5.1. Технология обеспечения повышения качества\nфункционирования управляемых систем\nНаибольшую эффективность применения разработанных в\nдиссертации методов наблюдается при моделировании и управлении\nсистемами, в которых происходят нерегулярные и хаотические явления,\nсвязанные, например, с процессами теплообмена, потока вязких жидкостей\nи химическими реакциями [260–280]. Конструкции промышленных\nустройств, в которых протекают указанные нелинейные явления, делают,\nкак правило, математическое моделирование задачей трудно выполнимой,\nреконструкция эволюционных уравнений позволяет строить модели\nкачественного поведения на минимальном инерциальном множестве.\nПроведенное компьютерное моделирование и анализ результатов\nвнедрения показал, что построенные модели позволяют реализовать\nтребования к качеству функционирования системы, связанные с\nуправлением нелинейных явлений. Учет нерегулярного поведения\nтехнических систем позволил уменьшить энергетические затраты на\nуправление, сократить время переходных процессов и обеспечить\nзаданные параметры надежности систем.\nРазработана оригинальная технология повышения качества\nфункционирования управляемых технических систем с нелинейной\nдинамикой. Технология включает шесть этапов.\n1. Проведение экспериментов и обработка исходных данных.\nВключает использование методов, разработанных и используемых в\nтеории нейронных сетей: методы предварительной обработки данных,\nметоды оценки статистических характеристик и др. [283], а также\nописанного в четвертой главе модифицированного метода реконструкции\nаттрактора и методов оценки инвариантных характеристик [251].\n2. Идентификация параметров модели системы. Используется\nпредложенный в четвертой главе метод моделирования по\nэкспериментальным данным систем, редуцированных на центральное\nмногообразие [278].\n105\n\n\f3. Формулировка требований к качеству управления. Представляет\nсобой формализацию критериев качества и надежности, а также\nприменение сформулированных в пятой главе геометрических критериев\nлокальной управляемости, наблюдаемости и критерия компактности\nмножества достижимости [281].\n4. Проектирование\nсистемы\nуправления\nс\nприменением\nгеометрических методов. Применяются разработанные в пятой главе\nметодики построения алгоритмов управления с учетом характеристик\nпостроенных моделей (аттрактора, спектра показателей Ляпунова,\nинфинитезимальных операторов и др.) и заданных критериев качества\n[283].\n5. Имитационное моделирование схемы управления и адаптация.\nЗаключается в настройке параметров управляющих механизмов в\nразличных режимах функционирования [276].\n6. Внедрение разработанной САУ. Обеспечивает анализ результатов\nвнедрения и возможность модификации параметров модели при\nзначительном изменении условий эксплуатации промышленной системы.\nПрименение в практических задачах обеспечения повышения\nкачества систем в различных областях промышленности рекомендуется в\nследующих случаях.\n1. В условиях функционирования существующих систем управления\nпромышленными объектами, когда необходимо уменьшить колебания\nвыходных параметров.\n2. В условиях существующих режимов функционирования, когда\nнеобходимо улучшить качество управления: увеличить быстродействие\nсистемы, удовлетворить заданным ограничениям на управляющие сигналы\nи контролируемые параметры.\n3. При наличии существующих моделей процессы теплообмена и\nпотока жидкостей (т. е. те, которые исходя из своей физической природы\nописываются уравнениями в частных производных, допускающих группы\nсимметрий), не удовлетворяющим заданным требованиям к качеству и\nнадежности объекта.\n4. Когда физические явления являются диссипативными (что может\nбыть проверено просто путем численного дифференцирования\nэкспериментальных данных).\n5. При конструктивной возможности изменения числа управляющих\nпараметров нелинейным явлением в условиях изменения режима\nфункционирования объекта.\n6. Для прогнозирования значений нелинейных диссипативных\nсистем или систем, движение которыми ограничивается центральным\nмногообразием.\nАнализ применения разработанных методов для различных\nвременных рядов показал, что в следующих случаях повышение качества\nуправления не обеспечивается.\n106\n\n\f1. При наличии ограничений на изменение контролируемых\nпараметров в режимах функционирования, в которых не проводились\nэксперименты.\n2. При наличии значительного постоянного неизменяемого\n(случайного) управляющего сигнала.\n3. В случаях, если нет возможности (если это требуется\nразработанным критерием управляемости) добавления управляющего\nмеханизма.\nОднако в ряде ситуаций модели могут быть использованы в\nсовокупности с другими методами и технологиями. Например, модели\nмогут быть автоматически настраиваемыми с использованием механизмов\nпереобучения.\nТаким образом, в условиях указанных ограничений внедрение\nразработанной\nтехнологии\nобеспечивает\nповышение\nкачества\nфункционирования управляемых технических систем и промышленных\nобъектов.\n5.2. Методика применения прогнозирующих моделей\nРазработанные\nмодели,\nредуцированные\nна\nцентральное\nмногообразие могут быть использованы в системах принятия решений и\nсовместно с методами управления существующих промышленных\nконтроллеров.\nМетоды и программные средства моделирования внедрены в НИИ\n«Энергия» Главного управления информационных систем Спецсвязи РФ в\nрамках информационной системы поддержки принятия решений.\nИсследования дали возможность построить аттракторы для моделей\nизменения параметров, анализ которых позволил увеличить качество\nпрогнозирования необходимых значений.\nПараметры модели имеет вид:\n⎡ 0,57622 0,13284\n⎢ −0,17116 0,31824\n⎢\n⎢ 0,069311 −0,24891\n⎢\nA = ⎢ 0,19068 −0,29474\n⎢ 0,17889 0,020402\n⎢\n⎢ −0,3242 0,79269\n⎢ 0,049435 0,36525\n⎣\n\n−0,20088\n0,27878\n−0,15565\n−0,62437\n0,43483\n−0,57987\n0,070016\n\n0,37433 −0,023112 0,37285\n−0,43661 −0,27536 −0,66245\n−0,35583 −0,9155\n0,10894\n−0,58184 0,30318 −0,29116\n−0,66442\n0,1957\n0,50556\n0,099279 0,066329 0,45846\n−0,04069 0,12744 0,078338\n\nΨ = 10−3 ( 9.4t 7 + t 4 + 0.059sin(t ) ) ×\n\n−0,31885 ⎤\n−0,70696 ⎥⎥\n−0,24585 ⎥\n⎥\n−0,35317 ⎥ ,\n0,11334 ⎥\n⎥\n0,68769 ⎥\n−0,63774 ⎥⎦\n\n× ( 0,0385; − 0,1320; − 0,4010; − 0,5072; − 0,2073;0,0841; − 0,0460 ) ,\nT\n\nC = 103 ⋅ [3,5234 −0.0210 2,1493 −2,3071 −0,0303 −3,1276 1,9016] .\n\nНа рис. 5.1 приведено сравнение динамики построенных моделей с\nреально принятыми значениями.\n107\n\n\fДля систем принятий решений самостоятельное значение имеют вид\nаттракторов, предоставляющие важную информацию о поведении\nсистемы. На рис. 5.2а приведена динамика одного из показателей,\nхарактеризующего энергетическую безопасность, на рис. 5.2б — пример\nпрогноза в области стационарности нелинейной модели, на рис. 5.2в —\nвосстановленный аттрактор.\n\nРис. 5.1. Сравнение динамика реальных значений показателей с моделями\n108\n\n\fПостроенная модель, также как и классические модели\n(регрессионные, разложение в ряды Фурье и Тейлора) для подобных\nвременных рядов не позволяет строить прогноз более чем на 2–3 шага, вид\nфазового портрета говорит о фазовом переходе состояний системы, в\nкотором система может находиться какое-то время (как это бы при t < 50),\nлибо продолжит переход на другую окрестность. Окончательное решение\nостается за лицом, принимающее решение, в распоряжении которого\nимеется экспертный анализ остальных показателей.\n\nа)\n\nб)\n\nв)\n\nРис. 5.2. Результаты моделирования\n109\n\n\fНа рис. 5.3 представлена функциональная схема системы принятия\nрешений. Цель исследования конкретных объектов определяет выбор\nмодели и набора параметров. Генератор модели на основе информации,\nхранящейся во БД, формирует задачи оптимизации и идентификации,\nзаписывая их в специальном формате. По существу генератор создает\nцелое семейство оптимизационных задач. Выходной файл генератора\nмоделей передается на вход блока, реализующего алгоритмы анализа и\nпринятия решений, который после решения выдает информацию также в\nспециальном виде. В базе знаний накапливается информация об\nисследованных задачах (правила агрегирования, поведение объектов в\nдинамике, значения оптимальных решений и параметров и т. д.) для\nпоследующего использования и построения новых моделей на основе\nполученных знаний.\nDocument\nDocument\nОтчеты\nи рекомендации\n\nВнешние данные\n\nОбработка\n\nАналитическая\nгруппа\n\nБаза знаний\n\nИнтерпритация и\nпредставление\nрезультатов\n\nБД\n\nВыбор модели\nи параметров\n\nИдентификация\nпараметров\n\nАлгоритмы анализа\nи принятия решений\n\nИнтерфейс\n\nРис. 5.3. Функциональная схема информационной системы поддержки\nпринятия решений\nРезультаты внедрения позволили улучшить качество принимаемых\nрешений в области энергетической безопасности.\nАналогично может быть использованы методы моделирования в\nусловиях, когда на выбор метода управления накладываются ограничения,\nвыдвигаемые типом используемых промышленных контроллеров [258].\nВ\nкачестве\nпромышленного\nконтроллера\nиспользовались\nконтроллеры Modicon, в которых на аппаратном уровне существует\nкоманда ПИД-регулирования. В команде PID2 реализуется схема\nуправления, цель которого – поддержка нулевого значения отклонения E,\nвычисляемого как разность измеренного параметра ξ и заданного значения\ns (рис. 5.4). При этом реализуется алгоритм управления с обратной связью,\n110\n\n\fаналогичный традиционным пневматическим и аналоговым электронным\nконтроллерам, с использованием фильтра высокочастотных параметров и\nисточников шума.\n\nРис. 5.4. Схема ПИД-регулирования: ξ– измеряемое значение; s – заданное\nзначение параметра; Mv – управляющее воздействие; Е – отклонение ξ от s\nПри пропорциональном регулировании управление получается как\nпроизведение отклонения E на константу Kp со смещением:\nMv = KpE + смещение\nПропорционально-интегральное управление устраняет смещение, интегрируя E по времени:\nt\n⎛\n⎞\nM v = K p ⎜ E + K i ∫ Edt ⎟ ,\n0\n⎝\n⎠\nгде Ki — константа интегральной составляющей регулятора.\nАлгоритм ПИД-управления имеет вид:\nt\n⎛\ndξ ⎞\nM v = K p ⎜ E + K i ∫ Edt + K d\n⎟,\ndt\n0\n⎝\n⎠\nгде Kd – постоянная дифференциальной составляющей регулятора.\nДля решения задачи моделирования и построения системы\nавтоматического регулирования сформирована методика, состоящая из\nчетырех шагов.\n1. Идентификация системы. По экспериментальным данным в\nпереходном режиме и в режиме нормального функционирования\nпроисходит построение системы.\n2. Проектирование системы управления. Выбирается схема\nуправления промышленной системой и, используя описанные выше\nсредства MATLAB, рассчитываются параметры ПИД–регулятора. К\nклассически применяемым методам можно отнести метод корневого\nгодографа и метод синтеза систем регулирования по желаемым ЛАХ\n(логарифмическим амплитудным характеристикам), которые реализованы\n\n111\n\n\fв Control System Toolbox в виде отдельного приложения SISO-tool с\nпользовательским интерфейсом.\n3. Имитационное моделирование схемы управления. Осуществляется\nдля тестирования и оптимизации параметров. Для наглядности\nрекомендуется использовать Simulink. Типовая схема приведена на\nрис. 5.5. В подсистеме «Объект» находится собственно идентифицированная модель промышленного объекта.\n4. Настройка параметров регулирования. Для выполнения расчета\nпараметров регулирования при заданных ограничениях и критериях\nкачества этапа используется пакет Nonlinear Control Design, в котором\nзадаются ограничения на вид переходных процессов (рис. 5.6).\n\nРис. 5.5. Модель схемы управления в Simulink\n\nРис. 5.6. Настройка параметров регулирования по заданному виду\nпереходного процесса в Nonlinear Control Design\nДля обоих рассмотренных случаев разработана типовая схема\nвзаимодействия программных средств, которая представлена на рис. 5.7.\nВ программной реализации используются следующие программные\nсредства:\n– среда визуального моделирования Simulink, предназначена для\nсоздания базы знаний моделей и занесения их с M-file;\n112\n\n\f– System Identification Toolbox — подпрограмма идентификации\nдинамических моделей;\n– Control System Toolbox — исследование и моделирования\nдинамических систем.\nПрименение этой методики было осуществлено на предприятиях\nфилиала Мосэнерго, кампании «Кампина», ООО «Энергостроймонтаж».\n\nРис. 5.7. Взаимодействие программных средств\nНа рис. 5.8. приведен результат сравнения результатов\nмоделирования системы разогрева котла. На вход котла подается\nотфильтрованная вода, а на вход турбины уже идет так называемый\nострый пар. На входе и выходе снимается температура воды и острого пара\nсоответственно.\nВ\nсостав\nвходит\nдве\nодинаковые\nцепочки\nподача в котёл—разогрев—подача в турбину\nпара.\nВключение\nоборудования происходит следующим образом. Последовательно\nосуществляется подача воды в котел до определенного уровня, разогрев\nгорелок и, соответственно, воды в котле, затем система управления\nосуществляет выход на плановый расход воды. Цель моделирования —\nобеспечение безопасного режима функционирования.\n113\n\n\fРис. 5.8. Функционирование системы запуска котла на ТЭЦ\nВ результате внедрения система автоматического управления\nобеспечивает заданным требованиям.\n5.3. Управление системой теплообмена с вязкой средой\nВ качестве объекта исследования использована система\nтеплообмена, осуществляющая процесс нагрева потока вязкой жидкости в\nкомпании «Марс». Исходными данными являются только показания\nдатчиков в переходных и нормальных режимах функционирования.\nСистема состоит из бесконтактного теплообменника, на вход\nкоторого в качестве теплоносителя подается пар. Измеряемыми\nпараметрами являются: давление пара (теплоносителя) и выходная\nтемпература нагретого продукта. Воздействие на процесс осуществляется\nуправляемым клапаном. Необходимо регулировать значения давления и\nтемпературы выходного продукта при заданных ограничениях на скорость\nроста давления пара и максимально допустимое значение температуры\nпродукта. В системе для повышения качества управления был\nдополнительный клапан и связанный с ним контур регулирования, что,\nоднако не позволяло обеспечивать заданные показатели качества.\n114\n\n\fa)\n\nб)\nРис. 5.9 Схема регулирования теплообменником:\nа) исходная; б) после применения созданной технологии\nСложность задачи — нелинейность процесса теплообмена, и, как\nследствие, динамически-сложное поведение системы. Это проявляется в\nтом, что при постоянном значении управления возникают колебания\nвыходных параметров. На рис. 5.10 показано изменение состояния\nуправляемого клапана (рис. 5.10а) давления (рис. 5.10б), температуры\nпродукта (рис. 5.10в) во времени. Показания аналоговых датчиков\nпромасштабированы для регистров промышленного контроллера от 0 до\n4000. Значения состояния клапана изменяется в пределах 1–100% (на\nграфике: реальное значение × 4000/100) давление пара изменяется в\nпределах от 1÷10 бар (на графике: реальное значение × 4000/10);\nтемпературы: 0÷150 0С (на графике: реальное значение × 4000/150).\nРазработана схема управления, представленная на рис. 5.9б. Для\nрассматриваемой системы реконструирован фазовый портрет (рис. 5.12);\nразмерность пространства состояний n = 6 .\n115\n\n\fа)\n\nб)\n\nв)\nРис. 5.10. Динамика процесса при «ручном» регулировании процесса\nПрименение разработанного в пятой главе метода дает уравнения с\nсоответствующими значениями:\n\n⎛ 0,998\n⎜ 0,008\n⎜\n⎜ 0,006\nA=⎜\n−5\n⎜ −8,12 ⋅ 10\n⎜ −0,001\n⎜⎜\n⎝ 0,002\n\n−0,001 −0,007 ⎞\n0,913 0,238 0,142\n0,091 −0,215 ⎟⎟\n0,132 0,091 −0,928 −0,459 −0,038 ⎟\n⎟;\n0,008 −0,177 0,356 −0,956 0,097 ⎟\n−0,06 0,803 0,203 −0,145 0,139 ⎟\n⎟\n−0,006 0,006 −0,003 0,010\n0,661 ⎟⎠\n0,006\n\n−0,009\n\n0,007\n\nΨ = exp(10t ) ( −7,24 ⋅ 10−4 ; −0,087;0,209; −0,074; −0, 440; −0,061) ;\nT\n\n⎛129,8 −144,2 18,43 −23, 21 −3,233 −4,911⎞\nC =⎜\n⎟.\n⎝ 582,1 4,995 2, 230 −1,596 2,587 4,100 ⎠\n116\n\n\fВ таб. 5.1\nхарактеристик\n\nпоказаны\n\nвычисленные\n\nзначения\n\nинвариантных\n\nТаблица 5.1. Значения инвариантных характеристик\nРазмерность Время\nСтарший\nСпектр показателей\nминимального\nпоказатель\nЛяпунова\nτ\nвложения\nЛяпунова\n6\n\n12\n\n0,03230\n\n0,02814\n–0,0823\n\n0,00217 –0,0537\n–0,0072 –0,0034\n\nа)\n\nб)\nРис. 5.11. Динамическое поведение системы теплообмена\nв установившемся режиме\n117\n\n\fСравнение моделируемого выходного процесса с реальными\nданными показано на рис. 5.13. Видно, что модель демонстрирует сложное\nдинамическое поведение, адекватное рассматриваемому теплообменному\nпроцессу. На рис. 5.14 и 5.15 показаны графики переходных процессов во\nвремени для решения задачи оптимального быстродействия.\n\nРис. 5.12. Реконструированный фазовый портрет\n\nРис. 5.13. Сравнение результатов моделирования с реальными данными\n118\n\n\fРис. 5.14. Динамика изменения давления при оптимальном\nбыстродействии\n\nРис. 5.15. Динамика изменения температуры при оптимальном\nбыстродействии\nПолученная модель позволила решить задачу проектирования\nсистемы регулирования теплообменника по заданным критериям качества.\nК системе для безопасного функционирования предъявляются следующие\nтребования: ограничения на максимально допустимое значение\nтемпературы, минимум скорости роста давления, минимально допустимое\nзначение быстродействия.\nПрименяя разработанные алгоритмы и модели, получены результаты\nфункционирования системы с учетом заданных критериев качества,\nкоторые показаны на рис. 5.16 и 5.17.\n119\n\n\fВнедрение подтвердило полученный результат. В результате выход\nсистемы на заданный режим функционирования уменьшился с 4000 с до\n2500 с (1,6 раза).\n\n.\nРис. 5.16. Динамика изменения давления с учетом критериев качества\n\nРис. 5.17. Динамика изменения температуры с учетом критериев\nкачества\nРазработан регулятор для технологического процесса нагрева вязкой\nжидкости, при этом контролируемым параметром является расход вязкой\nсреды. Работа выполнялась также для компании «Марс» по заказу\nподрядчика-изготовителя системы ООО «Центр передовых технологий\n«Базис».\nВходной параметр u(t) — состояние клапана (%); выходной — y(t) —\nскорость потока вязкой жидкости (кг/ч). Результат моделирования на\n120\n\n\fцентральном многообразии\nуправляемой модели 88%.\n\nприведен\n\nна\n\nрис.\n\n5.18.\n\nАдекватность\n\nОбъект\n\nРис. 5.18. Технологический процесс\n\nРис. 5.19. Сравнение динамики идентифицированной модели с реальным\nпроцессом\n121\n\n\fНаличие построенной модели позволило сократить затраты,\nсвязанные с браком выходного продукта при разогреве, изменение\nпараметров функционирования системы и обеспечить уменьшение\nбыстродействия при запуске в 3 раза.\n5.4. Управление процессом охлаждения алюминиевых слитков\n\nПроведено моделирование процесса охлаждения алюминиевых\nслитков, изготовляемых для оборонной промышленности ОАО\n«Ступинская\nметаллургическая\nкомпания».\nХимический\nсостав\nалюминиевых сплавов и температура, при которой он готовится, зависит\nот марки. Металл плавится в специальной печи. После готовности расплав\nперекачивается в другую печь (миксер), где отбираются пробы и при\nнеобходимости добавляются различные присадки. Отливка круглых\nсплошных слитков из алюминиевых деформируемых сплавов в\nкристаллизаторы с тепловыми насадками ПН–10 допускается только после\nполной готовности расплава, оснастки, инструмента и рабочего места, а\nтакже соответствия режимов литья при данной технологии (согласно\nТИ 410–025–02) (рис. 5.19).\nP, кгс/см2 – давление\nводы;\nR, м3/час –\nрасход воды\n\nМиксер\nФильтр\n\nПоворотная рама с кристаллизаторами\nПлатформа с поддонами\nt, oC – температура расплава\n\nυ, мм/мин –\nскорость\nдвижения\nплатформы\n\nРис. 5.19. Технология охлаждения\nРассмотрен процесс отливки круглых сплошных слитков диаметром\n190 мм из сплава АД31 (6063). Температура расплава контролируется по\nпоказаниям термоэлектрического термометра, расположенного в миксере\nи, для литья слитков данного диаметра, составляет от 690 до 710oC\nвключительно. Расплавленный металл через фильтр по литейному желобу\n122\n\n\fпоступает на поворотную раму с кристаллизаторами. В литейном желобе\nдобавляется лигатурный пруток и отбирается жидкая проба для контроля\nгазонасыщенности методом первого пузырька по ГОСТ 21132.0. На\nповоротной раме расположено 24 кристаллизатора цилиндрической\nформы, в каждый кристаллизатор подается вода для охлаждения металла и\nгазо-масляная смесь для получения более качественной поверхности\nслитка. Металл по желобам поступает в кристаллизаторы и\nкристаллизуется за счет охлаждения. В этот момент платформа,\nприводимая в движение электродвигателем, начинает опускаться со\nскоростью (скорость литья) от 90 до 100 мм/мин, пока не будут отлиты\nслитки необходимой длины. Скорость литья задается вручную\nлитейщиком. Для слитков диаметром 190 мм из сплава АД31 (6063) расход\nмасла составляет от 0,12 до 0,15 л/мин включительно, давление\nохлаждающей воды от 0,7 до 0,9 кгс/см2 включительно.\nТакие параметры, как температура расплава, давление и расход воды,\nскорость движения платформы и длина слитка визируются электроникой\nлитейной машины и записываются с периодичностью 3 с.\nДля слитков диаметром 190 мм из сплава АД31 (6063)\nосуществляется охлаждение, измеряемыми параметрами системы являются\nдавление воды (изм.— кгс/см2; рис. 5.20а), расход охлаждающей воды\n(изм. — л/мин; рис. 5.20б). Моделируется автономный процесс при\nпостоянном значении входного сигнала (постоянной скорости охлаждения\nсплава) и при определенных технологических условиях. До процесса\nмоделирования настройка этих параметров происходила вручную.\nПостроенная модель используется для построения дополнительного\nрегулятора в системе.\nВ результате применения разработанного алгоритма — размерность\nпространства состояний n = 6 , τ = 12, старший показатель 0,3454950.\nРеконструированный фазовый портрет системы приведен на рис. 5.21.\nРезультат идентификации показан на рис. 5.22.\n\nа)\n123\n\n\fб)\nРис. 5.20. Динамика параметров системы охлаждения\n\nРис. 5.21. Реконструированный аттрактор\nПолученный результат позволил построить автоматический\nробастный регулятор в виде наблюдателя, управляющий расходом\nохлаждающей жидкости, графики управления (рис. 5.23).\n124\n\n\fРис. 5.22. Сравнение результатов моделирования с реальными данными\n\nРис. 5.23. Динамика состояний и управлений: x1, x2, u1, u2\nВнедрение созданной методики позволило исключить ручную\nподрегулировку параметров. На основе модели построен регулятор, в\nрезультате улучшилось качество изделия, что связано с уменьшением\nколебаний при охлаждении.\n125\n\n\fЗаключение\nНа основании приведенных в работе результатов внедрения можно\nсделать вывод о решении практически важной задачи разработки теоретикометодологических основ повышения качества функционирования технических\nсистем с нелинейной динамикой. Внедрение результатов позволило значительно\nувеличить надежность и технико-экономические показатели систем, имеющих\nважное значение для различных отраслей промышленности.\nРабота над развитием разработанного геометрического метода не\nзакончена. В монографии приведены два приложения, являющихся\nпродолжением предложенного подхода.\nАвтор благодарит д. т. н., проф. М. В. Ульянова, д. ф.-м. н., проф. В. А.\nАнтонца за поддержку, внимание и проявленный интерес к результатам.\n\n126\n\n\fСписок использованных источников\n1.\n2.\n3.\n4.\n5.\n6.\n7.\n\n8.\n9.\n10.\n11.\n12.\n13.\n14.\n15.\n16.\n\nАграчеев А. А., Сачков Ю. Л. Геометрическая теория управления.— М.:\nФизматлит, 2005.\nАндриевский Б. Р., Фрадков Ф. Л. Управление хаосом: методы и\nприложения. I Методы // Автоматика и телемеханика.— 2003.— №1.—\n2003.— С. 3–45.\nАндронов А. А. Предельные циклы Пуанкаре и теория автоколебаний //\nСобрание трудов А. А. Андронова.— М.: Изд-во АН СССР, 1956.\nАндронов А. А., Леонтович Е. А., Гордон И. И., Майер А. Г. Качественная\nтеория динамических систем второго порядка.— М.: Наука, 1966.\nАндронов А. А., Леонтович Е. А., Гордон И. И., Майер А. Г. Теория\nбифуркаций динамических систем на плоскости.— М: Наука, 1967.\nАнищенко В. С. Знакомство с нелинейной динамикой: лекции соросовского\nпрофессора: учеб. пособие.— Саратов: Изд-во ГосУНЦ «Колледж», 2000.\nАнищенко В. С., Астахов В. В., Вадивасова Т. Е. и др. Нелинейные\nэффекты в хаотических и стохастических системах / Под ред.\nВ. С. Анищенко.— М.-Ижевск: Институт компьютерных исследований,\n2003.\nАнищенко В. С., Вадивасова Т. Е., Постнов Д. Э., Сафонова М. А. Внешняя\nи взаимная синхронизация хаоса // Радиотехника и электроника.— 1991.—\nТ. 36.— С. 338.\nАнищенко В.С., Павлов А.Н., Янсон Н.Б. Реконструкция динамических\nсистем в приложении к защите информации // ЖТФ.— 1998.— Т. 68.—\n№ 12.\nАрнольд В. И., Козлов В. В., Нейштадт А. И. Современные проблемы\nматематики. Фундаментальные направления. Т.3.— М.: ВНИТИ, 1985.\nАрнольд В. И. Доказательство теоремы Колмогорова о сохранении условнопериодических движений при малом изменении функции Гамильтона //\nУМН.— 1963.— Т. 18.— Вып. 5 (113).— С. 130.\nАрнольд В. И. Малые знаменатели и проблема устойчивости движения в\nклассической и небесной механике // УМН.— 1963.— Т. 18.— Вып.\n6(114).— С. 81–192.\nАрнольд В. И. Математические методы классической механики.— М:\nНаука, 1989.\nАссарин Е. А., Козякин В. С., Красносельский М. А., Кузнецов Н. А.\nАнализ устойчивости рассинхронизированных дискретных систем.— М.:\nНаука, 1992.\nАстахов В. В., Сильченко А. Н., Стрелкова Г. И., Шабунин А. В.,\nАнищенко В. С. Управление и синхронизация хаоса в системе связанных\nгенераторов // Радиотехника и электроника.— 1996.— Т. 41.— С. 1323.\nАфраймович B. C. Внутренние бифуркации и кризисы аттракторов // В сб.\nНелинейные\nволны\n/\nПод\nред.\nА. В. Гапонова-Грехова\nи\nМ. И. Рабиновича.— М.: Наука, 1987.— С. 189–213.\n127\n\n\f17. Афраймович B. C., Быков В. В., Шильников Л. П. О возникновении и\nструктуре аттрактора Лоренца // ДАН СССР.— 1977.— Т. 234.— № 2.—\nС. 336–339.\n18. Афраймович B. C., Гаврилов Н. К., Лукьянов В. И., Шильников Л. П.\nОсновные бифуркации динамических систем.— Горький: Изд-во ГГУ\n(ННГУ), 1985.\n19. Афраймович В.С., Веричев Н.Н., Рабинович М.И. Стохастическая\nсинхронизация колебаний в диссипативных системах // Изв. Вузов.\nРадиофизика.— 1986.— Т. 29.— С. 795.\n20. Ахромеева Т. С., Курдюмов С. П., Малинецкий Г. Г., Самарский А. А.\nНестационарные структуры и диффузионный хаос.— М.: Наука, 1992.\n21. Бабичев А. В., Бутковский А. Г., Похьолайнен С. К единой геометрической\nтеории управления.— М.: Наука, 2001.\n22. Батхин А. Б., Батхина А. Б., Сумароков С. И. Бифуркации удвоения периода\nв задаче Хилла // Вестник ВолГУ. Серия 1. Математика. Физика.— 2000.—\nВып. 5.— С. 6–11.\n23. Баутин Н. Н., Леонтович Е. А. Методы и приемы качественного\nисследования динамических систем на плоскости.— М.: Наука, 1976.\n24. Белюстина Л. Н., Белых В. Н. Качественное исследование динамических\nсистем на цилиндре // Дифф. уравнения.— 1973.— Т. 9.— № 3.— С. 403–\n415.\n25. Березовский С. В., Клепиков В. Ф., Середа Ю. В., Лысенко М. А.\nСимметрии в системах с несоразмерными фазами // Вестник Харьковского\nнационального университета. Серия: Физическая. Ядра, частицы, поля.—\n1999.— Т. 443.— Вып. 2 (6).\n26. Берже П., И. Поио, Видаль К. Порядок в хаосе. О детерминистском подходе\nк турбулентности: пер. с франц.— Череповец: Меркурий-ПРЕСС, 1998.\n27. Беркс У. Пространство — время, геометрия, космология.— М.: Мир. 1985.\n28. Биркгоф Дж. Д. Динамические системы.— М.–Ижевск: НИЦ «Регулярная и\nхаотическая динамика», 2002. (переизд. 1941).\n29. Бланк М. Л. Устойчивость и локализация в хаотической динамике.— М.:\nМЦНМО, 2001.\n30. Бобылев А. В., Ибрагимов Н. Х. Взаимосвязь свойств симметрии уравнений\nдинамики, кинетической теории газов и гидродинамики // Математическое\nмоделирование.— 1989.— Т. 1.— № 3.— С. 100–109.\n31. Борисов А. В., Мамаев И. С. Пуассоновы структуры и алгебры Ли в\nгамильтоновой механике.— Ижевск: Регулярная и хаотическая динамика,\nИзд. дом «Удмуртский университет», 1999.\n32. Бурбаки Н. Общая топология.— М: Наука, 1975 (1969).\n33. Бутковский А. Г. Фазовые портреты управляемых динамических систем.—\nМ.: Наука. 1985.\n34. Вишняков С., Геворкян\nВ., Казанцев Ю. Автоматизированное\nпроектирование высокодобротной колебательной системы транзисторного\nгенератора. Численные методы расчета электромагнитного поля //\nЭлектроника: Наука, Технология, Бизнес.— 2004.— № 2.— С. 52–56\n128\n\n\f35. Волович М. Е. Средства исследования диссипативных хаотических систем\nпо временным рядам // Дисс. .. канд. техн. наук: 05.13.01.— М.: МГАПИ,\n2003.\n36. Волосов В. М., Моргунов Б. И. Методы осреднения в теории нелинейных\nколебательных систем.— М.: Изд-во МГУ, 1971.\n37. Гайшун И. В. Вполне разрешимые многомерные дифференциальные\nуравнения.— Минск: Наука и техника, 1983.\n38. Гарев\nК. Г.\nПриложения\nнепрерывных\nгрупп\nсимметрий\nк\nдифференциальным уравнениям // Соросовский образовательный журнал.—\n1998.— № 12.— С. 113–118.\n39. Гелъфанд И. М., Фомин С. В. Вариационное исчисление.— М.: ГИФМЛ,\n1961.\n40. Гукенхеймер Дж. Странный, странный аттрактор // Кн. : Марсден Дж., МакКракен М. Бифуркация рождения цикла и ее приложения. Гл. 12.— М.:\nМир, 1980.— C. 284–293.\n41. Данилов Н. Ю., Павловский Ю. Н., Соколов В. И., Яковенко Г. Н. Геометрические и алгебраические методы в теории управления.— М.: Изд. МФТИ,\n1999.\n42. Данилов Ю. А. Лекции по нелинейной динамике.— М.: Постмаркет, 2001.\n43. Делюкова Я. В. Редукция определяющих систем при наличии симметрии //\nДифференциальные уравнения и процессы управления.— 2003.— № 2.—\nС. 1–7.\n44. Дмитриев М. Г., Коняев Ю. А. Асимптотика типа Биркгофа некоторых\nсингулярно\nвозмущенных\nзадач\nоптимального\nуправления\n//\nМатематическое моделирование.— 2000.— Т. 14.— № 3.— С. 27–29.\n45. Дородницын В. А. Групповые свойства разностных уравнений.— М.:\nФизматлит, 2001.\n46. Дородницын В. А. Конечно-разностный аналог теоремы Нётер // Доклады\nРАН.— 1993.— Т. 328.— № 6.— С. 678–682.\n47. Дородницын В. А., Еленин Г .Г. Симметрия в решениях уравнений\nматематической физики.— М.: Знание, 1985.\n48. Драгунов Т.Н., Морозов А.Д. К исследованию систем типа ХенонаХейлеса // Регулярная и хаотическая динамика.— 1997.— Т. 2.— № 1.—\nС. 43–54.\n49. Дубровин Б. А., Новиков С. П., Фоменко А. Т. Современная геометрия.—\nМ.: Наука, 1986.\n50. Елкин В. И. Редукция нелинейных управляемых систем: дифференциальногеометрический подход.— М.: Наука, 1997.\n51. Емельянова И. С. Проблема «симметрия–интегралы движения» в\nаналитической динамике: Монография.— Н. Новгород: Изд-во Нижегородского ун-та, 1992.\n52. Емельянова И. С. Свойства локальной группы Ли, имеющей в качестве\nинварианта\nфункцию\nГамильтона\nконечномерной\nсистемы\n//\nДифференциальные уравнения.— 1994.— Т. 3.— №. 10.— С. 1683–1686.\n129\n\n\f53. Жевакин С.А. Об отыскании предельных циклов в системах, близких к\nнекоторым нелинейным // ПММ.— 1951.— Т. 15.— Вып. 2.— С. 237–244.\n54. Заславский Г. М., Сагдеев Р. З., Усиков Д. А., Черников А. А. Слабый хаос\nи квазирегулярные структуры.— М.: Наука, 1981.\n55. Заславский Г. М., Чириков Б. В. Стохастическая неустойчивость нелинейных колебаний // УФН.— 1971.— Т. 105.— Вып. 1.— С. 3-39.\n56. Зубер И. Б. Терминальное управление по выходу для нелинейных\nнестационарных систем // Дифференциальные уравнения и процессы\nуправления.— 2004.— № 2.— С. 36–42.\n57. Ибрагимов Н. Х. Групповой анализ обыкновенных дифференциальных\nуравнений и принцип инвариантности в математической физике // УМН\nРAН.— 1992.— Т. 47.— Вып. 4 (286).— С. 83–144.\n58. Ибрагимов Н. Х. Группы преобразований в математической физике.— М.:\nНаука, 1983.\n59. Ибрагимов Н. Х. Инвариантные вариационные задачи и законы сохранения // Теор. и матем. физика.— 1969.— Т. 1.— № 3.— С. 350–359.\n60. Ибрагимов\nН. Х.\nОпыт\nгруппового\nанализа\nобыкновенных\nдифференциальных\nуравнений\nи\nпринцип\nинвариантности\nв\nматематической физике // Успехи математических наук.— 1992.— Т. 47.—\nВып. 4 (268).— С. 83–144.\n61. Иоффе А. Д., Тихомиров В. М. Теория экстремальных задач.— М: Наука,\n1974.\n62. Калошин Д. А. О построении бифуркационной поверхности существования\nгетероклинических контуров седло-фокусов в системе Лоренца //\nДифференциальные уравнения.— 2004.— Т. 40.— № 12.— С. 1705–1707.\n63. Капица П. Л. Динамическая устойчивость маятника при колеблющейся\nточке подвеса // ЖЭТФ.— 1951.— Т. 21.— С. 588.\n64. Капица П. Л. Маятник с вибрирующим подвесом // Успехи физических\nнаук.— 1951.— Т. 44.— С. 7.\n65. Каплан Д. Л., Йорке Дж. А. Предтурбулентность: режим, наблюдаемый в\nтечении жидкости, описываемой моделью Лоренца // В сб. «Странные\nаттракторы».— М.: Мир, 1981.\n66. Каток А. Б., Хассельблат Б. Введение в современную теорию динамических\nсистем.— М.: Факториал УРСС, 1999.\n67. Келли Дж. Общая топология.— М: Наука, 1968.\n68. Кириллов А. А. Элементы теории представлений.— М.: Наука, 1972.\n69. Козлов В.В. Симметрии, топология и резонансы в гамильтоновой\nмеханике.— Ижевск: Изд-во Удм. ун-та, 1995.\n70. Колесников А. А. Аналитическое конструирование агрегированных\nрегуляторов: управление хаосом // Управление и информационные\nтехнологии: Сб. докл. науч. конф.— СПб., 2003.— Т. 1.— С. 18–22.\n71. Колесников А. А. Синергетические методы управления сложными\nсистемами: Теория системного анализа.— М.: КомКнига, 2006.\n\n130\n\n\f72. Колмогоров А. Н. О сохранении условно-периодических движений при\nмалом изменении функции Гамильтона // ДАН СССР.— 1954.— Т. 98.—\nС. 527–530.\n73. Кондратьев Г. В. Геометрический подход к решению задачи оптимального\nсинтеза стационарных гладких систем управления // Дисс. .. докт. физ.-мат.\nнаук.— НГТУ, 2000.\n74. Кондратьев Г. В. Геометрическая теория синтеза оптимальных\nстационарных гладких систем управления.— М.: Физматлит, 2003.\n75. Коровин С. К., Бобылев Н. А., Емельянов С. В. Геометрические методы в\nвариационных задачах.— М.: Магистр, 1998.\n76. Костылев И. А., Малинецкий Г. Г., Потапов А. Б. Параметры порядка в\nнейронной сети Хопфилда // Журнал вычисл. математики и матем. физ.—\n1994.— Т. 34.— С. 1733–1740.\n77. Краснощеков В. И. Геометрические методы исследования систем\nуправления // Теория и компьютерные методы исследования\nстохастических систем / Пупков К. А. и др. Приложение 3.— М.:\nФизматлит, 2003.— С. 350–399.\n78. Крищенко А. П. Исследования управляемости и множества достижимости\nнелинейных систем управления // Автоматика и телемеханика.— 1984.—\n№ 6.— С. 30–36.\n79. Кузнецов А. П., Кузнецов С. П. Критическая динамика одномерных\nотображений. Ч.1. Сценарий Фейгенбаума // Известия вузов. Прикладная\nнелинейная динамика.— 1993.— Т. 1.—№ 1/2.— С. 15–33.\n80. Кузнецов С. П. Динамический хаос (курс лекций). Серия: современная\nтеория колебаний и волн.— М.: Наука, 2001.\n81. Кусюмов А. Н., Павлов В. Г. Частные симметрии системы внешних\nдифференциальных уравнений // Дифференциальные уравнения и процессы\nуправления.— 2002.— № 4.— С. 1–16.\n82. Кусюмов А. Н. Симметрии и законы сохранения невариационных систем\nуравнений // Дифференциальные уравнения и процессы управления.—\n2003.— № 1.— С. 100–107.\n83. Линчук Л. В. Формальные операторы, допускаемые обобщенными\nдифференциальными уравнениями, и принцип факторизации //\nДифференциальные уравнения и процессы управления.— № 1.— 2001.—\nС. 71–115.\n84. Лобри К. Динамические полисистемы и теория управления // Сб.:\nМатематические методы в теории систем.— М.: Мир, 1979.— С. 134–179.\n85. Лоренц Э. Н. Детерминированное непериодическое течение // В кн.:\n«Странные аттракторы».— М.: Мир. 1981.— С. 88–116.\n86. Магницкий Н. А. О стабилизации неподвижных точек хаотических\nдинамических систем // Докл. РАН.— 1997.— Т. 352.—№ 5.— С. 610–612.\n87. Магницкий Н. А., Сидоров С. В. Новые методы хаотической динамики.—\nМ.: Едикториал УРСС, 2004.\n\n131\n\n\f88. Магницкий Н. А., Сидоров С. В. О некоторых подходах к проблеме\nуправления диффузионным хаосом // Дифференциальные уравнения.—\n1999.— Т. 35.—№ 5.— С. 664–669.\n89. Макарычев\nП. П.\nМоделирование\nнепрерывных\nи\nдискретных\nдинамических систем: учебное пособие.— Пенза: Пенз. политехн. ин-т,\n1988.\n90. Малинецкий Г. Г. Математические основы синергетики. Хаос, структуры,\nвычислительный эксперимент. Изд. 4-е.— М.: КомКнига, 2005.\n91. Малинецкий Г. Г. Потапов А. Б. Современные проблемы нелинейной\nдинамики.— М.: Эдиториал УРСС. 2000.\n92. Малинецкий Г. Г., Курдюмов С. П. Нелинейная динамика и проблемы\nпрогноза // Доклады РАН.— 2001.— Т. 71.— № 3.— С. 210–232\n93. Мельников В. К. Об устойчивости центра при периодических по времени\nвозмущениях // Тр. моск. мат. об-ва.— 1963.— Т. 12.— С. 3–52.\n94. Методы классической и современной теории управления. Т.3. Методы\nсовременной теории автоматического управления / Под общ. ред.\nН. Д. Егупова.— М. МГТУ им. Н. Э. Баумана, 2000.\n95. Михайлов Л. В., Шабаш Л. Б., Ямилов Р. И. Симметрийный подход к\nклассификации нелинейных уравнений. Полные списки интегрируемых\nсистем // Успехи математических наук.— 1987.— Т.42.— Вып. 4(256).—\nС. 3–54.\n96. Мищенко А. С., Фоменко А. Т. Курс дифференциальной геометрии и\nтопологии.— М.: МГУ, 1980.\n97. Морозов А. Д., Драгунов Т. Н. Визуализация и анализ инвариантных\nмножеств динамических систем.— М.–Ижевск: Ин-т комп. иссл., 2003.\n98. Морозов А. Д. О резонансах и хаосе в параметрических системах // ПММ.—\n1994.— Т. 58.— Вып. 3.— С. 41–51.\n99. Музыкин С. Н., Родионова Ю. М. Моделирование систем.— М.: МГАПИ,\n2004.\n100. Неймарк Ю.И., Ланда П.С. Стохастические и хаотические колебания.— М.:\nНаука, 1987.\n101. Немыцкий В. В., Степанов В. В. Качественная теория дифференциальных\nуравнений.— М., Л.: Гостехиздат, 1947.\n102. Нётер Э. Инвариантные вариационные задачи // Сб.: Вариационные\nпринципы механики.— М.: Физматгиз, 1959.— С. 611–630.\n103. Новиков С. П., Фоменко А. Т. Элементы дифференциальной геометрии и\nтопологии.— М.: Наука, 1987.\n104. Овсянников JI. B. Групповые свойства уравнений нелинейной\nтеплопроводности // Доклады АН СССР.— 1959.— Т. 125.— № 3.— С. 492–\n495.\n105. Овсянников Л. В. Групповой анализ дифференциальных уравнений.— М.:\nНаука, 1978.\n106. Овсянников Л. В. Групповые свойства дифференциальных уравнений.—\nНовосибирск: СОАН СССР, 1962.\n132\n\n\f107. Оселедец\nВ.И.\nМультипликативная\nэргодическая\nтеорема.\nХарактеристические показатели Ляпунова динамических систем // Тр. моск.\nмат. об-ва.— 1968.— Т. 19.— С.179–210.\n108. Павлов А. Н., Янсон Н. Б., Анищенко В. С. Реконструкция динамических\nсистем // Радиотехника и электроника.— 1999.— Т. 44.— № 9.— С. 1075–\n1092.\n109. Павловский Ю. Н., Яковенко Г. Н. Группы, допускаемые динамическими\nсистемами // Методы оптимизации и их приложения.— Новосибирск:\nНаука, 1982.— С.155–189.\n110. Палис Ж., ди Мелу В. Геометрическая теория динамических систем:\nВведение.— М.: Мир, 1986.\n111. Плисс В. А., Пилюшин С. Ю. Сохраняющиеся структуры для\nдиффеоморфизмов с эргодической инвариантной мерой // Доклады РАН.—\n1995.— Т. 343.— № 3.— С. 312–313.\n112. Понтрягин Л. С., Болтянский В. Г., Гамкрелидзе Р. В., Мищенко Е. Ф.\nМатематическая теория оптимальных процессов.— М.: Физматгиз, 1961.\n113. Понтрягин Л.С. О динамических системах, близких к гамильтоновым //\nЖЭТФ.— 1934.— Т. 4.— Вып. 9.— С. 883–885.\n114. Постников М. М. Дифференциальная геометрия.— М.: Наука, 1989.\n115. Пуанкаре А. Избранные труды. Новые методы небесной механики. Т. 1,\nТ. 2.—М.: Наука, 1971.\n116. Рашевский П. К. Геометрическая теория уравнений с частными\nпроизводными.— М.: Гостехиздат, 1947.\n117. Рейсинг Р, Сансоне Г., Конти Р. Качественная теория нелинейных\nдифференциальных уравнений.— М.: Наука, 1974.\n118. Рокафеллер А.Ф. Выпуклый анализ.— М.: Мир, 1973.\n119. Рунд Х. Дифференциальная геометрия финслеровых пространств.— М.:\nНаука, 1981.\n120. Рюэль Д., Такенс Ф. О природе турбулентности. Странные аттракторы.—\nМ.: Мир, 1981.— С. 117–151\n121. Самарский А. А., Гулин А. В. Численные методы.— М.: Наука, 1989.\n122. Свирщевский С. Р. Групповые свойства модели теплопереноса с учетом\nрелаксации теплового потока.— М.: ИПМ АН СССР, 1988. 16 с.—(Препр.\nИПМ АН СССР; №105).\n123. Селезнев Е. П., Захаревич А. М. Динамика нелинейного осциллятора при\nквазипериодическом воздействии // Письма в ЖТФ.— 2005.— Т. 31.—\nВып. 17.— С. 13–18\n124. Серр Ж.-П. Алгебры Ли и группы Ли.— М.: Мир, 1969.\n125. Симметрии и законы сохранения уравнений математической физики / Под\nред. Виноградова А. М. и Красильщика И. С.— М.: Факториал, 1997.\n126. Симо К., Брур X., Джервер Дж., Джиорджилли А., Лазуткин В.Ф.,\nМонтгомери Р., Смейл С, Стучи Т., Шенсине А. Современные проблемы\nхаоса и нелинейности.— М.-Ижевск: Изд-во Института компьютерных\nисследований, 2002.\n127. Синг Дж.Л. Классическая динамика.— М.: Наука. 1963.\n133\n\n\f128. Смейл С. Дифференцируемые динамические системы // Успехи мат. наук.—\n1970.— Т. 25.— № 1.— С. 113–185\n129. Смейл С. Математические проблемы следующего столетия // Кн.:\nСовременные проблемы хаоса и нелинейности.— Ижевск: Изд-во\nИнститута компьютерных исследований, 2002.— С. 280–303.\n130. Спеньер Э. Алгебраическая топология / пер. с англ.— М.: Мир, 1972.\n131. Степанов В. В. Курс дифференциальных уравнений.— М.: Гостехтеориздат,\n1953.\n132. Трикоми Ф. Лекции по уравнениям в частных производных.— М.: ИЛ,\n1957.\n133. Трофимов В. В. Введение в геометрию многообразий с симметриями.— М.:\nИзд-во МГУ, 1989.\n134. Труды семинара «Софус Ли»: Теория алгебр Ли. Топология групп.— Л.:\nИИЛ, 1962.\n135. Хартман Ф. Обыкновенные дифференциальные уравнения.— М.: Мир,\n1970.\n136. Хенон М. Двумерное отображение со странным аттрактором // В сб.\n«Странные аттракторы».— М.: Мир, 1981.— С. 152–163.\n137. Хрящев С. М. Оценки времени управления в системах с хаотическим\nповедением. Часть 1, 2 // Автоматика и телемеханика.— 2004.— №10, 11.\n138. Чеботарев Н.Г. Теория групп Ли.— Л.: ГИТТЛ, 1940.\n139. Чернышев В. Е. Сильно устойчивые слоения над контурами лоренцова типа\n// Вестник СПб. гос. у-та. Сер. 1.— 1996.— Вып. 4 (№22).— С. 44–52.\n140. Чернышев В. Е. Структура окрестности гомоклинического контура с\nседловой точкой покоя // Дифференц. уравнения.— 1986.—Т. 22.— № 3.—\nС. 439–445.\n141. Чурин Ю. В. Об исчезновении периодических решений квазиоднородных\nсистем, имеющих лишь простые исключительные множества // Дифф.\nуравнения.— 1975.— Т.11.—№ 4.— С. 678–686.\n142. Шилов Г.Е. Введение в теорию линейных пространств.— М.:\nГостехтеориздат, 1956.\n143. Шильников Л. П., Шильников А. Л., Тураев Д. В., Чуа Л. Методы\nкачественной теории в нелинейной динамики.— М.–Ижевск: Институт\nкомпьютерных исследований, 2003.\n144. Шильников Л. П. К вопросу о структуре расширенной окрестности грубого\nсостояния равновесия типа седло-фокус // Матем. сборник.— 1970.—\nТ. 81(123).— № 1.— С. 92–103.\n145. Шильников Л. П. Теория бифуркаций и модель Лоренца // Марсден Дж.,\nМак-Кракен М. Бифуркация рождения цикла и ее приложения. Добавление\nII.— М.: Мир, 1980.— С. 317–335.\n146. Шмырин Д. А. Разработка симметричных моделей и алгоритмов\nсмешанного управления пространственно-распределенных систем //\nАвтореф... канд. физ.-мат. наук: 05.13.14,05.13.01.— Донецк, 1998.\n147. Шориков А. Ф. Минимаксное оценивание и управление в дискретных\nдинамических системах.— Екатеринбург: Изд-во уральского у-та, 1997.\n134\n\n\f148. Яковенко Г. Н. Обыкновенные дифференциальные уравнения и системы с\nуправлением — сравнительный групповой анализ // Дифференциальные\nуравнения и процессы управления.— 2002.— №3.— С. 40–83.\n149. Яковенко Г. Н. Принцип суперпозиций для нелинейных систем: Софус Ли и\nдругие.— М.: Изд. МФТИ, 1997.\n150. Яковенко Г. Н. Регулярные математические модели систем с управлением:\nинвариантность, симметрии // Автореф... доктора физ.-мат. наук:\n05.13.18.— М.: ВЦ РАН, 1995.\n151. Яковенко Г. Н.\nТеоретико-групповой\nанализ\nвзаимодействующих\nпопуляций // Электронный журнал «Исследовано в России».— 2003.—\nС. 981–990 (http://zhurnal.ape.relarn.ru/articles/2003/088.pdf).\n152. Янсон Н. Б., Павлов А. Н., Капитаниак Т., Анищенко В. С. Глобальная\nреконструкция по нестационарным данным // Письма в ЖТФ.— 1999.—\nТ. 25.— Вып. 10.— С. 75–81.\n153. Aguirre L. A., Mendes E. M. Global nonlinear polynomial models: structure,\nterm clustering and fixed points // Int. J. Bifurc. Chaos.— 1996.— V. 6(2).—\nP. 279–294.\n154. Aguirre L. A., Billings S. A. Identification of models for chaotic systems from\nnoisy data: implications for performance and nonlinear filtering // Physica D.—\n1995.— V. 85.—P. 239–258.\n155. Allie S., Mees A., Judd K., Watson D. Reconstructing noisy dynamical systems\nby triangulation // Phys. Rev. E.— 1997.— V. 55(1).— P. 87–93.\n156. Anderson I. M., Kamran M., Olver P. J. Internal, External and Generalized\nSymmetries.— Preprint, 9/4/90, 1990.\n157. Baker C. L., Collub J. P., Blackburn J. A. Inverting chaos extracting system\nparameters from experimental data // Chaos.— 1996.— N. 4.— P. 528–533.\n158. Baptista M.S., Caldas I. L. Easy-to-implement method to target nonlinear\nsystems // Chaos.— 1997.— V. 8.— N. 1.— P. 290–299.\n159. Brawn R., Rulkov N. F., Tracy E. R. Modelling and synchronizing chaotic\nsystems from time-series data // Pthys. Rev. E.— 1994.— V. 49.— P. 3784.\n160. Breeden J. L., Hubler A. // Phys. Rev. A.— 1990.— V. 42.— N. 10.— P. 5817–\n5826.\n161. Cao L. Practical method for determining the minimum embedding dimension of\na scalar time series // Physcai D.— 1997.— V. 110.— P. 43–50.\n162. Castro, R., Sauer T. Correlation dimension of attractors through interspike\nintervals // Phys. Rev. E.— 1997.— V. 55(1).— P. 287–290.\n163. Chason R. Suppresion of chaos by selective resonant parametric perturbations //\nPhys. Rev. E. — 1995.— V. 51.— P. 761.\n164. Chen С. С., Chuo Y. J., Wang F. L., Yen H. Y., Chen C. H. Correlation\ndimension and its temporal variations in geomagnetic total field during storms //\nTAO.— V. 16.— N. 2.— P. 435–443\n165. Chester W. A. General theory of resonance between weakly coupled oscillatory\nsystems. Part 2. Nonconservative systems // J. Inst. Math, and Appl.— 1980.—\nV. 26.— N. 2.— P. 199–207.\n135\n\n\f166. Chua L. O., Komyro M., Matsumoto T. The double scroll family // IEEE Trans.\nCircuits Syst., CAS-33. —1986.— P. 1072.\n167. Chua's Circuit: a Paradigm for Chaos ed. R.N.Madand. // World Sci. Ser. on\nNonlinear Sci. Series B. — 1993.—V. 1.\n168. Cicogna G., Fronzoni L. Effects of parametric perturbations on the onset of chaos\nin the Josephson—Junction model: Theory and analog experiments // Phys.\nRev.— 1990.— V. A42.— P. 1901.\n169. Cremers X., Hubler А. // Z. Naturforschung А.— 1987.— V. 42.— Р. 797–802.\n170. Crutchfield J.P., McNamara B.S. // Complex Systems.— 1987.— V. 1.— P 417–\n452.\n171. Davies M. E., Campbell K. M. Linear recursive filters and nonlinear dynamics //\nNonlinearity.— 1996.— N. 9.— P. 487–499.\n172. de Melo W., Pugh C. The C1 Brunovsky hypothesis // Diff. Equathions.—\n1993.— V. 112.— P. 300–337.\n173. de Sousa Viera M.C., Lichtenberg A.J., Lieberman M.A. Synchronization of\nregular and chaotic systems // Phys. Rev.— 1992.— V. A46.— P. R7359.\n174. Eckman J.-P., Ruelle D. Ergodic theory of chaos and stmge attractors // Rev. of\nModern Phys.— 1985.— V. 57.— N. 3.— P. 617–656.\n175. Elliott J. P., Dawber P. G. Symmetry in Physics.— London: The Macmillan\nPress Ltd., 1979.\n176. Falconer K. J. Fractal Geometry: Mathematical Foundations and Applications. —\nNew York: John Wiley, 1990.\n177. Farmer J. D., Sidorowich J. J. Predicting chaotic time series // Phys. Rev. Lett. —\n1987.— V.59.— P. 845–848.\n178. Fronzoni L., Giocondo M., Pettini M. Experimental evidence of suppression of\nchaos by resonant parametric perturbations // Phys. Rev. A.— 1991.—\nV. A43. — P.6483.\n179. Goodwin R.M. The nonlinear accelerator and the persistence of business cycles //\nEconometrica.— 1951.— V. 19.— P. 1–17.\n180. Gouesbet G, Maquet X. // Physica D.— 1992.— V. 58.— P. 202–215.\n181. Gouesbet G., Letellier С. // Phys. Rev. E.— 1994.— V. 49.— P. 4955–4972.\n182. Grassberger P., Hegger R., Kantz H., Schaffrath C., Schreiber T. On noise\nreduction methods for chaotic data // Chaos.— 1993.— V. 3.— P. 127–141.\n183. Grigoriev R. O. Symmetry and localized control of extended chaotic systems //\nThesis of Doctor Philosophy.— Pasadenta, California: California Inst. of\ntechnology, 1999.\n184. Grossman S. Fluctuation Dynamics Near Chemical Instabikities // Stochastic\nNonlinear Systems in Physics, Chemistry, and Biology. Proc. of the Workshop,\n(Bielefeld, Fed. Rep. of Germany, 1980) / Eds. L. Arnold, R. Lefever.—\nSpringer-Verlag, 1981.— P. 213–221.\n185. Guckenheimer J., Holmes P. Nonlinear oscillations, dynamical systems and\nbifurcations of vector fields.— N.–Y.: Springer, 1983.\n186. Guckenheimer J., Williams R. F. Structural stability of Lorenz attractors // Publ.\nMath. IHES.— 1979.— V. 50.— P. 59–72.\n136\n\n\f187. Haken H. Transition Phenomena in Nonlinear Systems // Stochastic Nonlinear\nSystems in Physics, Chemistry and Biology. Proc. of the Workshop (Bielefeld,\nFed. Rep. of Germany, 1980) / Eds. L. Arnold, R. Lefever.— Springer–Verlag,\n1981.— P. 12–19.\n188. Hegger R., Bunner M.J., Kantz K, Giaquinta A // Phys. Rev. Lett.— 1998.—\nV. 81.— P. 558–561.\n189. Henon M, Heiles С. The applicability of the third integral of motion. Some\nnumerical experiments // Astron J.— 1964.— V. 69.— P. 73–79.\n190. Henon M. Numerical study of quadratic area-preserving mappings // Quarterly of\nAppl. Math.— 1969.— V.27.— N.3.\n191. Hermann M. Mesure de Lebesgue et nombre de rotation // Proc. Symp. Geomerty\nand Topology; Lecture notes in Math.— Springer-Verlag: NY, 1971.—\nV. 597.— P. 371–395.\n192. Hirsch M., Smale S. Differential equations, dynamical systems and linear\nalgebra.— N.–Y.: Academic Press, 1974.\n193. Huerta R., Bazhenov M., Rabinovich M. Clusters of synchronization and\nbistability in lattices of chaotic neurons // Europhys. Lett.— 1998.—V. 43.—\nN. 6.— P. 719–724.\n194. Jaeger L., Kantz H. Effective deterministic models for chaotic dynamics\nperturbed by noise // Phys. Rev. E.— 1997.— V. 55(5).— P. 5234–5247.\n195. Judd K., Mees A. On selecting models for nonlinear time series // Physica D.—\n1995.— V. 82.— P. 426–444.\n196. Kantz H., Schreiber T. Nonlinear time series analysis.— Cambridge: Cambridge\nUniversity Press, 1997.\n197. Kaplan D., Schreiber T. Signal separation by nonlinear projections: The fetal\nelectrocardiogram // Phys. Rev. E.— 1996.— V. 53(5).— P. R4326–R4329.\n198. Kennel M. B., Brown R. ,Abarbanel H. D. I. Determining embedding dimension\nfor phase-space reconstruction using a geometrical construction // Phys.\nRev. A.— 1992.— V. 45.— P. 3403–3411.\n199. King G. P., Steward I. Phase space reconstruction for symmetric dynamical\nsystems // Physica D: Nonlinear Phenomena.— 1992.— V. 58.— P. 216–228.\n200. Kocarev L., Shang A., Chua L. O. Transitions in dynamical regimes by driving:\nA unified method of control and synchronization of chaos // Int. J. Bif. Chaos.—\n1993.— V. 3.— P. 479.\n201. Lakshmanan M. Chaos for engineering: theory, applications and control.—\nSingapore: World Scientific, 1999.\n202. Lai Y.-Ch., Grebogi C. Synchronization of chaotic trajectories using control //\nPhys. Rev.— 1993.— V. E47.— P. 2357.\n203. Lie S. Vorlesungen uber continuerliche Gruppen.— Leipzig: Teubner, 1893.\n204. Lima R., Pettini M. Suppression of chaos by resonant parametric perturbations //\nPhys. Rev.— 1990.— V. A41.— P. 726.\n205. Ljung L. System Identification — Theory for the User.— Prentice Hall, Upper\nSaddle River, N. J. 2nd edition, 1999.\n206. Lorenz E. N. Deterministic Nonperiodic Flow // J. Atmos. Sci.— 1963.—\nV. 20.— P. 130–141\n137\n\n\f207. Lorenz H.-W., Nusse H.E. Chaotic attractors, chaotic saddles, ands fractal basin\nboundaries: Goodwin nonlinear accelerator model reconsidered // Chaos, Solitons\nand Fractals.— 2002.— V. 13.— P. 957–965.\n208. Macau E. Targeting in chaotic scattering // Phisical Rewiew E.— 1998.—\nV. 57.— N. 5.— P. 5337–5347.\n209. Mandelbrot B.B. The Fractal Geometry of Nature.— Freeman & Co., 1983.\n210. Mira С, Gardini L., Barugola A., Chatala J.-C. Chaotic dynamics in twodimensional noninvertible maps.— World Sci., Series A. 1995.\n211. Murali K., Lakshmanan M. Drive - response scenario of chaos synchronization in\nidentical nonlinear systems // Phys. Rev.— 1994.— V. E49.— P. 4882.\n212. Olver P. J. Applications of Lie Groups to Differential Equations.— Springer,\nNew York, 1986.\n213. Ott E, Grebogi С, Yorke J. Controlling chaos // Physical Review Letters.—\n1990.— V. 64.— N. 11.— P. 1195–1199.\n214. Packard, N. H., Crutchfield, J. P., Farmer, J. D., Shaw, R. S. Geometry from a\ntime series // Phys. Rev. Lett.— 1980.— V. 45.— P. 712–716.\n215. Palus M., Dvorak I. Singular-value decomposition in attractor reconstruction:\npitfalls and precautions // Physica D.— 1992.— V. 55.— P. 221–234.\n216. Pecora L. M., Caroll T. I., Jonnson G. A, Mar D. J., Heagy J. F. Fundamentals of\nsynchronization in chaotic systems. Concepts and applications // Caos.— 1997.—\nV. 7.— N. 4.— P. 520–543.\n217. Pecora, L. M., Carroll T. L. Synchronization in chaotic systems // Phys. Rev.\nLett. — 1990— V. 64.— P. 821–824.\n218. Peng J. H., Ding E. J ., Ding M., Yang W. Synchronizing hiperchaos with a\nscalar transmitted signal // Ph. Rev. Lett.—1996.—V. 76.— № 6.— P. 904–907.\n219. Pikovsky A. S. On the interaction of strange attractors // Z. Phys.— 1984.—\nV. B55.— P. 149.\n220. Pragas K. Continuous control of chaos by self-controlling feedback // Phys. Lett.\nA. — 1992.— V. 170.— P. 421–428.\n221. Rosenstein M. T., Collins J. J., De Luca C. J. Reconstruction expansion as a\ngeometry-based framework for choosing proper delay times // Physica D.—\n1994.— V. 73.— P. 82–98.\n222. Rossler O.E. // Phys. Lett. A.— 1976.— V. 57.— P. 397–398.\n223. Rulkov N. F., Sushchik M. M., Tsimring L. S., Abarbanel H. D. Generalized\nsynchronization of chaos in directional coupled chaotic systems // Pthys.\nRev. E. — 1995.— V .51(2).— P. 980–994.\n224. Rul'kov N. F., Volkovskii A. R., Rodriguez-Lozano A., Del Rio E., Velarde\nM. G. Mutual synchronization of chaotic self — oscillators with dissipative\ncoupling // Int. J. Bif. Chaos.— 1992.— V. 2.— P. 669.\n225. Rychlik M. Lorenz attractors through a Shilnikov-type bufurcation, Part 1.\nErgodic theory dynamical systems— 1989.— V. 10.— P. 793–821.\n226. Sauer T. Reconstruction of dynamical systems from interspike intervals // Phys.\nRev. Lett.— 1994.— V. 72.— P. 3811–3814.\n227. Schreiber T. Constrained randomization of time series data // Phys. Rev. Lett.—\n1998.— V. 80(10).— P. 2105–2108.\n138\n\n\f228. Schroer C, Ott E. Tarketing in hamiltonian systems that have mixed\nregular/chaotic phase spaces // Chaos.— 1997.— V. 7.— N. 4.\n229. Shindort T., Gtebodi C., Yorke J. A. Using small perturbations to control chaos //\nNature.— 1993— V. 363.— N. 3— P. 411–417/\n230. Sparrow C. The Lorenz equations : Bifurcations, chaos and strange attractors.–\nN.-Y.: Springer Verlag, 1982.\n231. Szpiro G. G. Forecasting chaotic time series with genetic algorithms // Phys. Rev.\nE.— 1997.—V. 55(3).— P. 2557–2568.\n232. Takens F. Detecting strange attractors in turbulence // Dynamical Syst. and\nTurbulence / Eds.: Rand D.A., Young L.-S.— Berlin: Springer, 1981.— P. 366–\n381.\n233. Takens F. Detecting nonlinearities in stationary time series // Int. J. of Bifurcation\nand Chaos.— 1993.— V. 3.— P. 241–256.\n234. Tanaka K., Jkeda T., Wang H. O. A Unified Approach to Controlling Chaos via\nan LMIBased Fuzzy Control Systems Design // IEEE Trans. Circuits Syst. J.—\n1998.— V. 45.— N. 10.— P. 1021–1040.\n235. Tucker W. A rigorous ODE solver and Smale’s 14th problem // Found. Comput.\nMath.— 2002.— V. 2.— P. 53–117.\n236. Ushio T. Chaotic synchronization and controlling chaos based on contraction\nmappings // Phys. Lett.— 1995.— V. A198.— P. 14.\n237. Voss K, Kurths X // Phys. Lett. A.— 1997.— V. 234.— P. 336–344.\n238. Williams R. F. The structure of the Lorenz attractors // Publ. Math. IHES.—\n1979.— V. 50.— P. 321–347\n239. Wolf, A., J.B. Swift, L. Swinney, J.A. Vastano, Determining Lyapunov\nexponents from a time series // Physica D.— 1985.— V. 16.— P. 285–317.\n240. Yamada T., Fujisaka H. Stability theory of synchronized motions in coupled\noscillator systems // Progr. Theor. Phys.— 1984.— V. 69.— P. 32.\n241. Yang L., Liu Z., Zheng Y. «Midle»periodic orbit and its application to chaos\ncontrol // International Journal of Bifurcation and Chaos.— 2001.— V. 12.—\nN. 8.— P. 1869–1876.\n242. Yorke J. A., Yorke E. D. Metastable chaos : the transition to sustained chaotic\noscillations in a model of Lorenz // J. Stat. Phys.—1979.—V. 21.— P. 263–267.\n243. Young L.-S. Capacity of attractors / Ergod. Theory and Dyn. Syst., Part 1.—\n1981.— P. 381–388; Part 2.— 1982.— P. 109–124.\n244. Хныкин А.П., Никульчев Е. В. Исследование и синтез оптимального\nуправления\nсложными\nдинамическими\nсистемами\nв\nусловиях\nнеопределенности целей // Сб. науч. тр.: Математическое моделирование и\nуправление в сложных системах. Вып. 1 / Под общ. ред. С. Н. Музыкина.—\nМ.: МГАПИ, 1997.— С. 61–65.\n245. Никульчев Е. В. Хныкин А. П. Система распределения электроэнергии на\nпромышленных предприятиях // Наука–производству.— 1998.— № 11.—\nC. 46–48\n246. Хныкин А. П., Никульчев Е. В. О проблемах моделирования техпроцессов\nпо заданным критериям качества // Моделирование электронных приборов\nи техпроцессов, обеспечение качества, надежности и радиационной\n139\n\n\fстойкости приборов и аппаратуры: Докл. V межд. науч.-техн. конф.\n(Севастополь, 1998).— М.: МГАПИ, 1999.— Т. 2.— С. 74–78.\n247. Хныкин А. П., Никульчев Е. В. Построение компромиссной зависимости в\nзадаче управления промышленным манипулятором // Сб. науч. тр.: Математическое моделирование и управление в сложных системах. Вып. 4. / Под\nред. С. Н. Музыкина и др.— М.: МГАПИ, 2001.— С. 149–152.\n248. Никульчев Е. В., Волович М. Е. Идентификация фазовых портретов\nдинамических систем по временным рядам // Научные труды МАТИ им.\nК. Э. Циолковского.— Вып. 4 (76).— М.: ЛАТМЭС, 2001.— С. 463–467.\n249. Никульчев Е. В., Волович М. Е. Реконструкция фазового портрета системы\nтеплообмена // Наукоемкие технологии и интеллектуальные системы: Сб.\nнауч. трудов 5-ой молодеж. науч.-техн. конф. (Москва, 2003).— М.: МГТУ\nим. Н.Э. Баумана, 2003.— Т. 2.— С. 132–135.\n250. Никульчев Е. В., Волович М. Е. Модели хаоса для процессов изменения\nкурса акций // Exponenta Pro. Математика в приложениях.— 2003.— № 1.—\nС. 49–52.\n251. Никульчев Е. В., Никульчева Е. В. Моделирование процессов управления\nдинамическими системами в условиях безопасности // Моделирование и\nанализ безопасности в сложных системах: Труды 2-й международ. науч. шк.\n(С.-Петербург, 2002).— СПб.: Бизнес-Пресса, 2002.— С. 388–393.\n252. Никульчев Е. В., Кучаева Е. В. Метод моделирования нелинейных\nдинамических процессов в сложных экономических системах // Труды\nВсероссийской науч. конф. «Проектирование научных и инженерных\nприложений в среде MATLAB»: (Москва, 2004).— М.: ИПУ РАН, 2004.—\nС. 637–643.\n253. Никульчев Е. В. Синтез оптимального по времени управления сложных\nстохастических системам по заданным критериям качества //\nМоделирование и исследование сложных систем: Тез. докл. международ.\nнауч.-техн. конф. (Кашира, 1996).— М.: МГАПИ, 1996.— С. 66–68.\n254. Никульчев Е. В. Об одном математическом методе обеспечения качества\nуправления в многокритериальных системах // Математическое моделирование и управление в сложных системах: Сб. науч. трудов под ред.\nС. Н. Музыкина. Вып. 3.— М.: МГАПИ, 2000.— С. 33–39.\n255. Никульчев Е. В. Разработка многокритериальных систем управления\nдинамическими объектами // Информационные технологии в науке и\nобразовании: Матер. II межднарод. науч.-практ. конф.— Шахты: ЮРГУЭС,\n2001.— С. 42–44.\n256. Никульчев Е. В. Построение компромиссной зависимости в системах с\nнесколькими целями // Теория активных систем: Труды международ. науч.практ. конф. (Москва, 2001) / Под ред. В. Н. Буркова, Д. А. Новикова.— М.:\nИПУ РАН, 2001.— Т. 1.— С. 98–99.\n257. Никульчев Е. В. Разработка дифференциально-геометрических моделей и\nпрограммных средств исследования систем управления // 2-я всероссийск.\nконф. молодых ученых по математике, математическому моделированию и\n140\n\n\fинформатике: Материалы (Новосибирск, 2001).— Новосибирск: ИВТ СО\nРАН, 2001.— С. 32–33.\n258. Никульчев Е. В. Технология автоматизированного расчета параметров\nрегулирования технологическими процессами // Промышленные АСУ и\nконтроллеры.— 2001.— № 11.— С. 23–26.\n259. Никульчев Е. В. Применение методов дифференциальной геометрии к задачам управления в сложных системах // Rusycon. Российский журнал по\nсистемам и управлению (Электронный журнал ИПМАШ РАН).—\n5.1.2002.— C. 1–12. (www.rusycon.ru)\n260. Никульчев Е. В. Проектирование систем управления на основе групп и\nалгебр Ли // Проектирование научных и инженерных приложений в среде\nMATLAB: Труды 1-й всероссийск. научн. конф. (Москва, 2002).— М.: ИПУ\nРАН, 2002.— С. 452–457.\n261. Никульчев Е. В. Математическое моделирование и исследование процессов\nуправления на основе конечных алгебр Ли // Современные проблемы\nинформатизации в экономике и непромышленной сфере: Сб. трудов. Вып.\n7.— Воронеж: Центр.-Черн. кн. изд-во, 2002.— С. 84–85.\n262. Никульчев Е. В. К вопросу моделирования и синтеза управления на основе\nдифференциальной геометрии // Гироскопия и навигация.— 2002.—\n№ 3(34).— С. 134.\n263. Никульчев Е. В. Моделирование и исследование сложных систем методами\nтеории конечных групп и алгебр Ли // Современные технологии в задачах\nуправления, автоматики и обработки информации: Тр. XI Междн. науч.техн. семинара (Крым, Алушта, 2002).— М.: МГАПИ, 2002.— С. 21–22.\n264. Никульчев Е. В. Идентификация динамических систем на основе групп\nсимметрий // 3-я всероссийск. конф. молодых ученых по математическому\nмоделированию\nи\nинформационным\nтехнологиям:\nМатериалы\n(Новосибирск, 2002).— Новосибирск. ИВТ СО РАН, 2002.— С. 33.\n265. Никульчев Е. В. Управление сложными системами с использованием\nаппарата групп симметрий // Конференция по теории управления,\nпосвященная памяти академика Б. Н. Петрова: Сб. трудов (Москва,\n2003).— М.: ИПУ РАН, 2003.— С. 40–41.\n266. Никульчев Е. В. Применение методов групп симметрий для задач\nуправления // 2-я международ. конф. по проблемам управления: Сб. тр.\n(Москва, 2003).— М.: ИПУ РАН, 2003.— Т. 1.— С. 34.\n267. Никульчев Е. В. Применение аппарата групп симметрий к исследованию\nуправляемых процессов теплообмена // Методы и алгоритмы прикладной\nматематики в технике, медицине и экономике: Материалы 3-й международ.\nнауч.-практ. конф. (Новочеркасск, 2003).— Новочеркасск: ЮРГТУ, 2003.—\nЧ. 2.— С. 20–23.\n268. Никульчев Е. В. Математические методы и модели управления сложными\nсистемами: аппарат групп симметрий // IV всероссийск. конф. молодых\nученых по математическому моделированию и информационным\nтехнологиям: Материалы (Красноярск, 2003).— Новосибирск: ИВТ СО\nРАН, 2003.— С. 37.\n141\n\n\f269. Никульчев Е. В. Симметрии в динамических моделях систем управления //\nВестник Тамбовского гос. университета. Серия: Естественные и\nтехнические науки.— 2003.—Т.8.— №3.— С. 423.\n270. Никульчев Е. В. Групповой анализ и моделирование динамически-сложных\nсистем // Устойчивость и колебания нелинейных систем управления: Труды\n8-го международ. семинара, посвященного памяти Е. С. Пятницкого\n(Москва, 2004).— М.: ИПУ РАН, 2004.— С.134–136.\n271. Никульчев Е. В. Аппарат групп симметрий для идентификации\nквазистационарных режимов технических систем с хаотической\nдинамикой // Автоматизация и управление в технических системах: Труды\nмеждународ. науч.-техн. конф. (Пенза, 2004).— Пенза: ИИЦ ПГУ, 2004.—\nС. 241–244.\n272. Никульчев Е. В. Simulink как средство исследования дифференциальных\nмоделей // Exponenta Pro. Математика в приложениях.— 2004.— № 1.—\nС. 91–93.\n273. Никульчев Е. В. Применение геометрических методов в задачах управления\nдискретными системами // Exponenta Pro. Математика в приложениях.—\n2004.— № 3–4.— С. 178–180.\n274. Никульчев Е. В. Группы симметрий дискретных управляемых систем //\nВестник МГАПИ. Естественные и технические науки.— 2004.— № 1.—\nС. 150–161.\n275. Никульчев Е. В. Использование групп симметрий для идентификации\nсложных систем // Вычислительные технологии.— 2004.— Т. 9.— № 3.—\nС. 72–80.\n276. Никульчев Е. В. Проектирование систем регулирования и моделирование\nсложных технических систем в MATLAB // Автоматизация в\nпромышленности.— 2004.— № 7.— С. 46–47.\n277. Никульчев Е. В. Моделирование промышленной системы теплообмена //\nАвтоматизация в промышленности.— 2004.— № 7.— С. 48–50.\n278. Никульчев Е. В. Моделирование и идентификация динамически-сложных\nсистем на основе группового анализа // Мехатроника, автоматизация,\nуправление.— 2004.— № 10.— С. 2.–7.\n279. Никульчев Е. В. Технология моделирования сложных и хаотических\nпроцессов, допускающих группы симметрий // Автоматизация и современные\nтехнологии.— 2004.— № 11.— С. 29–33.\n280. Никульчев Е. В. Разработка геометрических методов синтеза управления\nдискретными системами // Информационные технологии: Материалы\nвсероссийск. науч.-техн. конф. (Воронеж, 2005).— Воронеж: Научная книга,\n2005.— С. 324–326.\n281. Никульчев Е. В. Многокритериальные системы принятия решений для\nзадач управления // Автоматизация в промышленности.— 2005.— №7.—\nС. 45–46.\n282. Никульчев Е. В. Метод управления системами с хаотической динамикой //\nМатематическое моделирование и управление в сложных системах: Сб.\n142\n\n\fнауч. трудов под ред. А. П. Хныкина. Вып. 8.—М.: МГАПИ, 2005.—\nC.58–62.\n283. Никульчев Е. В., Назаркин И. А. Обработка данных и формирование\nобучающей выборки для прогнозирования динамического поведения\nсложных технических систем // Вестник МГАПИ. Естественные и\nтехнические науки.— 2005.— № 1.— С. 150–161.\n284. Никульчев Е. В. Качественное исследование управляемых систем c\nнелинейной динамикой на центральном многообразии // Вестник МГАПИ.\nЕстественные и технические науки.— 2006.— № 1.— С. 150–161.\n285. Никульчев Е. В. Моделирование систем с нелинейной динамикой на\nосновании экспериментальных данных // Мехатроника, автоматизация,\nуправление.— 2006.— № 5.— С. 6–14.\n286. Никульчев Е. В. Геометрический метод реконструкции систем по\nэкспериментальным данным // Письма в Журнал технической физики.—\n2007.— Т. 33.— Вып. 6.— С. 83–89.\n\n143\n\n\fПРИЛОЖЕНИЯ\nБорисов Ю. Ю.\n\nМетодика построения нейросетевых прогнозирующих\nмоделей на основе анализа свойств аттракторов\nВведение\nЗадачи прогнозирования и построения математических моделей различных\nпроцессов и явлений имеет первостепенное значение во многих областях науки и\nжизнедеятельности человека.\nВ большинстве технических, экономических и социальных системах возникают\nпроцессы, являющиеся результатом взаимодействия множества составляющих, что не\nпозволяет строить адекватные математические модели исходя только из априорных\nзнаний. Вместе с тем часто имеется потребность строить не модели явлений, а\nэволюционные модели изменений динамики конкретного процесса, являющегося\nнаблюдаемым параметром сложной системы. Особый интерес представляют\nэволюционные модели, дающие качественные\nпрогнозирующие значения\nмоделируемого процесса. Такие модели могут быть использованы в системах принятия\nрешений, управлении, прогнозирования и оценки качества сложных систем. Таким\nобразом, прогнозирующие модели строятся на основании наблюдения за процессами с\nучетом имеющейся информации и предположений о структуре и классе системы.\nЗначительное время единственным доступным теоретическим и практическим\nсредством прогнозирования временных рядов являлись статистические методы. Однако\nфундаментальные ограничения статистических подходов, исходящие из сложности\nпроверки предположений о вероятностных характеристиках и стохастических\nзакономерностях исследуемой реализации приводят к невозможности объяснения\nмногих явлений, и, как следствие, устранению неточных прогнозов.\nВ последнее время все большее развитие получает теория нелинейных\nдинамических систем, в рамках которой разработаны методики, позволяющие по\nнаблюдаемой скалярной реализации восстанавливать аттрактор, качественно\nэквивалентный исследуемой детерминированной системе [1]. В тоже время, аппарат\nнейронных сетей является мощным практическим инструментом аппроксимации\nфункций и используется во многих работах для оценки оператора эволюции\nисследуемых динамических систем. Однако для построения нейросетевых моделей не\nдостаточно широко применяются современные методики нелинейной динамики.\nИсследование восстановленного аттрактора позволит выбрать его наиболее\nинформативные характеристики, минимизируя при этом размерность входного вектора\nи структуру нейросетевой модели [2–3].\nСовместное использование методов нелинейной динамики и аппарата\nнейронных сетей позволит разработать методику построения прогнозирующих моделей\nреальных технических, экономических систем.\nТаким образом, разработка и исследование методов построения нейросетевых\nпрогнозирующих моделей на основе исследования реконструированных по\nнаблюдаемым данным аттракторов является актуальной.\n\n144\n\n\fСтатья\nпосвящена\nразработке\nметодики\nпостроения\nнейросетевых\nпрогнозирующих моделей на основе исследования и обработки аттракторов,\nреконструированных по наблюдаемым данным методом Паккарда-Такенса.\n\n1. Постановка задачи\nПусть задан временной ряд, являющийся результатом функционирования\nструктурно-сложной наблюдаемой системы, обладающий автоколебательной\nрегулярной или хаотической динамикой для которого может быть восстановлен\nаттрактор.\nПусть в результате эксперимента получен скалярный временной ряд,\nявляющийся функцией состояния неизвестной динамической системы\nx(t + 1) = f ( x(t ) ) ,\ny (t ) = Φ ( x ( t ) ) .\n\nСогласно теореме Такенса и условию Мане, фазовый потрет, восстановленный в\nвиде\n\nZ (t ) = ( y (t ), y (t − τ),..., y (t − ( m − 1)τ) ) = ( z1 (t ), z2 (t ),..., zm (t ) ) ,\n\n(1.1)\n\nтопологически эквивалентен аттрактору исходной динамической системы, обеспечивая\nглобальное отображение состояние Z(t) в x(t). Здесь x(t) — вектор состояния исходной\nдинамической системы; y(t) — наблюдаемый скалярный временной ряд; zi(t) — i-ая\nкомпонента вектора, определяющего точку на восстановленном аттракторе; τ —\nпараметр задержки (время пересечения траекториями сечения Пуанкаре); m —\nразмерность реконструкции, определяемая из условия m ≥ n h + 1 ; n h — хаусдорфова\nразмерность.\nПредложено представить модель оператора эволюции в виде вектор-функции\nZ (t + 1) = ψ ( Z\u0004(t ) ) ,\n\nотображающей характеристики аттрактора в области Z\u0004 на координаты вектора\nсостояния в следующий момент времени, что обеспечивает возможность учета\nлокального, глобального и синтетического прогноза при реконструкции уравнений\nсостояния исследуемых систем.\nПроблема параметрической идентификации нейросетевой модели с заданной\nструктурой S определяется как поиск параметров WS , минимизирующих величину\nошибки\n\n(\n\n)\n\nE = ∑ Z ( t + 1) − ψ (WS , S , Z\u0004 ( t ) ) ,\nt\n\n2\n\nгде ψ — нейросетевая оценка оператора эволюции; S — структура нейросетевой\nмодели (количество слоев, число нейронов в каждом слое, вид функций активации);\nWS — набор матриц весовых коэффициентов; Z\u0004 ( t ) — область на восстановленном\nаттракторе системы.\n\n2. Построение прогнозирующих моделей\nВ работе исследуются авторегрессионные модели вида Z (t + 1) = ψ( Z\u0004) , где Z\u0004 —\nнекоторое подмножество точек на восстановленном аттракторе; ψ — преобразование,\nиспользующее предыдущие состояния для вычисления последующего состояния.\n\n145\n\n\fПоказано что, различные методы прогнозирования различаются лишь способом\nформирования окрестности Z\u0004 и методом оценки функции ψ .\nБудем рассматривать окрестность Z\u0004 как матрицу, центральный элемент которой\nявляется текущей точкой Z (t ) восстановленного аттрактора системы, а центральный\nстолбец состоит из упорядоченных по возрастанию расстояния от точки Z (t ) ее\nближайших соседей — Z1 (t1 ), Z 2 (t2 ),..., Z 2 r (t2 r ) . Строки матрицы Z\u0004 соответствуют\nнепрерывным участкам траекторий, центрами которых являются ближайшие соседи\nточки Z (t ) . Таким образом, матрица Z\u0004 имеет вид:\n⎛ Z 2 r −1 (t2 r −1 + k +t )\n⎜\n...\n⎜\n⎜ Z1 (t1 + k +t )\n⎜\n\u0004\nZ =⎜\n0\n⎜ Z 2 (t2 + k +t )\n⎜\n...\n⎜\n⎜ Z (t + k +t )\n⎝ 2r 2r\n\n... Z 2 r −1 (t2 r −1 ++t ) Z 2 r −1 (t2 r −1 ) Z 2 r −1 (t2 r −1 −+t )\n...\n...\n...\n...\n...\nZ1 (t1 ++t )\nZ1 (t1 )\nZ1 (t1 −+t )\n...\n0\nZ (t )\nZ (t −+t )\n...\nZ 2 (t2 ++t )\nZ 2 (t2 )\nZ 2 (t2 −+t )\n...\n...\n...\n...\n...\n\nZ 2 r (t2 r ++t )\n\nZ 2 r (t2 r )\n\nZ 2 r (t2 r −+t )\n\n... Z 2 r −1 (t2 r −1 − k +t ) ⎞\n⎟\n...\n...\n⎟\n...\nZ1 (t1 − k +t ) ⎟ .\n⎟\n...\nZ (t − k +t ) ⎟\n...\nZ 2 (t2 − k +t ) ⎟\n⎟\n...\n...\n⎟\n... Z 2 r (t2 r − k +t ) ⎠⎟\n\nНулевые элементы в центральной строке матрицы Z\u0004 объясняются отсутствием\nданных в точках Z (\u0004t ) , где \u0004t > t .\nДля глобальных моделей матрица Z\u0004 имеет вид:\n⎛0\n⎜\n⎜ ...\n⎜0\n⎜\nZ\u0004 = ⎜ 0\n⎜0\n⎜\n⎜ ...\n⎜0\n⎝\n\n... 0\n... ...\n... 0\n... 0\n... 0\n... ...\n... 0\n\n0\n...\n\n0\n...\n\n...\n...\n\n0\n0\n...\nZ (t ) Z (t −+t ) ...\n0\n0\n...\n...\n0\n\n...\n0\n\n...\n...\n\n⎞\n⎟\n⎟\n⎟\n0\n⎟\nZ (t − k +t ) ⎟ .\n⎟\n0\n⎟\n...\n⎟\n⎟\n0\n⎠\n0\n...\n\n(2.1)\n\nДля локальных моделей матрица Z\u0004 может быть записана в виде (2.2) или (2.3).\n⎛0\n⎜\n⎜ ...\n⎜0\n⎜\nZ\u0004 = ⎜ 0\n⎜0\n⎜\n⎜ ...\n⎜0\n⎝\n⎛0\n⎜\n⎜ ...\n⎜0\n⎜\n\u0004\nZ =⎜0\n⎜0\n⎜\n⎜ ...\n⎜0\n⎝\n\n... Z 2 r −1 (t2 r −1 ++t ) 0 0 ... 0 ⎞\n⎟\n...\n...\n... ... ... ... ⎟\nZ1 (t1 ++t )\n...\n0 0 ... 0 ⎟\n⎟\n...\n0\n0 0 ... 0 ⎟ ,\nZ 2 (t2 ++t )\n...\n0 0 ... 0 ⎟\n⎟\n...\n...\n... ... ... ... ⎟\n... Z 2 r (t2 r ++t )\n0 0 ... 0 ⎟⎠\n\n... Z 2 r −1 (t2 r −1 ++t ) Z 2 r −1 (t2 r −1 ) 0 ... 0 ⎞\n⎟\n...\n...\n...\n... ... ... ⎟\nZ1 (t1 ++t )\nZ1 (t1 )\n...\n0 ... 0 ⎟\n⎟\nZ (t )\n...\n0\n0 ... 0 ⎟ .\nZ 2 (t2 ++t )\nZ 2 (t2 )\n...\n0 ... 0 ⎟\n⎟\n...\n...\n...\n... ... ... ⎟\nZ 2 r (t2 r )\n... Z 2 r (t2 r ++t )\n0 ... 0 ⎟⎠\n\n(2.2)\n\n(2.3)\n\nМатрица (2.2) определяет метод локального прогнозирования, основанный на\nусреднении значений ближайших соседних точек на аттракторе системы, в то время как\nв (2.3) используются значения изменений данных точек. Математические модели,\nиспользующие виды окрестностей отличные от (2.1), (2.2), (2.3) являются\nсинтетическими.\n\n146\n\n\fРазработан алгоритм, позволивший экспериментально доказать эффективность\nприменения синтетических моделей в задачах прогнозирования. Алгоритм можно\nпереставить в виде шести шагов.\nШаг 1. Выполнить реконструкцию аттрактора системы по заданному\nвременному ряду.\nШаг 2. Сформировать очередную конфигурацию матрицы Z\u0004 .\nВыполнить N раз шаг 3 и шаг 4:\nШаг 3. Построить нейросетевую модель.\nШаг 4. Получить ошибку прогноза на тестовом множестве для построенной\nмодели.\nШаг 5. Оценить качество прогноза для данной конфигурации матрицы Z\u0004 с\nучетом качества N нейросетевых моделей.\nШаг 6. Выбрать k конфигураций матрицы Z\u0004 , для которых были получены\nнаилучшие результаты.\nПредложено преобразование ψ представить как последовательное выполнение\nэтапов предобработки и вычисления выходных характеристик имеющейся модели:\nψ ( Z\u0004) = ψ n ψ p ( Z\u0004 ) ,\n\n(\n\n)\n\nгде ψ p — функция предобработки локальной окрестности; ψ n — нейросетевая модель.\nУстановлено, что методики предобработки ψ p решают три следующие задачи:\nфильтрация, учет прогноза альтернативных моделей для текущей ситуации (положение\nна аттракторе) и учет ошибки прогноза идентичных по структуре моделей для похожих\nситуаций. Основная идея предлагаемой комбинированной методики предобработки\nзаключается в оценке характеристик локальной окрестности, направленных на решение\nкаждой из выделенных задач.\nДля каждой из выделенных задач введен соответствующий класс. Методы,\nосуществляющие фильтрацию, принадлежат классу 1 ( class1 ); методы, учитывающие\nпрогноз альтернативных моделей — классу 2 ( class 2 ); методы, использующие ошибки\nпрогноза идентичных по структуре моделей — классу 3 ( class3 ).\nСистематизированы методы предобработки в соответствии с введенной\nклассификацией (табл. 1.1).\nТаблица 1.1\nКлассификация методов предобработки\n№\nОписание\n1 Точки вида y ( t − k1 ) ,..., y ( t − kn ) : m1( k1 ,..., kn )\n\nКласс\nclass1\n\n2 Усреднение по k1 ,..., kn предыдущим точкам: m2 ( k1 ,..., kn )\n\nclass1 , class 2\n\n3 Усреднение по k1 ,..., kn ближайшим соседям: m3 ( k1 ,..., kn )\n\nclass1 , class 2\n\n4 Взвешенное усреднение по k1 ,..., kn предыдущим точкам:\n\nclass1 , class 2\n\n5 Ошибки прогнозирования идентичных по структуре моделей:\nm5 ( k1 ,..., kn )\n\nclass3\n\nm4 ( k1 ,..., kn )\n\nРазработана общая схема обучения и функционирования нейросетевой модели\n(рис. 2.1).\n\n147\n\n\fМетоды нелинейной динамики\nи эмпирические методы\n\nРис. 2.1. Общая схема обучения и функционирования нейросетевой модели\nАлгоритм поиска оптимальных методов предобработки является частью\nразработанной методики построения нейросетевых прогнозирующих моделей\nдинамических систем, которую можно представить в виде следующей\nпоследовательности шагов.\nШаг 1. Восстановить аттрактор системы как последовательность точек вида\nZ (t ) = ( z1 (t ), z2 (t ),..., zm (t ) ) .\nШаг 2. Выполнить алгоритм поиска оптимальных методов предобработки,\nопределив при этом минимальную размерность входного вектора M .\nПоследовательно изменяя количество скрытых нейронов i = 1, 2M + 1 , выполнить\nшаг 3.\nШаг 3. Провести L раз процесс обучения нейронной сети по алгоритму\nЛевенберга-Марквардта, используя найденный набор методов предобработки.\nШаг 4. Выбрать нейросетевую модель, обеспечивающую наименьшую ошибку\nна тестовом множестве данных.\nВ результате применения приведенной выше методики подбирается структура и\nпараметры нейросетевой модели так, чтобы минимизировать ошибку обобщения для\nминимально возможного числа нейронов в распределительном слое.\n\n3. Оценка качества прогнозирования\nПо выходной активности нейросетевых моделей не может быть сделан вывод о\nпредполагаемой точности получаемого прогноза. Предложено для оценки точности\nпрогноза нейросетевой модели использовать критерий\nв виде суммарной\n\n148\n\n\fсреднеквадратичной ошибки прогнозирования точек, принадлежащих ближайшим\nсоседним траекториям аттрактора:\n2\n(3.1)\nEψ = Eψ ( Z ( t ) , k ) = ∑ Z k ( tk + 1) − ψ ( Z\u0004k ( tk ) ) ,\nk\n\n(\n\n)\n\nгде Eψ ( Z ( t ) , k ) — суммарная ошибка прогнозирования k ближайших соседей точки\nZ ( t ) ; Z k ( tk + 1) — k -ая ближайшая соседняя точка по отношению к точке Z ( t ) в\nследующий момент времени; ψ ( Z\u0004k ( tk ) ) — прогноз для k -ой ближайшей соседней\n\nточки.\nРазработана методика выбора прогнозирующей модели, в соответствии с\nкоторой, на очередном шаге прогнозирования выбирается прогноз той модели, которая\nминимизирует критерий (3.1). Так, прогноз на основе методики выбора модели\nстроится в виде:\nyˆ = a1 yˆ1 + a2 yˆ 2 + ... + an yˆ n\n(3.2)\n\n(\n\n)\n\n⎧⎪1, если Eψ (i ) = min E\n, j = 1, n,\nψ( j )\nj\nai = ⎨\n⎪⎩0, иначе,\nгде yˆi — прогноз, построенный i -ой моделью; Eψ (i ) — ошибка\n\ni -ой модели при\n\nпрогнозировании будущих значений ближайших соседних траекторий.\nМетодику выбора модели для прогнозирования одномерных временных рядов\nможно представить в виде трех этапов.\nЭтап 1. Провести реконструкцию аттрактора.\nЭтап 2. Построить N нейросетевых моделей.\nЭтап 3. Провести выбор прогноза одной из N моделей с помощью функции\n(3.2).\nСхема применения методики выбора модели для прогнозирования одномерных\nвременных рядов показана на рис. 3.1.\n\na1\n\na2\n\naN\n+++\nРис 3.1. Схема применения методики выбора модели на каждом шаге прогнозирования\n\n149\n\n\fРазработан метод оценки параметра доходности в задаче портфельного\nинвестирования. Так, предложено доходность i -го финансового инструмента\nвычислять в виде:\n⎧⎪ yˆ , если Eψ (i ) < d ,\n(3.3)\nRi = ⎨\n⎪⎩0, Eψ (i ) ≥ d ,\nгде Ri — доходность i -го финансового инструмента; Eψ (i ) — ошибка i -ой модели\n\nпри прогнозировании будущих значений ближайших соседних траекторий; d —\nминимально допустимая величина суммарной среднеквадратичной ошибки\nпрогнозирования ближайших соседних траекторий на аттракторе, при которой\nполучаемый прогноз ŷ считается достоверным.\nОценка параметра доходности в виде (3.3) позволяет исключить из портфеля те\nценные бумаги, для которых построить достоверный прогноз с помощью имеющихся\nмоделей не представляется возможным.\nПредставлена методика пошаговой реконструкции, суть которой состоит в\nизменении параметров реконструкции на каждом шаге прогнозирования так, чтобы\nточки аттрактора на интервале прогнозирования принадлежали локальной области\nаттрактора с наибольшей степенью устойчивости. Если (1.1) записать в виде\nZ (t ) = Λ ( F ( y (t )) ) = ( f (t ), f (t − τ),..., f (t − ( m − 1)τ) ) = ( z1 (t ), z2 (t ),..., zm (t ) ) ,\n(3.4)\nто параметрами реконструкции будут являться m, τ, F , где F — некоторое\nпреобразование известного временного ряда.\nМетодика пошаговой реконструкции основывается на классификации локальных\nобластей фазового пространства. Предложено считать локальную область фазового\nпространства устойчивой, если принадлежащие ей соседние точки в момент времени t\nостаются соседними и в момент времени t + 1 . Если же на следующем временном\nинтервале изначально соседние траектории более не являются соседними, то локальная\nобласть считается неустойчивой. Количественная характеристика локальной\nустойчивости λ D определена как максимальное расстояние в следующий момент\nвремени между соседними точками, принадлежащими области D, и имеет вид:\n1\n(3.5)\nλD =\n, ( Z i (ti ), Z j (t j ) ) ⊂ D ,\nmax δ ( Z i (ti + 1), Z j (t j + 1) )\ni, j\n\nгде D — локальная область аттрактора системы; δ ( Z i (ti + 1), Z j (t j + 1) ) — функция\nрасстояния между точками Z i (ti + 1) и Z j (t j + 1) .\nДля эффективного применения локальных прогнозирующих моделей в области\nD необходимо максимизировать введенный коэффициент локальной устойчивости (3.5)\nв данной области. Вводится соответствующий критерий качества реконструкции\nJ1 (m, τ, F ) = λ D ,\nгде m, τ, F — параметры реконструкции.\nУстановлено, что точность оценки λ D в (3.5) зависит от количества локальных\nтраекторий, проходящих через область D. Таким образом, необходимо также увеличить\nконцентрацию локальных траекторий в области D. Данное требование определяет\nследующий критерий качества реконструкции:\nJ 2 (m, τ, F ) = D ,\nгде m, τ, F — параметры реконструкции; D — мощность множества D.\n\n150\n\n\fВ общем случае, максимизировать критерии J1 и J 2 одновременно не\nпредставляется возможным. Фиксируя величину локальной устойчивости (критерий\nJ1 ), и максимизируя точность ее оценки (критерий J 2 ), получим компромиссное\nрешение в виде параметров реконструкции m, τ, F . Формально это можно записать в\nвиде:\n⎧ J 2 , J1 ≥ λ d ,\n(3.6)\nJ (m, τ, F ) = ⎨\n⎩0, J1 < λ d ,\n\nгде λ d — заданная допустимая величина локальной устойчивости.\nДля аттрактора, соответствующего найденному компромиссному решению,\nстроится прогноз с помощью локальной модели вида:\n1 n\nZ (t + 1) = ∑ Z (t j + 1) , Z (t j ) ⊂ D ,\n(3.7)\nn j =1\n\nгде Z (t + 1) = ( z1 (t + 1), z2 (t + 1),..., zm (t + 1) ) — прогнозируемая точка аттрактора системы;\nZ (t j + 1) = ( z1 (t j + 1), z2 (t j + 1),..., zm (t j + 1) ) — ближайшие соседи прогнозируемой точки\n\nна аттракторе.\nДля визуализации структурной\nпредложен следующий вид оператора F :\nz1 (t ) =\n\nгде Eψ\n\nустойчивости\nsign ( yˆ (t + 1) )\nEψ\n\n,\n\nпрогнозирующей\n\nмодели\n\n(3.8)\n\nz2 (t ) = sign( y (t ) − y (t − 1)),\n— ошибка модели при прогнозировании будущих значений ближайших\n\nсоседних траекторий, yˆ (t + 1) — прогноз модели для момента времени t + 1.\nПредставлена разработанная методика пошаговой реконструкции для\nпрогнозирования временных рядов в виде следующей последовательности шагов.\nШаг 1. Применяя к исходному временному ряду y (t ) преобразования F ,\nизвлечь n признаков вида Z1 (t ), Z 2 (t ),..., Z n (t ) , t = 1, T .\nДля Cnm возможных комбинаций признаков Z1 (t ), Z 2 (t ),..., Z m (t ) и значений\nпараметра задержки τ = 1, τmax выполнить шаги 2–3.\nШаг 2. Выполнить реконструкцию аттрактора в m -мерном пространстве\nШаг 3. Для точки аттрактора Z (t ) найти ближайшие соседние точки и\nрассчитать для них композитный критерий J (m, τ, F ) (3.6).\nШаг 4. Восстановить аттрактор с параметрами, максимизирующими J (m, τ, F ) .\nШаг 5. Построить прогноз для точки Z (t + 1) с помощью локальной\nмодели (3.7).\nТаким образом, на основе анализа ближайших соседних траекторий\nвосстановленного аттрактора получены критерии оценки качества получаемого\nпрогноза и разработана методика выбора модели для прогнозирования временных\nрядов.\n\n4. Пример моделирования\nРассмотрим задачу прогнозирования поведения структурно-сложных систем на\nпримере американского фондового индекса S&P 500. На рис. 4.1a показан исходный\n\n151\n\n\fвид индекса S&P 500. Исключив тренд, получаем стационарную составляющую\nданного индекса (рис. 4.1b).\n1400\n\na).\n\nS&P500\n\n1200\n\n1000\n\n800\n\n600\n\n0\n\n200\n\n400\n\n600\n\n800\n\n1000\n\n1200\n\n1400\n\n800\n\n1000\n\n1200\n\n1400\n\nt\n\n60\n\nb).\n\ndetrend(S&P500)\n\n40\n20\n0\n-20\n-40\n-60\n\n0\n\n200\n\n400\n\n600\nt\n\nРис. 4.1. Исходный вид индекса S&P500 (a) и индекс S&P500 за вычетом тренда (b)\nАттракторы, восстановленные по исходной реализации и стационарной\nсоставляющей индекса, представлены на рис. 4.2 и рис. 4.3.\n\n60\n40\n\nz3\n\n20\n0\n-20\n-40\n-60\n60\n40\n\n60\n\n20\n\n40\n0\n\n20\n0\n\n-20\n\n-20\n\n-40\nz2\n\n-40\n-60\n\n-60\nz1\n\nРис. 4.2. Аттрактор, восстановленный по индексу S&P500\n\n152\n\n\f1400\n1300\n1200\n\nz3\n\n1100\n1000\n900\n800\n700\n1400\n1400\n\n1200\n\n1300\n1200\n\n1000\n\n1100\n1000\n\n800\nz2\n\n900\n600\n\n800\n700\nz1\n\nРис.4.3. Аттрактор, восстановленный по стационарной составляющей индекса\nS&P500\nНа практике интерес представляет не столько абсолютное значение индекса,\nсколько направление его движения через установленный интервал прогнозирования.\nРассмотрим применение разработанной методики построения нейросетевой\nмодели для прогнозирования знака дневного изменения стационарной составляющей\nиндекса S&P500.\nНа рис. 4.4 и рис. 4.5 показаны примеры работы полученной прогнозирующей\nмодели на обучающем и тестовом множестве данных.\n50\n\n40\n\n30\n\n20\n\nŷ\n\n10\n\n0\n\n-10\n\n-20\n\n-30\n\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n350\n\nt\nРис. 4.4. Прогнозирование нейросетевой моделью обучающего множества\nданных\n\n153\n\n\f30\n\n20\n\n10\n\nŷ\n0\n\n-10\n\n-20\n\n-30\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\nt\n\n30\n\n35\n\n40\n\n45\n\n50\n\nРис. 4.5. Прогнозирование нейросетевой моделью тестового множества данных\nМетоды предобработки, для которых получена наименьшая ошибка на тестовом\nмножестве данных, отображены в табл. 4.1. Структура наилучшей прогнозирующей\nмодели показана в табл. 4.2.\nТаблица 4.1\nМетоды предобработки, обеспечившие наименьшую\nошибку прогнозирования на тестовом множестве данных\n№\n1\n2\n3\n\nМетоды предобработки\nm1(0,1,3),m2(15,13,10,6),m3(1,2),m5(1,2,3)\nm1(0,3), m2(15,13,7,6,2), m3(1,2),m5(1,2,3)\nm1(0,1,2,3),m2(15,13,2),m3(1,2),m5(1,2,3)\n\nПроцент верно определенного\nнаправления на тестовом\nмножестве, %\n68\n64\n62\n\nТаблица 4.2\nСтруктура наилучшей прогнозирующей модели\nРазмерность\nвходного\nвектора\n12\n\nЧисло\nнейронов в\nскрытом слое\n24\n\nЧисло\nнейронов в\nвыходном слое\n1\n\nВид функции\nактивации\nСигмоидальная и\nлинейная функция\n\nСуммарное\nколичество\nсвязей\n312\n\nИсключение из интервала прогнозирования точек, для которых неверно\nпрогнозируется направление хотя бы одной из четырех ближайших соседних\n\n154\n\n\fтраекторий, увеличивает адекватность наилучшей прогнозирующей модели до 79%.\nТаким образом, разработана методика построения прогнозирующих моделей на основе\nанализа аттракторов нелинейных динамических систем, реконструированных на\nосновании экспериментальных данных.\nЛитература\n\n1. Никульчев Е. В. Геометрический метод реконструкции систем по\nэкспериментальным данным // Письма в ЖТФ.— 2007.— Т. 33.— Вып. 6.— С. 83–89.\n2. Борисов Ю. Ю. Построение прогнозирующих моделей динамических систем\nна основе исследования окрестностей реконструированных аттракторов //\nАвтоматизация и современные технологии.— 2007.— №2.— С. 32–37.\n3. Борисов Ю. Ю. Метод пошаговой реконструкции для построения локальных\nпрогнозирующих моделей хаотических временных рядов. // Известия вузов. Проблемы\nполиграфии и издательского дела.— 2007.— № 2.— С. 51–56.\n\nКозлов О. В.\n\nВыявление симметрий реконструированных фазовых\nтраекторий динамических систем\nДля реконструкции динамических моделей нелинейных систем по\nэкспериментальным данным в работе [1] предложен геометрический метод,\nоснованный на выделении локальных областей фазовых траекторий, близких к\nпериодическим, и построение конечнопараметрических преобразований, переводящих\nодну область в другую, т. е. построение группы симметрий фазовых траекторий,\nкоторая характеризуется преобразованием графиков.\nДоклад посвящен разработке методики, позволяющей выявлять практические\nзначимые симметрии (аффинные преобразования и поворот) при построении\nдинамических моделей по группам симметрий реконструированных аттракторов.\nПусть имеется траектория X(m) дискретной динамической системы (контур),\nсостоящая из m действительных точек в n-мерном фазовом пространстве, которую\nможно представить в виде:\nxn ,1 ⎤\n⎡ x1,1 x2,1\n⎢\n⎥\nx1,2 x2,2\nxn ,2 ⎥\n⎢\n.\nX ( m) =\n⎢\n⎥\n⎢\n⎥\nxn ,m ⎥⎦\n⎢⎣ x1,m x2,m\nРазработана\nметодика,\nрезультатом\nкоторой\nявляется\nполучение\nнормализованных характеристик траектории и показателей для наиболее практически\nважных симметрий (переноса, масштабирования и поворота) в условиях слабого\nнарушения симметрии. При формировании методики решения многомерной задачи\nвыявления преобразований симметрий контура был модифицирован подход,\nоснованный на применении дискретного преобразования Фурье к двумерной задаче [2].\nПрименим дискретное преобразование Фурье (ДПФ) для каждой точки\nрассматриваемой локальной области фазовой траектории в виде:\nm\n\nsd ,k = ∑ xd , p e\n\n−2 πi\n( k −1)( p −1)\nm\n\np =1\n\n155\n\n, 1≤ d ≤ n .\n\n(1)\n\n\fРезультатом преобразования будет спектр следующей структуры:\nSd = ⎡⎣ sd ,0 sd ,1\n\n(\n\nΤ\n\n)\n\nsd ,m ⎤⎦ , 1 ≤ d ≤ n .\n\nПары ( S1 Sn−1 ), ( S2 Sn−2 ), … , S( n−1) / 2 S n / 2 являются комплексно сопряженными\nчислами. Заметим, что они также определяют характеристики контура, но в другом\nпространстве. Переход от спектра к контуру осуществляется с помощью обратного\nдискретного преобразования Фурье (ОДПФ) вида:\n2 πi\n( k −1)( p −1)\n1 m\nxd ,k = ∑ sd , p e m\n, 1≤ d ≤ n .\n(2)\nm p=1\nРассмотрим получение симметрий переноса, масштабирования и поворота\nконтура относительно начала координат. Первый элемент спектра определяет\nположение центра контура, так как является усредненной суммой координат всех точек\nконтура. Собственно, первый элемент спектра и является показателем симметрии\nпереноса контура:\nK пер = ⎡⎣ s1,0 s2,0\nsn ,0 ⎤⎦ .\n(3)\nВоспользуемся следующим свойством преобразования Фурье: поворот или\nмасштабирование спектра приводит к повороту или масштабированию контура на тот\nже угол или коэффициент. Пары комплексно сопряженных элементов спектра\n( S1 Sn−1 ), ( S2 Sn−2 ) и т. д., после ОДПФ (2) представляют собой эллипсы (если не меняя\nразмера спектра просто приравнять к нулю все остальные элементы), в частности пара\n( S1 S m −1 ) — наибольший эллипс, аппроксимирующий контур. Коэффициент\nмасштабирования удобно рассматривать как отношение длины вектора второго\nэлемента к единичному вектору, такой же направленности:\n\nK мшт =\n\n2\n\n2\n\n2\n\ns1,1 + s2,1 + \" + sm ,1 .\n\n(4)\n\nПоказатель поворота контура удобно определять в виде набора углов вектора\nвторого элемента спектра к координатным осям. Положение контура определяется с\nпомощью n − 1 углов. Угол поворота контура к оси измерения k определяется\nследующим образом:\n⎧arctan ( sk ,2 sk +1,2 ) , sk ,2 > 0, sk +1,2 > 0,\n⎪\n⎪−\n⎪ arctan ( sk +1,2 sk ,2 ) + π / 2, sk ,2 ≤ 0, sk +1,2 > 0,\n(5)\nK пов,k = ⎨\n⎪arctan ( sk ,2 sk +1,2 ) + π, sk ,2 ≤ 0, sk +1,2 ≤ 0,\n⎪\n⎪⎩−arctan ( sk +1,2 sk ,2 ) + 3π / 2, sk ,2 > 0, sk +1,2 ≤ 0,\nгде 1 ≤ k ≤ n − 1 .\nВ условиях точного соблюдения симметрии для определения симметрий двух\nконтуров A и B достаточно применить к ним вышеописанную процедуру и получить\nпоказатели симметрий из разности полученных показателей относительно начала\nкоординат.\nK пер, AB = K пер, A − K пер, B ,\nK мшт, AB = K мшт, A − K мшт, B ,\n\n(6)\n\nK пов, AB = K пов, A − K пов, B .\nОднако при исследовании реальных систем имеют место нарушения симметрии\nили шум. При этом возникает необходимость в оценке близости контуров друг к другу.\nОценка близости должна осуществляться независимо от переноса, сдвига и\nмасштабирования, этого можно достичь путем сравнения спектров, нормализованным\n\n156\n\n\fпо этим преобразованиям. После нормализации из спектра S контура будет получен\nспектр Sнорм , инвариантный относительно переноса, масштаба и поворота исходного\nконтура.\n1. Нормализация по сдвигу. Необходимо перенести центр контура в начало\nкоординат, что достигается приравниванием к нулю первого элемента спектра:\ns1, 0 = 0, s2, 0 = 0,\n\n, sn,0 = 0 .\n\n(7)\n\n2. Нормализация по растяжению/сжатию. Необходимо смасштабировать\nспектр так, чтобы стал единичным вектор второго элемента, определяющего\nнаибольший эллипс, аппроксимирующий контур. При этом все элементы спектра\nумножаются на коэффициент масштабирования:\nS = S ⋅ K мшт .\n\n(8)\n\n3. Нормализация по повороту. Необходимо повернуть спектр так, чтобы вектор\nвторого элемента, определяющего наибольший эллипс, аппроксимирующий контур,\nсовпал с положительным направлением оси OX. Это достигается с помощью n − 1\nдвухмерных поворотов спектра. Матрица поворота спектра M k в плоскости измерений\n(k , k + 1) выглядит следующим образом:\n0⎤\n\"\n⎡1 0\n⎢0 %\n⎥\n⎢\n⎥\n⎢\n⎥\ncos(− K пов,k )\nsin(− K пов, k )\n(9)\nMk = ⎢\n⎥.\nsin(\n)\ncos(\n)\nK\nK\n#\n−\n−\n−\nпов,\nk\nпов,\nk\n⎢\n⎥\n⎢\n% 0⎥\n⎢\n⎥\n0 1 ⎦⎥\n\"\n⎣⎢0\nНеобходимо для каждого 1 ≤ k ≤ n − 1 составить матрицу M k и перемножить ее\nсо спектром S :\nS = S ⋅Mk .\n(10)\nПосле всех проведённых манипуляций спектр S инвариантен относительно\nпереноса, масштабирования и поворота исходного контура:\nSнорм = S .\n(11)\nДля сравнения двух нормализованных спектров контуров A и B необходимо\nввести критерий близости, например, сумму скалярных произведений векторов\nэлементов спектров с дисконтированием по важности элемента:\nm\n+1 G\nG\n2\ns A, k ⋅ s B , k\n.\n(12)\nK близ, AB = ∑\nk\nk =1\nРазработанная методика выявления основных симметрий и показателя схожести\nдвух контуров A и B имеет следующий вид:\n1. Заданы контуры A и B .\n2. Применяем к ним преобразование Фурье (1).\n3. Получаем их спектры S A и S B .\n4. Получаем показатели переноса K пер, А и K пер,B по (3).\n5. Получаем коэффициенты K мшт, А и K мшт, B масштабирования (4).\n6. Получаем углы поворота спектров относительно осей K пов, А,k и K пов, B ,k по\nформуле (5).\n\n157\n\n\f7. Получаем\n\nосновные\n\nсимметрии\n\nконтуров\n\nK пер, AB , K мшт, AB , K пов, AB\n\nв\n\nсоответствии с (6).\n8. Осуществляем нормализацию спектров S A и S B по переносу (7).\n9. Осуществляем нормализацию спектров S A и S B по масштабированию (8).\n10. Последовательно осуществляем поворот по каждой последовательной паре\nизмерений контура, рассчитывая матрицы M A,k и M B ,k (9) и умножая на них\nспектры (10).\n11. Получаем нормализованные спектры S A,норм и S B ,норм (11).\n12. Применяя критерий (12), получаем показатель близости контуров K близ, AB .\nНиже приведен пример реализации в среде MATLAB нормализации только\nодного трёхмерного ( n = 3) контура (шаги методики 2–11) и визуализации этапов этого\nпроцесса (рис. 1).\n\nРис. 1. Результат работы алгоритма для демонстрационного контура, состоящего\nиз 100 точек и параметром сглаживания K = 5\nОсновная функция, принимающая на вход трёхмерный контур и параметр\nсглаживания, определяющий количество занчимых пар коэффициентов Фурье при\nвосстановлении контура из спектра приведена ниже.\n%Curve - трёхмерный контур, матрица Mx3\n%K – параметр сглаживания\nfunction PreProc3D(Curve, K)\n%Получение кривой в комплексной плоскости\nM = size(Curve, 1);\n\n158\n\n\f%Применение быстрого одномерного преобразования Фурье\nFCurve = fft(Curve, [], 1);\n%Восстановление кривой по всем членам Фурье\nRCurveAll = ifft(FCurve, [], 1);\n%Восстановление кривой по K-парам членов Фурье\nFCuttedCurve = FCurve;\nFCuttedCurve(K+2:M-K, :) = 0;\nRCCurve = ifft(FCuttedCurve, [], 1);\n%Нормализация по перемещению и восстановление кривой K-парам членов Фурье\nFCMCurve = FCuttedCurve;\nFCMCurve(1, :) = 0;\nRCMCurve = ifft(FCMCurve, [], 1);\n%Нормализация по повороту и восстановление кривой K-парам членов Фурье\nN = FCMCurve(2,:);\nAngleXY = AngleToAxis([N(1) N(2)]);\n%Матрица поворота вокруг Z\nXYRotateMatrix = [ cos(AngleXY)\n-sin(AngleXY)\n0\n\nsin(AngleXY)\ncos(AngleXY)\n0\n\n0;\n0;\n1\n\n];\n\n%Поворот вокруг Z\nFCMRCurve = FCMCurve * XYRotateMatrix;\nN = N * XYRotateMatrix;\nAngleYZ = AngleToAxis([N(2) N(3)]);\n%Матрица поворота вокруг X\nYZRotateMatrix = [ 1\n0\n0\ncos(AngleYZ)\n0\n-sin(AngleYZ)\n\n0;\nsin(AngleYZ);\ncos(AngleYZ)];\n\n%Поворот вокруг X\nFCMRCurve = FCMRCurve * YZRotateMatrix;\nRCMRCurve = ifft(FCMRCurve, [], 1);\n%Нормализация по масштабу и восстановление кривой по по K-парам членов\nФурье\nScale = sqrt(FCMRCurve(2,1)^2 + FCMRCurve(2,2)^2 + FCMRCurve(2,3)^2);\nFCMRSCurve = FCMRCurve / real(Scale);\nRCMRSCurve = ifft(FCMRSCurve, [], 1);\n%Эллипс - главная компонента, образуемая при восстановлении контура по\n%первой паре членов Фурье\nFDCurve = FCurve;\nF1 = FDCurve(1,:);\nF2 = FDCurve(2,:);\nFM = FDCurve(M,:);\nFDCurve(:,:) = 0;\nFDCurve(1,:) = F1;\nFDCurve(2,:) = F2;\nFDCurve(M,:) = FM;\nFDCurve(1, :) = 0;\nRDCurve = ifft(FDCurve, [], 1);\n%РЕЗУЛЬТАТЫ:\n%FCurve - полный спектр исходного контура\n%RCurveAll - контур восстановленный по полному спектру\n\n159\n\n\f%FCuttedCurve - урезанный спектр состоящий только их K-пар членов Фурьё\n%RCCurve - контур восстановленный из урезанного спектра\n%FCMCurve - спектр, нормализованный по перемещению\n%RCMCurve - контур, нормализованный по перемещению\n%FCMRCurve - спектр, далее нормализованный по повороту\n%RCMRCurve - контур, нормализованный по перемещению и повороту\n%FCMRSCurve - спектр, далее нормализованный по масштабированию\n%RCMRSCurve - контур, нормализованный по перемещению, повороту и\nмасштабированию\n%FDCurve - спектр, состоящий только из нулевого члена и первой пары членов\nФурье\n%RDCurve - эллипс, главная компонента исходного контура\n%Вывод всех результатов\nCols = 2;\nRows = 3;\nsubplot(Cols, Rows, 1);\nRCurveAll = cat(1, RCurveAll, RCurveAll(1,:));\nplot3(RCurveAll(:,1), RCurveAll(:,2), RCurveAll(:,3));\ntitle('Full restore')\nset(gca, 'Projection', 'perspective');\nsubplot(Cols, Rows, 2);\nRDCurve = cat(1, RDCurve, RDCurve(1,:));\nRDCurve = real(RDCurve);\nplot3(RCMCurve(:,1), RCMCurve(:,2), RCMCurve(:,3), RDCurve(:,1),\nRDCurve(:,2), RDCurve(:,3));\ntitle('Main ellipse')\nset(gca, 'Projection', 'perspective');\nsubplot(Cols, Rows, 3);\nRCCurve = cat(1, RCCurve, RCCurve(1,:));\nRCCurve = real(RCCurve);\nplot3(RCurveAll(:,1), RCurveAll(:,2), RCurveAll(:,3), RCCurve(:,1),\nRCCurve(:,2), RCCurve(:,3));\ntitle('Aprroximation restore')\nset(gca, 'Projection', 'perspective');\nsubplot(Cols, Rows, 4);\nRCMCurve = cat(1, RCMCurve, RCMCurve(1,:));\nRCMCurve = real(RCMCurve);\nplot3(RCCurve(:,1), RCCurve(:,2), RCCurve(:,3), RCMCurve(:,1),\nRCMCurve(:,2), RCMCurve(:,3));\ntitle('Offset')\nset(gca, 'Projection', 'perspective');\nsubplot(Cols, Rows, 5);\nRCMRCurve = cat(1, RCMRCurve, RCMRCurve(1,:));\nRCMRCurve = real(RCMRCurve);\nplot3(RCMCurve(:,1), RCMCurve(:,2), RCMCurve(:,3), RCMRCurve(:,1),\nRCMRCurve(:,2), RCMRCurve(:,3));\ntitle('Rotate')\nset(gca, 'Projection', 'perspective');\nsubplot(Cols, Rows, 6);\nRCMRSCurve = cat(1, RCMRSCurve, RCMRSCurve(1,:));\nRCMRSCurve = real(RCMRSCurve);\nplot3(RCMRSCurve(:,1), RCMRSCurve(:,2), RCMRSCurve(:,3));\ntitle('Scale')\nset(gca, 'Projection', 'perspective');\n\n160\n\n\fВ основной функции используется вспомогательная функция, возвращающая\nугол между радиус-вектором точки и координатной осью:\n%Point – точка\n%Angle – угол к оси\nfunction Angle = AngleToAxis(Point)\nAngle = 0;\nX = real(Point(1));\nY = real(Point(2));\nif (X > 0 && Y > 0)\nAngle = -atan(X\\Y);\nend\nif (X <= 0 && Y > 0)\nAngle = atan(Y\\X) - pi/2;\nend\nif (X <= 0 && Y <= 0)\nAngle = -atan(X\\Y) - pi;\nend\nif (X > 0 && Y <= 0)\nAngle = atan(Y\\X) - 3*pi/2;\nend\n\nТаким образом, результаты тестирования реализации подтверждает\nэффективность решения выявления практически значимых симметрий (аффинные\nпреобразования и поворот) в предложенной методике.\nЛитература\n\n1. Никульчев Е. В. Геометрический метод реконструкции систем по\nэкспериментальным данным // Письма в ЖТФ.— 2007.— Т. 33.— Вып. 6.— С. 83–89.\n2. Осовский С. Нейронные сети для обработки информации. — М.: Финансы и\nстатистика, 2004.\n\n161\n\n\fНаучное издание\n\nНикульчев Евгений Витальевич\nГЕОМЕТРИЧЕСКИЙ ПОДХОД\nК МОДЕЛИРОВАНИЮ НЕЛИНЕЙНЫХ СИСТЕМ\nПО ЭКСПЕРИМЕНТАЛЬНЫМ ДАННЫМ\nМонография\n\nПечатается с оригинал-макета,\nподготовленного автором\n\nКорректор Е. Е. Бушуева\n\nИзд. Лиц. ИД № 04640 от 26.04.01. Подписано в печать 01.11.2007.\nФормат 60x84 /16. Бумага офсетная. Гарнитура «Times New Roman».\nУсл. печ. л. 6,75. Тираж 500 экз. (1-й завод – 100 экз). Заказ № 371/315.\nМосковский государственный университет печати\n127550, Москва, ул. Прянишникова, 2а\nОтпечатано в Издательстве МГУП\n\n6\n\n\f"
        ],
        [
         "17",
         "17",
         "cs.CE",
         "Computational Engineering",
         "1302.5941v1.pdf",
         "Optimization of thermal comfort in building\nthrough envelope design\nMilorad Bojića, Alexandre Patou-Parvedyb, Harry Boyerc\na\n\nFaculty of Engineering,Kragujevac , Serbia, milorad.bojic@gmail.com\nFaculty of Engineering, Kragujevac, Serbia, parvedyalexandre@gmail.com,\nc\nLPBS, Equipe Physique et ingénierie mathématique appliquée à l’énergie et l’environnement, University of\nReunion Island, Reunion, France, Harry.Boyer@univ-reunion.fr\nb\n\nAbstract:\nDue to the current environmental situation, energy saving has become the leading drive in modern\nresearch. Although the residential houses in tropical climate do not use air conditioning to maintain\nthermal comfort in order to avoid use of electricity. As the thermal comfort is maintained by adequate\nenvelope composition and natural ventilation, this paper shows that it is possible to determine the\nthickness of envelope layers for which the best thermal comfort is obtained. The building is modeled in\nEnergyPlus software and HookeJeves optimization methodology. The investigated house is a typical\nresidential house one-storey high with five thermal zones located at Reunion Island, France. Three\n\noptimizations are performed such as the optimization of the thickness of the concrete block layer,\nof the wood layer, and that of the thermal insulation layer. The results show optimal thickness of\nthermal envelope layers that yield the maximum TC according to Fanger predicted mean vote.\n\nKeywords:\nEnergy efficiency, Thermal insulation, Hooke-Jeeves optimization, EnergyPlus.\n\n1. Introduction\nDue to the current environmental situation, energy saving has become the leading drive in\nmodern research. To limit electricity consumption in tropical regions, it is important to achieve\nthe optimal thermal comfort in residential houses without using electricity. It is important in\ntropical Reunion as in the last fifteen years, the consumption of energy has been multiplied by 2.\nReunion demography has significantly increasing and will continue to increase, and the energy\ndemand will depend on it.\nResearch in the optimization of thermal comfort was performed by Kurian et al [1], Magnier and\nHaghighat[2] , Chantrelle et al [3], and Stavrakakis et al[4]. Kurian et al. showed that dimming\nwith a fuzzy logic based window blind controller provides complete optimization of thermal\ncomfort in the interior with energy savings. To optimize thermal comfort, Magnier and\nHaghighat performed multiobjective optimization of building design HVAC system settings,\nthermostat programming, and passive solar design. Chantrelle et al developed of a multicriteria\ntool for optimizing the renovation of buildings where one of criteria was the optimized thermal\ncomfort. Stavrakakis et al optimized window-openings design for thermal comfort in naturally\nventilated buildings. However, the literature does not report the research on optimization of\n\n1\n\n\fthermal comfort by the adequate selection of width of layers in the building envelope\nconstruction.\nIn this research, the objective is to enhance the thermal comfort in a house located in Reunion\nIsland without using electricity for air conditioning. In this direction, an optimization of the\nthickness of house walls has been done. The optimized layers of the wall are the customary\nmaterials used in Reunion Island namely concrete block, thermal insulation (glass wool) and\nwood. Three optimizations are performed such as the optimization of the thickness of the\nconcrete block layer, of the wood layer, and that of the thermal insulation layer. In these\noptimizations, other two layers are kept at constant width. These optimizations yield the\nthickness of each layer of the wall for the best thermal comfort inside the house. The\noptimization is performed by programming at GenOpt code where thermal comfort of the entire\nhouse as an objective function is maximized. This code is coupled with an external simulation\nprogram (EnergyPlus).\n\n2. Description\n2.1 House\nIn this study, the residential house has a floor surface of 162 m². The house consists of 2\nbedrooms and, a kitchen, a living room and a toilet. A family of 4 peoples: 2 children and 2\nparents live in the house. For each room, a occupancy schedule is recorded. The parents work\nfrom 8 am to 6.30 pm. They spend their lunch break at the house from 12 am to 1.30 pm. During\nthe week, the children are at school from 8 am to 5.30 pm. For the weekend, the entire family\nstays at the house. The infiltration is assumed to be 0.5 ach throughout these optimizations.\n\n(a)\n\n(b)\n\nFig. 1 The investigated Reunion residential house: (a) entire house, b) cut through the house\n\nThe wall layers in this house are of customary materials used in the wall construction in Reunion\nIsland. From the outside to the inside, the first layer is of wood, the second of glass wool, and the\nthird of concrete block. Their thickness, thermal conductivity, density and specific heat are given\nin Table 1. During optimization, the thickness of each layer of the wall is varied in an interval\nfrom 0.001m to 1m.\n\n2\n\n\fThe concrete blocks are covered by a mortar layer of 1 mm and the wood by a varnish layer of\n0.1 mm. These two layers are not considered in these optimizations.\nReunion is a hot and humid tropical island that lies to 21° south latitude and 55° east longitude.\nReunion Island has a yearly mean temperature of between 26ºC to 27ºC and has high daytime\ntemperatures of 29°C to 34°C and relative humidity of 70% to 90% throughout the year.\nTable 1 Characteristics of the layers of the wall of the investigated house\nThickness\nk\n\n1\n2\n3\n\nWood\nGlass wool\nConcrete block\n\nm\n0.025\n0.025\n0.2032\n\nW/m-K\n0.15\n0.04\n1.11\n\nkg/m3\n608\n11\n800\n\nc\nJ/kg-m3\n1630\n800\n920\n\n2.2 Software\nThe optimization is made with GenOpt software. GenOpt is an optimization program for the\nminimization of a cost function that is evaluated by an external simulation program, such as\nEnergyPlus, TRNSYS, SPARK, IDA-ICE or DOE-2. It has been developed for optimization\nproblems where the cost function is computationally expensive and its derivatives are not\navailable or may not even exist. GenOpt can be coupled to any simulation program that reads its\ninput from text files and writes its output to text files. The independent variables can be\ncontinuous variables (possibly with lower and upper bounds), discrete variables, or both.\nConstraints on dependent variables can be implemented using penalty or barrier functions.\n2.3Thermal comfort\nThe thermal comfort climate is defined as ―that state of mind which expresses satisfaction with\nthe thermal environment‖ [5]. So, thermal comfort is very difficult to objectively quantify\nbecause it relies on a wide range of environmental and personal factors that decides on what will\nmake people feel thermally comfortable.\nThermal comfort is maintained when the heat generated by human metabolism is allowed to\ndissipate, thus maintaining thermal equilibrium with the surroundings. It has been long\nrecognized that the sensation of feeling hot or cold is not just dependent on air temperature\nalone.\nAt present, two different approaches for the definition of thermal comfort coexist, each one with\nits potentialities and limits: the rational or heat-balance approach and the adaptive approach. The\nrational approach uses data from climate chamber studies to support its theory, best characterized\nby the works of Fanger while the adaptive approach uses data from field studies of people in\nbuilding. Fanger’s Predicted Mean Vote (PMV) model was developed in the 1970’s from\nlaboratory and climate chamber studies. In these studies, participants were dressed in\nstandardised clothing and completed standardised activities, while exposed to different thermal\nenvironments. In some studies the researchers chose the thermal conditions, and participants\nrecorded how hot or cold they felt, using the seven-point ASHRAE thermal sensation scale\nshown in Table 2. In other studies, participants controlled the thermal environment themselves,\n\n3\n\n\fadjusting the temperature until they felt thermally ‘neutral’ (i.e. neither hot nor cold; equivalent to\nvoting ‘0’ on the ASHRAE thermal sensation scale).\n\nTable.2: Seven point thermal sensation scale [6]\n\nMaintaining this heat balance is the first condition for achieving a neutral thermal sensation.\nHowever, Fanger noted that ‗‗man‘s thermoregulatory system is quite effective and will\ntherefore create heat balance within wide limits of the environmental variables, even if comfort\ndoes not exist‘‘.\nTo be able to predict conditions where thermal neutrality would occur, Fanger investigated the\nbody‘s physiological processes when it is close to neutral.\nThat comfort equation was expanded using data from 1296 participants. The resulting equation\ndescribed thermal comfort as the imbalance between the actual heat flow from the body in a\ngiven thermal environment and the heat flow required for optimum (i.e. neutral) comfort for a\ngiven activity. This expanded equation related thermal conditions to the seven-point ASHRAE\nthermal sensation scale, and became known as the ‗‗Predicted Mean Vote‘‘ (PMV) index.\nThe PMV index suggested by Fanger predicts the mean response of a large group of people\naccording to the ASHRAE thermal sensation scale.\n2.4 Objective function\nThe objective function that was minimised is the total PMV given as\nPMVtotal =\nC main room PMVmain room + Ckitchen PMVkitchen+ Cbedroom1 PMVbedroom1 + Cbedroom2 PMVbedroom2 (8)\nwhere C main room = 0.4169 is the weight coefficient for the part of the time that the family spends\nin the main room during one year, Ckitchen = 0.3533 is the weight coefficient for the part of the\ntime that the family spends in the kitchen during one year, Cbedroom1 =0.1896 is the weight\ncoefficient for the part of the time that the family spends in the bedroom 1 during one year,\nCbedroom1 = 0.0402 is the weight coefficient for the part of the time that the family spends in the\nbedroom 2 during one year. The part of the time that family spends in a room is a ratio of the\ntime that the family spends in one room, and the total time the family spends in the entire house.\nThe PMV for each zone is the output of EnergyPlus for the modelled house.\n\n4\n\n\f3. Results and discussion\nThe purpose of this research is to improve (decrease) the PMV (the thermal comfort) in the\nhouse for the optimized house. Three optimization runs are performed to find the compositions\nof the wall that give the best thermal comfort. As the external wall has three layers, Three\noptimizations are performed such as the optimization of the thickness of the concrete block layer,\nof the wood layer, and that of the thermal insulation layer. Each optimization run gives the\noptimum width of different layer while keeping other two at the constant width.\n\n2.5\n\nPMV\n\n2\n1.5\n\nPMV total\n\n1\n\nPMV MR\n\n0.5\n\nPMV KIT\nPMV BR1\n\n0\n\nPMV BR2\n\nFig.3: Comparison of PMV between house no optimized and house with different layer optimized\n\nFigure 3 shows the total PMV and the PMV for each room in the house without optimization and\nthree houses with optimization of a wall layer. Generally, the house with the optimized wall\nlayer has a better PMV than the house without any optimization. The PMV of the house without\nany optimization is around 1.45. With the optimization, this value drops to about 0.5, by 34.5 %.\nThe house with the optimized block layer seems to be the best constructive solution. The total\nPMV is around 0.55 instead to of 1.45 for house without an optimization. Then, the PMV of each\nroom drops significantly to be around 0.5. In the main room (MR), the PMV is around 0.7\ninstead 1.15 for the house without optimization. This drop by 60.86 % gives a neutral sensation\nof thermal comfort for the inhabitant instead previous slight warm sensation. The most\nnoticeable drop in PMV is in the kitchen (KIT). In the house without optimization, the PMV is\naround 2.10 which gives a warm sensation of thermal comfort. After the optimization the PMV\ndrops to 0.82.\nThe house with the optimized wood layer is also a good constructive solution. In comparison\nwith the previous case, the PMV in the kitchen is a little bit higher and is around 0.85. For the\nrest of rooms, the PMV is around 0.5 that is similar to the previous case.\n\n5\n\n\fThe house with the optimized insulation layer is the worst solution. Even if the total PMV and\nthat for the others rooms yield to a neutral sensation of comfort, the PMV of the kitchen is 1.17\nmeaning that occupants in kitchen have slightly warm sensation of thermal comfort.\nFigure 4 shows the thickness of the wall layers for a house without and with three optimizations.\nThe house without optimization has the thickness of concrete blocks of 160 mm, of thermal\ninsulation (glass wool) of 25mm, and wood of 25mm. However the thickness of the wall varies\nfor each optimization run. For the house where the concrete block is optimized, the thickness of\nthe concrete block is 200 mm which is 40mm thicker than that for the house without\noptimization. The thickness of the entire wall is now 250 mm instead 210mm-the rise of 19%.\nFor the house where the wood is optimized, the wood layer thickness is 100 mm instead of\n25mm for the house without optimization. The wood layer is normally used as a decoration so\nthe optimized layer width seems to be very thick for the practical applications. In the house\nwhere the thermal insulation layer is optimized, the thermal insulation width is also 100 mm\ninstead 25mm. The total thickness of the wall in each case goes up. In these two cases of the\noptimization, the total thickness of the wall is 285 mm that is a rise of 36%.\n250\n\nThickness (mm)\n\n200\n150\nBlock\n100\n\nWood\nInsulation\n\n50\n0\nHouse\nHouse with House with House with\nwithout\noptimization optimization optimization\noptimization\nof block\nof wood\nof insulation\n\nFig.4: Thickness of the wall layers for a house with optimizations and without optimization\n\n3. Conclusion\nThe optimization of the thickness of the wall gives better thermal comfort during the time that\nthe family is in the house. The total PMV drops by 34.5% and the PMV of each room by about\n60%.\nThe thermal comfort is better when the wall is thicker. In fact in the initial house, the thickness\nof the wall was 210 mm and in the optimized house the wall is equal and over 250 mm.\nThe best case is when the concrete block is optimized. Then, the thickness of the wall rises by\n40mm (19%), the total PMV is around 0.55 and the PMV of each room gives a neutral sensation\nof comfort. The optimized thermal insulation gives a good PMV, however its thickness is\nincreased by 300%. The optimized wood layer give the worst results.\n6\n\n\fAcknowledgment\nThis paper is a result of two project investigations: (1) project TR33015 of Technological\nDevelopment of Republic of Serbia, and (2) project III 42006 of Integral and Interdisciplinary\ninvestigations of Republic of Serbia. The first project is titled ―Investigation and development of\nSerbian zero-net energy house‖, and the second project is titled ―Investigation and development\nof energy and ecological highly effective systems of poly-generation based on renewable energy\nsources. We would like to thank to the Ministry of Education and Science of Republic of Serbia\nfor their financial support during these investigations.\n\nReferences\n[1] Kurian CP, Aithal RS, Bhat J, George VI (2008a). Robust control and optimization of energy\nconsumption in daylight—Artificial light integrated schemes. Lighting Research and\nTechnology, 40(1): 7 – 2.\n[2] Laurent Magnier, Fariborz Haghighat, Multiobjective optimization of building design using\nTRNSYS simulations, genetic algorithm, and Artificial Neural Network, Building and\nEnvironment 45 (2010) 739–746\n[3] Fanny Pernodet Chantrelle, Hicham Lahmidi, Werner Keilholz, Mohamed El Mankibi, Pierre\nMichel, Development of a multicriteria tool for optimizing the renovation of buildings, Applied\nEnergy 88 (2011) 1386–139\n[4] G.M. Stavrakakis, P.L. Zervas, H. Sarimveis, N.C. Markatos, Optimization of windowopenings design for thermal comfort in naturally ventilated buildings, Applied Mathematical\nModelling, Volume 36, Issue 1, January 2012, Pages 193–211\n[5] Patrick Taffé, A Qualitative Response Model of Thermal Comfort, Building and\nEnvironment, Vol. 32, No. 2, pp. 115-121. 1997, PII: S0360-1323(96)00035-2.\n[6] 2009 ASHRAE HANDBOOK Fundamentals\n\n7\n\n\f"
        ],
        [
         "18",
         "18",
         "cs.CE",
         "Computational Engineering",
         "1505.06282v1.pdf",
         "Are we far from correctly inferring gene interaction\nnetworks with Lasso?\nFrancesco Gadaleta 1∗\n\narXiv:1505.06282v1 [cs.CE] 23 May 2015\n\n1\n\nUniversity of Leuven, Belgium\n\nABSTRACT\nDetecting the interactions of genetic compounds like genes, SNPs,\nproteins, metabolites, etc. can potentially unravel the mechanisms\nbehind complex traits and common genetic disorders. Several\nmethods have been taken into consideration for the analysis of\ndifferent types of genetic data, regression being one of the most\nwidely adopted. Without any doubt, a common data type is\nrepresented by gene expression profiles, from which gene regulatory\nnetworks have been inferred with different approaches. In this work\nwe review nine penalised regression methods applied to microarray\ndata to infer the topology of the network of interactions. We evaluate\neach method with respect to the complexity of biological data. We\nanalyse the limitations of each of them in order to suggest a number\nof precautions that should be considered to make their predictions\nmore significant and reliable.\nContact: francesco.gadaleta@gmail.com\n\n1 INTRODUCTION\nComplexity of biological systems is dictated by their interactions.\nIn the field of gene regulatory networks, detecting significant\ninteractions means understanding the biological mechanisms that\nregulate complex genetic disorders. Graphical models are a common\nmathematical abstraction that allow researchers to visualise those\ninteractions, detect groups of similar predictor variables, discover\npathways or assess the conditional dependency between covariates.\nAll this information is at the researcher’s disposal by means of\ngraphs, formed by nodes, edges and the connections between them\nvia the adjacency matrix. The values of each entry (i, j) in such a\nmatrix indicate the magnitude of the interaction between nodes i and\nj. There have been several attempts to analyse biological data with\ngraph theory. The current trends consist in regressing the phenotype\nof a number of individuals against their genetic profile or regressing\nclinical data to perform survival analysis. The types of data might\nbe heterogeneous and can include expression profiles, RNAseq data,\nSNPs, proteins or metabolites. In this work we refer to the study\nof genetic interaction networks and review some methods that have\nbeen specifically designed to infer the topology of such networks.\nWe refer to inferring network topologies with methods based on\npenalised regression from gene expression data.\nIn Section 1 we introduce L1−norm penalised regression. In\nSection 2 we provide a description of methods derived from it.\nIn Section 3 we discuss the performance of the aforementioned\nmethodologies and highlight the ones that perform the best on\nsynthetic datasets. Section 5 draws our conclusion, paving the way\n∗ to\n\nwhom correspondence should be addressed\n\nc Francesco Gadaleta 2014.\n\nto potential improvements in terms of accuracy and computational\nburden whenever dealing with real-life data.\n\n2 METHODS\nPenalised regression has been considered within several domains\nof computational biology. Some of these contributions include the\nanalysis of kidney cancer microarray data, regressed against survival\ntimes of each individual [36]. In such a context, a Lasso method has\nbeen applied to preconditioned response variables [22]. Penalised\nregression has been applied also in genome-wide association studies\nunder the name of Lasso logistic regression [32], in which main\neffects are analysed together with interaction effects for SNPs data,\nand hyper-lasso [12]. A contribution within the field of neuroscience\napplies Lasso regression to evaluate genetic effects with respect to\nbrain images, using MRI-derived temporal lobe volume measure as\nresponse variable [16]. Several attempts to improve the performance\nof the Lasso procedure led to a very efficient algorithm developed\nby Friedman et al. [7], called graphical lasso that maximises the\npenalised log-likelihood function through coordinate-descent. An\nattempt to detect gene-gene interactions with a combination of\nLasso and Principal Component Analysis is provided in [4]. In this\nspecific work we focus on the application of penalised regression\nmethods applied to variable selection and structure inference, with\nthe purpose of discovering the network topology that regulates the\ninteraction of genetic compounds. Within this context, seminal\nwork of Meinshausen et al. [19] paved the way to a simple yet\neffective approach that performs Lasso iteratively on each response\nvariable. According to their methodology, given the expression\nprofiles of individuals known to be affected by complex genetic trait,\neach gene is first considered as response and regressed against the\nremaining ones. The problem of iterative regression is translated\ninto the more intuitive one of neighbourhood selection: only the\ngenes that are directly associated to the response are selected and\ntheir coefficients estimated. These associations can be visualised as\na graph in which nodes and edges represent genes and interactions,\nrespectively. Nevertheless, a number of limitations affect such a\nprocedure, especially when it is applied to real life data. We will\naddress a number of such limitations in Section 2.2.\n\n2.1 L1-norm: the common form of penalised regression\nOne common form of penalised regression in computational biology\nis the Lasso procedure introduced by Tibshirani [26]. This procedure\nhas an attractive feature referred to as regularisation by L1-norm.\nGiven a n × p n-dimensional matrix X and a n-dimensional\nresponse vector Y , the Lasso estimate is given by\n\n1\n\n\fGadaleta et al\n\n1\nΘ̂a,λ = argmin ( kYi − XΘk22 + λkΘk1 )\ns. t. Θ:Θa =0 n\n\n(1)\n\nP\nwhere ||Θ||1 = j θj is the L1-norm of the coefficient vector.\nThe minimisation problem of Equation 1 finds the best T heta\nthat minimises the least squares with a penalty factor. A property of\nthe L1-norm of the minimisation problem above is that it tends to\nshrink the coefficients of a number of variables to zero. By doing so,\nit discards them from the set of selected variables associated to the\nresponse, making the model simpler. As a matter of fact, a simpler\nmodel is affected by smaller variance of the regressed coefficients, at\nthe cost of an increased bias of the predicted response. However, in a\nvariable selection procedure, a lower number of variables is usually\npreferred to a less biased prediction. In the context of microarray\nanalysis, the terms X and Y of Equation 1 correspond to the p\ngene expression profiles of n individuals and the expressions of the\nresponse gene of each individual, respectively. Correctly estimating\nthe shrinkage factor λ is critical and challenging. In fact, the value of\nλ directly determines the rate of false positives and false negatives of\nthe predictive model. A small penalty factor will allow many more\ngenes to be added to the model. In contrast, a larger λ will shrink\na higher number of θj to zero, resulting in the selection of fewer\ngenes as influential.\nOne of the main reasons for which Lasso is widely accepted in\nthe field of computational biology is because the shrinkage factor λ\nhas an intuitive counterpart in biological terms. The shrinkage factor\nfits relatively well with the widely accepted biological assumption\nthat a small number of main effects are associated to the response\n[20, 33]. Moreover, fewer genes make the model easier to interpret,\nwith respect to a model with a high number of degrees of freedom.\nUnfortunately, as we will explain later in this section, Lasso does not\nprovide consistent predictions within a large number of scenarios.\nTo begin with, regardless the convexity of the set of solutions\nprovided by Lasso, those are not necessarily unique. As we will\nsee, this is an issue more and more often neglected when analysing\nreal biological data with Lasso.\n\n2.2 Limitations of Lasso\nIn this section we explore some of the limitations of penalised\nregression applied to high dimensional data and more specifically\nto genetic data, such as gene expression profiles, gene methylation\ndata, SNPs, CNVs, etc.\nUniqueness of the solution One problem that arises when the\nnumber of predictors exceeds the number of observations, usually\nreferred to as p >> n problems, is that the Lasso criterion is not\nstrictly convex [28]. This fact leads to a fundamental consequence\nthat is not always taken into account during the analysis: the\noptimisation problem might not have a unique minimum.\nSpecifically, the L1-norm Lasso solution is unique only when\nrank(X) = p. If the rank(X) < p there can be more than one\nminimiser of the optimisation problem of Equation 1. This occurs\nwhenever there is sort of a structure within the data (and p = n) or\nwhenever the number of observations is higher than the number of\npredictors (p > n). Multiple solutions Θ that give the same fitted\nvalue ŷ = XΘ make it impossible to interpret the results of a Lasso\nregression. For the sake of completeness, what two different Lasso\nsolutions cannot do is to attach opposite signs to the coefficients of\n\n2\n\nthe same variable. An important finding reported in [28] consists\nin the fact that a unique solution exists with probability one only if\nthe predictors are drawn from a continuous probability distribution.\nMoreover, the uniqueness of solution occurs regardless of the sizes\nof n and p and the maximum number of selected predictors (the\nnonzero components) is min(n, p). It comes without saying that\nsuch a condition is rarely fulfilled in genetics, where data might\ncontain discrete variables or it might have been post-processed\nbefore regression.\nSignificance All procedures of the Lasso family lack of the usual\nconstructs to assess significance of estimated predictors, such\nas p-values or confidence intervals. One common approach to\nevaluate the significance of predictors relies on resampling and data\nsplitting. The major limitation addressed by such methods is the\nhigh computational burden, which becomes prohibitive for numbers\nof predictors that exceed 104 . The lack of a statistical significance\nprocedure for Lasso has been partially solved by a number of\nmethods such as the one described in [31] that estimates p-values in\nhigh dimensional models based on data splitting; two more methods\nthat derive confidence intervals for the regression coefficients are\ndescribed in [23] and [35]; a method called stability selection that\ncontrols false positives by resampling in the space of predictors\nhas been proposed in [18]; another method that constructs p-values\nof predictors starting from a ridge estimate and then corrects the\nprediction bias with Lasso has been proposed by [2]; and two\nmethods that give a simple statistic of Lasso coefficients without\nrelying on sampling nor splitting data, as described in [13] and [17].\nMulticollinearity occurs whenever gene expression profiles are\naffected by the presence of highly correlated predictors [29, 5].\nMulticollinearity can degrade the performance and the stability of\nregression estimates, giving rise to non-sensical results or incorrect\nmagnitude and sign of regression coefficients. When the number\nof predictors increases, such critical scenarios gain greater chances\nto occur. It is widely accepted that strong genetic correlations are\nfrequent in genetics (specifically in microarray datasets). Moreover,\ncomplete independence between gene expression measurements\nis rare [9]. The assumption of functionally related genes being\ncorrelated to each other is realistic. Therefore, it is expected that\nsuch genes might be co-expressed in the datasets at hand. We\nhave seen that Lasso procedures that are based on the L1-norm\ntend to shrink the number of significant predictors of the model.\nUnfortunately, such procedures also tend to select only one or a\nfew in a group of highly correlated predictors. Approaches that rely\non the L2-norm do not entirely solve the issue, since they select\nall or none of them, increasing false positive or false negatives,\nrespectively. In [10] it is shown that correlation within the data can\nconsistently influence the Lasso prediction. An important finding\nregards the relation between correlation and the shrinkage factor\nλ of Equation 1: high correlations tend to lead to smaller tuning\nparameters. The ability to optimally estimate λ by cross-validation\ndoes not hold anymore with the presence of highly correlated\nvariables.\nDeviation from normality The use of penalised regression to infer\ngraphical models of associations between predictor variables, has\nbecome increasingly popular after the work published in [7, 19, 6].\nThe core idea of such methods is to provide a solution to the variable\nselection problem by inferring a graph of conditionally dependent\n\n\fLasso for gene interaction networks\n\npredictors. When the complexity of data is also determined by\na phenomenon that statisticians call deviation from normality,\ninference and predictions can be significantly impacted by it.\nSpecifically to the problem of inferring a graph of interactions,\ncontamination of a number of variables can lead to a drastically\nwrong graph [6].\nDegrees of freedom As previously mentioned in Section 2, the main\npurpose of penalised regression methods is to reduce the variance\nof the estimated predictors while controlling the bias by minimising\nthe training error. The best performance is usually reached when an\noptimal compromise between error and degrees of freedom is found.\nOne should expect an increase of the error while decreasing the\nnumber of predictors. However, there are counter examples in which\nmore regularisation can, in fact, increase the degrees of freedom. In\nsuch cases the regularisation can raise both the error and the degrees\nof freedom. Examples for Lasso and ridge regression are provided\nin [15].\n\n2.3 Ridge Regression\nChanging the L1-norm to the L2-norm in the penalty term of\nEquation 1 is referred to as ridge regression or basis pursuit. The\nconvex optimisation problem to be solved is\n1\nΘ̂a,λ = argmin ( kYi − XΘk22 + λkΘk2 )\ns. t. Θ:Θa =0 n\n\n(2)\n\nThe L2-norm has the property of shrinking the regression\ncoefficients without performing selection. Therefore, the number\nof predictors initially included in the model will stay constant after\nregression. Ridge regression regularisation is performed to control\nthe variance of predictors, preventing their coefficients to grow\nindefinitely. The original motivation for the ridge penalty is to make\nthe problem of regression computable. As a matter of fact, the λ\nshrinkage factor can make the matrix X T X not invertible, making\nthe calculation of βλ = (X T X + λIp )−1 X T y not possible [11].\nThe ridge procedure is slightly easier to implement and faster to\ncompute than Lasso. Generally speaking, ridge regression is to be\npreferred whenever a high number of minor effects is a realistic\nhypothesis (even more so, if supported by expert knowledge). In\ncontrast, datasets with a small number of significant predictors\n(main effects) should be regressed with Lasso (L1-norm penalty).\nWhenever this information is available, the choice of the best\nprediction method is, therefore, straightforward.\nAs explained in Section 2.2 a phenomenon that is commonly\nobserved in computational biology is multicollinearity [29, 5],\nwhich leads to high variance of the estimator. Ridge regression deals\nwell with highly correlated predictors due to the fact that it will\nselect all of them or none. As a consequence, the mean squared\nerror (MSE) is usually lower than the one of a Lasso procedure.\nThis comes at the cost of including more predictor variables and\nconsequently making the model more complex. Ridge regression is\nbest indicated for those applications in which smaller variance is\npreferred, paying the cost of a more biased prediction. In contrast,\nin all those procedures that rely on permutation tests to improve the\nstability of the selected predictors, the ridge penalty is not the best\nchoice, as we will explain in Section 2.8.\nSome of the advantages of ridge regression used to discover\ngenetic interactions on simulated and real datasets are illustrated in\n\n[21]. The authors modified the hierarchy rule to add new predictors\nto the model, allowing an interaction term even in the case in which\none of the two genes is present with a strong individual effect.\nOf course, more complicated rules can be applied, such as those\ndescribed later in the section that presents hierarchical lasso. Within\nthe same work, a comparison with other tools specifically designed\nwith dimensionality reduction in mind is provided, showing that\nL2-norm penalties usually have reasonable predictive accuracy.\n\n2.4 Elastic Net\nA method that takes the benefits of both Lasso and Ridge penalties\nis referred to as Elastic Net. The optimisation problem to be solved\nin this case is\n1\nΘ̂a,λ = argmin ( kYi − XΘk22 + λ1 kΘk2 + λ2 kΘk2 ) (3)\ns. t. Θ:Θa =0 n\nElastic net is a method that enforces sparsity, due to the L1-norm\nwhile favouring the grouping effect of highly correlated predictors,\ndue to the L2-norm factor. The double shrinkage is more demanding\nin computational terms and more challenging to perform with\nrespect to a pure Lasso or Ridge regression procedure.\n\n2.5 Fused Lasso\nA generalisation of Lasso that has been designed specifically for\npredictor variables that can be ordered is referred to as fused\nLasso [27]. The core idea of fused Lasso consists in penalising the\ncoefficients of the single predictors, as in a regular Lasso procedure,\nwhile favouring the sparsity of their differences.\nThe optimisation problem to be solved is\np\np\nX\nX\n1\nkθj −θj−1 k)\nkθk1 +λ2\nΘ̂a,λ = argmin ( kYi −XΘk22 +λ1\ns. t. Θ:Θa =0 n\nj=2\nj=1\n\n(4)\nFused Lasso is particularly useful for cases in which the number\nof predictors is much larger than the number of observations (p >>\nn problems). By penalising the differences of adjacent predictors\nit is assumed that a limited number of covariates needs to be\nconsidered. One limitation of this approach is that the order of\nthe predictors should be set prior to the regression. Often this\ninformation is not available. However, an estimate can be computed\ndirectly from the data, i.e. by ordering genes via a correlation\nmetric and applying hierarchical clustering to mark predictors of\nthe same group as adjacent nodes of a graph. This strategy can be\nalso applied to the Group Lasso procedure described in the next\nsection. The presence of the double penalty factors λ1 and λ2\nrequires a more demanding cross-validation procedure, in order to\noptimally estimate both the parameters. Researchers have applied\nfused Lasso to synthetic and real data sets with number of predictors\nin the range between 102 to 4 × 104 . A direct comparison with L1norm Lasso shows that fused Lasso slightly contains the test error\nwhile improving overall sensitivity (true positive rate) [27]. In the\nsame simulation study of controlled predictors, fused Lasso does\nnot seem to improve the specificity (true negative rate), compared\nto the original Lasso procedure. Results from a real dataset of\nLeukemia microarray of 7129 genes show how fused Lasso reduces\n\n3\n\n\fGadaleta et al\n\nthe degrees of freedom of the model to 37 significant genes and\nperforms with the smallest test error, compared to other methods.\nHowever, an observation is needed to better frame the consistent\nimprovements of the fused Lasso solution. The initial 7125 genes\nhave been filtered down by a variance-based measure to 1000.\nHierarchical clustering applied to the filtered set of genes has been\nused to estimate the initial order. As a matter of fact, the pure\nfusion estimate, without any filtering, performs at the worst, as\nauthors show in the same work. Another limitation of fused Lasso is\ncomputational speed, which becomes less practicable for a number\nof predictors higher than 2000.\n\nL\n\nX\nX\n1\nΘ̂a,λ = argmin ( kYi −\nkΘij , Θji k2 )\nXl Θl k22 + λ1\ns. t. Θ:Θa =0 2\nj<i\nl=1\n\n(7)\nwith the diagonal elements Θii = 0. The overall performance\nof paired grouped lasso is reported as the best with respect to the\noriginal version of grouped lasso as well as the symmetric version.\n\n2.7 Hierarchical Lasso\n2.6 Group Lasso\nGroup Lasso, proposed in [34] considers the n−dimensional vector\nof responses Y and p predictors which are divided into L groups.\nThis version of Lasso solves the convex optimisation problem\n\nL\nL\nX\nX\n√\n1\nΘ̂a,λ = argmin ( kYi −\nnl kΘl k2 ) (5)\nXl Θl k22 + λ\n2\ns. t. Θ:Θa =0\nl=1\nl=1\n\n√\nwhere nl is the number of predictors per group and the nl\ntakes into account the group size. Group Lasso performs selection\nat the group level, namely an entire group of genes will be selected\nor discarded. It comes without saying that a critical aspect of the\nGroup Lasso consists in selecting the groups beforehand. This\ncomplicates the tuning even further compared to a regular Lasso\nin which only parameter λ needs to be estimated. Moreover, there\nis no cross-validation procedure to learn an optimal set of groups,\nmaking Group Lasso more challenging when this information is not\navailable. Another feature worth mention is that the Group lasso\nprocedure does not provide sparsity within the group due to the\nL2−norm in the penalty. However, sparsity can be re-established\nby another version of the penalty such as\n\nThe problem of detecting pairwise interactions between predictors\nhas received a lot of attention in recent years. When the number\nof predictors is large, the number of potential interactions grows\nexponentially with the order of the interaction itself. As\n\u0001 a recall, the\nnumber of k-order interactions from p predictors is kp .\nA strategy used by researchers in order to mitigate the curse\nof dimensionality, consists in testing the interactions of those\npredictors that show significant individual effects, discarding those\nthat do not. It turns out that choosing the correct threshold for main\neffects is a not-well-defined problem.\nJacob Bien et al. [1] provide a convex formulation that models\nmain effects and interactions together in hierarchical fashion.\nThe method is an extension of Lasso that incorporates pairwise\ninteractions in order to explain the cases in which 1) two or more\ngenes are expressed together and 2) their contribution to explaining\nthe response in not simply additive. Starting from a non-hierarchical\napproach, that they call all-pairs lasso, and that we report in\nEquation 8\n\nn\n\n1 X\n1\nλ\nΘ̂a,λ = argmin ( k\nYi −XiT β− XiT ΘXi k2 +λ1 kβk1 + kΘk1 )\n2\n2\n2\ns. t. Θ:Θa =0\ni=1\n\n(8)\nthey extend it by splitting the main effects βj as βj+ − βj− and by\nan additional constraint kΘj k1 ≤ βj+ + βj− .\nL\nL\nX\nX\n√\n1\na,λ\n2\nΘ̂\n= argmin ( kYi −\nXl Θl k2 +λ1\nnl kΘl k2 )+λ2 kΘk1 The former transformation replaces a non-convex version with a\ns. t. Θ:Θa =0 2\nconvex relaxation. The latter constraint emphasises the hierarchy\nl=1\nl=1\nof the interactions: the regression coefficient of the j-th interaction\n(6)\nis a lower bound of the main effects on predictor j. It is shown\nin which the L1−norm will shrink to zero predictors of the\nthat hierarchy favours models that tend to reuse measured variables.\nsame group. The choice of the sparsity factor requires to optimally\nIn a direct comparison between hierarchical lasso and all-pairs\nestimate an additional parameter λ2 , usually performed with crosslasso, it is shown that parameter sparsity, defined as the number of\nvalidation.\nnonzero parameters in the model, does not change between the two\nA relevant application of the Group Lasso approach has been\napproaches. On the other hand, practical sparsity, defined as the\nperformed by Friedman et al. in [14]. Starting from the work\nsmallest number of variables needed to make predictions, is always\ndescribed in [19], they propose a symmetrised version of it, which\nlower in hierarchical lasso. An illustrative example is shown in [1].\nthey call symmetric lasso. In addition, they adapt the original\nAs a conclusion, the degrees of freedom of the hierarchical model\nversion of the grouped lasso described in [34], in order to estimate\nis always lower than the ones introduced by Lasso. Regardless\nsparse graphical models. The penalty proposed by Friedman et al.\nthe useful simplification of the model, hierarchical lasso becomes\ngroups all of the edges connected to a given node, obtaining a graph\nprohibitive with a relatively high number of predictors. For instance,\nthat is sparse in its nodes, not in its edges. The convex optimisation\na network of 1000 genes, SNPs or proteins will give rise to 4995004\nproblem that they try to solve is like the one in Equation 5 with\npotential interactions. Considering that a network with 106 nodes is\ngroups of equal sizes. The minimisation of the loss and penalty\nconsidered only relatively large in biology, it comes without saying\nfunction is performed by means of block-wise coordinate descent.\nthat exploring all possible interactions is not feasible with such an\nAnother method based on Equation 5 is called paired grouped\napproach.\nlasso, given by the minimisation of\n\n4\n\n\fLasso for gene interaction networks\n\n2.8 Stability of gene regulatory networks\nInferring genetic interactions within a high dimensional context\nis a challenging task that often has to deal with the problem\nof sensitivity. As the number of predictors is increased the true\npositive rate of all Lasso-based methods explained so far decreases\nuntil it approaches a predictive performance comparable to random\nguessing. Several methods have been designed to deal with the issue\nof high dimensionality.\nThe work described in [25] performs a permutation-based\nprocedure to test marginal interactions with Lasso regression of a\nbinary response. The significance of each interaction is compared\nagainst a null distribution built with A permutations of the response\nvariable y and re-calculating a new set of p(p−1)\nstatistics. The\n2\nexpected number of false rejections is computed by taking the\naverage number of these statistics that lie above a cutoff value.\nThe authors call this procedure TMIcor, which stands for Testing\nMarginal Interactions correlation. When applied to real data of\nColitis gene expression profiles, the initial 22283 genes are filtered\nand only the genes on chromosomes 5 and 10 are considered, as\nthey are known to be related to Crohn’s disease. In total, only 663\ngenes and 219453 pairwise interactions are considered. Despite\nthe consistent reduction of the overall dimensionality, the authors\nclaim that TMIcor better controls False Discovery Rate, compared\nto logistic regression.\nAnother method that leverages the strengths of penalised\nregression for sparse network structures is described in [30].\nInspired by the node wise lasso approach of [19], they use a Huber or\nLeast Absolute Deviation (LAD) loss function together with an L1norm penalty to encourage sparsity. A number of approaches, such\nas tLasso, GLasso, Adalasso, AdaLAD, AdaHuber and Copula,\nare compared in a simulation study with a number of predictors\nup to p = 100. However, a performance evaluation on real data\nis facilitated by the fact that the authors restrict their attention to\n8 genes already known to be associated to the regulation under\nstudy. Another result is collected by performing the list of Lassobased methods on a dataset with different degrees of contamination.\nSpecifically, the data are generated by a N (0, Θ) distribution and\nthe contamination occurs by using a N (µ, 0.2) distribution for\ndifferent numbers of predictors. Interestingly, the Copula method\nshows stable performance across various degrees of contamination.\nThe authors of LABNet [8] present a Lasso-based approach\nto detect main genetic interactions from gene expression profiles.\nThey leverage penalised regression together with a permutationbased procedure that determines whether the predicted interactions\nare stable across experiments. It is shown that the higher number\nof permutations not only improves the sensitivity of the method\nby reducing the number of false negatives, but it also affects the\noverall number of predicted edges. Unfortunately, a high number\nof permutations has high computational burden, making the method\nprohibitive for genome wide studies.\nA more robust graphical model of gene networks is achieved\nby using classical and alternative T −distributions in [6]. In their\nwork, the authors demonstrate that penalised likelihood inference\ncombined with an application of the EM algorithm provides a\ncomputationally efficient approach to model selection in the tdistribution case.\nRegardless of the impossibility for ridge regression procedures to\nperform variable selection, they have been considered to estimate a\n\nregression coefficient for each predictor variable. The purpose of the\nauthors of [3] consists in obtaining an estimate of the significance\nof each ridge regression coefficient. Specifically they develop and\nevaluate a test of significance for ridge regression coefficients.\nUsing simulation studies, they demonstrate that the performance\nof the test is comparable to that of a permutation test, with the\nadvantage of reduced computational cost. The p-value trace is\nplot for several values of the shrinkage parameter, providing an\nimmediate evaluation of both the estimated λ and the significance\nof the predictors.\n\n3 A COMPARISON OF PENALISED REGRESSION\nMETHODS ON KNOWN GENE-INTERACTION\nNETWORKS\nIn order to evaluate benefits and drawbacks of the methods reported\nthus far, we sampled synthetic microarrays of 15, 50, 100 and 200\ngenes each from subnetworks created from a template of E.Coli\nbacterium. Therefore, we applied the nine penalised regression\nmethods described in Section 2 to the same datasets and inferred\nthe networks of genetic interactions. In order to infer such networks\nwe followed the strategy used in [19], being so far one of the\nmost intuitive approaches. We subsequently compared each inferred\nnetworks to the true network at our disposal. Gene Net Weaver [24]\nhas been used to generate both the gold standard and datasets. By\ndoing so, the comparison of inferred and true networks becomes\nstraightforward.\nAll the methods described thus far have been applied directly\nto the datasets at hand, with the exception of Group Lasso and\nFused Lasso. As previously mentioned these two methods require\nthe predictor variables to be ordered according to some criterium in\norder to apply the convex optimisation described by Equation 5 and\nEquation 4. In both cases, we group all predictors by a correlation\nmetric. Specifically, for each regression iteration, we build the\ncorrelation matrix C of the p − 1 predictors. We then perform\nhierarchical clustering on C, and generate k clusters by using\nthe Euclidean distance measure. The optimal number of clusters\nhas been empirically estimated to k = 3 for Group Lasso and\nk = 10 for Fused Lasso. Building the vector of grouped predictors\ndirectly from the output of the hierarchical clustering procedure\nis straightforward. Finally, we use a 10-fold cross-validation to\nestimate the penalty factor λ. We are aware of the fact that a\nbetter grouping metric might exist. However, without any prior\nknowledge, this metric is challenging to obtain or infer. Performance\nbenchmarks with timing measures are reported in Table 1, Table 3\nand Table 3.\nThe number of predicted edges has been normalised across\nall methods with a quantile-based selection that filters out small\nregression coefficients. This normalisation procedure allows all the\nmethods to predict a comparable number of edges.\nThe method that performs the best in the 15-gene network is\nLABnet [8], a mixture of L1−norm Lasso and permutation test\nthat increases the stability of the inferred topology. The number\nof predicted edges of LABnet is also one of the lowest, improving\nits overall performance, measured by the Matthew Correlation\nCoefficient. As expected and already mentioned by its authors,\nthe permutation test of LABnet is heavily detrimental with ridge\n\n5\n\n\fGadaleta et al\n\nTable 1. Performance of all penalised-regression based methods inferring a network of 15 genes. True indicates the number of edges in the gold standard\nnetwork; Pred are the edges of the predicted network; TP, FP, TN and FN stand for true positives, false positive, true negatives and false negatives, respectively;\nMCC is the Matthew Correlation Coefficient; TPR, FPR and ACC are the true positive rate, false positive rate and accuracy; Time is the amount of seconds to\nperform the computation.\n\nfused\nhier\ngroup\nLABnet\nridge perm\nenet perm\nlasso\nridge\nenet\n\nTrue\n13\n13\n13\n13\n13\n13\n13\n13\n13\n\nPred\n24\n24\n24\n24\n0\n24\n24\n24\n24\n\nTP\n0\n3\n1\n4\n0\n2\n0\n0\n0\n\nFP\n24\n21\n23\n20\n0\n22\n24\n24\n24\n\nTN\n188\n191\n189\n192\n212\n190\n188\n188\n188\n\nFN\n13\n10\n12\n9\n13\n11\n13\n13\n13\n\nMCC\n0\n0.099\n0\n0.16\nNA\n0.037\n0\n0\n0\n\nTPR\n0\n0.23\n0.08\n0.31\n0\n0.15\n0\n0\n0\n\nFPR\n0.11\n0.09\n0.10\n0.09\n0\n0.10\n0.11\n0.11\n0.11\n\nACC\n0.83\n0.86\n0.84\n0.87\n0.94\n0.85\n0.83\n0.83\n0.83\n\nTime [sec]\n1.17\n7.55\n1.51\n12.6\n12.20\n12.87\n0.17\n0.16\n0.14\n\nTable 2. Performance of all penalised-regression based methods inferring a network of 50 genes. Same acronyms as in Table 1\n\nfused\nhier\ngroup\nLABnet\nridge perm\nenet perm\nlasso\nridge\noneenet\n\nTrue\n48\n48\n48\n48\n48\n48\n48\n48\n48\n\nPred\n252\n224\n414\n98\n0\n86\n254\n254\n254\n\nTP\n5\n21\n21\n16\n0\n10\n8\n8\n8\n\nFP\n247\n203\n393\n82\n0\n76\n246\n246\n246\n\nTN\n2205\n2249\n2059\n2370\n2452\n2376\n2206\n2206\n2206\n\nFN\n43\n27\n27\n32\n48\n38\n40\n40\n40\n\nMCC\n0.001\n0.170\n0.102\n0.212\nNA\n0.133\n0.030\n0.030\n0.030\n\nTPR\n0.10\n0.4375\n0.4375\n0.33\n0\n0.20\n0.16\n0.16\n0.16\n\nFPR\n0.100\n0.08\n0.16\n0.03\n0\n0.030\n0.10\n0.10\n0.10\n\nACC\n0.884\n0.908\n0.832\n0.9544\n0.9808\n0.9544\n0.8856\n0.8856\n0.8856\n\nTime[sec]\n177.66\n544.71\n17.84\n206.2\n202.60\n201.93\n1.345\n1.28\n1.52\n\nregression and slightly less with elastic net due to the fact that\nL2-norm penalties do not shrink to zero the regression coefficients.\nLABnet performs with the highest MCC in the 50-gene dataset\ntoo. Hierarchical Lasso performs equally like with a computational\noverhead 2x as larger. This is due to the fact that hierarchical\nLasso performs regression on p2 predictor variables (it considers\nall pairwise interactions).\nLABnet is also the best method in the 200-gene network with\nM CC = 0.349. Hierarchical Lasso has comparable performance\nwith execution time 20x as higher than LABnet. As expected the\ncomputational burden of hierarchical Lasso is exponential in the\nnumber of predictors. Regardless the undeniable computational\nburden required by hierarchical Lasso to regress a quadratic number\nof covariates, hierNet - the R package that implements it - has\nnot been designed with parallelisation in mind. On the other side,\nLABnet takes advantage of multi core processor to parallelise the\npermutation-based procedure.\n\nbeen applied to genetic datasets with a limited number of covariates.\nOur primary goal is to provide an unbiased comparison among all\nthe methods under investigation. We are aware that preselecting\nvariables might improve the stability and the overall performance\nin terms of prediction and reduce computation time. Preselection,\nhowever, is an open problem in genetics and it can lead to complete\nremoval of detectable signals or biased results, depending on the\nstrength of the preselection filter. Even in the case of moderate\ncorrelation, quite rare in biology, applying penalised regression\nwithin a high dimensional context might become prohibitive.\nAll permutation-based procedures impose a computational burden\nthat makes them impossible to be considered for real world\ndatasets (number of predictors approaching 109 ). The limitations of\npenalised regression methods suggest that constraining the problem\nof variable selection with diverse datasets or with prior knowledge\ncan reduce the variance of the predictions and, in turn, increase their\nsignificance.\n\n4 DISCUSSION\n\n5 CONCLUSION\n\nDespite active research in the field of genetic interaction networks\nby penalised regression methods, many limitations still need to be\naddressed. More sophisticated solutions need to be provided when\ndealing with high dimensional data and highly correlated variables.\nMulticollinearity is a recurrent problem in genetics and, according\nto the relatively poor performance of the methods reported thus\nfar, simply applying penalised regression does not seem to provide\nacceptable solutions. All the methods reported in this review have\n\nWe provided a review of nine penalised regression methods applied\nto gene expression data to infer the topology of the network of\ngene-gene interactions. The different types of penalties are indicated\nwithin diverse contexts, according to initial hypotheses of strong\npresence of main effects rather than weak interactions. We found a\nlimitation that is common to all approaches and that regards high\ndimensionality and multicollinearity within the datasets at hand.\nNone of the methods described seem to deal with both at the same\n\n6\n\n\fLasso for gene interaction networks\n\nTable 3. Performance of all penalised-regression based methods inferring a network of 200 genes. Same acronyms as in Table 1\n\nfused\nhier\ngroup\nLABnet\nlasso\nridge\nenet\n\nTrue\n212\n212\n212\n212\n212\n212\n212\n\nPred\n398\n428\n432\n398\n400\n400\n400\n\nTP\n76\n95\n3\n103\n38\n38\n38\n\nFP\n322\n333\n429\n295\n362\n362\n362\n\nTN\n39466\n39455\n9670\n39493\n39426\n39426\n39426\n\ntime. Due to the nature of genetic data - high number of highly\ncorrelated variables - we suggest to consider penalised regression\nonly in more constrained problems, where prior knowledge and\ndata integration play a fundamental role. As a consequence, more\nsophisticated regression-based approaches need to be designed to\nmake the prediction of gene regulatory networks more reliable.\nFunding:\n\nREFERENCES\n[1]J. Bien, J. Taylor, and R. Tibshirani. A lasso for hierarchical interactions. Appl\nStat, 41(3):1111–1141, 2013.\n[2]P. Bhlmann. Statistical significance in high-dimensional linear models. Bernoulli,\n19(4):1212–1242, 09 2013.\n[3]E. Cule, P. Vineis, and M. De Iorio. Significance testing in ridge regression for\ngenetic data. BMC Bioinformatics, 12(1):372, 2011.\n[4]G. M. D’Angelo, D. Rao, and C. C. Gu. Combining least absolute shrinkage\nand selection operator (lasso) and principal-components analysis for detection of\ngene-gene interactions in genome-wide association studies. BMC Proc., 2009.\n[5]D. Farrar and R. Glauber. Multicollinearity in Regression Analysis: The Problem\nRevisited. Working paper (Sloan School of Management). M.I.T., 1964.\n[6]M. Finegold and M. Drton. Robust graphical modeling of gene networks\nusing classical and alternative T-distributions. The Annals of Applied Statistics,\n5(2A):1057–1080, Aug. 2011.\n[7]J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation\nwith the graphical lasso. Biostatistics, 9(3):432–441, July 2008.\n[8]F. Gadaleta and K. Van Steen. Discovering main genetic interactions with labnet\nlasso-based network inference. PLoS ONE, 9(11):e110451, 11 2014.\n[9]J. J. Goeman and P. Bühlmann. Analyzing gene expression data in terms of gene\nsets: methodological issues. Bioinformatics, 23(8):980–987, Apr. 2007.\n[10]M. Hebiri and J. C. Lederer. How correlations influence lasso prediction. 2012.\n[11]A. E. Hoerl and R. W. Kennard. Ridge regression: Biased estimation for\nnonorthogonal problems. Technometrics, 12:55–67, 1970.\n[12]C. J. Hoggart, J. C. Whittaker, M. De Iorio, and D. J. Balding. Simultaneous\nanalysis of all snps in genome-wide and re-sequencing association studies. PLoS\nGenet, 4(7):e1000130, 07 2008.\n[13]A. Javanmard and A. Montanari. Hypothesis testing in high-dimensional\nregression under the gaussian random design model: Asymptotic theory. CoRR,\nabs/1301.4240, 2013.\n[14]F. Jerome, H. Trevor, and T. Robert. Applications of the lasso and grouped lasso\nto the estimation of sparse graphical models. Technical report, 2010.\n[15]S. Kaufman and S. Rosset. When does more regularization imply fewer degrees\nof freedom? sufficient conditions and counter examples from lasso and ridge\nregression. arXiv: 1311.2791, 2013.\n[16]O. Kohannim, D. P. Hibar, J. L. Stein, N. Jahanshad, X. Hua, P. Rajagopalan,\nA. Toga, C. R. Jack Jr, M. W. Weiner, G. I. de Zubicaray, K. L. McMahon,\nN. K. Hansell, N. G. Martin, M. J. Wright, and P. M. Thompson. Discovery and\n\nFN\n136\n117\n78\n109\n174\n174\n174\n\nMCC\n0.256\n0.310\n0.046\n0.349\n0.124\n0.124\n0.124\n\nTPR\n0.358\n0.448\n0.103\n0.485\n0.179\n0.179\n0.179\n\nFPR\n0.008\n0.008\n0.024\n0.007\n0.009\n0.009\n0.009\n\nACC\n0.98\n0.988\n0.96\n0.98\n0.986\n0.986\n0.986\n\nTime[sec]\n10761\n72000\n734\n3650\n43.5\n48.2\n50.4\n\nreplication of gene influences on brain structure using lasso regression. Frontiers\nin Neuroscience, 6(115), 2012.\n[17]R. Lockhart, J. Taylor, R. J. Tibshirani, and R. Tibshirani. A significance test for\nthe lasso. Technical report, arXiv, 2013.\n[18]N. Meinshausen and P. Bühlmann. Stability selection. Journal of the Royal\nStatistical Society B, 72(4):417–473, 2010.\n[19]N. Meinshausen and P. Bhlmann. High dimensional graphs and variable selection\nwith the lasso. ANNALS OF STATISTICS, 34(3):1436–1462, 2006.\n[20]P. Michalak. Coexpression, coregulation, and cofunctionality of neighboring\ngenes in eukaryotic genomes. Genomics, 91(3):243 – 248, 2008.\n[21]M. Y. Park and T. Hastie. Penalized logistic regression for detecting gene\ninteractions. Biostatistics, 9(1):30–50, 2008.\n[22]D. Paul, E. Bair, T. Hastie, and R. Tibshirani. ?preconditioning? for feature\nselection and regression in high-dimensional problems. The Annals of Statistics,\n36(4):1595–1618, 08 2008.\n[23]B. M. Ptscher and U. Schneider. Confidence sets based on penalized maximum\nlikelihood estimators in gaussian regression. Electronic Journal of Statistics,\n4:334–360, 2010.\n[24]T. Schaffter, D. Marbach, and D. Floreano.\nGeneNetWeaver: In silico\nbenchmark generation and performance profiling of network inference methods.\nBioinformatics, 27(16):2263–2270, 2011.\n[25]N. Simon and R. Tibshirani. A Permutation Approach to Testing Interactions in\nMany Dimensions. ArXiv e-prints, June 2012.\n[26]R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the\nRoyal Statistical Society, Series B, 58:267–288, 1994.\n[27]R. Tibshirani, M. Saunders, S. Rosset, J. Zhu, and K. Knight. Sparsity and\nsmoothness via the fused lasso. Journal of the Royal Statistical Society Series\nB, pages 91–108, 2005.\n[28]R. J. Tibshirani. The lasso problem and uniqueness. Electronic Journal of\nStatistics, 7(0):1456–1490, 2013.\n[29]K. Van Steen, D. Curran, J. Kramer, G. Molenberghs, A. Van Vreckem,\nA. Bottomley, and R. Sylvester. Multicollinearity in prognostic factor analyses\nusing the eortc qlq-c30: identification and impact on model selection. Statistics in\nMedicine, 21(24):3865–3884, 2002.\n[30]V. Vinciotti and H. Hashem. Robust methods for inferring sparse network\nstructures. Computational Statistics and Data Analysis, 67(C):84–94, 2013.\n[31]L. Wasserman and K. Roeder. High dimensional variable selection. The Annals\nof Statistics, 37(5A):2178–2201, 2009.\n[32]T. T. Wu, Y. F. Chen, T. Hastie, E. Sobel, and K. Lange. Lange k: Genomewide\nassociation analysis by lasso penalized logistic regression. Bioinformatics.\n[33]G. Yi, S.-H. Sze, and M. R. Thon. Identifying clusters of functionally related\ngenes in genomes. Bioinformatics, 23(9):1053–1060, 2007.\n[34]M. Yuan, M. Yuan, Y. Lin, and Y. Lin. Model selection and estimation in\nregression with grouped variables. Journal of the Royal Statistical Society, Series\nB, 68:49–67, 2006.\n[35]C.-H. Zhang and S. Zhang. Confidence intervals for low-dimensional parameters\nwith high- dimensional data. arXiv: 1110.2563, 2011.\n[36]H. Zhao, B. Ljungberg, K. Grankvist, T. Rasmuson, R. Tibshirani, and J. D.\nBrooks. Gene Expression Profiling Predicts Survival in Conventional Renal Cell\nCarcinoma. PLoS Medicine, 3(1):e13+, Dec. 2005.\n\n7\n\n\f"
        ],
        [
         "19",
         "19",
         "cs.CE",
         "Computational Engineering",
         "0504378v2.pdf",
         "arXiv:math/0504378v2 [math.PR] 28 Sep 2005\n\nA Short Proof that Phylogenetic Tree Reconstruction\nby Maximum Likelihood is Hard\nSébastien Roch\nDepartment of Statistics\nUniversity of California, Berkeley\nFebruary 1, 2008\n\nAbstract\nMaximum likelihood is one of the most widely used techniques to infer evolutionary histories.\nAlthough it is thought to be intractable, a proof of its hardness has been lacking. Here, we give\na short proof that computing the maximum likelihood tree is NP-hard by exploiting a connection\nbetween likelihood and parsimony observed by Tuffley and Steel.\n\n1\n\nIntroduction\n\nIn a series of seminal works, Edwards and Cavalli-Sforza [9], Neyman [15], and Felsenstein [11] applied the maximum likelihood methodology to the problem of inferring phylogenies from molecular\nsequences. Since, the many variants of this approach have gained increasing popularity in the systematics literature. This is due in part to the flexibility of the technique in accomodating a variety of\nmodels of evolution as well as to good practical performance. Nevertheless, the approach is not without\na flaw: it has been observed to be highly demanding computationally. Remarkably, the computational\ncomplexity status of the problem has remained elusive. Partial progress was made recently in [1] where\na variant of the problem, known as Ancestral Maximum Likelihood, was shown to be NP-hard. Here,\nwe resolve the issue by proving that computing the maximum likelihood tree is NP-hard. Moreover,\nwe show that the log-likelihood is NP-hard to approximate within a constant ratio. Our proof—which\nis mostly elementary—combines a connection between likelihood and parsimony observed by Tuffley\nand Steel [17] with a result on the hardness of approximating parsimony obtained by Wareham [18].\nGeneral references on inferring phylogenies are [12, 16]. For background on NP-completeness and\nhardness of approximation, refer to [14, 3]. Many other popular phylogenetic techniques have been\nshown to be NP-hard, including parsimony [13, 7], compatibility [8], and distance-based methods [2].\nRemark. While writing this paper, Mike Steel brought to our attention that Benny Chor and\nTamir Tuller have recently given an independent proof of this result which now appears in the proceedings of RECOMB 2005 [5]. Similarly to the proof presented here, the Chor-Tuller paper uses\nresults from [17] and the hardness of approximating vertex cover (from which follows the hardness of\napproximating parsimony), but their argument proceeds from a sequence of rather involved constructions. Our reduction has the advantage of being short and elementary. It also sheds some more light\non the interesting connection between likelihood and parsimony.\n\n2\n\nDefinitions and Results\n\nThe use of maximum likelihood requires the choice of a statistical model of evolution. Here, we consider\nthe simple binary symmetric model generally known as the Cavender-Farris model [4, 10]. We are given\n1\n\n\fa tree T on n leaves and probabilities of transition on edges p = {pe }e∈E(T ) ∈ [0, 1/2]ET , where E(T )\nis the set of edges of T and ET ≡ |E(T )| is the cardinality of E(T ). (All trees considered here have\nno internal vertex of degree 2.) A realization of the model is obtained as follows: choose any vertex\nas a root; pick a state for the root uniformly at random in {0, 1}; moving away from the root, each\nedge e flips the state of its ancestor with probability pe . Let [n] denote the set of leaves. A character\nχ assigns to each leaf a state in {0, 1}. An extension of χ is an assignment of states in {0, 1} to all\nvertices of T which is equal to χ on the leaves. The set of all extensions of χ is denoted H(χ).\nIn the Cavender-Farris model, the (modified) log-likelihood of χ on (T, p) is\n\n\nX\nY\ne T, p) ≡ − ln 2P[χ | T, p] = − ln \n(1)\np1e {χ̂(u)6=χ̂(v)} (1 − pe )1{χ̂(u)=χ̂(v)}  ,\nL(χ;\nχ̂∈H(χ) e=(u,v)∈E(T )\n\nwhere 1{A} is 1 if A occurs, and 0 otherwise. The data consists of a set of characters X = {χi }ki=1 .\nAssuming the characters are independent and identically distributed,\nPktheelog-likelihood of the data\ne\nis the sum of the log-likelihood of all characters, viz. L(X; T, p) = i=1 L(χi ; T, p). The maximum\ne\nlikelihood (ML) problem consists in computing (T ∗ , p∗ ) minimizing L(X;\nT, p) over all trees and transition probability vectors. Note that this is equivalent to minimizing the standard likelihood, without\nthe factor of 2.\nContrary to ML, maximum parsimony (MP) is not based on a statistical model. Denote by ch(χ̂)\nthe number of flips in χ̂, i.e. ch(χ̂) = |{(u, v) ∈ E(T ) : χ̂(u) 6= χ̂(v)}|. Let l(χ, T ) be the smallest\nnumber of flips in any extension of χ on T , i.e. l(χ, T ) = minχ̂∈H(χ) ch(χ̂). The parsimony score of T\nP\nis then l(X, T ) = ki=1 l(χi , T ). The problem MP consists in finding the tree T ∗∗ minimizing l(X, T )\nover all trees.\nA useful connection between ML and MP was noted by Tuffley and Steel in [17]: if one adds\nsufficiently many constant sites (i.e. χ(i) = α, ∀i ∈ [n] for some α ∈ {0, 1}) to the data and applies\nthe maximum likelihood technique, then one necessarily chooses the most parsimonious tree. This\ncould serve as the basis for a reduction, except that the Tuffley-Steel bounds require an exponential\nnumber of constant sites. Our contribution is to show that a polynomial number of sites imposes\na weaker relationship between likelihood and parsimony, but that this is sufficient for the following\nreason. Parsimony is in fact hard to approximate, that is, even the seemingly easier task of obtaining\na solution close to optimal is hard. This result is due to Wareham [18].\nWe prove the following theorem. We first define the notion of approximation algorithm.\nDefinition 1 Let Π be an optimization problem (minimization). Let I denote an instance of Π and\nOPT(I), the optimal value of a solution to I. For c > 0, a (1 + c)-approximation algorithm for Π is\na polynomial-time algorithm that is guaranteed to return, for any instance I, a solution with objective\nvalue m satisfying m ≤ (1 + c)OPT(I).\nTheorem 1 There exists a c > 0 sufficiently small so that there is no (1 + c)-approximation algorithm\nfor ML unless P = NP. In particular, ML is NP-hard. (The approximation claim relates to the\ne\nmodified log-likelihood L.)\n\n3\n\nProof\n\nIn this section, we prove our main result. The proof follows easily from the following propositions. The\nfirst proposition borrows heavily from [17] although we need somewhat tighter estimates. The second\nproposition follows directly from the work of [18, 6].\nProposition 1 Let c′ > c > 0 be constants. If there is a (1 + c)-approximation algorithm for ML then\nthere is a (1 + c′ )-approximation algorithm for MP.\n2\n\n\fProposition 2 ([18, 6]) There exists a c′ > 0 sufficiently small so that there is no (1+c′ )-approximation\nalgorithm for MP unless P = NP.\nAs in [17], the reduction from MP to ML consists in adjoining a large number of constant sites\nto the data. Let ε > 0 be a small constant and M = max{2n, k}. Fix Nc = M 1/ε . Denote by\nc\nX0 = {χi }k+N\nthe set X augmented with Nc all-0 characters. For all χ, let Nχ be the number of\ni=1\ncharacters equal to χ in X. To avoid the factors of 2 from the probability of the root state, we define\ne 0 | p, T ] = 2k+Nc P[X0 | p, T ] for p ∈ [0, 1/2]ET . Also, let f˜χ = 2P[χ | p, T ]. Let 0 be the all-0\nP[X\ncharacter and 1, the all-1 character. We make three claims, from which Proposition 1 follows.\nClaim 1 Let ε > 0 and Nc = M 1/ε for M = max{2n, k}. Let pe = q =\nThen\n\ne\nP[X0 | p,T ]\n− lnln(k+N\nc)\n\nl(X,T )\nET (k+Nc ) ,\n\nfor all e ∈ E(T ).\n\n≤ (1 + 2ε)l(X, T ), for M large enough.\n\nProof. Note that, by a calculation identical to [17, Lemma 5],\n\n\n\u0011\n\u0010\nX\n\u0001\nq ch(χ̂) (1 − q)ET −ch(χ̂)  ≥ ln q l(χ,T ) (1 − q)ET ≥ l(χ, T ) ln q − ET q + 2q 2 ,\nln f˜χ = ln \nχ̂∈H(χ)\n\nwhere we have used a standard Taylor expansion (note that we have q ≤ 1/2 by definition). This\nbound applies in particular to the case χ = 0. Then, as in [17, Lemma 5] again,\n\n\nY Nχ\ne 0 | p, T ]\nln P[X\n1\n−\nf˜χ \n= −\nln f˜0N0 +Nc\nln(k + Nc )\nln(k + Nc )\nχ6=0\n\n\u0001\n1\n(k + Nc )ET (q + 2q 2 ) − l(X, T ) ln q\nln(k + Nc )\n\u0012\n\u0012\n\u0013\u0013\n1\nln ET − ln l(X, T )\nl(X, T )\n+\n= l(X, T ) 1 +\n1+2\nln(k + Nc )\nln(k + Nc )\nET (k + Nc )\n\u0012\n\u0013\u0013\n\u0012\n1\nM\nln M\n+\n1 + 2 1/ε\n≤ l(X, T ) 1 +\n1/ε\n1/ε\nln M\nln M\nM\n≤ (1 + 2ε)l(X, T ),\n≤\n\ne 0 | p, T ] ≤ 1 because P[χ | p, T ] ≤ 1/2 by symmetry.) \u0004\nfor M large enough. (Note that P[X\ne\n\nP[X0 | p,T ]\nClaim 2 For all p ∈ [0, 1/2]ET such that − lnln(k+N\n≤ l(X, T ), one has pe ≤ p̄, ∀e ∈ E(T ), with\nc)\n\np̄ ≡\n\nl(X,T ) ln(k+Nc )\n.\nNc\n\nProof. Assume edge e is such that pe > p̄. Take any two leaves u, v joined by a path going through\ne. As observed in [17, Formula (11)], the probability that χ(u) 6= χ(v) is at least pe . In particular, the\nprobability that a character is constant is less than 1 − pe and − ln f˜0 ≥ − ln(1 − pe ) ≥ pe (by the 0 − 1\nsymmetry). Therefore,\n\n\n\u0010\n\u0011\nY\ne\nln P[X0 | p, T ]\n1\n1\nNχ \nN0 +Nc\nNc\n˜\n˜\n˜\n\n−\nln f0\nfχ\n> l(X, T ),\n=−\nln f0\n≥−\nln(k + Nc )\nln(k + Nc )\nln(k + Nc )\nχ6=0\n\nby pe > p̄ and f˜χ ≤ 1 (by the 0 − 1 symmetry). This contradicts the assumption. \u0004\n\ne\n\nP[X0 | p,T ]\n≥\nClaim 3 Let ε > 0 and Nc = M 1/ε for M = max{2n, k}. For all p ∈ [0, 1/2]ET , we have − lnln(k+N\nc)\n(1 − 5ε)l(X, T ) for M large enough.\n\n3\n\n\fProof. For this proof, we need a better estimate than [17, Lemma 6]. From Claim 2, the result holds\nwhenever maxe pe > p̄. Therefore, we can assume that for all e ∈ E(T ), pe ≤ p̄. Then,\n\u0013\nET −l(χ,T ) \u0012\nET −l(χ,T )\nX\nX\nX\nET\nch(χ̂)\nα+l(χ,T )\n˜\nfχ ≤\np̄\n≤\np̄\n≤\n(ET p̄)α+l(χ,T ) ≤ ET (ET p̄)l(χ,T ) ,\nα\n+\nl(χ,\nT\n)\nα=0\nα=0\nχ̂∈H(χ)\n\nwhen M is large enough so that p̄ < 1/ET . For constant sites, we use the bound f˜0 , f˜1 ≤ 1 (by the\n0 − 1 symmetry). Therefore,\n\n\nY\ne 0 | p, T ]\n1\nln P[X\nN\nf˜χ χ \n= −\nln f˜0N0 +Nc\n−\nln(k + Nc )\nln(k + Nc )\nχ6=0\n\n\nX\n1\n\n≥ −\nNχ ln(ET (ET p̄)l(χ,T ) )\nln(k + Nc )\nχ6=0,1\n\n\n\n\nX\n1\nET l(X, T ) ln(k + Nc ) \nl(X, T ) \n\nNχ ln ET  − ln\n−\n≥\nln(k + Nc )\nl(X, T )\nNc\nχ6=0,1\n\n\u0001\nl(X, T )\n− ln ET + ln Nc − ln(ET2 k ln(k + Nc ))\n1 + ln Nc\n\u0011\nl(X, T ) \u0010\n1/ε\n1/ε\n≥\nln\nM\n−\n4\nln\nM\n−\nln(1\n+\nln\nM\n)\n1 + ln M 1/ε\n≥ l(X, T )(1 − 5ε),\n≥\n\nfor M large enough. \u0004\nProof. (Proposition 1) Let T ∗ be a maximum likelihood tree with corresponding edge probabilities\np∗ , and T ∗∗ , be a maximum parsimony tree. Assume we have a polynomial-time algorithm which is\nguaranteed to return a tree T ′ and edge probabilities p′ such that\n\u0010\n\u0011\ne 0 | T ′ , p′ ] ≤ (1 + c) − ln P[X\ne 0 | T ∗ , p∗ ] .\n− ln P[X\n\nThen the claims above and the optimality of T ∗ imply that, if p∗∗ is chosen as in Claim 1 (for T = T ∗∗ ),\n!\ne 0 | T ′ , p′ ]\n− ln P[X\n1\n′\nl(X, T ) ≤\n1 − 5ε\nln(k + Nc )\n!\ne 0 | T ∗ , p∗ ]\n1 + c − ln P[X\n≤\n1 − 5ε\nln(k + Nc )\n!\ne 0 | T ∗∗ , p∗∗ ]\n1 + c − ln P[X\n≤\n1 − 5ε\nln(k + Nc )\n(1 + c)(1 + 2ε)\nl(X, T ∗∗ )\n1 − 5ε\n≤ (1 + c′ )l(X, T ∗∗ ),\n\n≤\n\nfor ε small enough. \u0004\nProof. (Proposition 2) Wareham [18, Theorem 45 Part 3] gives a reduction from vertex cover with\nbounded degree B (B-VC) to maximum parsimony. (Wareham actually defines MP as a Steiner tree\nproblem on the Hamming cube {0, 1}k but the correspondence with our definition is straightforward.)\nThe reduction is such that the existence of a (1 + c′ )-approximation algorithm for maximum parsimony\nimplies the existence of a (1 + 2Bc′ )-approximation algorithm for B-VC. By [6], for a sufficiently large\nB, there is no 1.16-approximation algorithm for B-VC unless P = NP. \u0004\n4\n\n\fAcknowledgment\nI thank Elchanan Mossel, Mike Steel and Tandy Warnow for discussions and encouragements. I\ngratefully acknowledge the partial support of CIPRES (NSF ITR grant # NSF EF 03-31494), NSERC,\nNATEQ, and a Loève Fellowship. This project was initiated at a CIPRES retreat. I also thank Martin\nNowak and the Program for Evolutionary Dynamics at Harvard University where part of this work\nwas done.\n\nReferences\n[1] L. Addario-Berry, B. Chor, M. T. Hallett, J. Lagergren, A. Panconesi, and T. Wareham, “Ancestral\nMaximum Likelihood of Evolutionary Trees is Hard,” Journal of Bioinformatics and Computational\nBiology, vol. 2, no. 2, pp. 257-271, 2004.\n[2] R. Agarwala, V. Bafna, M. Farach, B. Narayanan, M. Paterson, and M. Thorup, “On the approximability of numerical taxonomy (fitting distances by tree metrics),” SIAM Journal on Computing,\nvol. 28, pp. 1073-1085, 1999.\n[3] G. Ausiello, P. Crescenzi, G. Gambosi, V. Kann, A. Marchetti-Spaccamela, and M. Protasi, Complexity and Approximation, Springer, Berlin, 1999.\n[4] J. Cavender, “Taxonomy with confidence,” Mathematical Biosciences, vol. 40, pp. 271-280, 1978.\n[5] B. Chor and T. Tuller, “Maximum Likelihood of Evolutionary Trees is Hard,” In: Proceedings\nof the 9th International Conference on Computational Molecular Biology (RECOMB 2005), ACM\nPress, Cambridge, 2005.\n[6] A. Clementi, and L. Trevisan, “Improved Non-Approximability Results for Minimum Vertex Cover\nwith Density Constraints,” Theor. Comput. Sci., vol. 225, no. 1-2, pp. 113-128, 1999.\n[7] W. Day, D. Jonhson, and D. Sankoff, “The computational complexity of inferring rooted phylogenies\nby parsimony,” Mathematical Biosciences, vol. 81, pp. 33-42, 1986.\n[8] W. Day and D. Sankoff, “The computational complexity of inferring phylogenies by compatibility,”\nSystematic Zoology, vol. 35, pp. 224-229, 1986.\n[9] A. W. F. Edwards and L. L. Cavalli-Sforza, “Reconstruction of evolutionary trees,” In: Phenetic\nand Phylogenetic Classification, eds. V. H. Heywood and J. McNeill, Systematics Association,\nLondon, vol. 6, pp. 67-76, 1964.\n[10] J. S. Farris, “A probability model for inferring evolutionary trees,” Systematic Zoology, vol. 22,\npp. 250-256, 1973.\n[11] J. Felsenstein, “Evolutionary trees from DNA sequences: a maximum likelihood approach,” J.\nMolecular Evolution, vol. 17, pp. 368-376, 1981.\n[12] J. Felsenstein, Inferring Phylogenies, Sinauer Associates, Sunderland, 2004.\n[13] L. Foulds and R. Graham, “The Steiner problem in phylogeny is NP-complete,” Advances in\nApplied Mathematics, vol. 3, pp. 43-49, 1982.\n[14] M. R. Garey and D. S. Johnson, Computers and Intractability. A Guide to the Theory of NPcompleteness, W. H. Freeman, San Francisco, 1976.\n\n5\n\n\f[15] J. Neyman, “Molecular studies of evolution: a source of novel statistical problems,” In: Statistical\ndecision theory and related topics, eds. S.S Gupta and J. Yackel, Academic Press, New York, pp.\n1-27, 1971.\n[16] C. Semple and M. Steel, Phylogenetics, Oxford University Press, 2003.\n[17] C. Tuffley and M. Steel, “Links between maximum likelihood and maximum parsimony under a\nsimple model of site substitution,” Bull Math Biol., vol. 59, no. 3, pp. 581-607, 1997.\n[18] H. T. Wareham, On the Computational Complexity of Inferring Evolutionary Trees, M.Sc. thesis,\nTechnical Report no. 9301, Department of Computer Science, Memorial University of Newfoundland, 1993.\n\n6\n\n\f"
        ],
        [
         "20",
         "20",
         "cs.CE",
         "Computational Engineering",
         "1501.05810v1.pdf",
         "Computational Particle Mechanics manuscript No.\n(will be inserted by the editor)\n\nUltrascale Simulations of Non-smooth Granular Dynamics\n\narXiv:1501.05810v1 [cs.CE] 23 Jan 2015\n\nTobias Preclik · Ulrich Rüde\n\nReceived: 31.12.2014 / Accepted:\n\nAbstract This article presents new algorithms for massively parallel granular dynamics simulations on distributed memory architectures using a domain partitioning approach. Collisions are modelled with hard\ncontacts in order to hide their micro-dynamics and thus\nto extend the time and length scales that can be simulated. The multi-contact problem is solved using a nonlinear block Gauss-Seidel method that is conforming to\nthe subdomain structure. The parallel algorithms employ a sophisticated protocol between processors that\ndelegate algorithmic tasks such as contact treatment\nand position integration uniquely and robustly to the\nprocessors. Communication overhead is minimized through\naggressive message aggregation, leading to excellent strong\nand weak scaling. The robustness and scalability is assessed on three clusters including two peta-scale supercomputers with up to 458 752 processor cores. The simulations can reach unprecedented resolution of up to ten\nbillion (1010 ) non-spherical particles and contacts.\n\n1 Introduction\n\nT. Preclik\nLehrstuhl für Informatik 10 (Systemsimulation), FriedrichAlexander-Universität Erlangen-Nürnberg, Cauerstr. 11,\n91052 Erlangen, Germany\nE-mail: tobias.preclik@fau.de\n\nTwo fundamentally different model types must be\ndistinguished: Soft and hard contacts. Soft contacts allow a local compliance in the contact region, whereas\nhard contacts forbid penetrations. In the former class\nthe contact forces can be discontinuous in time, leading to non-differentiable but continuous velocities after\nintegration. The differential system can be cast e.g. as\nan ordinary differential equation with a discontinuous\nright-hand side or as differential inclusions. However,\n\nGranular matter exhibits intriguing behaviours akin to\nsolids, liquids or gases. However, in contrast to those\nfundamental states of matter, granular matter still cannot be described by a unified model equation homogenizing the dynamics of the individual particles [26]. To\ndate, the rich set of phenomena observed in granular\nmatter, can only be reproduced with simulations that\nresolve every individual particle. In this paper, we will\nconsider methods where also the spatial extent and geometric shape of the particles can be modelled. Thus\nin addition to position and translational velocity the\norientation and angular velocity of each particle constitute the state variables of the dynamical system. The\nshapes of the particles can be described for example\nby geometric primitives, such as spheres or cylinders,\nwith a low-dimensional parameterization. Composite\nobjects can be introduced as a set of primitives that\nare rigidly glued together.Eventually, even meshes with\nKeywords Granular Dynamics · High Performance\na higher-dimensional parameterization can be used. In\nComputing · Non-smooth Contact · Parallel Comthis article the shape of the particles does not change\nputing · Message Passing Interface\nin time, i.e. no agglomeration, fracture or deformation\ntakes place. The rates of change of the state variables\nMathematics Subject Classification (2000) 65Y05 · are described by the Newton-Euler equations, and the\n70F35 · 70F40 · 70E55\nparticle interactions are determined by contact models.\n\nU. Rüde\nLehrstuhl für Informatik 10 (Systemsimulation), FriedrichAlexander-Universität Erlangen-Nürnberg, Cauerstr. 11,\n91052 Erlangen, Germany\nE-mail: ulrich.ruede@fau.de\n\n\f2\n\nthe resulting differential system is typically extremely\nstiff if realistic material parameters are employed.\nIn the latter class, discontinuous forces are not sufficient to accomplish non-penetration of the particles. Instead, impulses are necessary to instantaneously change\nvelocities on collisions or in self-locking configurations\nif Coulomb friction is present [33]. Stronger mathematical concepts are required to describe the dynamics. For\nthat purpose, Moreau introduced the measure differential inclusions in [27].\nHard contacts are an idealization of reality. The\nrigidity of contacts has the advantage that the dynamics of the micro-collisions does not have to be resolved\nin time. However, this also introduces ambiguities: The\nrigidity has the effect that the force chains along which\na particle is supported are no longer unique [29]. If energy is dissipated, this also effects the dynamics. To\nintegrate measure differential inclusions numerically in\ntime, two options exist: In the first approach the integration is performed in subintervals from one impulsive\nevent to the next [25, 11]. At each event an instantaneous impact problem must be solved whose solution serves as initial condition of the subsequent integration subinterval. Impact problems can range from\nsimple binary collisions, to self-locking configurations,\nto complicated instantaneous frictional multi-contact\nproblems with simultaneous impacts. The dynamics between events are described by differential inclusions,\ndifferential algebraic equations or ordinary differential\nequations. Predicting the times of the upcoming events\ncorrectly is non-trivial in general and handling them in\norder in parallel is impeding the scalability [25]. In the\nsecond approach no efforts are made to detect events,\nbut the contact conditions are only required to be satisfied at discrete points in time. This approach is commonly referred to as a time-stepping method.\nThis article focuses on the treatment of hard contacts in order to avoid the temporal resolution of microcollisions and thus the dependence of the time-step length\non the stiffness of the contacts. In order to avoid the resolution of events a time-stepping method is employed.\nThis considerably pushes the time scales accessible to\ngranular flow simulations for stiff contacts.\nTo estimate the order of a typical real-life problem\nsize of a granular system, consider an excavator bucket\nwith a capacity of 1 m3 . Assuming sand grains with\na diameter of 0.15 mm, and assuming that they are\npacked with a solid volume fraction of 0.6, the excavator bucket contains in the order of 1010 particles. In\nsuch a dense packing the number of contacts is in the\nsame order as the number of particles. Only large scale\nparallel systems with distributed memory can provide\nenough memory to store the data and provide sufficient\n\nTobias Preclik, Ulrich Rüde\n\ncomputational power to integrate such systems for a\nrelevant simulation time. Consequently a massive parallelization of the numerical method for architectures\nwith distributed memory is absolutely essential.\n\nIn the last half decade several approaches were published suggesting parallelizations of the methods integrating the equations of motion of rigid particles in hard\ncontact [38, 39, 20, 34, 15, 16, 28]. The approach put forward in this article builds conceptually on these previous approaches but exceeds them substantially by consistently parallelizing all parts of the code, consistently\ndistributing all simulation data (including the description of the domain partitioning), systematically minimizing the volume of communication and the number\nof exchanged messages, and relying exclusively on efficient nearest-neighbor communication. The approach\ndescribed here additionally spares the expensive assembly of system matrices by employing matrix-free computations. All this is accomplished without sacrificing\naccuracy. The matrix-freeness allows the direct and straight\nforward evaluation of wrenches in parallel and thus reduces the amount of communicated data. Furthermore,\nan exceptionally robust synchronization protocol is defined, which is not susceptible to numerical errors. The\nexcellent parallel scaling behaviour is then demonstrated\nfor dilute and dense test problems in strong- and weakscaling experiments on three clusters with fundamentally different interconnect networks. Among the test\nmachines are the peta-scale supercomputers SuperMUC\nand Juqueen. The results show that given a sufficient\ncomputational intensity of the granular setup and an\nadequate interconnect, few hundred particles per process are enough to obtain satisfactory scaling even on\nmillions of processes.\n\nIn Sect. 2 of this paper the underlying differential\nequations and the time-continuous formulation of the\nhard contact models are formulated. Sect. 3 proposes\na discretization scheme and discrete constraints for the\nhard contact model. The problem of reducing the number of contacts in the system for efficiency reasons is\naddressed in Sect. 4. Subsequently, an improved numerical method for solving multi-contact problems in\nparallel is introduced in Sect. 5 before turning to the\ndesign of the parallelization in Sect. 6. The scalability\nof the parallelization is then demonstrated in Sect. 7 by\nmeans of dilute and dense setups on three different clusters. Finally, the algorithms and results are compared\nto previous work by other authors in Sect. 8 before summarizing in Sect. 9.\n\n\fUltrascale Simulations of Non-smooth Granular Dynamics\n\n2 Continuous Dynamical System\nThe Newton-Euler equations for a system with νb particles are [22]\n\u0012\n\u0013 \u0012\n\u0013\nẋ(t)\nv(t)\n=\n,\nϕ̇(t)\nQ(ϕ(t))ω(t)\n\u0012\n\u0013 \u0012\n\u0013\nv̇(t)\nf(s(t), t)\nM(ϕ(t))\n=\n,\nω̇(t)\nτ(s(t), t) − ω(t) × I(ϕ(t))ω(t)\nwhere the positions x(t) ∈ R3νb , the rotations ϕ(t) ∈\nR4νb , translational velocities v(t) ∈ R3νb , and angular\nvelocities ω(t) ∈ R3νb are the state variables at time t.\nDifferent parameterizations exist for the rotations,\nbut quaternions having four real components are the\nparameterization of choice here. Independent of the parameterization, the derivatives of the rotation components can be expressed in terms of a matrix-vector product between a block-diagonal matrix and the angular\nvelocities [10]. If the rotation of particle i is described\nby the quaternion qw + qx i + qy j + qz k ∈ H then, according to [10], the i-th diagonal block of Q(ϕ(t)) is\n\n\n−qx −qy −qz\n1  qw qz −qy \n.\nQii (ϕi (t)) = \n2  −qz qw qx \nqy −qx qw\nEach particle has an associated body frame whose\norigin coincides with the body’s center of mass and\nwhose axes are initially aligned with the axes of the observational frame. The body frame is rigidly attached\nto the body and translates and rotates with it. All of\nthe state variables and other quantities are expressed\nin the observational frame unless noted otherwise. Furthermore, the matrix\n\n\ndiag mi 1\n\nM(ϕ(t)) = i=1..νb\ndiag Iii (ϕi (t))\ni=1..νb\n\nis the block-diagonal mass matrix, where 1 denotes the\n3 × 3 identity matrix. The mass matrix contains the\nconstant particle masses mi and the particles’ inertia\nmatrices Iii (ϕi (t)) about the particles’ centers of mass.\nThe latter can be calculated by similarity transformations from the constant body frame inertia matrices I0ii .\nIf the body frames are attached such that they coincide\nwith the principal axes of their particles, then the body\nframe inertia matrices are diagonal, and floating-point\noperations as well as memory can be saved. The lowerright block of the mass matrix corresponds to the matrix I(ϕ(t)). f(s(t), t) and τ(s(t), t) are the total forces\nand torques (together they are referred to as wrenches)\nacting at the particles’ centers of mass. Both may depend on any of the state variables s(t) of the system\n\n3\n\nand time t. The wrench contributions from contact reactions are summed up with external forces fext and\ntorques τext such as fictitious forces from non-inertial\nreference frames.\nLet λj (t) ∈ R3 be the contact reaction of a contact j ∈ C, where C = {1 . . νc } is the set of potential contact indices. Let (j1 , j2 ) ∈ B2 be the index pair\nof both particles involved in the contact j, where B =\n{1 . . νb } is the set of body indices. Let x̂j (x(t), ϕ(t)) ∈\nR3 be the location of contact j, then the wrench on\nbody i is\n\u0012\n\u0013 \u0012\n\u0013 X\u0014\n\u0015\nfi (s(t), t)\nfi,ext (s(t), t)\n1\n=\n+\nλ (t)\nτi (s(t), t)\nτi,ext (s(t), t)\n(x̂j (x(t), ϕ(t)) − xi (t))× j\nj∈C\nj1 =i\n\n−\n\nX\u0014\nj∈C\nj2 =i\n\n|\n\n\u0015\n1\nλ (t),\n(x̂j (x(t), ϕ(t)) − xi (t))× j\n{z\n\nwrench contributions\n\n(1)\n\n}\n\n×\n\nwhere ( · ) is a matrix, which when multiplied to a\nvector corresponds to the cross product between its\noperand ( · ) and the vector.\nIn contrast to soft contact models, the contact reactions in hard contact models cannot be explicitly expressed as a function of the state variables but are defined implicitly, e.g. by implicit non-linear functions [21],\ncomplementarity constraints [1, 3], or inclusions [36].\nIn any case, the constraints distinguish between reactions in the directions normal to the contact surfaces\nand reactions in the tangential planes of the contact\nsurfaces. The former are used to formulate the nonpenetration constraints, and the latter are used to formulate the friction constraints. For that reason, each\ncontact j is associated with a contact frame, where\nthe axis nj (x(t), ϕ(t)) ∈ R3 points along the direction normal to the contact surface, and the other two\naxes tj (x(t), ϕ(t)) ∈ R3 and oj (x(t), ϕ(t)) ∈ R3 span\nthe tangential plane of the contact.\nLet Si be the set of points in the observational frame\ndefining the shape of particle i, and let fi (xi (t), ϕi (t), y) ∈\nR be the associated signed distance function for a point\ny in the observational frame. The signed distance function shall be negative in the interior of Si . Assuming that all particles are (strictly) convex with sufficiently smooth boundaries, then for a pair of particles\n(j1 , j2 ) involved in a contact j, the contact location\nx̂j (x(t), ϕ(t)) is defined by the optimization problem\nx̂j (t) := x̂j (x(t), ϕ(t))\n=\n\narg min\n\nfj1 (xj1 (t), ϕj1 (t), y),\n\n(2)\n\nfj2 (xj2 (t),ϕj2 (t),y)≤0\n\nwith associated contact normal\n\nnj (t) := nj (x(t), ϕ(t)) = ∇y fj2 (xj2 (t), ϕj2 (t), x̂j (t)),\n\n\f4\n\npointing outwards with respect to Sj2 and associated\nsigned contact distance\n\nξj (t) := ξj (x(t), ϕ(t)) = fj1 (xj1 (t), ϕj1 (t), x̂j (t))\nwhich is negative in the case of penetrations.\nFor convex particles each pair of bodies results in\na potential contact, and thus the total number of contacts νc is limited by 12 νb (νb − 1). Non-convex objects\ne.g. can be implemented as composite objects of convex particles. By convention a positive reaction in normal direction is repulsive, and thus the contact reaction\nλj (t) acts positively on particle j1 and negatively on j2 ,\nthus explaining the signs in (1). By applying the opposite reactions at the same point in the observational\nframe, not only the linear momentum can be conserved\nbut also the angular momentum of the system. Conservation of energy can only hold if the contact model\ndoes not include dissipative effects. Hard-contact models require the Signorini condition to hold. Written as\na complementarity condition for a contact j, it reads\nξj (t) ≥ 0 ⊥ λj,n (t) ≥ 0,\nwhere λj,n (t) = nj (t)T λj (t). The signed contact distance is required to be non-negative, resulting in a nonpenetration constraint. The contact reaction in direction of the contact normal is also required to be nonnegative, resulting in non-adhesive contact reactions.\nFurthermore, both quantities must be complementary,\nmeaning that either of them must be equal to zero. This\neffects that the contact reaction can only be non-zero\nif the contact is closed.\nHowever, the Signorini condition does not determine\nthe contact reaction force if the contact is closed. In\nthat case the non-penetration constraint on the velocity\nlevel,\nξ˙j+ (t) ≥ 0 ⊥ λj,n (t) ≥ 0,\nmust be added to the system, where ξ˙j+ is the right\nderivative of the signed contact distance with respect\nto time. The constraint allows the contact to break\nonly if no reaction force is present and otherwise forces\nξ˙j+ (t) = 0. In the latter case the reaction force is still\nnot fixed. The non-penetration constraint on the acceleration level,\nξ¨j+ (t) ≥ 0 ⊥ λj,n (t) ≥ 0,\nthen determines the force also if ξ¨j+ (t) = 0. When considering impacts, a non-penetration constraint for the\nreaction impulse in the direction normal to the contact\nsurface must be formulated, and, if the contact is closed,\nan additional constraint modelling an impact law such\nas Newton’s impact must be added.\n\nTobias Preclik, Ulrich Rüde\n\nThese non-penetration conditions can be complemented by a friction condition. The most prominent\nmodel for dry frictional contact is the Coulomb model\nwhich restricts the relative contact velocity in the tangential plane of the contact. The relative contact velocity for a pair of particles (j1 , j2 ) involved in a contact j\nis\nδvj+ (s(t)) =vj+1 (t) + ωj+1 (t) × (x̂j (x(t), ϕ(t)) − xj1 (t))\n−vj+2 (t) − ωj+2 (t) × (x̂j (x(t), ϕ(t)) − xj2 (t)).\nLet\n+\n+\nδvj,to\n(t) := δvj,to\n(s(t)) =\n\n\u0012\n\ntj (x(t), ϕ(t))T δvj+ (s(t))\noj (x(t), ϕ(t))T δvj+ (s(t))\n\n\u0013\n\nbe the relative contact velocity in the tangential plane\nafter application of the contact impulses, then the Coulomb conditions for a non-impulsive point in time t are\nkλj,to (t)k2 ≤ µj λj,n (t) and\n+\n+\nkδvj,to\n(t)k2 λj,to (t) = −µj λj,n (t)δvj,to\n(t).\n+\nHowever, if kδvj,to\n(t)k2 = 0 these conditions must be\nsupplemented by the constraint\n\n˙ + (t)k2 λj,to (t) = −µj λj,n (t)δv\n˙ + (t)\nkδv\nj,to\nj,to\non acceleration level in order to determine the friction\nforce. Likewise constraints for the friction impulse are\nnecessary. At this point we refrain from formulating the\nmeasure differential inclusion in detail since it would\nnot contribute information essential to the remaining\npaper which only deals with the discrete-time system.\n3 Discrete Dynamical System\nIn simulations of granular matter impulsive reactions\nare abundant. Higher-order integrators for time-stepping\nschemes are still subject to active research [31]. In particular, discontinuities pose problems for these integrators. Hence, the continuous dynamical system is discretized in the following with an integrator of order one,\nresembling the semi-implicit Euler method and similar\nto the one suggested in [2].\nLet s, x, ϕ, v and ω denote the given discrete-time\nstate variables at time t and λ the contact reactions at\ntime t. Then the state variables at time t + δt are functions depending on the contact reactions: s0 (λ), x0 (λ),\nϕ0 (λ), v 0 (λ) and ω 0 (λ). The discrete-time NewtonEuler equations integrated by the proposed scheme are\n\u0013 \u0012 \u0013\n\u0012\n\u0013\nx0 (λ)\nx\nv 0 (λ)\n=\n+ δt\n,\n0\n0\nϕ (λ)\nϕ\nQ(ϕ)ω (λ)\n\u0012 0\n\u0013 \u0012 \u0013\n\u0012\n\u0013\nv (λ)\nv\nf(s, λ, t)\n=\n+ δtM(ϕ)−1\n.\n0\nω (λ)\nω\nτ(s, λ, t) − ω × I(ϕ)ω\n\u0012\n\n(3)\n\n\fUltrascale Simulations of Non-smooth Granular Dynamics\n\n5\n\nPositions and orientations at time t + δt appear exclusively on the left-hand side of the position and orientation integration. Velocities at time t + δt appear on\nthe left-hand side of the velocity integration and additionally in the integration of positions and orientations.\nThe numerical integration of the quaternion has the effect that the quaternion gradually looses its unit length.\nThis deficiency can be compensated by renormalizing\nthe quaternions after each integration.\nInstead of discretizing each of the five intermittently\nactive continuous-time complementarity constraints, the\nSignorini condition is only required to hold at the end\nof each time step. This has the effect that impulsive reactions are no longer necessary to satisfy the condition\nsince the condition is no longer required to be fulfilled\ninstantaneously. Furthermore, the signed distance function gets linearized, resulting in\n\nA detailed discussion of solution algorithms for onecontact problems is out of the scope of this article. However, splitting methods, where non-penetration and friction constraints are solved separately, are prone to slow\nconvergence or cycling. In [7] Bonnefon et al. solve the\none-contact problem by finding the root of a quartic\npolynomial. Numerous other approaches exist for modified friction laws, notably those where the friction cone\nis approximated by a polyhedral cone and solution algorithms for linear complementarity problems can be\nused [1, 30]. In any case the algorithm of choice should\nbe extremely robust in order to successfully resolve νc\ncontacts per iteration and time step, where νc can be\nin the order of 1010 in this article.\n\nξj (t + δt) = ξj (t) + δtξ˙j (t) + O(δt2 ),\n\nThe contact problem F (λ) = 0 has O(νb2 ) non-linear\nequations. Thus, already the setup of the contact problem would not run in linear time, much less the solution algorithm even if it were optimal. The contact\nconstraints of a contact j can be removed from the system without altering the result if the contact is known\nto stay open (λj = 0) within the current time step. Let\n\b\nSi (t) = y ∈ R3 fi (xi (t), ϕi (t), y) ≤ 0\n\nwhere the time derivative of the signed contact distance\ncan be determined to be\nξ˙j (t) = nj (t)T δvj+ (s(t))\nunder the assumption that the contact point x̂j (t) translates and rotates in accordance with body j2 , such that\nx̂˙ j (t) = vj2 (t) + ωj2 (t) × (x̂j (t) − xj2 (t)).\n\nbe the set of points in space corresponding to the rotated and translated shape of particle i at time t and\nlet\n\b\nHi (t) = Si (t) + y ∈ R3 kyk2 ≤ hi (t)\n\nLet the time-discrete relative contact velocity be\nδvj0 (λ) =vj0 1 (λ) + ωj0 1 (λ) × (x̂j − xj1 )\n−vj0 2 (λ) − ωj0 2 (λ) × (x̂j − xj2 ),\nwhere the velocities are discretized implicitly. The discrete non-penetration constraint then is\nξj\n0\n+ nT\nj δvj (λ) ≥ 0 ⊥ λj,n ≥ 0.\nδt\n\n(4)\n\nξ\n\nThe term δtj acts as an error correction term if penetrations are present (ξj < 0). In that case it can be\nscaled down to avoid introducing an excessive amount\nof energy. If no numerical error is present, the contact\nis inelastic. The frictional constraints translate into\nkλj,to k2 ≤ µj λj,n and\n0\n0\nkδvj,to\n(λ)k2 λj,to = −µj λj,n δvj,to\n(λ).\n\n4 Contact Detection\n\n(5)\n\nLet Fj (λ) = 0 denote a non-linear system of equations equivalent to the constraints from (4) and (5) of a\nsingle contact j, and let F (λ) denote the collection of\nall Fj (λ). Neither F (λ) = 0 nor Fj (λ) = 0 for given\nλj have unique solutions. Let Fj−1 (0, λj ) be a possible\nsolution of the one-contact problem of contact j, given\nthe contact reactions λj of all other contacts j.\n\nbe an intersection hull that spherically expands the particle shape by the radius hi (t) > 0. If hi (t) is chosen\nlarge enough then an algorithm finding intersections between the hulls can detect all contacts that can potentially become active in the current time step. A possible\nchoice for the expansion radius is\nhi (t) = δt(kvi (t)k2 + kωi (t)k2 ri ) + τ,\n\n(6)\n\nwhere ri = maxy∈Si (0) kyk2 is the bounding radius of\nparticle i, and τ is a safety margin. The safety margin becomes necessary since an explicit Euler step is\nunderlying the derivation of (6). In practice, the usage of intersection hulls reduces the number of contacts\nconsiderably. E.g., monodisperse spherical particles can\nhave at most 12 contacts per particle if the expansion\nradii are small enough [32], resulting in O(νb ) potential\ncontacts.\nBroad-phase contact detection algorithms aim to\nfind as few as possible candidate particle pairs for contacts by using e.g. spatial partitioning approaches or exploiting temporal coherence of the particle positions [9].\n\n\f6\n\nTobias Preclik, Ulrich Rüde\n\n1: k ← 0\n2: λ(k) ← 0\n3: while convergence criterion not met do\n4:\nfor j ← 1 to νc do\n5:\nfor l ∈ C do (\n(k+1)\nλl\nif l < j ∧ sc (l) = sc (j)\n(k,j)\n6:\nλ̃l\n←\n(k)\nλl\nelse\n7:\nend for\n(k,j)\n8:\ny ← Fj−1 (0, λ̃j\n)\n(k+1)\n\n(k)\n\n9:\nλj\n← ωy + (1 − ω)λj\n10:\nend for\n11:\nk ←k+1\n12: end while\n\nAlgorithm 1: The subdomain NBGS method with relaxation parameter ω.\n\nThe candidate pairs are then checked in detail in the\nnarrow-phase contact detection, where (2) is solved for\neach pair, leading to the contact location x̂j , normal nj\nand signed distance ξj for a contact j.\nTo solve (2) for non-overlapping particles, the Gilbert-Johnson–Keerthi (GJK) algorithm can be used [13,\n4]. For overlapping particle shapes the expanding polytope algorithm (EPA) computes approximate solutions [5].\nFor simple geometric primitives like spheres, the optimization problem can be solved analytically. The indices of all contacts found that way form the set of potential contacts C = {1 . . νc } at time t. Let F (λ) = 0\nfrom now on denote the contact problem where all contact conditions and contact reactions whose indices are\nnot part of C have been filtered out.\n\nThe algorithm is of iterative nature and needs an\nappropriate stopping criterion to terminate. In each iteration k a sweep over all contacts is performed, where\neach contact j is relaxed, given an approximation of all\nother contact reactions λ̃(k,j) . In the subdomain NBGS,\nthe approximation of contact reaction l is taken from\nthe current iteration if it was already relaxed (l < j)\nand if it is associated with the same subdomain as the\ncontact j to be relaxed (sc (l) = sc (j)). In all other\ncases, the approximation is taken from the previous it(k+1)\neration. The contact reaction λj\nis then a weighted\nmean between the previous approximation and the relaxation result. If all contacts are associated with the\nsame subdomain and ω = 1 then Alg. 1 corresponds to\na classic NBGS. If each contact is associated to a different subdomain then Alg. 1 corresponds to a non-linear\nblock Jacobi (NBJ) with relaxation parameter ω.\n\n6 Parallelization Design\nSect. 6.1 introduces the domain partitioning approach.\nSect. 6.2 then discusses requirements that must be met\nin order to be able to treat all contacts exactly once in\nparallel. Sect. 6.3 explains how accumulator and correction variables can be used in order to reduce data\ndependencies to other processes. In Sect. 6.4 conditions\nare discussed under which the set of communication\npartners can be reduced to the nearest neighbors. Timeintegration and the subsequent necessity of synchronization are addressed in Sect. 6.5 before summarizing\nthe time-stepping procedure in Sect. 6.6.\n\n5 Numerical Solution Algorithms\nTo solve the multi-contact problem, when suitable solution algorithms for the one-contact problems Fj−1\nare given, a non-linear block Gauss-Seidel (NBGS) can\nbe used as propagated by the non-smooth contact dynamics (NSCD) method [18]. Unfortunately, the GaussSeidel algorithm cannot be efficiently executed in parallel for irregular data dependencies as they appear in\ncontact problems [20].\nAs an alternative, a more general variant is proposed here, accommodating the subdomain structure\nthat will arise in the domain partitioning. Therefore,\neach contact j ∈ C is associated with a subdomain\nnumber sc (j) ∈ P, where P = {1 . . νp } is the set of\nsubdomain indices for νp subdomains. Alg. 1 presents\npseudo-code for the subdomain NBGS with the relaxation parameter ω > 0. The initial solution is chosen to\nbe zero, however, any other initialization can be used,\nin particular contact reactions from the previous time\nstep.\n\n6.1 Domain Partitioning\nUnder the assumption that no contacts are present,\nthere exists no coupling between the data of any two\nparticles, and the problem becomes embarrassingly parallel: Each process integrates b ννpb c or d ννpb e particles.\nLet sb (i) ∈ P determine the process responsible for the\ntime-integration of particle i as of now referred to as the\nparent process. All data associated with this particle,\nthat is the state variables (position, orientation, velocities) and constants (mass, body frame inertia matrix,\nshape parameters), are instantiated only at the parent\nprocess in order to distribute the total memory load.\nHowever, contacts or short-range potentials introduce\ndata dependencies to particles that in general are not\ninstantiated on the local process nor on a process close\nto the local one, rendering a proper scaling impossible.\nA domain partitioning approach alleviates this problem.\n\n\fUltrascale Simulations of Non-smooth Granular Dynamics\n\nLet Ω denote the computational domain within which\nall particles are located and Ωp ⊆ Ω, p ∈ P, a family\nof disjoint subdomains into which the domain shall be\npartitioned. In this connection subdomain boundaries\nare associated to exactly one process. One process shall\nbe executed per subdomain. The number of processes\ncan e.g. correspond to the number of compute nodes in\na hybrid parallelization or to the total number of cores\nor even threads in a homogeneous parallelization. In the\ndomain partitioning approach the integration of a particle whose center of mass xi is located in a subdomain\nΩp at time t is calculated by process p. That way data\ndependencies typically pertain the local or neighboring\nsubdomains since they are considered to be of short\nrange. Let sb (i) be adapted accordingly. Special care\nis required when associating a particle to a subdomain\nwhose center of mass is located on or near subdomain\ninterfaces. Especially, periodic boundary conditions can\ncomplicate the association process since the finite precision of floating-point arithmetics does in general not\nallow a consistent parametric description of subdomains\nacross periodic boundaries. Sect. 6.5 explains how the\nsynchronization protocol can be used to allow a reliable\nassociation.\nThe domain partitioning should be chosen such that\nan equal number of particles is located initially in each\nsubdomain and sustained over the course of the simulation in order to balance the computational load which\nis directly proportional to the number of particles. Particles now migrate between processes if their positions\nchange the subdomain. Migration can lead to severe\nload imbalances that may need to be addressed by dynamically repartitioning the domain. Such load-balancing\ntechniques are beyond the scope of this article.\n\n6.2 Shadow Copies\nA pure local instantiation of particles has the effect that\ncontacts cannot be detected between particles that are\nnot located on the same process. A process can detect a\ncontact if both particles involved in the contact are instantiated on that process. In order to guarantee that\nat least one process can detect a contact, the condition that a contact j must be detected by all processes\nwhose subdomains intersect with the hull intersection\nHj1 ∩ Hj2 is sufficient if the intersection of the hull intersection and the domain is non-empty. This condition\ncan be fulfilled by the following requirement:\nRequirement 1 A particle i must be instantiated not\nonly on the parent process but also on all processes\nwhose subdomains intersect with the particle’s hull.\n\n7\n\nThese additional instantiations shall be termed shadow\ncopies in the following. They must be kept in synchronization with the original instantiation on the parent\nprocess. In order to agree upon the detecting process\nresponsible for treating the contact without communication a rule is needed. Here, the statement that a\nprocess is responsible for treating a contact refers to\nthe responsibility of the process for executing the relaxation of the respective contact in Alg. 1. The typical\nchoice for this rule requires that the process whose subdomain contains the point of contact is put in charge\nto treat the contact [34].\nHowever, this rule only works if the process whose\nsubdomain contains the point is able to detect the contact. This is only guaranteed if the point of contact is\nlocated within the hull intersection. Also, if the point\nof contact is located outside of the domain Ω, then no\nprocess will treat it.\nA more intricate drawback of this approach is that\nit can fail in case of periodic boundary conditions: If\nthe contact point is located near the periodic boundary, the periodic image of the contact point will be\ndetected at the other end of the simulation box. Due\nto the shifted position of the contact point image and\nthe limited numerical precision, the subdomains can no\nlonger consistently decide the subdomain affinity.\nA more robust rule to determine the subdomain\naffinity can be established by fulfilling the following requirement:\nRequirement 2 All shadow copy holders of a particle maintain a complete list of all other shadow copy\nholders and the parent process of that particle.\nThen each process detecting a contact can determine\nthe list of all processes detecting that very same contact, which is the list of all processes with an instantiation of both particles involved in the contact. This\nlist is exactly the same on all processes detecting the\ncontact and is not prone to numerical errors. The rule\ncan then e.g. appoint the detecting process with smallest rank to treat the contact. In order to enhance the\nlocality of the contact treatment, the rule should favor the particle parents if they are among the contact\nwitnesses. Any such rule defines a partitioning of the\ncontact set C. Let Cp be the set of all contacts treated\nby process p ∈ P. Then process p instantiates all contacts j ∈ Cp .\n6.3 Accumulator and Correction Variables\nThe contact relaxations in Alg. 1 exhibit sums with\nnon-local data dependencies. In the following, the redundant evaluation of these sums is prevented by intro-\n\n\f8\n\nTobias Preclik, Ulrich Rüde\n\nducing accumulator variables and the non-local data dependencies are reduced by introducing correction variables.\nThe relaxation of a contact j depends on the data\nof the state variables of both particles (j1 , j2 ) involved\nin the contact, their constants and shape parameters,\nas can be seen by inspecting (4), (5) and the definitions\nof the terms appearing therein. All of these quantities\nare instantiated on the detecting process, either as a\nshadow copy or as an original instance. The contact\nvariables of contact j (location, signed distance and\nthe contact frame) are also required. They are available on the detecting process since they result from\nthe positions, orientations, and the shape parameters\nof the particles (j1 , j2 ) in the contact detection. Furthermore, the force and torque terms from (1) acting\non these particles additionally depend on the locations\n(k,j)\nx̂l and reaction approximations λ̃l\nof all other contacts l involving one of the particles (j1 , j2 ). Neither the\nlocations nor the reaction approximations of these contacts are necessarily available on the process treating\ncontact j. To rectify this deficiency, one can introduce\ncontact shadow copies so that location and reaction approximation can be mirrored at every instantiation of\nboth particles involved in the contact. However, the organisational overhead of contact shadow copies can be\ncircumvented. It is not necessary that the process treating the contact evaluates all the wrench contributions\nto the particles involved in the contact. Instead, parts\nof the wrench contribution sum can be evaluated on the\nprocesses actually treating the remote contacts and can\nsubsequently be communicated:\n\u0012\n\n\u0013 \u0012\n\u0013 X\u0014\n\u0015\n\u0015\nX\u0014\nfi (λ)\nfi,ext\n1\n1\n=\n+\n× λl −\n× λl\nτi (λ)\nτi,ext\n(x̂l − xi )\n(x̂l − xi )\nl∈C\nl1 =i\n\nl∈C\nl2 =i\n\n\n\u0012\n=\n\nfi,ext\nτi,ext\n\n\u0013\n+\n\nl∈Cp\nl1 =i\n\nl∈Cp\nl2 =i\n\n{z\n\n|\n\n}\n\nwrench contribution (fi,p τi,p )T to particle i from process p\n\nThe total wrench on particle i can also be expressed\nin terms of the total wrench on particle i at the beginning of iteration k:\n\u0012\n\n\n\u0013 \u0012\n\u0013 X X\u0014\n\u0015\n(k)\n\nfi (λ)\nfi (λ )\n1\n(k)\n\n=\n+\n(λl − λl )\n\nτi (λ)\n(x̂l − xi )×\nτi (λ(k) )\np∈P\n\nl∈Cp\nl1 =i\n\n\n−\n\nX\u0014\nl∈Cp\nl2 =i\n\n\u0012\n\u0013 \u0012\n\u0013\n\u0015\nX \u0014\nfi (λ̃(k,j) )\nfi (λ(k) )\n1\n(k+1)\n(k)\n=\n+\n− λl )\n× (λl\n(k)\n(k,j)\n(x̂l − xi )\nτi (λ )\nτi (λ̃\n)\nl∈Csc (j)\nl<j\nl1 =i\n\n−\n\n\u0015\n\n1\n(k) \n(λl − λl )\n\n(x̂l − xi )×\n\nWhen relaxing the contact j in iteration k of the\nsubdomain NBGS, the wrench on particle i ∈ {j1 , j2 }\nis evaluated with the reaction approximation λ̃(k,j) as\n\nX \u0014\nl∈Csc (j)\nl<j\nl2 =i\n\n\u0015\n1\n(k+1)\n(k)\n− λl )\n× (λl\n(x̂l − xi )\n\nOur implementation instantiates variables on process p for the reaction approximations λ[p] ∈ R3|Cp | of\nall contacts treated by process p. Any updates to the\nreaction approximations occur in place. Furthermore,\nan implementation can instantiate accumulator variables f [p] , τ [p] ∈ R3|Bp | on process p for the wrenches\nfrom the last iteration of all instantiated particles (shadow\ncopies and original instances), where Bp contains the indices of all shadow copies and original instances instantiated on process p. This set is partitioned into Bp,local\nand Bp,shadow , containing the indices of the original instances and the shadow copies respectively.\nInstead of evaluating the wrench contribution sums\neach time when calculating the total wrench on particle i anew, the contributions can be accumulated as the\ncontacts are relaxed. For that purpose, implementations\ncan instantiate corrections variables δf [p] ∈ R3|Bp | and\nδτ [p] ∈ R3|Bp | . Then, after line 9 of Alg. 1, these wrench\ncorrections can be updated by assigning\nδfj1 c\n[s (j)]\nδτj1 c\n\n!\n\n[s (j)]\n\n!\n\n[s (j)]\n\n\n\n\u0015\n\u0015\nX X \u0014\nX\u0014\n\n1\n1\n\n\n× λl −\n× λl \n\n(x̂l − xi )\n(x̂l − xi )\n\np∈P\n\nparameter. Since the subdomain NBGS respects the\nsubdomain affinity of the contacts, the remote wrench\ncontributions to particle i cancel out, and just the total\nwrench on particle i from the last iteration is needed\nin addition to corrections stemming from contacts that\nwere already relaxed by the same process.\n\nδfj2 c\n[s (j)]\nδτj2 c\n\n[s (j)]\n\n←\n←\n\n!\n\n\u0015\n1\n(k+1)\n(k)\n− λj ),\n× (λj\n(x̂j − xj1 )\n! \u0014\n\u0015\n[s (j)]\nδfj2 c\n1\n(k+1)\n(k)\n−\n(λj\n− λj ).\n[s (j)]\n(x̂j − xj2 )×\nδτj c\n\nδfj1 c\n[s (j)]\nδτj1 c\n\n\u0014\n\n+\n\n2\n\nThe evaluation of the total wrench on particle i in\nline 8 of Alg. 1 when relaxing contact j in iteration k\nbecomes\n!\n!\n\u0012\n\u0013\n[s (j)]\n[s (j)]\nfi (λ̃(k,j) )\nfi c\nδfi c\n=\n+\n[s (j)] ,\n[s (j)]\nτi (λ̃(k,j) )\nτi c\nδτi c\nthat is the sum of the accumulator and the correction\nvariables.\nAt the end of each iteration the wrench corrections\nfor each body have to be reduced and added to the accumulated wrench from the last iteration. This can be performed in two message exchanges. In the first message\nexchange each process sends the wrench correction of\n\n\fUltrascale Simulations of Non-smooth Granular Dynamics\n\neach shadow copy to its parent process. Then each process sums up for each original instance all wrench corrections obtained from the shadow copy holders, its own\nwrench correction, and the original instance’s accumulated wrench. Subsequently, the updated accumulated\nwrench of each original instance is sent to the shadow\ncopy holders in a second message-exchange communication step. The wrench corrections are then reset everywhere.\nThe accumulated wrenches f [p] , τ [p] are initialized\non each process p before line 3 in Alg. 1 to\n[p]\n\nfi\n[p]\nτi\n\n!\n\n\u0012\n←\n\nfi,ext\nτi,ext\n\n\u0013\n∀i ∈ Bp\n\nunless the initial solution is chosen to be non-zero. The\nwrench corrections are initially set to 0. If the external\nforces and torques are not known on each process or are\nscattered among the processes having instantiated the\nparticles, the initialization requires another two message exchanges, as they are necessary at the end of each\niteration.\nAn alternative to storing accumulated wrenches and\nwrench corrections is to store accumulated velocities\nand velocity corrections. In that case, a process p instantiates variables v [p] , ω [p] , δv [p] , δω [p] ∈ R3|Bp | . The\naccumulated velocities are set to vi0 (λ(k) ) and ωi0 (λ(k) )\nfor all i ∈ Bp in each iteration. They are initialized\nand updated accordingly. The velocity corrections are\ninitialized and updated analogously to the wrench corrections. Hereby, the velocity variables can be updated\nin place. In the classic NBGS no wrench or velocity\ncorrection variables would be necessary, but the corrections could be added to the velocity variables right away\nwhich is similar to the approach suggested by Tasora et\nal. in [37].\n\n6.4 Nearest-Neighbor Communication\nIn the following we describe how the strict locality of\nparticle interactions can be used to optimize the parallel communication and synchronization by exchanging\nmessages only between nearest neighbors. So far the\nshadow copies can be present on any process, and the\ncorrections in the summation over wrench or velocity\ncorrections can originate from a long list of processes.\nHowever, by requiring that the particle hulls do not\nextend past any neighboring subdomains, all message\nexchanges can be reduced to nearest-neighbor communications. Let\nNp = {q ∈ P \\ {p} | inf {kyp − yq k2 | yp ∈ Ωp , yq ∈ Ωq } = 0}\n\n9\n\nbe the set of process indices in direct neighborhood of\nprocess p’s subdomain, and let\n\nldd\n\n\n\n= min inf kyp − yq k2 yp ∈ Ωp , yq ∈\np∈P\n\n\n[\nq∈P\\(Np ∪{p})\n\nΩq\n\n\n\n\n,\n\n\n\nbe the shortest distance from a point inside a subdomain to a non-nearest neighbor. Then the condition\nri + kvi (t)k2 δt + τ < ldd\n\n∀i ∈ B\n\n(7)\n\nensures in the first approximation that no hull extends\npast neighboring subdomains. This immediately defines\na hard upper limit of ri < ldd − τ for the bounding radius and thus for the size of all objects. Furthermore,\ngiven the particle shapes, velocities, and safety margins, the condition defines an upper limit for the timestep length. The introduction of condition (7) entails\nthat on a process p \b\nonly the description of the subdomains within Ωp + y ∈ R3 kyk2 ≤ ldd needs to be\navailable, meaning that the description of non-nearestneighbor subdomains can be dispensed with, and that\nthe description of nearest-neighbor subdomains do not\nhave to be correct outside of the ldd -surrounding of Ωp .\nThis leads to a localized description of the domain partitioning on each process, describing the surrounding\nsubdomains only.\nTypically, the size limit stemming from (7) is not a\nproblem for the particles of the granular matter themselves, but very well for boundaries or mechanical parts\nthe granular matter interacts with. However, the number of such enlarged bodies is typically significantly\nsmaller than and independent of the number of smallsized particles, suggesting that they can be treated globally. Let Bglobal be the set of all body indices exceeding\nthe size limit. These bodies will be referred to as being\nglobal in the following. All associated state variables\nand constants shall be instantiated on all processes and\ninitialized equally. The time-integration of these global\nbodies then can be performed by all processes equally.\nIf a global body i has infinite inertia (mi = ∞ and\nI0ii = ∞1), such as a stationary wall or a non-stationary\nvibrating plate, the body velocities are constant, and no\nwrenches need to be communicated. Global bodies having a finite inertia can be treated by executing an allreduce communication primitive whenever reducing the\nwrench or velocity corrections of the small-sized bodies.\nInstead of only involving neighboring processes, the allreduce operation sums up the corrections for each global\nbody with finite inertia from all processes and broadcasts the result, not requiring any domain partitioning\ninformation.\n\n\f10\n\n6.5 Time-Integration and Synchronization Protocol\nHaving solved the contact problem F (λ) = 0 by Alg. 1,\nthe time-integration defined in (3) needs to be performed. If the NBGS implementation uses velocity accumulators, the integrated velocities are at hand after\nthe final communication of the velocity corrections. If\ninstead the NBGS implementation uses wrench accumulators, the wrenches are at hand, and the velocities\nof all local bodies can be updated immediately.\nSubsequently, the time-integration of the positions\ncan take place. Updating a body’s position or orientation effects that the list of shadow copy holders changes\nsince the intersection hull possibly intersects with different subdomains. Also, the body’s center of mass can\nmove out of the parent’s subdomain. In order to restore\nthe fulfillment of the requirements 1 and 2, a process\nmust determine the new list of shadow copy holders\nand the new parent process for each local body after\nthe position update. Shadow copy holders must be informed when such shadow copies become obsolete and\nmust be removed. Analogously, processes must be notified when new shadow copies must be added to their\nstate. In this case copies of the corresponding state variables, constants, list of shadow copy holder indices, and\nindex of the parent process must be transmitted.\nAll other shadow copy holders must obtain the new\nstate variables, list of shadow copy holder indices, and\nindex of the parent process. Hereby, the condition from\n(7) guarantees that all communication partners are neighbors. All information can be propagated in a single aggregated nearest-neighbor message-exchange. The information should be communicated explicitly and should\nnot be derived implicitly, in order to avoid inconsistencies. This is essential to guarantee a safe determination\nof the contact treatment responsibilities as well as timeintegration responsibilities.\nOur implementation of the synchronization protocol makes use of separate containers for storing shadow\ncopies and original instances in order to be able to enumerate these different types of bodies with good performance. Both containers support efficient insertion,\ndeletion and lookup operations for handling the fluctuations and updates of the particles efficiently. Furthermore, the determination of the new list of shadow copy\nholders involves intersection tests between intersection\nhulls of local bodies and neighboring subdomains as requirement 1 explains in Sect. 6.2. However, determining\nthe minimal set of shadow copy holders is not necessary.\nAny type of bounding volumes can be used to ease intersection testing. In particular bounding spheres either\nwith tightly fitting bounding radii ri + hi (t) or even\nwith an overall bounding radius maxi∈B ri + hi (t) as\n\nTobias Preclik, Ulrich Rüde\n1: procedure simulateTimeStep\n2:\nCp,bp = broadPhaseCollisionDetection\n3:\nCp,np = narrowPhaseCollisionDetection(Cp,bp )\n4:\nCp = filterContacts(Cp,np )\n5:\ninitializeAccumulatorAndCorrectionVariables\n6:\nk←0\n7:\nλ[p] ← 0\n8:\nwhile convergence criterion not met do\n9:\nfor j ← 1 to νc ∧ j ∈ Cp do\n[p]\n[p]\n[p]\n10:\nλj ← ωFj−1 (0, λj ) + (1 − ω)λj\n11:\nend for\n12:\nreduceCorrections\n13:\nk ←k+1\n14:\nend while\n15:\nintegrateStateVariables\n16:\nsynchronize\n17: end procedure\n\nAlgorithm 2: A single time step of the simulation on\nprocess p.\n\nproposed by Shojaaee et al. in [34] are canonical. Concerning the geometry of the subdomains at least the\nsubdomain closures can be used for intersection testing. In our implementation we chose to determine almost minimal sets of shadow copy holders by testing\nthe intersections of the actual hull geometries of the\nparticles with the closures of the subdomains. This reduces the number of shadow copies and thus the overall\ncommunication volume in exchange for more expensive\nintersection tests.\n\n6.6 Summary\nAlg. 2 summarizes the steps that need to be executed on\na process p when time-integrating the system for a single time step δt in parallel. The algorithm requires that\nall shadow copies are instantiated on all subdomains\ntheir hull intersects with. Furthermore, the shadow copies\nmust be in sync with the original instance, and the\nglobal bodies must also be in sync to each other. The\npositions of all local bodies must be located within the\nlocal subdomain. The time step proceeds by executing\nthe broad-phase contact detection which uses the positions, orientations, shapes, hull expansion radii, and\npossibly information from previous time steps, in order\nto determine a set of contact candidates (body pairs)\nCp,bp on process p in near-linear time.\nThen, in the narrow-phase contact detection, for\nall candidates the contact location, associated contact\nframe, and signed contact distance is determined if the\nhulls actually intersect. Finally, this set of detected contacts Cp,np needs to be filtered according to one of the\nrules presented above, resulting in Cp , the set of contacts\nto be treated by process p. Before entering the itera-\n\n\fUltrascale Simulations of Non-smooth Granular Dynamics\n\ntion of the subdomain NBGS, the accumulator, correction, and contact reaction variables must be initialized.\nThe initialization of the accumulator variables requires\nan additional reduction step if the external forces or\ntorques cannot be readily evaluated on all processes.\nEach iteration of the subdomain NBGS on process p\ninvolves a sweep over all contacts to be treated by the\nprocess. The contacts are relaxed by a suitable onecontact solver. The j indexing indicates that such a\nsolver typically needs to evaluate the relative contact\nvelocity under the assumption that no reaction acts at\nthe contact j. This can be achieved by subtracting out\nthe corresponding part from the accumulator variables.\nThe weighted relaxation result is then stored in place.\nThe update of the wrench or velocity correction variables is not explicitly listed. After the sweep the wrench\nor velocity corrections are sent to the respective parent\nprocess and summed up per body including the accumulator variables. Then the accumulator variables are\nredistributed to the respective shadow copy holders in\na second message-exchange step.\nAfter a fixed number of iterations or some prescribed\nconvergence criterion is met, the time step proceeds by\nexecuting the time-integration for each local body. The\nchanges of the state variables must then be synchronized in a final message-exchange step, after which the\npreconditions of the next time step are met. Any user\nintervention taking place between two time steps needs\nto adhere to these requirements.\n\n11\n\n7.1 Weak and Strong Scalability\nTo demonstrate the scalability of the algorithms and\ntheir implementation, we perform weak-scaling experiments, where the problem size is chosen directly proportional to the number of processes and such that the\nload per process stays constant. Thus, if ideal scaling\nwere achieved, the time to solution would stay constant.\nLet tp be the time to solution on p processes, then the\nparallel efficiency ep,ws in a weak-scaling experiment is\ndefined to be\nep,ws =\n\nIn strong-scaling experiments, in contrast, the problem size is kept constant, effecting a decreasing work\nload per process when increasing the number of processes. Thus, ideally the time to solution on p processes\nshould be reduced by p in comparison to the time to\nsolution on a single process. The speedup sp on p processes is defined to be\nsp =\n\nt1\n.\ntp\n\nThe parallel efficiency ep,ss in a strong-scaling experiment is then the fraction of the ideal speedup actually\nachieved\nep,ss =\n\n7 Experimental Validation of Scalability\nThis section aims to assess the scalability of the parallelization design presented in Sect. 6 as we implemented\nit in the pe which is an open-source software framework for massively parallel simulations of rigid bodies [16, 17]. The implementation is based on velocity accumulators and corrections, as introduced in Sect. 6.3.\nThe accumulator initialization performs an additional\ninitial correction reduction step in all experiments.\nIn Sect. 7.1 the idea behind weak- and strong-scaling\nexperiments is explained before presenting the test problems for which those experiments are executed in Sect. 7.2.\nThe scaling experiments are performed on three clusters whose properties are summarized and compared\nin Sect. 7.3. Sect. 7.4 points out the fundamental differences in the scalability requirements of the two test\nproblems. Finally, in Sect. 7.5 the weak-scaling and in\nSect. 7.6 the strong-scaling results are presented for\neach test problem and cluster.\n\nt1\n.\ntp\n\nsp\nt1\n=\n.\np\nptp\n\nSometimes speedup and parallel efficiency are also\nstated with respect to a different baseline, that is, a\nsingle central processing unit (CPU) or a single node\nrather than a single hardware thread or core – the\nprinciple remains the same. The parallel efficiency in\na weak- and strong-scaling context is a simple performance metric that will serve in the following to assess\nthe quality of the parallelization.\n\n7.2 Test Problems\nThe scalability of the parallelization algorithm as it is\nimplemented in the pe framework is validated based on\ntwo fundamentally different families of test problems.\nSect. 7.2.1 describes a family of dilute granular gas setups whereas Sect. 7.2.2 describes a family of hexagonal\nclose packings of spheres corresponding to structured\nand dense setups. We chose these setups because their\ndemands towards the implementation vary considerably\nwhich will be analyzed in detail in Sect. 7.4.\n\n\f12\n\n7.2.1 Granular Gas\nGranular material attains a gaseous state when sufficient energy is brought into the system, for example by\nvibration. Consequently, granular gases feature a low\nsolid volume fraction and are dominated by binary collisions. When the energy supply ceases, the system cools\ndown due to dissipation in the collisions. Granular gases\nare not only observed in laboratory experiments, but\nappear naturally for example in planetary rings [35] and\nin technical applications such as granular dampers [19].\nThese systems in general exhibit interesting effects like\nthe inelastic collapse [24] or other clustering effects as\nthey e.g. can be observed in the Maxwell-demon experiment [41].\nAs initial conditions, a rectangular domain with confining walls is chosen. The domain contains a prescribed\nnumber of non-spherical particles arranged in a Cartesian grid. Random initial\n√ velocities are assigned to the\nparticles with up to 2103 m/s ≈ 0.35 m/s. The particles\nare composed of two to four spheres of varying radius\nin the range [0.6 cm, 0.8 cm], arranged at the boundary of a bounding sphere with a diameter of 1 cm. The\ndistance between the centers of two granular particles\nalong each spatial dimension is 1.1 cm, amounting to\na solid volume fraction of 23% on average. In [16] almost the same family of setups served as a scalability\ntest problem. However, there the granular gases had a\nsolid volume fraction of 3.8% on average. In order to\ntest a higher collision frequency, a more dense granular\n1\ngas was chosen here. The system is simulated for 10\ns,\nand the time step is kept constant at 100 µs, resulting\nin 1 000 time steps in total. Since the contacts are dissipative and no energy is added, the system is quickly\ncooling down. The coefficient of friction is 0.1 for any\ncontact.\nFor this test problem, the subdomain NBGS solver\nrequires a slight underrelaxation in order to prevent divergence. Using an underrelaxation parameter of 0.75\nproduces good results. For binary collisions, a single iteration of the solver would suffice, but because particles\ncluster due to the inelastic contacts, more iterations are\nrequired. This could be determined by a dynamic stopping criterion, but in the scenario presented here it was\nfound to be more efficient to perform a fixed number of\n10 iterations.\nFor particle simulations, the work load strongly depends on the number of particles and contacts. For the\nweak-scaling experiments, each process is responsible\nfor a rectangular subdomain, initially containing a fixed\nnumber of particles arranged in a Cartesian grid. For\nthe strong-scaling experiments, the total number of particles in x-, y-, and z-dimension should be divisible by\n\nTobias Preclik, Ulrich Rüde\n\nthe number of processes in x-, y-, and z-dimension that\nis used in the experiment. With this arrangement the\ninitial load is perfectly balanced. Statistically, the load,\nthat is the number of particles and contacts per subdomain, remains balanced if the subdomains are large\nenough, and clustering effects have not yet progressed\ntoo far. The duration of the simulation was chosen such\nthat the load remains well balanced throughout.\n7.2.2 Hexagonal Close Packing of Spheres\nThis setup aims to assess the scalability of the parallelization for dense granular setups. To demonstrate\nthe scalability, the initial setup should be easily and\nefficiently generateable for arbitrary problem sizes and\nshould feature a good load balance over a longer period of time. Hence, a hexagonal close packing of equal\nspheres was chosen, for which simple formulas for the\nposition of the spheres are available. The packing denπ\nsity is known to be 3√\n≈ 74.0%. According to the Ke2\npler conjecture, a hexagonal close packing is the densest\npossible packing of spheres. To avoid load imbalances\nat the boundaries, a domain is chosen that is periodic\nin the x- and y-dimension. In z-dimension the packing\nis confined by walls that are in direct contact with the\nspheres on both sides. Assuming an even number of\nparticles in y-direction, the number of contacts is permanently nx ny (6nz − 1) for nx × ny × nz particles. The\ndomain is decomposed in x- and y-dimensions only. The\nobjects are subject to gravity. However, the gravity is\ntilted in the x-z-plane such that the setup corresponds\nto a ramp inclined by 30◦ including a lid. The magnitude of gravity is 9.81 m/s2 . The time step is 10 µs\nconstantly. The radii of the particles are 1 mm, and\ntheir density is 2.65 g/cm3 . All particles get an initial\ndownhill velocity of 10 cm/s. The coefficient of friction\nis 0.85 constantly for any contact. The high coefficient\nof friction causes a slip-stick transition shortly after the\nsimulation begins. As in the granular gas setups the\nsubdomain NBGS uses an underrelaxation of 0.75. The\nsolver unconditionally performs 100 iterations in each\ntime step. This intentionally disregards that the iterative solver converges faster for smaller problems. A\nmultigrid solver could possibly remedy the dependence\non the problem size, but the successful construction of\nsuch a solver needs substantial further research.\n7.3 Test Machines\nIn the following all test machines are presented. Tab. 1\nsummarizes the basic information. The Emmy cluster\nis located at the Regional Computing Centre in Erlangen (RRZE) in Germany which is associated to the\n\n\fUltrascale Simulations of Non-smooth Granular Dynamics\ncluster name\n\n13\n\nEmmy\n\nSuperMUC\n\nJuqueen\n\nbest TOP 500 ranking\npeak performance in PFlop/s\nnumber of nodes\nnumber of sockets\nname of CPU\nclock rate in GHz\nnumber of cores per CPU\nnumber of threads per core\ntotal RAM in TiB\n\nRegional Computing Centre in\nErlangen (RRZE), Germany\n0.23\n560\n2\nIntel Xeon E5-2660 v2\n2.2\n10\n2\n35\n\nJülich Supercomputing\nCentre (JSC), Germany\n5th (November 2012)\n5.9\n28 672\n1\nIBM PowerPC A2\n1.6\n16\n4\n448\n\ninterconnection fabric\n\nInfiniband QDR\n\nnetwork topology\n\nnon-blocking tree\n\nLeibniz Supercomputing\nCentre (LRZ), Germany\n4th (June 2012)\n3.2\n9 216\n2\nIntel Xeon E5-2680\n2.7\n8\n2\n288\nInfiniband QDR/\nInfiniband FDR 10\nnon-blocking tree/\n4:1 pruned tree\n\ncomputing centre\n\nBlueGene/Q\n5D torus\n\nTable 1: The test machines used for performing the weak- and strong-scaling experiments.\nFriedrich-Alexander-Universität Erlangen-Nürnberg. The\ncluster comprises 560 compute nodes. Each node has a\ndual-socket board equipped with two Xeon E5-2660 v2\nprocessors. Each processor has 10 cores clocked at 2.2 GHz.\nThe processors offer 2-way simultaneous multithreading (SMT). The peak performance of the cluster is\n0.23 PFlop/s. Each node is equipped with 64 GiB of random access memory (RAM). The cluster features a fully\nnon-blocking Infiniband interconnect with quad data\nrate (QDR) and 4× link aggregation, resulting in a\nbandwidth of 40 Gbit/s per link and direction. In all experiments on the Emmy cluster, each core is associated\nwith a subdomain since preliminary tests showed that\nwe could not take advantage of the SMT features by\nassociating each hardware thread with a subdomain.\nThe Emmy cluster has the smallest peak performance\namong the test machines and was never among the 500\nworld’s largest commercially available supercomputers.\nHowever, it is the only machine with the largest nonblocking tree network topology and the largest amount\nof RAM per core.\nThe second test machine is the SuperMUC supercomputer which is located at the Leibniz Supercomputing Centre (LRZ) in Germany and was best ranked on\nthe 4th place of the TOP 500 list in June 2012. The\ncluster is subdivided into multiple islands. The majority of the compute power is contributed by the 18 thinnode islands. Each thin-node island consists of 512 compute nodes (excluding four additional spare nodes) connected to a fully non-blocking 648 port FDR10 Infiniband switch with 4× link aggregation, resulting in a\nbandwidth of 40 Gbit/s per link and direction. Though\nQDR and FDR10 use the same signaling rate, the effective data rate of FDR10 is more than 20% higher\nsince it uses a more efficient encoding of the transmitted data. The islands’ switches are each connected via\n\n126 links to 126 spine switches. This results in a blocking switch-topology. Thus, if e.g. all nodes within an\nisland send to nodes located in another island, then\nthe 512 nodes have to share 126 links to the spine\nswitches, effecting that the bandwidth is roughly one\nquarter of the bandwidth that would be available in an\noverall non-blocking switch-topology. Each (thin) compute node has two sockets, each equipped with an Intel Xeon E5-2680 processor having 8 cores clocked at\n2.7 GHz. The processors support 2-way SMT. In the\nfollowing, as in the case of the Emmy cluster, each core\nis associated with a single subdomain. The peak performance of the cluster is stated to be 3.2 PFlop/s. Each\nnode offers 32 GiB of RAM, summing up to 288 TiB in\ntotal. The SuperMUC supercomputer has an interesting blocking tree network-topology and the processors\nwith the highest clock rate among the processors in the\ntest machines.\nThe third test machine is the Juqueen supercomputer which is located at the Jülich Supercomputing\nCentre (JSC) in Germany and was best ranked on the\n5th place of the TOP 500 list in November 2012. The\ncluster is a BlueGene/Q system with 28 672 compute\nnodes since 2013 [14, 40]. Each node features a single\nIBM PowerPC A2 processor having 18 cores clocked at\n1.6 GHz, where only 16 cores are available for computing. The processors support 4-way SMT. The Juqueen\nsupercomputer is the only machine, where we decided\nto associate each hardware thread with a subdomain\nin the scaling experiments. The machine’s peak performance is 5.9 PFlop/s. Each node offers 16 GiB of RAM,\nsumming up to 448 TiB in total. The interconnect fabric\nis a 5D torus network featuring a bandwidth of 16 Gbit/s\nper link and direction [8]. The Juqueen supercomputer\nis the machine with the highest peak performance, the\nlargest number of cores and threads and the only ma-\n\n\f%\n9.5\n%\n\n25\n\n.8%\n\n8.0\n\n(a) Time-step profile of the\ngranular gas executed with\n5 × 2 × 2 = 20 processes on\na single node.\n\n%\n.7\n\n16.\n\n.3%\n%8\n5.9\n\n22\n\n%\n\n5%\n\n30.6%\n\nIn this section we clarify how much time is spent in\nthe various phases of the time-step procedure and how\nthis time changes in a weak scaling depending on the\ntest problem. Fig. 1 breaks down the wall-clock times\nof various time step components in two-level pie charts\nfor the granular gas scenario. The times are averaged\nover all time steps and processes. The dark blue section\ncorresponds to the fraction of the time in a time step\nused for detecting and filtering contacts. The orange\nsection corresponds to the time used for initializing the\nvelocity accumulators and corrections. The time to relax the contacts is indicated by the yellow time slice. It\nincludes the contact sweeps for all 10 iterations without the correction reductions. The time used by all correction reductions is shown in the green section which\nincludes the reductions for each iteration and the reduction after the initialization. The time slice is split\nup on the second level in the time used for assembling,\nexchanging, and processing the first correction reduction message (dark green section) and the time used\nfor assembling, exchanging, and processing the second\n\n12.6\n\n16.0%\n\n7.4 Time-Step Profiles\n\n%\n\n18.1%\n\nchine among our test machines with a torus interconnect.\nTab. 2 presents a summary of the domain partitionings used for the scaling experiments on the various clusters. The number of nodes are always a power\nof two except when using the whole machine or when\nperforming intra-node scalings. The intra-node scaling\nbehaviour is analyzed by means of weak-scaling experiments choosing the granular gas as a test problem and\nthe Emmy cluster as a test machine. The influence of\nthe number of dimensions in which the domain is partitioned is also only analyzed for this configuration.\nAll further scaling tests of the granular gas scenario\nuse three-dimensional domain partitionings. All internode weak-scaling experiments start with a single node\nand extend to the full machine where possible. The experiments on the SuperMUC supercomputer were obtained at the Extreme Scaling Workshop in July 2013\nat the LRZ, where at most 16 islands corresponding\nto 8 192 nodes were available. All strong-scaling experiments start on a single node except on the Juqueen\nsupercomputer, where we chose to start at 32 nodes\nwhich is the minimum allocation unit in the batch system on Juqueen. The experiments extend to a number\nof nodes where a notable efficiency degradation is observed. Since the results on the SuperMUC were obtained well before the other experiments, no scaling\nexperiments with the hexagonal close packing scenario\nwere performed.\n\nTobias Preclik, Ulrich Rüde\n\n25\n.9\n\n14\n\n(b) Time-step profile of the\ngranular gas executed with\n8 × 8 × 5 = 320 processes on\n16 nodes.\n\nFig. 1: The time-step profiles for two weak-scaling executions of the granular gas on the Emmy cluster with\n253 particles per process.\n\ncorrection reduction message (light green section). The\ntime slices are depicted counterclockwise in the given\norder. The message-exchange communications have a\ndotted border to distinguish them from the rest. A\nsingle message-exchange communication time measurement started, when sending the first message buffer to\nthe neighbors, and ended, when having received the last\nmessage buffer from the neighbors. The dark red section\ncorresponds to the time used by the time-integration\nof the positions, and the final blue section indicates\nthe time used by the position synchronization. The latter is split up into assembling, exchanging, and processing of the message in the inner ring. The messageexchange communication is highlighted by the dashed\nborder again. The first pie chart in Fig. 1a corresponds\nto the time-step profile of an execution in the weakscaling experiment with the three-dimensional domain\npartitioning 5 × 2 × 2 on a single node of the Emmy\ncluster. Fig. 1b shows the time-step profile of an execution in the weak-scaling experiment with the threedimensional domain partitioning 8 × 8 × 5 on 16 nodes.\nThe two time slices involving communication need more\ntime in comparison to Fig. 1a, especially the framed\nslices on the second level which amount to the communication. The wall-clock time for the components\ninvolving no communication was roughly the same in\nboth runs. The enlarged synchronization time-slices in\nFig. 1b then approximately amount to the increased\ntime-step duration on 16 nodes. Overall, computations\nin the time step of this granular gas scenario prevail.\nBut since the collision frequency is low, the 10 contact\nsweeps, marked by the yellow and green sections, are\ndominated by communication.\n\n\fUltrascale Simulations of Non-smooth Granular Dynamics\n\n1D\n\n2D\n\n3D\n\n15\n\nnodes\n\n1\n20\n\n2\n20\n\n4\n20\n\n8\n20\n\n10\n20\n\n16\n20\n\n1\n\npx\n\n1\n\n2\n\n4\n\n8\n\n10\n\n16\n\n20\n\nnodes\n\n4\n20\n\n8\n20\n\n10\n20\n\n16\n20\n\n1\n\n2\n\n4\n\npx\npy\n\n2\n2\n\n4\n2\n\n5\n2\n\n4\n4\n\n5\n4\n\n8\n10 16 20 32\n40\n64\n80\n5\n8\n10 16 20\n32\n40\n64\nweak-scaling granular gas\nweak-scaling hexagonal close packing\nstrong-scaling hexagonal close packing\n\nnodes\n\n8\n20\n\n16\n20\n\n1\n\n2\n\n4\n\n8\n\npx\npy\npz\n\n2\n2\n2\n\n4\n2\n2\n\n5\n2\n2\n\n5\n4\n2\n\n2\n\n4\n\n8\n\n16\n\n32\n\n40 80 160 320 640\nweak-scaling granular gas\n8\n\n16\n\n16\n\n32\n\n32\n\n64\n\n5\n8\n8\n10 16\n4\n5\n8\n8\n10\n4\n4\n5\n8\n8\nweak-scaling granular gas\nstrong-scaling granular\n\n64\n\n128\n\n128\n\n256\n\n512\n\n16\n16\n10\n\n20\n16\n16\n\n32\n20\n16\n\n64\n\n128\n\n256\n\n512\n\n1 280\n\n2 560\n\n5 120\n\n10 240\n\n256\n\n512\n128\n80\n\ngas\n\n(a) Domain partitionings used on the Emmy cluster.\n\n2D\n\n3D\n\nnodes\n\n1\n\n2\n\n4\n\n8\n\n16\n\n32\n\n64\n\n8 192\n\n16 384\n\n28 672\n\npx\npy\n\n8\n8\n\n16\n8\n\n16\n16\n\n32\n16\n\n32\n32\n\n64\n32\n\n64 128 128 256 256\n512\n512\n64 64\n128 128 256\n256\n512\nweak-scaling hexagonal close packing\nstrong-scaling hexagonal close packing\n\n128\n\n256\n\n512\n\n1 024\n512\n\n1 024\n1 024\n\n1 024\n1 792\n\nnodes\n\n1\n\n2\n\n4\n\n8\n\n16\n\n32\n\n64\n\n128\n\n4 096\n\n8 192\n\n16 384\n\n28 672\n\npx\npy\npz\n\n4\n4\n4\n\n8\n4\n4\n\n8\n8\n4\n\n8\n8\n8\n\n16\n8\n8\n\n16\n16\n8\n\n16\n16\n16\n\n32\n16\n16\n\n64\n64\n64\n\n128\n64\n64\n\n128\n128\n64\n\n128\n128\n112\n\n2 048\n\n4 096\n\n8 192\n\n32\n32\n32\n\n64\n32\n32\n\n64\n64\n32\n\n256\n\n512\n\n1 024\n\n1 024\n\n2 048\n\n2 048\n\n32\n32\n64\n64\n32\n32\n32\n64\n16\n32\n32\n32\nweak-scaling granular gas\nstrong-scaling granular gas\n\n4 096\n\n(b) Domain partitionings used on the Juqueen supercomputer.\n\n3D\n\nnodes\n\n1\n\n2\n\n4\n\n8\n\n16\n\n32\n\npx\npy\npz\n\n4\n2\n2\n\n4\n4\n2\n\n4\n4\n4\n\n8\n4\n4\n\n8\n8\n4\n\n8\n8\n8\n\n64\n\n128\n\n256\n\n512\n\n1 024\n\n16 16\n16\n32\n32\n8\n16\n16\n16\n32\n8\n8\n16\n16\n16\nweak-scaling granular gas\nstrong-scaling granular gas\n\n(c) Domain partitionings used on the SuperMUC supercomputer.\n\nTable 2: Summary of the domain partitionings used on all test clusters.\nFig. 2 presents time-step profiles for two weak-scaling\nexecutions of the hexagonal close packing scenario. The\ntime-step profiles use the same color coding as in Fig. 1.\nIn contrast to the time-step profiles of the granular gas\nscenario, the time step is dominated by the 100 contact\nsweeps (yellow section) and the 100 correction reductions (green section). Contact detection, position integration, and synchronization play a negligible role. In\nFig. 2a the time-step profile of a weak-scaling execution\nwith again 20 processes on a single node of the Emmy\ncluster is presented, whereas in Fig. 2b the time-step\nprofile of a weak-scaling execution with again 320 processes on 16 nodes is shown. The wall-clock time spent\nin the contact sweep was roughly the same in both executions, hence the increased communication costs are\n\nmainly responsible for the larger time slice of the correction reduction.\n\nThe time-step profiles showed that for the dilute\ngranular gas scenario the time spent in the various timestep components is well balanced and the time spent in\nthe communication routines moderately increases as the\nproblem size is increased. For the hexagonal close packings most of the time is spent in the contact sweeps and\nthe reduction of the velocity corrections. Components\nsuch as the position integration and the final synchronization play a negligible role due to the higher number\nof iterations in comparison to the granular gas scenario.\n\n\f16\n\nTobias Preclik, Ulrich Rüde\nGranular Gas\nEmmy\nnumber of particles per process\nnumber of time steps\nmaximum number of particles\ninitial number of contacts\nsolid volume fraction\n\n3\n\n25\n1 000\n1.6 · 108\n0\n23%\n\nHexagonal Close Packing\n\nJuqueen\n3\n\n10\n1 000\n1.8 · 109\n0\n23%\n\nSuperMUC\n3\n\nEmmy\n3\n\n10\n10 000\n1.3 · 108\n0\n3.8%\n\n10\n1 000\n1.0 · 107\n6.0 · 107\n74%\n\nJuqueen\n103\n100\n1.8 · 109\n1.1 · 1010\n74%\n\nTable 3: Summary of the test problem parameters used for the weak-scaling experiments.\n\n70000\n\nmemory bandwidth in GB/s\n\n67\n.5\n\n%\n\n30\n\n%\n.8\n\n%\n\n22\n\n%\n.9\n\n75\n.3\n\n60000\n50000\n40000\n30000\n20000\n10000\n0\n\nmeasured bandwidth of triad (ﬁrst series)\nmeasured bandwidth of triad (second series)\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\n14\n\n16\n\n18\n\n20\n\nnumber of processes\n\n(a) Time-step profile of the\nhexagonal close packing scenario executed with 5 × 2 ×\n2 = 20 processes on a single\nnode.\n\n(b) Time-step profile of the\nhexagonal close packing scenario executed with 8 × 8 ×\n5 = 320 processes on 16\nnodes.\n\nFig. 2: The time-step profiles for two weak-scaling executions of the hexagonal close packing scenario on the\nEmmy cluster with 103 particles per process.\n\n7.5 Weak-Scaling Results\n\nIn the following subsections the weak-scaling results\nfor both test problems on the clusters are presented.\nTab. 3 gives an overview of the employed parameters.\nThe experiments differ in terms of the number of particles generated per process depending on the amount of\nmemory available. In order to control the overall wallclock time the number of time steps performed varies\nbetween 100 and 10 000. All wall-clock times presented\nin the following subsections correspond to the average\nwall-clock time needed to perform a single time step\nper 1 000 particles facilitating the comparison of the\ncharts. The wall-clock times exclude the time needed to\nsetup the systems and generate the simulation output.\nThe scaling experiments of the granular gas scenario on\nSuperMUC differs from the other granular gas experiments in that the gas is considerably more dilute and a\nlonger period of time is simulated.\n\nFig. 3: Measured bandwidth of the triad in the stream\nbenchmark computed with a varying number of cores\non a single node of the Emmy cluster.\n\n7.5.1 Granular Gas\nFirst, we pay special attention to the intra-node weakscaling before turning to the inter-node weak-scaling\nsince the former is subject to the non-linear scaling\nbehaviour of the memory bandwidth. As a test problem we chose the granular gas scenario and as the test\nmachine the Emmy cluster. A single node in the cluster is equipped with two processors each one having\na single on-chip memory controller. The total memory\nbandwidth available to both sockets is exactly twice\nthe bandwidth of a single socket. However, for a single\nsocket a simple stream benchmark [23] reveals that the\nmemory architecture is designed such that for x cores\nmore than x1 of the socket’s total memory bandwidth is\navailable. Fig. 3 plots the measured memory bandwidth\nof computations of the triad as defined in the stream\nbenchmark. The computations were performed by a\nvarying number of cores in parallel. The first series of\nmeasurements minimized the number of sockets in use\nmeaning that the processor affinities of the processes\nwere adjusted such that all processes in measurements\nwith less or equal to 10 processes shared the same memory controller. In the second series of measurements the\nprocesses were pinned to the sockets alternately such\nthat 2x processes had twice the bandwidth at their dis-\n\n\fav. time per time step and 1000 particles in s\n\nUltrascale Simulations of Non-smooth Granular Dynamics\n0.0075\n\n3D\n2D\n1D\n3D\n2D\n1D\n\n0.007\n0.0065\n0.006\n0.0055\n\npartitioning\npartitioning\npartitioning\npartitioning\npartitioning\npartitioning\n\nmain partitionings are indeed consistently slightly better than the timings for two-dimensional domain partitionings, which are in turn slightly better than the\ntimings for three-dimensional domain partitionings.\n\n(ﬁrst series)\n(ﬁrst series)\n(ﬁrst series)\n(second series)\n(second series)\n(second series)\n\n0.005\n0.0045\n0.004\n0.0035\n0.003\n0.0025\n0.002\n\n1\n\n2\n\n4\n\n17\n\n8\n\n10\n\n16\n\n20\n\nnumber of processes\n\nFig. 4: Intra-node weak-scaling graphs for a granular\ngas on the Emmy cluster.\n\nposal as x processes in the first series. Indeed, the lowerleft part of the first series’ graph very well matches the\nsecond series’ graph with proper scaling. The measured\nbandwidth in the first series increases for an increasing\nnumber of processes until the available memory bandwidth of the first memory controller is saturated. Measurements with more than 10 processes start to make\nuse of the second memory controller and the measured\nbandwidth continues to increase linearly.\nAn analogous behavior can be observed in the intranode weak-scaling graphs. Fig. 4 plots the average wallclock time needed for a single time step and 1 000 particles. In the first series of executions again the pinning strategy minimizing the number of sockets in use\nwas employed. The average wall-clock time needed per\ntime step increases considerably for executions with\nup to 10 processes. However, beyond that point the\nweak-scaling graph continues almost ideally. The second series of executions used as before the pinning strategy minimizing the maximum number of processes per\nsocket meaning that executions with 2x processes in\nthe second series have twice the bandwidth at their\ndisposal as the executions with x processes in the first\nseries. The graphs of the second series show that the\nwall-clock times needed per time step on 2x processes\nindeed closely match the wall-clock times on x processes\nin the first series. This indicates that our implementation is limited by the available memory bandwidth.\nThe figure also distinguishes between weak-scaling\ngraphs with one-, two-, and three-dimensional domain\npartitionings since their communication volumes differ.\nHigher-dimensional non-periodic domain partitionings\nhave typically a higher communication volume in comparison to lower dimensional non-periodic domain partitionings with the same number of processes, due to\nthe larger area of the interfaces between the subdomains. The plotted timings for the one-dimensional do-\n\nEven though the intra-node weak-scaling results reveal an underperforming parallel efficiency between 30.8%\nand 32.9% when computing on all cores of an Emmy\nnode, the correlation with the measured memory bandwidth of a triad suggests that a good intra-node scaling can be expected as long as the available bandwidth\nscales. With corresponding pinning this is the case as\noff the first full socket on the Emmy cluster.\nFig. 5a extends the weak-scaling experiment to almost the full Emmy cluster for one-, two-, and threedimensional domain partitionings. The scaling experiment for the one-dimensional domain partitionings performs best and achieves on 512 nodes a parallel efficiency of 98.3% with respect to the single-node performance. The time measurements for two-dimensional\ndomain partitionings are consistently slower, but the\nparallel efficiency does not drop below 89.7%. The time\nmeasurements for three-dimensional domain partitionings come in last, and the parallel efficiency goes down\nto 76.1% for 512 nodes. This behaviour can be explained\nby the differences in the communication volumes of\none-, two-, and three-dimensional domain partitionings.\nThe results attest that the problem can be efficiently\nscaled (almost) up to the full machine if the load per\nprocess is sufficiently large.\nFig. 5b shows the results of the inter-node weakscaling experiments on the Juqueen supercomputer. The\nscaling experiments were only performed with the more\ndemanding three-dimensional domain parititionings. In\nthe first series of measurements the average wall-clock\ntime per time step increases as expected up to 2 048 nodes.\nBut then the average time-step duration for setups with\n4 096 nodes and beyond is significantly shorter than the\naverage time-step duration with fewer nodes. The time\nsteps are even computed faster than on a single node,\nwhere no inter-node communication takes place at all.\nAssuming that intra-node communication is faster than\ninter-node communication, this is a puzzling result. In\nfact, it turned out that the intra-node communication\nwas responsible for the behaviour: The default mechanism for intra-node communication is via shared memory on the Juqueen. In the second series of measurements we disallowed the usage of shared memory for\nintra-node communication. This resulted in the measurements that are consistently faster than the measurements from the first series, and the parallel efficiency is more or less monotonically decreasing with an\nexcellent parallel efficiency of at least 92.9%.\n\n\f18\n\nTobias Preclik, Ulrich Rüde\n1.2\n\nav. time per time step (3D partitioning)\nav. time per time step (2D partitioning)\nav. time per time step (1D partitioning)\nparallel eﬃciency (3D partitioning)\n\n0.0085\n\n1\n0.8\n\n0.008\n0.6\n0.0075\n0.4\n0.007\n\n0.0065\n\nparallel eﬃciency\n\nav. time per time step and 1000 particles in s\n\n0.009\n\n0.2\n\n1\n\n2\n\n4\n\n8\n\n16\n\n32\n\n64\n\n128\n\n256\n\n0\n512\n\nnumber of nodes\n\n0.116\n\n1.2\n\n0.114\n1\n\n0.112\n0.11\n\n0.8\n\n0.108\n0.106\n\n0.6\n\n0.104\n0.4\n\n0.102\n0.1\n\n0.096\n\n0.2\n\nav. time per time step (ﬁrst series)\nav. time per time step (second series)\nparallel eﬃciency (second series)\n\n0.098\n1\n\n4\n\n16\n\n64\n\n256\n\n1024\n\n4096\n\nparallel eﬃciency\n\nav. time per time step and 1000 particles in s\n\n(a) Weak-scaling graph on the Emmy cluster.\n\n16384\n\n0\n\nnumber of nodes\n\n0.0075\n\n1.2\n\n0.007\n1\n0.0065\n0.006\n\n0.8\n\n0.0055\n0.6\n0.005\n0.0045\n\n0.4\n\nparallel eﬃciency\n\nav. time per time step and 1000 particles in s\n\n(b) Weak-scaling graph on the Juqueen supercomputer.\n\n0.004\n0.2\n0.0035\n0.003\n\nav. time per time step\nparallel eﬃciency\n1\n\n2\n\n8\n\n32\n\n128\n\n512\n\n2048\n\n0\n8192\n\nnumber of nodes\n\n(c) Weak-scaling graph on the SuperMUC supercomputer.\n\nFig. 5: Inter-node weak-scaling graphs for a granular\ngas on all test machines.\n\nThe reason why the measured times in the first\nseries became shorter for 4 096 nodes and more is revealed when considering how the processes get mapped\nto the hardware. The default mapping on Juqueen is\nABCDET, where the letters A to E stand for the five\ndimensions of the torus network, and T stands for the\nhardware thread within each node. The six-dimensional\ncoordinates are then mapped to the MPI ranks in a\nrow-major order, that is, the last dimension increases\n\nfastest. The T coordinate is limited by the number of\nprocesses per node, which was 64 for the above measurements. Upon creation of a three-dimensional communicator, the three dimensions of the domain partitioning are mapped also in row-major order. This effects, if\nthe number of processes in z-dimension is less than the\nnumber of processes per node, that a two-dimensional\nor even three-dimensional section of the domain partitioning is mapped to a single node. However, if the number of processes in z-dimension is larger or equal to the\nnumber of processes per node, only a one-dimensional\nsection of the domain partitioning is mapped to a single\nnode. A one-dimensional section of the domain partitioning performs considerably less intra-node communication than a two- or three-dimensional section of the\ndomain partitioning. This matches exactly the situation for 2 048 and 4 096 nodes. For 2 048 nodes, a twodimensional section 1×2×32 of the domain partitioning\n64×64×32 is mapped to each node, and for 4 096 nodes\na one-dimensional section 1 × 1 × 64 of the domain partitioning 64 × 64 × 64 is mapped to each node. To substantiate this claim, we confirmed that the performance\njump occurs when the last dimension of the domain partitioning reaches the number of processes per node, also\nwhen using 16 and 32 processes per node.\nFig. 5c presents the weak-scaling results on the SuperMUC supercomputer. The setup differs from the\ngranular gas scenario presented in Sect. 7.2.1 in that it\nis more dilute. The distance between the centers of two\ngranular particles along each spatial dimension is 2 cm,\namounting to a solid volume fraction of 3.8% and consequently to less collisions. As on the Juqueen supercomputer only three-dimensional domain partitionings were\nused. All runs on up to 512 nodes were running within a\nsingle island. The run on 1 024 nodes also used the minimum number of 2 islands. The run on 4 096 nodes used\nnodes from 9 islands, and the run on 8 192 nodes used\nnodes from 17 islands, that is both runs used one island\nmore than required. The graph shows that most of the\nperformance is lost in runs on up to 512 nodes. In these\nruns only the non-blocking intra-island communication\nis utilised. Thus this part of the setup is very similar\nto the Emmy cluster since it also has dual-socket nodes\nwith Intel Xeon E5 processors and a non-blocking tree\nInfiniband network. Nevertheless, the intra-island scaling results are distinctly worse. The reasons for these\ndifferences were not yet further investigated. However,\nthe scaling behaviour beyond a single island is decent\nfeaturing a parallel efficiency of 73.8% with respect to\na single island. A possible explanation of the underperforming intra-node scaling behaviour could be that\nsome of the Infiniband links were degraded to QDR,\nwhich was a known problem at the time the extreme-\n\n\f0.135\n\n1.2\n\n0.13\n\n1\n\n0.125\n\n0.8\n\n0.12\n\n0.6\n\n0.115\n\n0.4\n\n0.11\n\n0.105\n\n19\n\nparallel eﬃciency\n\nav. time per time step and 1000 particles in s\n\nUltrascale Simulations of Non-smooth Granular Dynamics\n\n0.2\naverage time per time step\nparallel eﬃciency\n1\n\n2\n\n4\n\n8\n\n16\n\n32\n\n64\n\n128\n\n256\n\n0\n512\n\nnumber of nodes\n\n7.6 Strong-Scaling Results\n\n1.38\n\n1.2\n\n1.36\n\n1\n\n1.34\n\n0.8\n\n1.32\n0.6\n1.3\n0.4\n\n1.28\n\nparallel eﬃciency\n\nav. time per time step and 1000 particles in s\n\n(a) Weak-scaling graph on the Emmy cluster.\n\nIn the following subsections the strong-scaling results\nfor both test problems on the clusters are presented.\nTab. 4 gives an overview of the employed parameters.\nThe experiments differ in terms of the number of particles generated in total and the number of time steps\nused for averaging. As in the weak-scaling experiments\nthe granular gas scenario on SuperMUC is considerably\nmore dilute than on the other machines.\n\n0.2\n\n1.26\n1.24\n\nto a single node stayed above 91.4% for all measurements. This result is almost as good as the 92.9% parallel efficiency in the scaling experiments of the granular gas. The largest execution ran 1 024 × 1 792 × 1 =\n1 835 008 processes on all 28 672 nodes of the machine,\nwhere 10 240×17 920×10 = 1 835 008 000 particles were\nspawned, in total leading to 10 826 547 200 ≈ 1.1 · 1010\ncontacts – again a possibly record-breaking number for\nnon-smooth contact dynamics.\n\naverage time per time step\nparallel eﬃciency\n1\n\n4\n\n16\n\n64\n\n256\n\n1024\n\n4096\n\n16384\n\n0\n\nnumber of nodes\n\n(b) Weak-scaling graph on the Juqueen supercomputer.\n\nFig. 6: Inter-node weak-scaling graphs for hexagonal\nclose packings of spheres.\nscaling workshop took place. The communication routines then need 54 ·· 64\n66 ≈ 1.21 times longer to complete.\nThis could also explain the high variability of the runs’\nwall-clock times.\nSubsequently, a second series of measurements was\nperformed with 603 non-spherical particles per process.\nThe scaling behaviour is comparable to the scaling behaviour observed in Fig. 5c. However, the largest weakscaling run simulated 28 311 552 000 ≈ 2.8 · 1010 nonspherical particles – a possibly record-breaking number\nfor non-smooth contact dynamics.\n7.5.2 Hexagonal Close Packings of Spheres\nFig. 6a shows the average wall-clock time needed for a\nsingle time step in the hexagonal close packing test on\nthe Emmy cluster. The parallel efficiency with respect\nto a single node remains above 79.9% for all executions. This is slightly better than the parallel efficiency\nof 76.1% for the granular gas.\nThe weak-scaling results of the hexagonal close packing scenario on the Juqueen supercomputer are presented in Fig. 6b. The parallel efficiency with respect\n\n7.6.1 Granular Gas\nFig. 7 presents the strong-scaling results of the granular gas scenario on all clusters. The strong-scaling graph\non the Emmy cluster is presented in Fig. 7a. A total of\n320 × 160 × 160 = 8 192 000 particles was used, leading\nto at most 64 × 80 × 80 = 409 600 particles per process\non a single node and at least 10 × 8 × 10 = 800 particles\nper process on 512 nodes. The speedup is ideal for up to\n64 nodes and then gradually becomes more inefficient.\nHowever, no turnover is observed. Some time measurements exceed the optimal speedup, which can happen\nfor example if the problem becomes small enough to fit\ninto one of the caches. In conclusion, the scaling experiments for this dilute setup on the Emmy cluster suggest\nthat one obtains a satisfactory parallel efficiency on the\nwhole cluster, as long as several thousand particles are\npresent per process.\nFig. 7b presents the results of the strong-scaling experiments on the Juqueen supercomputer for the granular gas. The total number of particles was 32 768 000\nparticles. In the execution on 32 nodes each of the\n16×16×8 = 2 048 processes initially had 20×20×40 =\n16 000 non-spherical particles, and in the execution on\n4 096 nodes each of the 64 × 64 × 64 = 262 144 processes spawned 5 × 5 × 5 = 125 particles. The parallel\nefficiency is plotted with respect to 32 nodes and stays\nabove 80.7% for up to 1 024 nodes and 500 particles\nper process before rapidly decreasing. On 4 096 nodes\nthe efficiency is at 55.4%. The weak- and strong-scaling\nresults are both better in comparison to the Emmy clus-\n\n\f20\n\nTobias Preclik, Ulrich Rüde\nGranular Gas\n\nnumber of particles\nnumber of time steps\nsolid volume fraction\n\nHexagonal Close Packing\n\nEmmy\n\nJuqueen\n\nSuperMUC\n\nEmmy\n\nJuqueen\n\n320 × 160 × 160\n1 000\n23%\n\n320 × 320 × 320\n1 000\n23%\n\n128 × 128 × 128\n100\n3.8%\n\n1 280 × 640 × 10\n50\n74%\n\n2 048 × 2 048 × 10\n20\n74%\n\nTable 4: Summary of the test problem parameters used for the strong-scaling experiments.\n\n512\n\n1.2\n\n256\n1\n\nspeedup\n\n64\n\n0.8\n\n32\n0.6\n16\n8\n\n0.4\n\nparallel eﬃciency\n\n128\n\n4\n\n1\n\n0.2\n\nideal speedup\nspeedup\nparallel eﬃciency\n\n2\n1\n\n2\n\n4\n\n8\n\n16\n\n32\n\n64\n\n128\n\n256\n\n0\n512\n\nnumber of nodes\n\n(a) Strong-scaling graph on the Emmy cluster.\n128\n\nspeedup\n\n0.8\n\n16\n0.6\n8\n0.4\n\n4\n\n0.2\n\nideal speedup\nspeedup\nparallel eﬃciency\n\n2\n\n32\n\n64\n\n128\n\n256\n\n512\n\n1024\n\nparallel eﬃciency\n\n1\n\n32\n\n1\n\n7.6.2 Hexagonal Close Packings of Spheres\n\n1.2\n\n64\n\n2048\n\n0\n4096\n\nnumber of nodes\n\n(b) Strong-scaling graph on the Juqueen supercomputer.\n1024\n\n1.2\n\n512\n\nspeedup\n\n128\n\n0.8\n\n64\n32\n\n0.6\n\n16\n0.4\n\n8\n4\n\n1\n\n0.2\n\nideal speedup\nspeedup\nparallel eﬃciency\n\n2\n1\n\n2\n\n4\n\n8\n\n16\n\n32\n\n64\n\n128\n\n256\n\nparallel eﬃciency\n\n1\n\n256\n\n512\n\nter, owed to the torus network which performs excellent\nfor the nearest-neighbor communication.\nThe results of the strong-scaling experiments on the\nSuperMUC supercomputer are shown in Fig. 7c. In total 1283 non-spherical particles were simulated. Hence,\nin the single-node run each process owned 32×64×64 =\n131 072 particles, and in the run on 1 024 nodes, each\nprocess owned 8 × 4 × 4 = 128 particles. The parallel\nefficiency is at 90.0% on 256 nodes. Beyond that point\nit decreases dramatically, indicating that the scaling is\nfine as long as at least about 500 particles are present\nper process.\n\n0\n1024\n\nnumber of nodes\n\n(c) Strong-scaling graph on the SuperMUC supercomputer.\n\nFig. 7: Strong-scaling graphs for a granular gas test\nproblem on all test machines.\n\nIn the strong-scaling experiment on the Emmy cluster\nin total 1 280 × 640 × 10 = 8 192 000 particles were generated. The experiment was run for 1 to 512 nodes, such\nthat the smallest setup with 5 × 4 × 1 = 20 processes on\na single node generated 256 × 160 × 10 = 409 600 spherical particles per process, and the largest setup with\n128 × 80 × 1 = 10 240 processes on 512 nodes generated\n10 × 8 × 10 = 800 particles per process. Fig. 8a presents\nthe results. A super-linear speedup is observed for several executions, which is likely due to caching effects,\nsince the working set size becomes very small. In the\nstrong-scaling experiment for 512 nodes of the granular gas scenario on Emmy also only 800 particles were\ngenerated per process. However, the computational intensity here is much higher in comparison to that of the\ngranular gas, because far more contacts have to be resolved. This explains the high parallel efficiency of 113%\nin comparison to the disappointing parallel efficiency of\n37.7% from Fig. 7a. In conclusion, the scaling experiments suggest that a few hundred particles per process\nare enough to achieve a very good parallel efficiency on\nthe Emmy cluster if the granular flow is dense.\nFor the strong-scaling experiment on the Juqueen\nsupercomputer a hexagonal close packing with 41 943 040 particles in total was created. The smallest execution ran\n64 × 32 × 1 = 2 048 processes on 32 nodes, where 32 ×\n64 × 10 = 20 480 spherical particles were generated per\nprocess. The largest execution ran 512 × 512 × 1 =\n262 144 processes on 4 096 nodes, where 4 × 4 × 10 =\n160 particles were generated per process. Fig. 8b shows\n\n\fUltrascale Simulations of Non-smooth Granular Dynamics\n\n21\n\n1.2\n\n512\n256\n\n1\n0.8\n\nspeedup\n\n64\n32\n\n0.6\n\n16\n0.4\n\n8\n4\n\n1\n\n0.2\n\nideal speedup\nspeedup\nparallel eﬃciency\n\n2\n1\n\n2\n\n4\n\n8\n\n16\n\n32\n\n64\n\n128\n\nparallel eﬃciency\n\n128\n\n256\n\n0\n512\n\nnumber of nodes\n\n(a) Strong-scaling graph on the Emmy cluster.\n128\n\n1.2\n\n64\n\nspeedup\n\n0.8\n\n16\n0.6\n8\n0.4\n\n4\nideal speedup\nspeedup\nparallel eﬃciency\n\n2\n1\n\n32\n\n64\n\n128\n\n256\n\n512\n\n1024\n\n2048\n\nparallel eﬃciency\n\n1\n\n32\n\n0.2\n\n0\n4096\n\nnumber of nodes\n\n(b) Strong-scaling graph on the Juqueen supercomputer.\n\nFig. 8: Strong-scaling graphs for hexagonal close packings of spheres.\n\nthe speedup and the parallel efficiency on the second\naxis, both with respect to 32 nodes. A parallel efficiency of 75.0% on 4 096 nodes is achieved, where only\n160 particles were owned per process. This suggests that\na reasonable good efficiency can be achieved for a dense\nsetup on the Juqueen supercomputer, as long as several\nhundred particles are created per process.\n\n8 Related Work\nOther authors have proposed approaches for parallelizing non-smooth contact dynamics on architectures with\ndistributed memory. All of them are based on domain\npartitionings. A parallelization strategy termed nonsmooth contact domain decomposition (NSCDD) implemented in the renowned LMGC90 code was lately\npresented in [38, 39] by Visseq et al. The approach\nis inspired by the finite element tearing and interconnect (FETI) method for solving partial differential equations in computational mechanics. The authors suggest to decouple the multi-contact problem such that\non each process a multi-contact problem is solved hav-\n\ning the same structure as a multi-contact problem that\nis solved sequentially. Particles with multiple contacts\nthat are associated with different subdomains are duplicated, similar to shadow copies used in this article.\nHowever, the mass and inertia are split among all instantiations. The coupling is recovered by adding linear\nequations gluing the duplicates back together through\nadditional Lagrange multipliers. In contrast to the contact constraints, the interface equations are linear, and\na block-diagonal system of linear equations must be\nsolved after several sweeps over all contacts. In [39], the\nauthors present simulations with up to 2 · 105 spherical particles and 2 · 106 contacts, time-integrated on\nup to 100 processes. The NSCDD allows non-nearestneighbor communication in order to allow enlarged rigid\nbodies instead of introducing a concept analogous to\nglobal bodies.\nPrior to Visseq et al., Koziara et al. presented the\nparallelization implemented in the solfec code [20]. This\napproach dispenses with the separation into interface\nproblems and local multi-contact problems. A classic\nNBGS is parallelized with a non-negligible but inevitable\namount of serialization. Bodies are instantiated redundantly on all processes, prohibiting scaling beyond the\nmemory limit. Instead of using accumulator and correction variables, as proposed in this paper, the authors\nsynchronize dummy particles (particles that are in contact with shadow copies or original instances) in addition to shadow copies in order to implement contact\nshadow copies. As in the NSCDD, the system matrix\n(Delassus operator) is set up explicitly instead of using\nmatrix-free computations as proposed here. Simulations\nare presented with up to 1 · 104 polyhedral particles or\n6 · 105 contacts time-integrated on up to 64 processes.\nAt the same time, Shojaaee et al. presented another\ndomain partitioning method in [34]. The presentation\nis restricted to two-dimensional problems. The solver\nin the paper corresponds to a subdomain NBGS with\nrelaxation parameter ω = 1, where the authors argue\nthat divergence does typically not occur. At least for\nthree-dimensional simulations this is in our experience\nnot sufficient. Shadow copies are created not only if the\nhulls overlap the neighboring subdomain but also if the\nparticles approach the subdomain boundaries, simplifying the intersection testing but introducing excessive\nshadow copies. Shojaaee et al. also introduce contact\nshadow copies instead of using accumulator and correction variables as proposed here. Simulations are presented with up to 1 · 106 circular particles in a dense\npacking on up to 256 processes.\nThe approach presented in this paper improves in\ngeneral the robustness and scalability of previously published parallel algorithms. The matrix-free approach fa-\n\n\f22\n\ncilitates the evaluation of the particle wrenches in parallel as suggested in Sect. 6.3 and thus reduces the\namount of communicated data. The separation of bodies into global and local bodies allows to restrict messageexchange communications to nearest neighbors as detailed in Sect. 6.4 and thus maps well to various interconnect networks. Furthermore, the synchronization\nprotocol defined in Sect. 6.2 and Sect. 6.5 is not susceptible to numerical errors in contrast to the conventional\nrules which are based on contact locations. Last but not\nleast the scaling experiments from Sect. 7 with up to\n2.8 · 1010 non-spherical particles or 1.1 · 1010 contacts on\nup to 1.8 · 106 processes exceed all previously published\nnumbers by a factor of 103 to 104 .\n9 Summary\nThis article presents models and algorithms for performing scalable direct numerical simulations of granular matter in hard contact as we implemented them\nin the pe open-source software framework for massively\nparallel simulations of rigid bodies. The pe framework\nalready has been successfully used to simulate granular systems with and without surrounding fluid in the\npast [12, 6].\nThe discretization of the equations of motion underlying the time-stepping scheme use an integrator\nof order one. Contacts are modelled as inelastic and\nhard contacts with Coulomb friction. The hard contact model avoids the necessity to resolve the collision\nmicro-dynamics and the time-stepping scheme avoids\nthe necessity to resolve impulsive events in time. The\none-step integration can be split into the integration\nof the velocities and the subsequent integration of the\npositions and orientations.\nThe velocity integration requires the solution of a\nnon-linear system of equations per time step. In order to reduce the size of the system in the first place\nconventional broad-phase contact detection algorithms\nare applied to exclude contacts between intersection\nhulls. To solve the non-linear system of equations the\nsubdomain non-linear block Gauss-Seidel is used. The\nnumerical solution algorithm is a mixture between a\nnon-linear block Gauss-Seidel (NBGS) and a non-linear\nblock Jacobi with underrelaxation. In contrast to a pure\nnon-linear block Jacobi it only requires a mild underrelaxation and in contrast to a non-linear block GaussSeidel it accommodates the subdomain structure of the\ndomain partitioning and thus allows an efficient parallelization avoiding irregular data dependencies across\nsubdomains. The implementation of the subdomain NBGS\nin the pe is matrix-free and thus avoids the expensive assembly of the Delassus operator. Furthermore, the use\n\nTobias Preclik, Ulrich Rüde\n\nof accumulators and correction variables enables the\nevaluation of the particle wrenches in parallel, reuses\npartial results and reduces the number of particles that\nneed to be synchronized.\nThe integration of the positions and orientations is\nentailed by the execution of an exceptionally robust\nsynchronization protocol. The key to obtain this robustness is to add the rank of the parent process and\nthe ranks of the shadow copy holders to the state of\neach particle and to explicitly communicate the state\nchanges. Only then processes can reliably agree upon\nresponsibilities such as contact treatment and particle\nintegration without being susceptible to numerical errors.\nBeyond that, all messages are aggressively aggregated in order to reduce the communication overhead of\nsmall messages and all messages are restricted to nearest neighbors. The latter is achieved by splitting bodies\ninto local and global bodies and identifying appropriate\nrequirements. Both measures improve the scalability of\nthe implementation.\nFinally, the scalability was demonstrated for dilute\nand dense setups on three clusters, two of them having\nbeen in the top 10 of the world’s largest publicly available supercomputers. The parallel efficiency on Juqueen\nis outstanding. T he inter-island scaling results on SuperMUC are satisfactory, however, the intra-island scaling results show room for improvement. That this is not\ninherently caused by the parallelization approach can\nbe seen by inspecting the results of the Emmy cluster,\nwhose architecture is close to a single island of SuperMUC.\nThe largest scaling experiments demonstrate that\nsimulations of unprecedented scale with up to 2.8 · 1010\nnon-spherical particles and up to 1.1 · 1010 contacts are\npossible using up to 1.8 · 106 processes. The systmatic\nevaluation also confirms that good parallel efficiency\ncan be expected on millions of processes even if only\na few hundred particles are allocated to each process\nprovided that the computation exhibits a sufficiently\nhigh computational intensity and the architecture has\na good interconnect network .\nThe favourable scalability results do not account for\nthe fact that the NBGS solver does not scale (algorithmically) in terms of the number of iterations needed to\nachieve a given error bound. Possible future developments arise out of that: The convergence rate of multigrid methods is independent of the number of unknowns\nand is in that sense optimal. A successful construction\nof such a multigrid method for hard contact problems\nwould be invaluable for simulating every-increasing system sizes.\n\n\fUltrascale Simulations of Non-smooth Granular Dynamics\n\nReferences\n1. Anitescu M, Potra F (1997) Formulating dynamic\nmulti-rigid-body contact problems with friction as\nsolvable linear complementarity problems. Nonlinear Dynamics 14(3):231–247\n2. Anitescu M, Potra F (2002) A time-stepping\nmethod for stiff multibody dynamics with contact\nand friction. International Journal for Numerical\nMethods in Engineering 55(7):753–784\n3. Anitescu M, Tasora A (2010) An iterative approach\nfor cone complementarity problems for nonsmooth\ndynamics. Computational Optimization and Applications 47(2):207–235\n4. van den Bergen G (1999) A fast and robust GJK\nimplementation for collision detection of convex objects. Journal of Graphics Tools 4(2):7–25\n5. van den Bergen G (2001) Proximity queries and\npenetration depth computation on 3D game objects. In: Game Developers Conference, vol 170\n6. Bogner S, Mohanty S, Rüde U (2015) Drag correlation for dilute and moderately dense fluid-particle\nsystems using the lattice boltzmann method. International Journal of Multiphase Flow 68(0):71–79\n7. Bonnefon O, Daviet G (2011) Quartic formulation\nof Coulomb 3D frictional contact. Technical Report\nRT-0400, INRIA\n8. Chen D, Eisley N, Heidelberger P, Senger R, Sugawara Y, Kumar S, Salapura V, Satterfield D,\nSteinmacher-Burow B, Parker J (2012) The IBM\nBlue Gene/Q interconnection fabric. IEEE Micro\n32(1):32–43\n9. Cohen J, Lin M, Manocha D, Ponamgi M (1995) ICOLLIDE: An interactive and exact collision detection system for large-scale environments. In: Proceedings of the 1995 symposium on Interactive 3D\ngraphics, ACM, pp 189–ff\n10. Diebel J (2006) Representing attitude: Euler angles, unit quaternions, and rotation vectors\n11. Esefeld B (2014) Numerische Integration von\nMehrkörpersystemen mit mengenwertigen Kraftgesetzen. Herbert Utz Verlag\n12. Fischermeier E, Bartuschat D, Preclik T, Marechal\nM, Mecke K (2014) Simulation of a hardspherocylinder liquid crystal with the pe. Computer\nPhysics Communications 185(12):3156–3161\n13. Gilbert E, Johnson D, Keerthi S (1988) A fast procedure for computing the distance between complex\nobjects in three-dimensional space. IEEE Journal of\nRobotics and Automation 4(2):193–203\n14. Gilge M, et al (2013) IBM System Blue Gene Solution Blue Gene/Q Application Development. IBM\nRedbooks\n\n23\n\n15. Iglberger K, Rüde U (2009) Massively parallel rigid\nbody dynamics simulations. Computer ScienceResearch and Development 23(3-4):159–167\n16. Iglberger K, Rüde U (2010) Massively parallel\ngranular flow simulations with non-spherical particles. Computer Science-Research and Development\n25(1-2):105–113\n17. Iglberger K, Rüde U (2011) Large-scale rigid\nbody simulations. Multibody System Dynamics\n25(1):81–95\n18. Jean M (1999) The non-smooth contact dynamics\nmethod. Computer Methods in Applied Mechanics\nand Engineering 177(3–4):235–257\n19. Kollmer J, Sack A, Heckel M, Pöschel T (2013)\nRelaxation of a spring with an attached granular\ndamper. New Journal of Physics 15(9):093,023\n20. Koziara T, Bićanić N (2011) A distributed memory\nparallel multibody contact dynamics code. International Journal for Numerical Methods in Engineering 87(1–5):437–456\n21. Leyffer S (2006) Complementarity constraints as\nnonlinear equations: Theory and numerical experience. In: Optimization with Multivalued Mappings,\nSpringer, pp 169–208\n22. Liu C, Jain S (2012) A quick tutorial on multibody\ndynamics. Tech. rep., Georgia Institute of Technology\n23. McCalpin J (1995) Memory bandwidth and machine balance in current high performance computers. IEEE Computer Society Technical Committee\non Computer Architecture (TCCA) Newsletter pp\n19–25\n24. McNamara S, Young W (1994) Inelastic collapse in\ntwo dimensions. Physical Review E 50(1):R28–R31\n25. Miller S, Luding S (2004) Event-driven molecular dynamics in parallel. Journal of Computational\nPhysics 193(1):306–316\n26. Mitarai N, Nakanishi H (2012) Granular flow: Dry\nand wet. The European Physical Journal Special\nTopics 204(1):5–17\n27. Moreau J, Panagiotopoulos P (1988) Nonsmooth\nMechanics and Applications, vol 302. Springer\n28. Negrut D, Tasora A, Mazhar H, Heyn T, Hahn P\n(2012) Leveraging parallel computing in multibody\ndynamics. Multibody System Dynamics 27(1):95–\n117\n29. Popa C, Preclik T, Rüde U (2014) Regularized solution of LCP problems with application to rigid\nbody dynamics. Numerical Algorithms pp 1–12\n30. Sauer J, Schömer E (1998) A constraint-based approach to rigid body dynamics for virtual reality\napplications. In: Proceedings of the ACM Symposium on Virtual Reality Software and Technology,\n\n\f24\n\npp 153–162\n31. Schindler T, Acary V (2014) Timestepping schemes\nfor nonsmooth dynamics based on discontinuous\nGalerkin methods: Definition and outlook. Mathematics and Computers in Simulation 95(0):180–199\n32. Schütte K, van der Waerden B (1952) Das Problem der dreizehn Kugeln. Mathematische Annalen\n125(1):325–334\n33. Shen Y, Stronge W (2011) Painlevé paradox during\noblique impact with friction. European Journal of\nMechanics - A/Solids 30(4):457–467\n34. Shojaaee Z, Shaebani M, Brendel L, Török J, Wolf\nD (2012) An adaptive hierarchical domain decomposition method for parallel contact dynamics simulations of granular materials. Journal of Computational Physics 231(2):612–628\n35. Spahn F, Petzschmann O, Schmidt J, Sremčević M,\nHertzsch JM (2001) Granular viscosity, planetary\nrings and inelastic particle collisions. In: Granular\nGases, Springer, pp 363–385\n36. Studer C (2009) Numerics of Unilateral Contacts\nand Friction: Modeling and Numerical Time Integration in Non-smooth Dynamics, Lecture Notes\nin Applied and Computational Mechanics, vol 47.\nSpringer\n37. Tasora A, Anitescu M (2011) A matrix-free\ncone complementarity approach for solving largescale, nonsmooth, rigid body dynamics. Computer\nMethods in Applied Mechanics and Engineering\n200(5):439–453\n38. Visseq V, Martin A, Dureisseix D, Dubois F,\nAlart P (2012) Distributed nonsmooth contact domain decomposition (NSCDD): Algorithmic structure and scalability. In: Proceedings of the International Conference on Domain Decomposition Methods\n39. Visseq V, Alart P, Dureisseix D (2013) High performance computing of discrete nonsmooth contact dynamics with domain decomposition. International Journal for Numerical Methods in Engineering 96(9):584–598\n40. Wautelet P, Boiarciuc M, Dupays J, Giuliani S,\nGuarrasi M, Muscianisi G, Cytowski M (2014) Best\nPractice Guide – Blue Gene/Q. v1.1.1 edn\n41. van der Weele K, van der Meer D, Versluis M, Lohse\nD (2001) Hysteretic clustering in granular gas. Europhysics Letters 53(3):328\n\nTobias Preclik, Ulrich Rüde\n\n\f"
        ],
        [
         "21",
         "21",
         "cs.CE",
         "Computational Engineering",
         "1407.2098v1.pdf",
         "Jäger et al.\n\nRESEARCH\n\ninPHAP: Interactive visualization of genotype\nand phased haplotype data\nGünter Jäger* , Alexander Peltzer and Kay Nieselt\n\nAbstract\nBackground: To understand individual genomes it\nis necessary to look at the variations that lead to\nchanges in phenotype and possibly to disease.\nHowever, genotype information alone is often not\nsufficient and additional knowledge regarding the\nphase of the variation is needed to make correct\ninterpretations. Interactive visualizations, that\nallow the user to explore the data in various ways,\ncan be of great assistance in the process of making\nwell informed decisions. But, currently there is a\nlack for visualizations that are able to deal with\nphased haplotype data.\nResults: We present inPHAP, an interactive\nvisualization tool for genotype and phased\nhaplotype data. inPHAP features a variety of\ninteraction possibilities such as zooming, sorting,\nfiltering and aggregation of rows in order to explore\npatterns hidden in large genetic data sets. As a\nproof of concept, we apply inPHAP to the phased\nhaplotype data set of Phase 1 of the 1000\nGenomes Project. Thereby, inPHAP’s ability to\nshow genetic variations on the population as well\nas on the individuals level is demonstrated for\nseveral disease related loci.\nConclusions: As of today, inPHAP is the only\nvisual analytical tool that allows the user to explore\nunphased and phased haplotype data interactively.\nDue to its highly scalable design, inPHAP can be\napplied to large datasets with up to 100 GB of\ndata, enabling users to visualize even large scale\ninput data. inPHAP closes the gap between\ncommon visualization tools for unphased genotype\ndata and introduces several new features, such as\nthe visualization of phased data.\ninPHAP is available for download at\nhttp://bit.ly/1iJgKmX.\nKeywords: Genotype data; Phased haplotype\ndata; Interactive visualization; 1000 genomes\nproject\n\nBackground\nCombinations of genetic variants occurring on the\nsame DNA molecule are known as haplotypes. The\nterm haplotype was first used in 1967 in conjunction\nwith the Human Leukocyte Antigen (HLA) system,\na set of genes located close together on chromosome\n6. This system of genes is important for determining\ntissue compatibility for transplants [1]. When studying haplotypes one distinguishes phased haplotypes\nand unphased genotypes. For a phased haplotype both\nthe maternal and paternal alleles are known, either\nby directly inferring the information or using haplotype phasing tools. In contrast to that, for unphased\ngenotypes the chromosomal origin for each allele is unknown.\nEspecially collecting and comparing single nucleotide\nvariations (SNV) between different human populations\nhas become of central interest. Abecasis et al. showed\nthat human individuals have around 4 × 106 variants\non average [2]. These variants can have great influence\non genes, leading to malfunction or even complete loss\nof function and consequently to genetically related diseases such as cancer. To fully understand the mechanisms leading to disease, a catalog of all existing variants, especially of rare ones that are only seen in a single or very few individuals is required [2]. In addition,\nhumans are diploid organisms, which means that they\nhave two copies of each chromosome. Genes or other\nnon-coding sequences constituted by two homologous\nchromosomes can be genetically very different.\nOften the term haplotype is also used to refer to\nclusters of inherited single nucleotide polymorphisms\n(SNPs). By examining haplotypes, researchers wish to\nidentify patterns of genetic variation that are associated with descent, phenotype or disease state. However, studying diploid, omni- or even polyploid organisms requires additional phase information, linking a\nspecific genetic variation to its respective chromosome.\nOnly by including such information one is able to understand the impact of genetic variations.\nFurthermore, a widely used strategy in this context\nis to compare samples from several populations and to\nidentify genomic loci or regions with significant genetic\ndifferentiation between these populations.\n*\n\nCorrespondence: guenter.jaeger@uni-tuebingen.de\nIntegrative Transcriptomics, Center for Bioinformatics, University of\nTübingen, 72076 Tübingen, DE\nFull list of author information is available at the end of the article\n\n\fJäger et al.\n\nMany studies that genotype individuals have already been and are currently performed. The International HapMap Project [3] for example is an international consortium of scientists who catalog the complete genetic variation in the human genome. As of\ntoday more than 26.3 million SNPs have been listed in\nHapMap. Another example is the Collaborative Oncological Gene-environment Study (COGS) which tries\nto understand the genetic susceptibility of different\nhormone-related cancers [4, 5, 6, 7, 8]. Most haplotypes do not span more than one gene, so studying\nlocal relationships of SNPs is the most common use\ncase.\nGenome-wide association studies (GWAS) have been\nused successfully for dissecting the genetic causes underlying certain traits and diseases. Work by the\nWellcome Trust Case Control Consortium[1] has identified variations-associated phenotypes ranging from\nmalaria [9] to myocardial infarction (Myocardial Infarction Genetics Consortium, 2009) [10]. Typically,\nGWAS data are displayed using Manhattan plots, a\ntype of scatter plot to display dense data, usually with\nnon-zero amplitude. In GWAS Manhattan plots, genomic coordinates are displayed along the x-axis, and\nthe y-axis represents the negative logarithm of the associated p-value for each polymorphism in the data set.\nBecause strong associations have very small p-values,\ntheir negative logarithms will be the largest and visibly\nmost prominent [11]. A number of tools or even whole\nsuites are specifically designed to visually investigate\nvariants, either separately or in their haplotype contexts. The SNP & Variation Suite [12] is a collection\nof analytical tools for managing, analyzing and visualizing genomic and phenotypic data. However, only\nwell-established visualizations for SNP data are provided, most do not scale well with big data. Flapjack\noffers interactive visualization of large-scale genotype\ndata with a focus on plant data [13]. Its emphasis is put\non real-time rendering of the data and combining genotype data with phenotype or QTL data. Some genome\nbrowsers also offer additional visualization modes that\nallow visualization of genotype cohort data by agglomerating data from many individual genomes. Savant\n[14] in its latest version offers visualization for multiindividual genotype data sets by agglomerating SNPs\nfrom larger genomic regions and linking them with a\nlinkage disequilibrium (LD) plot as originally introduced by Haploview [15].\nWhile all described genotype and haplotype visualization tools so far mostly focus on showing raw\ndata, Haploscope visualizes haplotype cluster frequencies that are estimated by statistical models for population haplotype variation [16]. Another example in\n[1]\n\n(http://www.wtccc.org.uk)\n\nPage 2 of 16\n\nthis area is iXora [17], which is a framework for inferring haplotypes from genotyped population data and\nfor associating observed phenotypes with the inferred\nhaplotypes. It features statistical tests, such as Fisher’s\nexact test, and visualization methods that help to\nstudy parental haplotype distributions or to spot unexpected distortions. These visualizations basically include line charts for haplotype frequency distributions\nas well as bar plots for haplotype visualization. The\nuser can easily observe haplotypes, missing data, position of the markers on chromosome maps and colocalization with QTL.\nIn general, the analysis of haplotype data is a challenging scientific endeavor, since it involves the scalable processing of very large, heterogeneous, incomplete, and potentially conflicting data. Clearly, visualizing the data has been shown to aid in gaining better\nunderstanding of it. Furthermore, researchers wish to\nview all facets of haplotype data, including the spatial\ndistribution of the loci along a chromosome, the specificity of the genotypes, the different frequencies of haplotypes in different subgroups, and possibly also correlation of occurring haplotypes. For this, static visualizations are insufficient, since such complex data needs\nto be addressed on many different levels, and here in\nparticular interactivity is of utmost importance.\nThe challenges of visualizing haplotype data could be\nexacerbated when it comes to analyzing phased haplotype data that are for example derived from studies [18] such as the 1000 genomes project. Until today an interactive tool for the visualization of phased\nhaplotype data has been missing. To fill the gap, we\nimplemented inPHAP, short for (interactive P hased\nHAP lotype Viewer). inPHAP can be used in several\nways, ranging from the investigation of phased haplotypes or unphased genotypes on the single nucleotide\nlevel to the visualization of the data in a more general way showing the similarities and dissimilarities\nbetween several subject groups of interest. In the following, inPHAP and its features are presented, accompanied by a proof of concept application to data from\nPhase 1 of the 1000 Genomes Project.\n\nMethods\nThis section presents the general framework and the\ndesign choices we made for inPHAP.\ninPHAP is an interactive visualization tool written\nin the JAVA programming language. It makes use of\nthe general idea of iHAT [19], our previously published\ntool for the visualization and analysis of genome wide\nassociation (GWA) data. In iHAT we introduced the\nconcept of interactive aggregation of subsets of the\ndata in order to reveal hidden patterns that are not\nclearly visible when displaying the whole data set at\n\n\fJäger et al.\n\nPage 3 of 16\n\nFigure 1 The inPHAP graphical user interface. It consists of six components which are highlighted with boxes of different color.\nBlue (1): The haplotype visualization panel providing color-encoded base information for phased haplotype or unphased genotype\ndata, green (2): the subject meta-information panel next to the haplotype visualization panel, red(3): the SNV meta-information\npanel below the haplotype visualization panel, purple(4): the overview panel, displaying the viewers current focus in the haplotype\nvisualization panel, black(5): the settings panel, which allows the user to quickly change between settings, yellow(6): the data set\nsummary panel, providing general information for the currently loaded data set.\n\nonce. Based on the concept of aggregating the information content of data based on meta-information, we\nimplemented inPHAP, a new interactive visualization\ntool which is capable of visualizing unphased genotypes as well as phased haplotypes.\nIn the following the design of the inPHAP tool itself,\nas well as its features are described in detail.\nThe inPHAP Graphical User Interface\nOne of the key features of inPHAP is that it supports\na broad range of interaction with the data. Therefore, we implemented a graphical user interface (GUI)\nwhich consists of six components (see Figure 1 for an\noverview of all the components): the haplotype visualization panel, the subject meta-information panel, the\nsingle nucleotide variation (SNV) meta-information\n\npanel, the overview panel, the settings panel, and last\nbut not least the summary panel. The largest and most\nimportant component is the haplotype visualization\npanel located in the center of inPHAP. It consists of\na heatmap-like haplotype visualization, together with\nrow and column headers showing subject and the SNV\nidentifiers, respectively. Detailed information on the\nvisual representation of haplotype data is given in the\nGeneral Visual Encoding section. The second component is the subject meta-information panel, which displays numerical and categorical meta-data of the subjects. Each meta-information type is represented as a\nsingle column in the subject meta-information panel\nand different color gradients for numerical data or\nmaps for categorical data can be chosen by the user\nto distinguish sub-groups in the data. The SNV meta-\n\n\fJäger et al.\n\ninformation panel is used to enhance the haplotype\nvisualization by displaying meta-information for variants. In the case of phased data for example, variants on the paternal and maternal chromosome can\nbe distinguished. This information is then used to automatically create a meta-information row below the\nhaplotype view with “P/M” as identifier to enhance\nidentification of paternal and maternal alleles in the\nhaplotype visualization panel. The fourth component\nin the upper left is the overview panel, an interactive\nzoomed out representation of the whole haplotype visualization. It shows the current view of the user in\nthe haplotype visualization panel and gives an estimate of the proportion of the visualized data using a\nrectangle as visual clue. The settings panel on the right\nallows for quick changes of the most often needed settings. Here the user can change the way the data is\npresented. Amongst others, colors can be adjusted according to the users’ needs and different visual representations for haplotype data are available. The last\ncomponent is the data set summary panel. It provides\ngeneral information for the current data set, including\nthe number of subjects and SNVs in the data set as\nwell as the number of different meta-information (MI)\ntypes, separated into “MI columns” and “MI rows” for\nsubject and SNV meta-information. These panels are\ncomplemented by a button bar at the top of the GUI\nthat provides convenient access to further useful and\noften needed functions, such as filtering, changing the\nsubject or SNV MI color gradients or the export of the\nhaplotype visualization. Additional functionality that\nis not available in the settings panel or the button bar\nis provided in the inPHAP menu bar. Furthermore,\nan information bar at the very bottom shows the last\nchange made by the user. Thereby, it provides information on what has been changed and how this change\naffected the underlying data. A complete log of all interactions performed on the data is also available in\nthe help menu located in the inPHAP menu bar.\nData formats and structures\nData can be imported in inPHAP in two different formats: The VCF file format containing haplotype information for different subjects as separated columns and\nthe IMPUTE2 format, the default haplotype text file\nformat used by the IMPUTE2 program [18] to encode\ngenotype information from the 1000 Genomes Project.\nThe example files that have been used in our paper to\ndemonstrate inPHAP have either been generated using SHAPEIT2 [20, 21] or BEAGLE [22, 23], which\ncan both be used to infer phased haplotypes and are\nable to output the results in the IMPUTE2 or VCF file\nformat. Since such files can get very large, implementation of the underlying data structures has been per-\n\nPage 4 of 16\n\nformed with respect to the overall memory consumption. In general, haplotype data consist of two different characters from the alphabet Σ = {A, T, C, G},\none character for the paternal allele and one for the\nmaternal allele. In some cases also the character “ − ”\nis allowed, to indicate that no second allele is present.\nThis is for example the case for many SNVs for the\nhuman X chromosome, especially for males. Encoding these characters as character primitives in Java\nwould require 2 Bytes per character. For a dataset\nconsisting of around 4 × 106 SNVs and about 1000\nsubjects this would lead to a memory consumption of\n2 × 4 × 106 × 103 × 2 = 16 GByte just for storing allele combinations. State of the art computers currently\nhave between 8 − 16 GBytes of RAM installed. To allow users to use inPHAP on their desktop computers,\nit was necessary to introduce a binary encoding of the\nhaplotype data in order to reduce the amount of consumed memory. In inPHAP each character c ∈ Σ is\nencoded using only two bits. With this strategy only\n4 bits are needed to store the paternal and maternal allele for one SNV and subject. As a result inPHAP consumes for 4 × 106 SNVs and 103 subjects\nonly (4 × 106 × 103 )/2 = 2 GByte for storing the raw\nallele combinations, which is 8 times less than using a\nnaive memory storage approach.\nTo keep interactions smooth even on the lowest zoom\nlevel, where each cell of the haplotype visualization is\n1 × 1 pixel in size, only those data that are needed\nfor the currently visible submatrix are decompressed\nfrom their binary form. All other data is kept in the\ncompressed form in memory. Furthermore, the visualization of the subject specific haplotypes has been optimized to perform very fast repainting. For this, each\nbase c ∈ Σ is rendered as a colored image in memory.\nWhen drawing the visible submatrix only already prerendered images are drawn, decreasing calculation and\npainting time to a minimum. To allow for smooth interaction with the visualization, selection boxes as well as\ndifferent saturation values have also been implemented\nas pre-calculated images that can be drawn on top of\nthe nucleotide images. With this strategy typical interactions, such as resorting the matrix, moving the\nsliders, or selecting specific columns or rows, do not\nrequire to recalculating the pre-rendered images but\nonly repainting them in the current view. Changes that\nrequire a recalculation of the images, such as changing\nthe color for the bases, then only requires to recalculate 4 images, which can be used multiple times for\na single repaint event. Altogether, these mechanisms\nenable instantaneous updates of the haplotype visualization panel and smooth interaction in inPHAP.\nIn addition to haplotype data, meta-information\ndata can be imported for subjects and for SNVs. Currently inPHAP accepts only tab-delimited text files\n\n\fJäger et al.\n\nwith two header lines, with column names in the first\nheader line and declaration of the type of data (categorical or numerical) for each column in the second\nheader line, and subject and/or SNV identifiers in the\nfirst column.\nOn aggregated data, inPHAP utilizes a further visualization method to provide the user with feedback\non the relative frequency of a certain nucleotide for the\naggregated group of individuals in form of displaying a\nheight of a bar within the respective cells. This can be\nchanged by selecting the ‘’Saturation’‘ based visualization, which visualizes the most common SNV within\nthe group by changing the color saturation from very\nlow (= there are a lot of other SNVs within the group\ndisagreeing with the shown SNV) to very high (= most\nof the SNVs within the aggregated group agree with\nthe shown color), providing useful feedback as well for\nthe user.\nGeneral Visual Encoding\nIn the haplotype visualization panel there are two different visualizations available, one for phased data and\none for unphased data. For phased data, each SNV\nis represented by two different columns, one for the\npaternal allele and one for the maternal allele. This\ndesign choice is motivated by the 1000 genomes data\nfrom Abecasis et al. who used two rows for each allele in their publication [24]. For unphased data only\none column per SNV is needed. In addition, inPHAP\noffers two different color encodings for phased data\nand one for unphased data. In the default visual representation for phased data, each base is assigned a\nunique color. By default green is used for A, blue for\nC, red for T and yellow for G. Missing nucleotides, as\nit might be the case for males on the X chromosome\nare colored white. This encoding allows the user to\ncompare different SNVs as well as to spot differences\nbetween the maternal and paternal allele quickly. The\nsecond visual representation for phased data is more\nconvenient for visualizing differences to the reference\nbase. If for one of the SNVs either the maternal or paternal allele differs from the respective reference base,\nthen yellow color is used in the haplotype visualization panel, otherwise the respective cell is painted in\nblue. The third visual representation is more focused\non unphased data, but can be applied to phased data\nas well. Here only one column is required for each SNV.\nIf the phase is unknown, only three different cases can\noccur, namely homozygous and heterozygous SNVs as\nwell as SNVs for which both alleles are equal to the reference base. Homozygous SNVs are colored red, while\nheterozygous SNVs are shown in yellow. If both alleles\nare equal to the reference the respective cell is colored\ngreen. For each of the three visual encodings, the default colors are selected based on ColorBrewer color\n\nPage 5 of 16\n\nmaps [25], such that differences as well as similarities\nin the haplotype visualization panel can be spotted\nquickly. However, all colors can easily be changed in\nthe settings panel to fulfill user specific needs. In case\nof a user defined selection of subjects of SNVs a colored border is drawn around cells in the haplotype\nvisualization panel and the respective column or row\nidentifiers are overlaid by a colored box. The default\nselection color is black, but it can also be changed by\nthe user if needed.\nIn contrast to haplotype data, meta-information\ndata is encoded in a different way. Here, for each\nmeta-information the user can choose the appropriate\ncolor encoding. For numerical meta-data, the values\nare mapped directly to a color from the chosen color\ngradient. For categorical meta-data, first each category\nis assigned a unique numerical value. Then these numerical values are used for the selection of colors from\nthe chosen color map.\nInteraction possibilities\nGeneral interaction features\ninPHAP is a highly interactive tool, allowing the user\nto change the current view on the data in various ways.\nInteraction possibilities include the navigation along\nthe subject (vertical) axis as well as along the SNV\n(horizontal) axis using the navigation bars. Furthermore, navigation is also possible using the overview\npanel. There, the current view is indicated by a red\nrectangle. This rectangle can be dragged to the desired location inducing a change in the position of the\nnavigation bars in the haplotype visualization panel.\nFurther interaction possibilities are zooming in two different dimensions, i.e. the width and height of each\ncell in the haplotype visualization panel can be adjusted. In addition, width and height of the metainformation cells can be changed separately from the\nvisualization panel, allowing the user to see the metainformation assigned to subjects or SNVs even for very\nsmall cell sizes in the haplotype visualization. Width\nand height changes can be made either by using the\nsettings panel or via the mouse wheel if the mouse\nis placed above the haplotype visualization panel or\none of the meta-information panels, respectively. Subjects as well as SNVs of interest can be selected with\nthe click of a mouse button on the respective identifier or via dragging over a series of identifiers. Selection thereby also affects the meta-information panels and the corresponding meta-information cells are\nhighlighted as well. Furthermore, rows and columns\nin the haplotype visualization panel can be sorted according to the provided meta-information by doubleclicking on one of the meta-information identifiers. For\nthe sorting we use a stable sort. If the user for example chooses a meta-information group for sorting, the\n\n\fJäger et al.\n\nPage 6 of 16\n\nFigure 2 Two inPHAP visualizations showing SNVs for the MLD associated gene ARSA. Data was taken from Phase 1 of the 1000\nGenomes Project [24]. For both visualizations the reference color encoding was used. A: shows a non-aggregated view of the data, B:\nshows an aggregated view of the data. Individuals have been aggregated according to their population affiliation using the\n“minimum” aggregation method for SNVs. Bar heights represent rarity of the aggregated consensus base.\nPopulation abbreviations: ASW, African ancestry in Southwest United States; CEU, Utah residents with ancestry from Northern and\nWestern Europe; CHB, Han Chinese in Beijing, China; CHS, Han Chinese South, China; CLM, Colombians in Medellin, Colombia;\nFIN, Finnish in Finland; GBR, British from England and Scotland; IBS, Iberian populations in Spain; LWK, Luhya in Webuye, Kenya;\nJPT, Japanese in Tokyo, Japan; MXL, people with Mexican ancestry in Los Angeles, California; PUR, Puerto Ricans in Puerto Rico;\nTSI, Tuscani in Italy; YRI, Yoruba in Ibadan, Nigeria. Superpopulation abbreviations: AFR, African; AMR, Americas; ASN, East\nAsian; EUR, European.\n\norder of the elements that belong to the same subgroup in the chosen meta-information group is preserved. This allows users to sort according to different\nmeta-information groups consecutively. These general\ninteraction possibilities are assisted by several interactive filtering and aggregation methods, which will be\nexplained in the following.\nFiltering\nFiltering is a crucial step in the analysis of large data\nsince it allows reducing the overall amount of data\nthat has to be investigated by displaying only those\nvariants that are of interest to the user. Consequently,\ndata that is currently not of interest is removed from\nthe view. If for example the user is interested in the\nvariants that are shared by whole population groups\nrather than by only very few individuals, using a frequency filter can help in the selection of the respective\nSNVs and thereby reduce the overall amount of data\nthat has to be visually assessed. To enable filtering in\ninPHAP, we implemented several different filter methods for single nucleotide variants. Filtering based on\nchromosomal location allows the user to concentrate\non those SNVs that are located in a specific region\non a chromosome, e.g. a gene or promoter region. If\n\na list of interesting SNVs is already available, i.e. the\nuser is interested in a specific haplotype, this list can\nbe passed to inPHAP. Then only the intersection of\nSNVs in the given list with SNVs in the data set will\nbe shown in the haplotype visualization panel. In addition, filtering based on SNV identifiers can also be\ndone by providing a regular expression for the SNV\nidentifier. We also included a frequency based filter, to\nshow only those SNVs where the respective genotype\nfrequency lies above or below a user-defined threshold. This is especially useful when the user wants to\nconcentrate on rare variants only for example.\nAggregation\nUsing visualization to identify patterns in large data\nsuch as those from the 1000 Genomes Project is a\nchallenging task, since structures often remain hidden when visualizing them on a global level. Therefore,\nmethods to reduce the overall complexity of the data\nare needed to improve visual assessment of underlying patterns. In iHAT [19] we have demonstrated that\naggregation is a rich technique when it comes to revealing hidden structures in the data. inPHAP allows the\nuser to aggregate rows interactively, where for example meta-data can be used to guide this process. Especially for genotype as well as haplotype data where\n\n\fJäger et al.\n\nPage 7 of 16\n\nFigure 3 Example workflow for the inPHAP tool, showing how data is loaded, processed and visualized using the inPHAP core\nfeatures import, sorting, filtering and aggregation. A: The inPHAP graphical user interface after starting inPHAP and selecting\n“New” from the button menu on the top, in order to load a new data set in the VCF file format, B: View on the data, after loading\na data set in the VCF file format and adding additional meta-information for individuals and SNVs in the data set. Rows have been\nsorted according to Population and Super Population by double-clicking the corresponding meta-information identifiers. “Filtering”\nfrom the button menu has been selected to initiate the filtering for SNVs with a frequency ≥ 0.5%, C: After filtering, the\n“Aggregate” button from the menu bar has been clicked to start aggregating the rows based on the provided meta-information. Here\nthe population affiliation of the individual subjects is used for aggregation, D: Aggregated view on the filtered data set. In addition,\nzooming with the mouse wheel on the haplotype visualization was performed to increase cell height. The new height values are\ndisplayed in the settings panel.\n\n\fJäger et al.\n\ndifferences between whole populations or subgroups\nof populations are hard to compare, aggregation can\nhelp to unravel the hidden structures and thereby help\nto interpret the genetic differences. In inPHAP several different aggregation methods have been implemented, such as maximum, minimum or mean. A typical use case of aggregation of haplotype data would be\nto take subjects from a common group, e.g. from the\nsame population, and look for differences in the haplotypes of these populations possibly revealing recombination events on a global level. In inPHAP the user\ncan combine subjects of interest into subject groups by\naggregating the corresponding haplotypes. These subject groups can either be based on user selection or on\nmeta-information that has been additionally assigned\nto each subject. The aggregation of haplotypes is performed on a per SNV base. For each SNV the base with\nthe highest frequency among the selected subjects is\nchosen as the consensus and the respective frequency is\nstored as an indication of how representative this base\nis given the underlying base distribution. In the haplotype visualization panel, aggregations can be encoded\nin two different ways, depending whether more attention shall be drawn to the consensus base itself or to\nthe differences in SNV frequency in the combined subject group. If one is interested in the consensus base itself rather than in the differences in frequency between\naggregated SNVs, aggregations can be represented as\ncolored boxes where their saturation is adjusted based\non the frequency of the consensus base. This visual\nrepresentation is the default representation that was\nshown to work well on genotype data [19]. However, in\na study conducted by Mackinlay it was shown that positioning along a common scale is more effective than\nsaturation when comparing quantitative values [26].\ninPHAP therefore offers an alternative way to represent aggregations. Instead of filled boxes, bars are\ndrawn, whose color represents the consensus base and\nthe height of the bar displays the underlying consensus base frequency. With this second visual encoding,\ndifferences in frequency stand out more clearly, which\nis especially useful for the comparison of maternal and\npaternal allele frequencies. Aggregated individuals are\nassigned a new identifier in the haplotype visualization\npanel constructed from the prefix “AGN” followed by\na number. This number corresponds to the number of\nindividuals included in the aggregation.\nThe aggregation of haplotypes is accompanied by the\naggregation of corresponding meta-information values.\nMeta-information can also be aggregated based on a\nuser defined aggregation method which may differ from\nthe method chosen for the haplotype visualization. In\nFigure 2 SNVs for the MLD associated gene ARSA\nare shown. Figure 2B shows the data after applying\n\nPage 8 of 16\n\nthe minimum aggregation method to subjects that belong to the same population. This view is compared\nto a non-aggregated version showing the same data\n(see Figure 2A). After aggregation it becomes clearer,\nwhich SNVs are rare for specific populations, and how\nrare variants differ between the populations.\nTypical inPHAP workflow\nAn example workflow, showing how data is loaded into\ninPHAP, how filtering is applied to SNVs of interest\nand how aggregation is used to enhance visualization\nusing meta-information is shown in Figure 3. This figure is split into four sub-figures showing the different stages of a typical inPHAP workflow. The quick\nbutton bar provides helpful features for processing the\ndata. First data can be loaded into inPHAP with the\n“New” button. This opens up the settings dialog, from\nwhich the user can select what type of data he wants\nto load (see Figure 3A). As soon as data has been\nloaded (including meta-data), the user can interact\nwith it, for example by sorting the rows based on metainformation. This can easily be done by double-clicking\non one of the meta-information identifiers. To concentrate on SNVs of interest several different filters can be\napplied. Via the “Filtering” button in the quick button\nbar, the user gets access to the filter settings dialog,\nfrom which a filter of choice can be selected and parameters for the filter can be set (see Figure 3B). Data\ncan be explored at any time, by navigating through the\nvisualization using the corresponding navigation bars\nor by zooming in and out either with the mouse wheel\nor using the settings panel on the right of the graphical user interface. If needed, aggregation, e.g. based\non meta-data, can be performed to obtain an aggregated view where individual subjects are grouped together based on the selected subject meta-information\ncolumn and consensus values are calculated. This can\nbe achieved by clicking the “Aggregate Rows” button from the quick button menu and setting up the\ncorresponding aggregation parameters in the aggregation settings panel that shows up (see Figure 3C). The\ncalculations for the aggregations are performed in the\nbackground, keeping the visualization usable at any\ntime. A resulting view on the data after filtering, sorting, aggregation and zooming is shown in Figure 3D.\nExport\nWith inPHAP the user can generate graphics in publication ready quality as either bitmapped images\n(PNG, JPEG and TIFF formats) or as scalable vector\ngraphics (SVG or PDF format). During the export the\nuser is provided with a preview of the resulting image\nas well as further options to adjust the image size. Furthermore, the user can decide whether to export the\nfull visualization or just the region of the visualization\ncurrently visible in the inPHAP GUI.\n\n\fJäger et al.\n\nPage 9 of 16\n\nFigure 4 inPHAP phased haplotype view for the 100-kb region on chromosome 2 spanning the genes ALMS1, NAT8 and ALMS1P,\nafter filtering of only those variants with a frequency > 0.5% across the 1096 human individuals of Phase 1 of the 1000 Genomes\nProject. SNVs that differ from the reference base are colored yellow, while alleles equal to the reference are colored blue. A:\nIndividuals are sorted according to their affiliation with a common population. B: Shows the same visualization as in A, but\nindividuals are aggregated based on their population affiliation, using “maximum” as aggregation method. For abbreviations of the\npopulation names see Figure 2.\n\nResults\nVisualization of genetic variation between populations\nWe applied inPHAP to haplotype data as generated\nby the 1000 Genomes Project. In the Phase 1 publication, Abecasis et al. provide a detailed view of the\nvariation across several populations [24]. During their\nanalysis they highlighted a 100-kB region on chromosome 2 spanning the genes ALMS1 and NAT8. Variations in those genes have been associated with kidney\ndisease in earlier studies [27]. As a proof of concept\nwe used inPHAP to generate a similar visualization\nas Figure 2a in [24]. We first loaded the complete vcf\nfile of chromosome 2 as provided on the ftp site of the\n1000 Genomes project website. We then filtered only\nthe respective 100 kB chromosomal region of the two\ngenes. Next we applied two SNV filters: one for variants with a frequency > 0.5% across all individuals and\none for rare variants with a frequency < 0.5%. The resulting inPHAP visualizations are shown in Figure 4\nfor variants with a frequency > 0.5% and in Figure 5\nfor rare variants with a frequency < 0.5%. As in Figure 2a of Abecasis et al. differences in common single\nnucleotide variants between different populations are\n\nclearly visible. Especially in the African (AFR) super\npopulation there are substantially more SNVs in the\nALMS1 region than for the other populations. This\neffect is even more pronounced after aggregation (see\nFigure 4B). Interestingly, for the Asian (ASN) population only very few variants are found in the central part of the ALMS1 gene, while these are more\nlikely in Europeans (EUR) and Americans (AMR). In\ncontrast to all the other populations variant locations\nin this 100-kb region are more uniformly distributed,\nwhile for the other population groups variants are located mainly across two different sub-regions, namely\nthe first part of the ALMS1 gene and an approximate\n20-kb region at the end of the selected 100-kb region\nspanning the genes NAT8 and ALMS1P. These observations correlate well with the findings of Abecasis et\nal., who showed that highly frequent variants in the\n100-kb region are differently distributed across several\npopulations.\nTaking a closer look at the rare variants with a frequency < 0.5%, one can see that the African population (AFR) again shows a higher number of variants\nthan the rest (see bottom three rows in Figure 5). In\naddition, the degree of rare variants varies between dif-\n\n\fJäger et al.\n\nPage 10 of 16\n\nFigure 5 inPHAP phased haplotype view for the 100-kb region on chromosome 2 spanning the genes ALMS1, NAT8 and ALMS1P,\nafter filtering of rare variants with a frequency < 0.5% across the 1096 human individuals of Phase 1 of the 1000 Genomes Project\n[24]. The bases A,C,T,G are colored green, blue, red and yellow respectively. Individuals are sorted according to their affiliation with\na common population, and subsequently aggregated according to a specific population using the “minimum” aggregation method for\nSNVs. A: SNVs on the paternal chromosome are shown. B: SNVs on the maternal chromosome are shown. For abbreviations of the\npopulation names see Figure 2.\n\nferent populations, even for those from a common super population. For example, the Iberian population\nin Spain (IBS) shows only very few rare variants in\nthis region (third row in Figure 5) whereas the numbers are much higher for the other European (EUR)\npopulations. Interestingly, variations in the IBS population usually are limited to a single chromosome,\nwhich means that the SNV can either be found on the\npaternal or on the maternal chromosome, but rarely\non both. This leads to the assumption that those variants have been introduced only recently, which correlates with the findings by Abecasis et al., who argue\nthat recent events, such as clan breeding structures or\nadmixture of diverged populations are the main reason for rare variants in the Spanish (IBS) and Finnish\n(FIN) population [24].\n\nVisualization of MLD variations\nEspecially of interest for researchers are not common\nvariants, that can be easily found in haplotype data,\nbut rather rare alleles that can only be found in smaller\nsubsets of populations or individuals. Finding such\nrare alleles can be difficult, due to the total number\nof subjects in common haplotype datasets, that might\nnot include individuals with such rare alleles and furthermore the difficulty to filter out common alleles that\nare not as alluring as rare ones. inPHAP provides different methods in order to ease the search for rare alleles in large haplotype datasets, such as the frequency\nfiltering feature together with the powerful aggregation methods included in the tool.\nMetachromatic leukodystrophy (MLD) is an inherited disorder, that directly effects the growth and\ndevelopment of myelin, which is a crucial insulator\naround nerve fibers in human central and peripheral\nnervous systems [28]. The disease is caused by several\n\n\fJäger et al.\n\nPage 11 of 16\n\nFigure 6 Two inPHAP visualizations showing SNVs for the MLD associated gene ARSA. SNVs have been filtered based on their\nfrequency across the 1096 individuals in the data set, showing only those SNVs with a frequency > 0.5%. Individuals have been\naggregated according to their population affiliation (for abbreviations of the population names see Figure 2). Bar heights for each\nSNV display the frequency of the aggregated consensus base. The arrow points to the maternal allele of the central SNV with\ndbSNP ID rs743616 which is assumed to be one of the causative mutations leading to MLD. A: shows the selected SNVs using the\nreference- based visual encoding. Four of these SNVs show large differences to the reference base, which is shared across all\npopulations, indicated by yellow bars ranging over a whole column, while some SNVs differing from the reference are restricted to\nfew or even single populations, B: shows the selected SNVs using the nucleotide-based color encoding. In both visualizations\ndifferences between maternal and paternal alleles stand out clearly.\n\nmissense mutations on Chromosome 22, causing defects of the enzyme arylsulfatase A (ARSA) [29]. One\nof the SNPs with dbSNP ID rs743616 that is the supposedly responsible mutation for MLD, is a C → G\nsubstitution, leading to an amino acid change of Threonine → Serine in the corresponding protein ARSA.\nUsing inPHAP we aggregated the dataset of chromosome 22 according to the population and then compared the resulting aggregations with respect to their\nsuper populations. Interestingly, as can be seen in Figure 6, there exist differences between super population groups, for example the Asian (ASN) and African\n(AFR) super populations show low pathogenic allele\ncounts for MLD, whereas the European (EUR) and\nAmerican (AMR) super populations show significantly\nhigher total counts of pathogenic alleles, most pronounced is the Puerto Rican (PUR) population group\nin the American super population. On the single individual level the variations between subgroups are\ndifficult to spot, as the allele patterns themselves in\npopulations look entirely random without the aggregation. After aggregation in inPHAP the pattern becomes nicely visible. Furthermore, with this visualization the origin of the corresponding (pathogenic) allele\ncan be distinguished, as both maternal and paternal\nallele frequencies can be observed in our visualization.\nFor example for this SNP it seems to be of mostly\nmaternal origin for Mexican individuals living in Los\n\nAngeles (MXL), as can be seen in Figure 6 (bottom\nfourth row).\nPerformance\nThe inPHAP tool has been designed in a way to keep\nthe performance on a high level throughout the whole\nanalysis. As an extreme use case, we tested inPHAP\nwith the VCF file of chromosome 2 from Phase 1 of\nthe 1,000 Genomes Project with 3.2 Mio SNVs and\n103 GByte file size (for the VCF). inPHAP needs\nabout 21 Gbytes of RAM, which can be explained by\nthe fact that besides the raw allele data, all images are\nheld in RAM as well. inPHAP still remains interactive\nand reacts smoothly when users switch between views,\nor apply functions such as filtering or aggregation.\n\nDiscussion\nWe have designed inPHAP as a tool following Ben\nFry’s computational information design approach to\nunderstand large and complex data [30], which consists\nof the following seven main steps: acquire, parse, filter, mine, represent, refine and interact. With inPHAP,\ndata can be loaded from different files formats, several filters can be applied, aggregations can be calculated, different representations for the underlying data\nare available, data can be sorted according to metainformation and interaction is possible at every stage\nof the analysis.\n\n\fJäger et al.\n\ninPHAP can visualize phased haplotype data in order to study the influences of certain alleles. This is\nachieved by introducing two columns for SNVs, one\nfor the maternal and one for the paternal allele. This\ndesign choice was motivated by the 1000 genomes data\nfrom Abecasis et al. [24]. Although inPHAP is designed for diploid organisms, its visualization concept\ncan easily be extended to more complex genomes, as\nfor example from omniploid organisms, by extending\nthe number of columns used for single variations. Furthermore, the decision to split SNVs that are located\non different homologous chromosomes into two different columns in the visualization has several further\nadvantages. First of all, visual clarity is maintained\nthroughout the whole analysis and comparisons between SNVs on homologous chromosomes can easily be\nmade, by placing them next to each other in the haplotype visualization. Additionally, reordering of SNVs\nbased on meta-information, such as the affiliation with\na chromosome, enables the user to study single haplotypes without getting confused by the information\nfrom other homologous chromosomes. This would for\nexample not be possible by adding two rows for each\nsubject, as it was done in Figure 2a from Abecasis et\nal. [24], rather than adding two columns for each variant in the case of a diploid organism. In addition, comparison of haplotypes on homologous chromosomes is\nmuch easier, when the corresponding variations can be\nplaced into chromosome based groups. The drawback\nof this approach is that comparison between patterns\non the paternal and the maternal chromosome can become difficult, when the haplotype regions are large.\nIn order to identify patterns on the paternal or maternal allele one would have to sort the SNVs according\nto their allele affiliation. However, this places maternal\nand paternal haplotypes far away from each other in\nthe inPHAP visualization panel. Due to a limitation\nin the users screen size displaying both, the maternal\nand paternal haplotype, at the same time would be impossible. This could be overcome, by allowing the user\nto split the haplotype visualization panel in such cases\ninto two parts, one for the maternal and one for the\npaternal allele, which is however currently not possible. Although, inPHAP was designed for phased data,\nit is not limited to those and can easily be applied to\nunphased data as well. Then of course, only a single\ncolumn in the haplotype visualization panel is needed.\nThe possibility to decide whether specific allele combinations have an influence on an individual phenotype, is of great advantage and may lead to more precise interpretations. For this, we have shown that aggregations are a valuable tool to assess hidden patterns\nin the data and thereby help the user to draw better conclusions. However, aggregation techniques also\n\nPage 12 of 16\n\nbear risks. Depending on the aggregation method that\nis used, valuable information is potentially lost. During the analysis of the Abecasis et al. data set we have\nshown how aggregation can be used to display specific\npatterns hidden in whole populations. However, using\na single aggregation technique did not allow us to reveal all the hidden information. Using the maximum\naggregation technique, for example, enabled the comparison of common features, but has the disadvantage\nof loosing information on rare variants. In order to\nconcentrate on rare variants, we had to apply the minimum aggregation technique. Therefore, the question\nwhether to use aggregation for data exploration and\nwhich aggregation method is applicable, largely depends on the data and the question one wants to solve.\nFor the visual encoding of aggregations we have implemented two different alternatives, a saturation based\napproach and the possibility to display nucleotide frequencies by using bars of different height. Using bar\nheights has the advantage that aggregated frequencies\nare much easier to compare between specific SNVs of\ninterest. However, for a more general overview, e.g.\nover a whole genomic region, using saturation is more\nsuitable, because depending on the number of SNVs\nand aggregated sub-groups in the overview, nucleotide\nboxes can become very small.\nThe application of different visualization strategies\nrequires to be able to switch between data transformations and visual representations interactively. Since\nvisualizing too much information in a single view easily leads to unnecessary clutter, which exacerbates the\nprocess of making decisions on the data, we follow a\ndifferent strategy in inPHAP. By offering the user a\nvariety of visual encodings and interaction techniques\nto process the data, he can generate different views\non the data and switch between them in a fast and\ninteractive way. In inPHAP we provide two different\nvisual encodings for phased haplotype data, a reference\nbased encoding where only similarities with and differences to the reference nucleotides are displayed and a\nnucleotide based representation that provides detailed\nbase information. Only by the interplay of these two\nrepresentations one is able to locate SNVs of interest and get nucleotide information at the same time.\nAgain, in order to compare different representations,\nit would be of advantage to place them next to each\nother, which is currently only possible by exporting\nthe visualized data using one of the available image\nformats in inPHAP. However, with that approach interactivity would be lost.\nIn addition to the visualization of phased haplotype\nor genotype data, meta-information, such as gene affiliation of SNVs or population information for individuals can provide further insight into the data.\n\n\fJäger et al.\n\nSo far inPHAP supports numerical and categorical\nmeta-information for SNVs and individuals. Due to\nthe generic design of meta-information for subjects, inPHAP can also handle quantitative meta-information,\nenabling the study of QTLs (quantitative trait loci) or\neQTLs (expression quantitative trait loci). However,\nmore complex meta-information, such as SNV associations, structural variations or individual relationships,\ncan currently not be visualized without larger modifications of the tool itself.\nFuture Work\nAn important step to evaluate and improve inPHAP\nwill be the execution of a user study, which we will conduct next. Furthermore, we plan to improve inPHAP\nby adding more features. First of all we will add an\nadditional component to the GUI showing the location of variations on the chromosome. This helps to\nidentify SNVs in close proximity to each other which\nis of interest, since those variants are more likely to be\nin linkage disequilibrium. A further step in this direction would be to include additional visualizations in\ninPHAP, as for example an interactive LD-plot that\ncan be linked to the haplotype visualization panel to\nimprove identification and assessment of LD blocks.\nBut also statistically motivated visualizations, such as\ncharts that display the SNV frequencies for specific\nsubgroups can largely improve inPHAP’s efficiency, by\nmaking it easier to estimate differences between these\ngroups.\nIn the current version of inPHAP we concentrated\nprimarily on single nucleotide variations. However, also\ninsertions and deletions (INDELs) are important variations that can lead to changes in gene function and\nconsequently to disease. In future versions, we plan to\nextend inPHAP to be able to visualize INDELs together with SNVs, by adding a separate visual encoding for INDELs. Since INDELs can also differ between\nthe maternal and paternal chromosome, the general\nconcept of representing phased variations in different\ncolumns does also apply.\nTo improve interactivity with the visualization we\nalso plan to add the possibility to keep user-defined\nregions in the visualization fixed, such that those regions are presented to the user at any time. In this, one\nwould be able to navigate through the visualization in\norder to compare structures at different locations to\nthe fixed region more easily. Another possibility would\nbe to allow the user to split the haplotype visualization\npanel and link the resulting two sub-panels to each\nother, such that navigating in one panel would also\nchange the view in the other panel. With this strategy\ninteractivity would be maintained at any time.\n\nPage 13 of 16\n\nConclusion\nWe have presented inPHAP, a tool for the visualization and interactive exploration of phased haplotype\ndata for large scale genome projects. Through a variety of different interaction and data transformation\npossibilities, inPHAP allows the user to study the influences of variants either on the individual level or\non a more general level that can for example be defined by meta-information. Since identical genotypes\nmay have different impact, depending on their phase,\nvisual assessment of the phase information can help\nresearchers to make well-informed decisions. To our\nknowledge inPHAP so far is the only available interactive visualization tool capable of visualizing phased\nhaplotype data.\nList of abbreviations used\nARSA: Arylsulfatase A, COGS: Collaborative Oncological Gene\nenvironment Study, eQTL: expression Quantitative Trait Locus, GUI:\nGraphical User Interface, GWA: Genome Wide Association, GWAS: Genome\nWide Association Study, HLA: Human Leukocyte Antigen, INDEL:\nInsertion / Deletion, MI: Meta-Information, MLD: Metachromatic\nleukodystrophy, SNP: Single Nucleotide Polymorphism, SNV: Single\nNucleotide Variation, QTL: Quantitative Trait Locus\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthors contributions\nGJ and KN extended the general idea of iHAT to phased data and\ndeveloped the concept of inPHAP. GJ designed the graphical user interface\nof inPHAP and the different visual encoding strategies. GJ and AP\nimplemented inPHAP in the Java programming language. GJ, AP, and KN\nanalysed the Abecasis et al. data set. AP, and KN investigated the genetic\nvariations at the ARSA gene locus. All authors wrote, read and approved\nthe final manuscript.\nAcknowledgements\nWe acknowledge support by Deutsche Forschungsgemeinschaft and Open\nAccess Publishing Fund of Tübingen University.\nReferences\n1. Ceppellini, R., Curtoni, E., Mattiuz, P., Miggiano, V., Scudeller, G.,\nSerra, A.: Genetics of leukocyte antigens: a family study of segregation\nand linkage. Histocompatibility testing 1967, 149 (1967)\n2. ”1000 Genomes Project Consortium”, Abecasis, G.R., Altshuler, D.,\nAuton, A., Brooks, L.D., Durbin, R., Gibbs, R., Hurles, M., McVean,\nG.: A map of human genome variation from population-scale\nsequencing. Nature 467(7319), 1061–1073 (2010).\ndoi:10.1038/nature09534\n3. The International HapMap Consortium: The international hapmap\nproject. Nature 426, 789–796 (2003)\n4. Eeles, R.A., Olama, A.A.A., Benlloch, S., Saunders, E.J.,\nLeongamornlert, D.A., Tymrakiewicz, M., Ghoussaini, M., Luccarini,\nC., Dennis, J., Jugurnauth-Little, S., Dadaev, T., Neal, D.E., Hamdy,\nF.C., Donovan, J.L., Muir, K., Giles, G.G., Severi, G., Wiklund, F.,\nGronberg, H., Haiman, C.A., Schumacher, F., Henderson, B.E.,\nMarchand, L.L., Lindstrom, S., Kraft, P., Hunter, D.J., Gapstur, S.,\nChanock, S.J., Berndt, S.I., Albanes, D., Andriole, G., Schleutker, J.,\nWeischer, M., Canzian, F., Riboli, E., Key, T.J., Travis, R.C., Campa,\nD., Ingles, S.A., John, E.M., Hayes, R.B., Pharoah, P.D.P., Pashayan,\nN., Khaw, K.-T., Stanford, J.L., Ostrander, E.A., Signorello, L.B.,\nThibodeau, S.N., Schaid, D., Maier, C., Vogel, W., Kibel, A.S.,\nCybulski, C., Lubinski, J., Cannon-Albright, L., Brenner, H., Park,\nJ.Y., Kaneva, R., Batra, J., Spurdle, A.B., Clements, J.A., Teixeira,\nM.R., Dicks, E., Lee, A., Dunning, A.M., Baynes, C., Conroy, D.,\nMaranian, M.J., Ahmed, S., Govindasami, K., Guy, M., Wilkinson,\n\n\fJäger et al.\n\nR.A., Sawyer, E.J., Morgan, A., Dearnaley, D.P., Horwich, A.,\nHuddart, R.A., Khoo, V.S., Parker, C.C., As, N.J.V., Woodhouse, C.J.,\nThompson, A., Dudderidge, T., Ogden, C., Cooper, C.S.,\nLophatananon, A., Cox, A., Southey, M.C., Hopper, J.L., English,\nD.R., Aly, M., Adolfsson, J., Xu, J., Zheng, S.L., Yeager, M., Kaaks,\nR., Diver, W.R., Gaudet, M.M., Stern, M.C., Corral, R., Joshi, A.D.,\nShahabi, A., Wahlfors, T., Tammela, T.L.J., Auvinen, A., Virtamo, J.,\nKlarskov, P., Nordestgaard, B.G., Røder, M.A., Nielsen, S.F., Bojesen,\nS.E., Siddiq, A., Fitzgerald, L.M., Kolb, S., Kwon, E.M., Karyadi,\nD.M., Blot, W.J., Zheng, W., Cai, Q., McDonnell, S.K., Rinckleb,\nA.E., Drake, B., Colditz, G., Wokolorczyk, D., Stephenson, R.A.,\nTeerlink, C., Muller, H., Rothenbacher, D., Sellers, T.A., Lin, H.-Y.,\nSlavov, C., Mitev, V., Lose, F., Srinivasan, S., Maia, S., Paulo, P.,\nLange, E., Cooney, K.A., Antoniou, A.C., Vincent, D., Bacot, F.,\nTessier, D.C., (part of GAME-ON) Initiative, C.O.G.S.R.U.G.,\nBioresource, A.P.C., of Urological Surgeons’ Section of Oncology,\nU.K.G.P.C.S.C.A., testing for cancer, U.K.P.P., Collaborators, T.S.,\nto Investigate Cancer-Associated Alterations in the\nGenome) Consortium, P.R.A.C.T.I.C.A.L.P.C.A.G., Kote-Jarai, Z.,\nEaston, D.F.: Identification of 23 new prostate cancer susceptibility\nloci using the iCOGS custom genotyping array. Nat Genet 45(4),\n385–9139112 (2013). doi:10.1038/ng.2560\n5. Pharoah, P.D.P., Tsai, Y.-Y., Ramus, S.J., Phelan, C.M., Goode, E.L.,\nLawrenson, K., Buckley, M., Fridley, B.L., Tyrer, J.P., Shen, H.,\nWeber, R., Karevan, R., Larson, M.C., Song, H., Tessier, D.C., Bacot,\nF., Vincent, D., Cunningham, J.M., Dennis, J., Dicks, E., Aben, K.K.,\nAnton-Culver, H., Antonenkova, N., Armasu, S.M., Baglietto, L.,\nBandera, E.V., Beckmann, M.W., Birrer, M.J., Bloom, G., Bogdanova,\nN., Brenton, J.D., Brinton, L.a., Brooks-Wilson, A., Brown, R.,\nButzow, R., Campbell, I., Carney, M.E., Carvalho, R.S., Chang-Claude,\nJ., Chen, Y.A., Chen, Z., Chow, W.-H., Cicek, M.S., Coetzee, G.,\nCook, L.S., Cramer, D.W., Cybulski, C., Dansonka-Mieszkowska, A.,\nDespierre, E., Doherty, J.a., Dörk, T., du Bois, A., Dürst, M., Eccles,\nD., Edwards, R., Ekici, A.B., Fasching, P.a., Fenstermacher, D.,\nFlanagan, J., Gao, Y.-T., Garcia-Closas, M., Gentry-Maharaj, A., Giles,\nG., Gjyshi, A., Gore, M., Gronwald, J., Guo, Q., Halle, M.K., Harter,\nP., Hein, A., Heitz, F., Hillemanns, P., Hoatlin, M., Hø gdall, E.,\nHø gdall, C.K., Hosono, S., Jakubowska, A., Jensen, A., Kalli, K.R.,\nKarlan, B.Y., Kelemen, L.E., Kiemeney, L.a., Kjaer, S.K., Konecny,\nG.E., Krakstad, C., Kupryjanczyk, J., Lambrechts, D., Lambrechts, S.,\nLe, N.D., Lee, N., Lee, J., Leminen, A., Lim, B.K., Lissowska, J.,\nLubiński, J., Lundvall, L., Lurie, G., Massuger, L.F.a.G., Matsuo, K.,\nMcGuire, V., McLaughlin, J.R., Menon, U., Modugno, F., Moysich,\nK.B., Nakanishi, T., Narod, S.a., Ness, R.B., Nevanlinna, H., Nickels,\nS., Noushmehr, H., Odunsi, K., Olson, S., Orlow, I., Paul, J., Pejovic,\nT., Pelttari, L.M., Permuth-Wey, J., Pike, M.C., Poole, E.M., Qu, X.,\nRisch, H.a., Rodriguez-Rodriguez, L., Rossing, M.A., Rudolph, A.,\nRunnebaum, I., Rzepecka, I.K., Salvesen, H.B., Schwaab, I., Severi, G.,\nShen, H., Shridhar, V., Shu, X.-O., Sieh, W., Southey, M.C.,\nSpellman, P., Tajima, K., Teo, S.-H., Terry, K.L., Thompson, P.J.,\nTimorek, A., Tworoger, S.S., van Altena, A.M., van den Berg, D.,\nVergote, I., Vierkant, R.a., Vitonis, A.F., Wang-Gohrke, S.,\nWentzensen, N., Whittemore, A.S., Wik, E., Winterhoff, B., Woo,\nY.L., Wu, A.H., Yang, H.P., Zheng, W., Ziogas, A., Zulkifli, F.,\nGoodman, M.T., Hall, P., Easton, D.F., Pearce, C.L., Berchuck, A.,\nChenevix-Trench, G., Iversen, E., Monteiro, A.N.a., Gayther, S.a.,\nSchildkraut, J.M., Sellers, T.a.: GWAS meta-analysis and replication\nidentifies three new susceptibility loci for ovarian cancer. Nature\ngenetics 45, 362–7037012 (2013). doi:10.1038/ng.2564\n6. Bojesen, S.E., Pooley, K.a., Johnatty, S.E., Beesley, J., Michailidou,\nK., Tyrer, J.P., Edwards, S.L., Pickett, H.a., Shen, H.C., Smart, C.E.,\nHillman, K.M., Mai, P.L., Lawrenson, K., Stutz, M.D., Lu, Y.,\nKarevan, R., Woods, N., Johnston, R.L., French, J.D., Chen, X.,\nWeischer, M., Nielsen, S.F., Maranian, M.J., Ghoussaini, M., Ahmed,\nS., Baynes, C., Bolla, M.K., Wang, Q., Dennis, J., McGuffog, L.,\nBarrowdale, D., Lee, A., Healey, S., Lush, M., Tessier, D.C., Vincent,\nD., Bacot, F., Vergote, I., Lambrechts, S., Despierre, E., Risch, H.a.,\nGonzález-Neira, A., Rossing, M.A., Pita, G., Doherty, J.a., Alvarez, N.,\nLarson, M.C., Fridley, B.L., Schoof, N., Chang-Claude, J., Cicek, M.S.,\nPeto, J., Kalli, K.R., Broeks, A., Armasu, S.M., Schmidt, M.K., Braaf,\nL.M., Winterhoff, B., Nevanlinna, H., Konecny, G.E., Lambrechts, D.,\n\nPage 14 of 16\n\nRogmann, L., Guénel, P., Teoman, A., Milne, R.L., Garcia, J.J., Cox,\nA., Shridhar, V., Burwinkel, B., Marme, F., Hein, R., Sawyer, E.J.,\nHaiman, C.a., Wang-Gohrke, S., Andrulis, I.L., Moysich, K.B., Hopper,\nJ.L., Odunsi, K., Lindblom, A., Giles, G.G., Brenner, H., Simard, J.,\nLurie, G., Fasching, P.a., Carney, M.E., Radice, P., Wilkens, L.R.,\nSwerdlow, A., Goodman, M.T., Brauch, H., Garcia-Closas, M.,\nHillemanns, P., Winqvist, R., Dürst, M., Devilee, P., Runnebaum, I.,\nJakubowska, A., Lubinski, J., Mannermaa, A., Butzow, R.,\nBogdanova, N.V., Dörk, T., Pelttari, L.M., Zheng, W., Leminen, A.,\nAnton-Culver, H., Bunker, C.H., Kristensen, V., Ness, R.B., Muir, K.,\nEdwards, R., Meindl, A., Heitz, F., Matsuo, K., du Bois, A., Wu, A.H.,\nHarter, P., Teo, S.-H., Schwaab, I., Shu, X.-O., Blot, W., Hosono, S.,\nKang, D., Nakanishi, T., Hartman, M., Yatabe, Y., Hamann, U.,\nKarlan, B.Y., Sangrajrang, S., Kjaer, S.K., Gaborieau, V., Jensen, A.,\nEccles, D., Hø gdall, E., Shen, C.-Y., Brown, J., Woo, Y.L., Shah, M.,\nAzmi, M.A.N., Luben, R., Omar, S.Z., Czene, K., Vierkant, R.a.,\nNordestgaard, B.r.G., Flyger, H., Vachon, C., Olson, J.E., Wang, X.,\nLevine, D.a., Rudolph, A., Weber, R.P., Flesch-Janys, D., Iversen, E.,\nNickels, S., Schildkraut, J.M., Silva, I.D.S., Cramer, D.W., Gibson, L.,\nTerry, K.L., Fletcher, O., Vitonis, A.F., van der Schoot, C.E., Poole,\nE.M., Hogervorst, F.B.L., Tworoger, S.S., Liu, J., Bandera, E.V., Li,\nJ., Olson, S.H., Humphreys, K., Orlow, I., Blomqvist, C.,\nRodriguez-Rodriguez, L., Aittomäki, K., Salvesen, H.B., Muranen,\nT.a., Wik, E., Brouwers, B., Krakstad, C., Wauters, E., Halle, M.K.,\nWildiers, H., Kiemeney, L.a., Mulot, C., Aben, K.K., Laurent-Puig, P.,\nAltena, A.M., Truong, T., Massuger, L.F.a.G., Benitez, J., Pejovic, T.,\nPerez, J.I.A., Hoatlin, M., Zamora, M.P., Cook, L.S.,\nBalasubramanian, S.P., Kelemen, L.E., Schneeweiss, A., Le, N.D.,\nSohn, C., Brooks-Wilson, A., Tomlinson, I., Kerin, M.J., Miller, N.,\nCybulski, C., Henderson, B.E., Menkiszak, J., Schumacher, F.,\nWentzensen, N., Le Marchand, L., Yang, H.P., Mulligan, A.M.,\nGlendon, G., Engelholm, S.A., Knight, J.a., Hø gdall, C.K., Apicella,\nC., Gore, M., Tsimiklis, H., Song, H., Southey, M.C., Jager, A., den\nOuweland, A.M.W., Brown, R., Martens, J.W.M., Flanagan, J.M.,\nKriege, M., Paul, J., Margolin, S., Siddiqui, N., Severi, G.,\nWhittemore, A.S., Baglietto, L., McGuire, V., Stegmaier, C., Sieh, W.,\nMüller, H.: Multiple independent variants at the TERT locus are\nassociated with telomere length and risks of breast and ovarian cancer.\nNature genetics 45, 371–8438412 (2013). doi:10.1038/ng.2566\n7. Garcia-Closas, M., Couch, F.J., Lindstrom, S., Michailidou, K.,\nSchmidt, M.K., Brook, M.N., Orr, N., Rhie, S.K., Riboli, E., Feigelson,\nH.S., Le Marchand, L., Buring, J.E., Eccles, D., Miron, P., Fasching,\nP.a., Brauch, H., Chang-Claude, J., Carpenter, J., Godwin, A.K.,\nNevanlinna, H., Giles, G.G., Cox, A., Hopper, J.L., Bolla, M.K., Wang,\nQ., Dennis, J., Dicks, E., Howat, W.J., Schoof, N., Bojesen, S.E.,\nLambrechts, D., Broeks, A., Andrulis, I.L., Guénel, P., Burwinkel, B.,\nSawyer, E.J., Hollestelle, A., Fletcher, O., Winqvist, R., Brenner, H.,\nMannermaa, A., Hamann, U., Meindl, A., Lindblom, A., Zheng, W.,\nDevillee, P., Goldberg, M.S., Lubinski, J., Kristensen, V., Swerdlow,\nA., Anton-Culver, H., Dörk, T., Muir, K., Matsuo, K., Wu, A.H.,\nRadice, P., Teo, S.H., Shu, X.-O., Blot, W., Kang, D., Hartman, M.,\nSangrajrang, S., Shen, C.-Y., Southey, M.C., Park, D.J., Hammet, F.,\nStone, J., Veer, L.J.V., Rutgers, E.J., Lophatananon, A.,\nStewart-Brown, S., Siriwanarangsan, P., Peto, J., Schrauder, M.G.,\nEkici, A.B., Beckmann, M.W., Dos Santos Silva, I., Johnson, N.,\nWarren, H., Tomlinson, I., Kerin, M.J., Miller, N., Marme, F.,\nSchneeweiss, A., Sohn, C., Truong, T., Laurent-Puig, P., Kerbrat, P.,\nNordestgaard, B.r.G., Nielsen, S.F., Flyger, H., Milne, R.L., Perez,\nJ.I.A., Menéndez, P., Müller, H., Arndt, V., Stegmaier, C., Lichtner,\nP., Lochmann, M., Justenhoven, C., Ko, Y.-D., Muranen, T.a.,\nAittomäki, K., Blomqvist, C., Greco, D., Heikkinen, T., Ito, H., Iwata,\nH., Yatabe, Y., Antonenkova, N.N., Margolin, S., Kataja, V., Kosma,\nV.-M., Hartikainen, J.M., Balleine, R., Tseng, C.-C., Berg, D.V.D.,\nStram, D.O., Neven, P., Dieudonné, A.-S., Leunen, K., Rudolph, A.,\nNickels, S., Flesch-Janys, D., Peterlongo, P., Peissel, B., Bernard, L.,\nOlson, J.E., Wang, X., Stevens, K., Severi, G., Baglietto, L., McLean,\nC., Coetzee, G.a., Feng, Y., Henderson, B.E., Schumacher, F.,\nBogdanova, N.V., Labrèche, F., Dumont, M., Yip, C.H., Taib, N.A.M.,\nCheng, C.-Y., Shrubsole, M., Long, J., Pylkäs, K., Jukkola-Vuorinen,\nA., Kauppila, S., Knight, J.a., Glendon, G., Mulligan, A.M., Tollenaar,\nR.a.E.M., Seynaeve, C.M., Kriege, M., Hooning, M.J., van den\n\n\fJäger et al.\n\nOuweland, A.M.W., van Deurzen, C.H.M., Lu, W., Gao, Y.-T., Cai,\nH., Balasubramanian, S.P., Cross, S.S., Reed, M.W.R., Signorello, L.,\nCai, Q., Shah, M., Miao, H., Chan, C.W., Chia, K.S., Jakubowska, A.,\nJaworska, K., Durda, K., Hsiung, C.-N., Wu, P.-E., Yu, J.-C.,\nAshworth, A., Jones, M., Tessier, D.C., González-Neira, A., Pita, G.,\nAlonso, M.R., Vincent, D., Bacot, F., Ambrosone, C.B., Bandera,\nE.V., John, E.M., Chen, G.K., Hu, J.J., Rodriguez-Gil, J.L., Bernstein,\nL., Press, M.F., Ziegler, R.G., Millikan, R.M., Deming-Halverson, S.L.,\nNyante, S., Ingles, S.a., Waisfisz, Q., Tsimiklis, H., Makalic, E.,\nSchmidt, D., Bui, M., Gibson, L., Müller-Myhsok, B., Schmutzler,\nR.K., Hein, R., Dahmen, N., Beckmann, L., Aaltonen, K., Czene, K.,\nIrwanto, A., Liu, J., Turnbull, C., Rahman, N., Meijers-Heijboer, H.,\nUitterlinden, A.G., Rivadeneira, F., Olswold, C., Slager, S., Pilarski, R.,\nAdemuyiwa, F., Konstantopoulou, I., Martin, N.G., Montgomery,\nG.W., Slamon, D.J., Rauh, C., Lux, M.P., Jud, S.M., Bruning, T.,\nWeaver, J., Sharma, P., Pathak, H., Tapper, W., Gerty, S., Durcan, L.:\nGenome-wide association studies identify four ER negative-specific\nbreast cancer risk loci. Nature genetics 45, 392–839812 (2013).\ndoi:10.1038/ng.2561\n8. Michailidou, K., Hall, P., Gonzalez-Neira, A., Ghoussaini, M., Dennis,\nJ., Milne, R.L., Schmidt, M.K., Chang-Claude, J., Bojesen, S.E., Bolla,\nM.K., Wang, Q., Dicks, E., Lee, A., Turnbull, C., Rahman, N.,\nFletcher, O., Peto, J., Gibson, L., Dos Santos Silva, I., Nevanlinna, H.,\nMuranen, T.a., Aittomäki, K., Blomqvist, C., Czene, K., Irwanto, A.,\nLiu, J., Waisfisz, Q., Meijers-Heijboer, H., Adank, M., van der Luijt,\nR.B., Hein, R., Dahmen, N., Beckman, L., Meindl, A., Schmutzler,\nR.K., Müller-Myhsok, B., Lichtner, P., Hopper, J.L., Southey, M.C.,\nMakalic, E., Schmidt, D.F., Uitterlinden, A.G., Hofman, A., Hunter,\nD.J., Chanock, S.J., Vincent, D., Bacot, F., Tessier, D.C., Canisius, S.,\nWessels, L.F.a., Haiman, C.a., Shah, M., Luben, R., Brown, J.,\nLuccarini, C., Schoof, N., Humphreys, K., Li, J., Nordestgaard, B.r.G.,\nNielsen, S.F., Flyger, H., Couch, F.J., Wang, X., Vachon, C., Stevens,\nK.N., Lambrechts, D., Moisse, M., Paridaens, R., Christiaens, M.-R.,\nRudolph, A., Nickels, S., Flesch-Janys, D., Johnson, N., Aitken, Z.,\nAaltonen, K., Heikkinen, T., Broeks, A., Veer, L.J.V., van der Schoot,\nC.E., Guénel, P., Truong, T., Laurent-Puig, P., Menegaux, F., Marme,\nF., Schneeweiss, A., Sohn, C., Burwinkel, B., Zamora, M.P., Perez,\nJ.I.A., Pita, G., Alonso, M.R., Cox, A., Brock, I.W., Cross, S.S., Reed,\nM.W.R., Sawyer, E.J., Tomlinson, I., Kerin, M.J., Miller, N.,\nHenderson, B.E., Schumacher, F., Le Marchand, L., Andrulis, I.L.,\nKnight, J.a., Glendon, G., Mulligan, A.M., Lindblom, A., Margolin, S.,\nHooning, M.J., Hollestelle, A., van den Ouweland, A.M.W., Jager, A.,\nBui, Q.M., Stone, J., Dite, G.S., Apicella, C., Tsimiklis, H., Giles,\nG.G., Severi, G., Baglietto, L., Fasching, P.a., Haeberle, L., Ekici,\nA.B., Beckmann, M.W., Brenner, H., Müller, H., Arndt, V., Stegmaier,\nC., Swerdlow, A., Ashworth, A., Orr, N., Jones, M., Figueroa, J.,\nLissowska, J., Brinton, L., Goldberg, M.S., Labrèche, F., Dumont, M.,\nWinqvist, R., Pylkäs, K., Jukkola-Vuorinen, A., Grip, M., Brauch, H.,\nHamann, U., Brüning, T., Radice, P., Peterlongo, P., Manoukian, S.,\nBonanni, B., Devilee, P., Tollenaar, R.a.E.M., Seynaeve, C., van\nAsperen, C.J., Jakubowska, A., Lubinski, J., Jaworska, K., Durda, K.,\nMannermaa, A., Kataja, V., Kosma, V.-M., Hartikainen, J.M.,\nBogdanova, N.V., Antonenkova, N.N., Dörk, T., Kristensen, V.N.,\nAnton-Culver, H., Slager, S., Toland, A.E., Edge, S., Fostira, F., Kang,\nD., Yoo, K.-Y., Noh, D.-Y., Matsuo, K., Ito, H., Iwata, H., Sueta, A.,\nWu, A.H., Tseng, C.-C., Van Den Berg, D., Stram, D.O., Shu, X.-O.,\nLu, W., Gao, Y.-T., Cai, H., Teo, S.H., Yip, C.H., Phuah, S.Y.,\nCornes, B.K., Hartman, M., Miao, H., Lim, W.Y., Sng, J.-H., Muir,\nK., Lophatananon, A., Stewart-Brown, S., Siriwanarangsan, P., Shen,\nC.-Y., Hsiung, C.-N., Wu, P.-E., Ding, S.-L., Sangrajrang, S.,\nGaborieau, V., Brennan, P., McKay, J., Blot, W.J., Signorello, L.B.,\nCai, Q., Zheng, W., Deming-Halverson, S., Shrubsole, M., Long, J.,\nSimard, J., Garcia-Closas, M., Pharoah, P.D.P., Chenevix-Trench, G.,\nDunning, A.M., Benitez, J., Easton, D.F.: Large-scale genotyping\nidentifies 41 new loci associated with breast cancer risk. Nature\ngenetics 45, 353–6136112 (2013). doi:10.1038/ng.2563\n9. Jallow, M., Teo, Y.Y., Small, K.S., Rockett, K.A., Deloukas, P., Clark,\nT.G., Kivinen, K., Bojang, K.A., Conway, D.J., Pinder, M., Sirugo, G.,\nSisay-Joof, F., Usen, S., Auburn, S., Bumpstead, S.J., Campino, S.,\nCoffey, A., Dunham, A., Fry, A.E., Green, A., Gwilliam, R., Hunt, S.E.,\nInouye, M., Jeffreys, A.E., Mendy, A., Palotie, A., Potter, S.,\n\nPage 15 of 16\n\n10.\n\n11.\n12.\n13.\n\n14.\n\n15.\n\n16.\n\nRagoussis, J., Rogers, J., Rowlands, K., Somaskantharajah, E.,\nWhittaker, P., Widden, C., Donnelly, P., Howie, B., Marchini, J.,\nMorris, A., SanJoaquin, M., Achidi, E.A., Agbenyega, T., Allen, A.,\nAmodu, O., Corran, P., Djimde, A., Dolo, A., Doumbo, O.K., Drakeley,\nC., Dunstan, S., Evans, J., Farrar, J., Fernando, D., Hien, T.T.,\nHorstmann, R.D., Ibrahim, M., Karunaweera, N., Kokwaro, G., Koram,\nK.A., Lemnge, M., Makani, J., Marsh, K., Michon, P., Modiano, D.,\nMolyneux, M.E., Mueller, I., Parker, M., Peshu, N., Plowe, C.V.,\nPuijalon, O., Reeder, J., Reyburn, H., Riley, E.M., Sakuntabhai, A.,\nSinghasivanon, P., Sirima, S., Tall, A., Taylor, T.E., Thera, M.,\nTroye-Blomberg, M., Williams, T.N., Wilson, M., Kwiatkowski, D.P.:\nGenome-wide and fine-resolution association analysis of malaria in\nWest Africa. Nature genetics 41, 657–665 (2009). doi:10.1038/ng.388\nKathiresan, S., Voight, B.F., Purcell, S., Musunuru, K., Ardissino, D.,\nMannucci, P.M., Anand, S., Engert, J.C., Samani, N.J., Schunkert, H.,\nErdmann, J., Reilly, M.P., Rader, D.J., Morgan, T., Spertus, J.A.,\nStoll, M., Girelli, D., McKeown, P.P., Patterson, C.C., Siscovick, D.S.,\nO’Donnell, C.J., Elosua, R., Peltonen, L., Salomaa, V., Schwartz,\nS.M., Melander, O., Altshuler, D., Ardissino, D., Merlini, P.A.,\nBerzuini, C., Bernardinelli, L., Peyvandi, F., Tubaro, M., Celli, P.,\nFerrario, M., Fetiveau, R., Marziliano, N., Casari, G., Galli, M.,\nRibichini, F., Rossi, M., Bernardi, F., Zonzin, P., Piazza, A., Mannucci,\nP.M., Schwartz, S.M., Siscovick, D.S., Yee, J., Friedlander, Y., Elosua,\nR., Marrugat, J., Lucas, G., Subirana, I., Sala, J., Ramos, R.,\nKathiresan, S., Meigs, J.B., Williams, G., Nathan, D.M., MacRae,\nC.A., O’Donnell, C.J., Salomaa, V., Havulinna, A.S., Peltonen, L.,\nMelander, O., Berglund, G., Voight, B.F., Kathiresan, S., Hirschhorn,\nJ.N., Asselta, R., Duga, S., Spreafico, M., Musunuru, K., Daly, M.J.,\nPurcell, S., Voight, B.F., Purcell, S., Nemesh, J., Korn, J.M.,\nMcCarroll, S.A., Schwartz, S.M., Yee, J., Kathiresan, S., Lucas, G.,\nSubirana, I., Elosua, R., Surti, A., Guiducci, C., Gianniny, L., Mirel, D.,\nParkin, M., Burtt, N., Gabriel, S.B., Samani, N.J., Thompson, J.R.,\nBraund, P.S., Wright, B.J., Balmforth, A.J., Ball, S.G., Hall, A.S.,\nSchunkert, H., Erdmann, J., Linsel-Nitschke, P., Lieb, W., Ziegler, A.,\nKönig, I., Hengstenberg, C., Fischer, M., Stark, K., Grosshennig, A.,\nPreuss, M., Wichmann, H.-E., Schreiber, S., Schunkert, H., Samani,\nN.J., Erdmann, J., Ouwehand, W., Hengstenberg, C., Deloukas, P.,\nScholz, M., Cambien, F., Reilly, M.P., Li, M., Chen, Z., Wilensky, R.,\nMatthai, W., Qasim, A., Hakonarson, H.H., Devaney, J., Burnett,\nM.-S., Pichard, A.D., Kent, K.M., Satler, L., Lindsay, J.M., Waksman,\nR., Knouff, C.W., Waterworth, D.M., Walker, M.C., Mooser, V.,\nEpstein, S.E., Rader, D.J., Scheffold, T., Berger, K., Stoll, M., Huge,\nA., Girelli, D., Martinelli, N., Olivieri, O., Corrocher, R., Morgan, T.,\nSpertus, J.A., McKeown, P., Patterson, C.C., Schunkert, H., Erdmann,\nE., Linsel-Nitschke, P., Lieb, W., Ziegler, A., König, I.R.,\nHengstenberg, C., Fischer, M., Stark, K., Grosshennig, A., Preuss, M.,\nWichmann, H.-E., Schreiber, S., Hólm, H., Thorleifsson, G.,\nThorsteinsdottir, U., Stefansson, K., Engert, J.C., Do, R., Xie, C.,\nAnand, S., Kathiresan, S., Ardissino, D., Mannucci, P.M., Siscovick,\nD., O’Donnell, C.J., Samani, N.J., Melander, O., Elosua, R., Peltonen,\nL., Salomaa, V., Schwartz, S.M., Altshuler, D.: Genome-wide\nassociation of early-onset myocardial infarction with single nucleotide\npolymorphisms and copy number variants. Nature genetics 41,\n334–341 (2009). doi:10.1038/ng.327\nGibson, G.: Hints of hidden heritability in GWAS. Nature genetics\n42(7), 558–60 (2010). doi:10.1038/ng0710-558\nGolden Helix: SNP and Variation Suite (SVS 7).\nhttp://www.goldenhelix.com (March 13, 2014, last accessed).\nMilne, I., Shaw, P., Stephen, G., Bayer, M., Cardle, L., Thomas,\nW.T.B., Flavell, A.J., Marshall, D.: Flapjack–graphical genotype\nvisualization. Bioinformatics (Oxford, England) 26, 3133–3134 (2010).\ndoi:10.1093/bioinformatics/btq580\nFiume, M., Smith, E.J., Brook, A., Strbenac, D., Turner, B., Mezlini,\nA.M., Robinson, M.D., Wodak, S.J., Brudno, M.: Savant genome\nbrowser 2: visualization and analysis for population-scale genomics.\nNucleic acids research 40(W1), 615–621 (2012)\nBarrett, J.C., Fry, B., Maller, J., Daly, M.: Haploview: analysis and\nvisualization of ld and haplotype maps. Bioinformatics 21(2), 263–265\n(2005)\nSan Lucas, F.A., Rosenberg, N.A., Scheet, P.: Haploscope: a tool for\nthe graphical display of haplotype structure in populations. Genetic\n\n\fJäger et al.\n\nepidemiology 36(1), 17–21 (2012). doi:10.1002/gepi.20640\n17. Utro, F., Haiminen, N., Livingstone, D., Cornejo, O.E., Royaert, S.,\nSchnell, R.J., Motamayor, J.C., Kuhn, D.N., Parida, L.: iXora: exact\nhaplotype inferencing and trait association. BMC genetics 14, 48\n(2013). doi:10.1186/1471-2156-14-48\n18. Howie, B.N., Donnelly, P., Marchini, J.: A flexible and accurate\ngenotype imputation method for the next generation of genome-wide\nassociation studies. PLoS Genet 5(6), 1000529 (2009).\ndoi:10.1371/journal.pgen.1000529\n19. Heinrich, J., Vehlow, C., Battke, F., Jäger, G., Weiskopf, D., Nieselt,\nK.: iHAT: interactive hierarchical aggregation table for genetic\nassociation data. BMC bioinformatics 13 Suppl 8, 2 (2012).\ndoi:10.1186/1471-2105-13-S8-S2\n20. Delaneau, O., Marchini, J., Zagury, J.-F.: A linear complexity phasing\nmethod for thousands of genomes. Nature methods 9(2), 179–81\n(2012). doi:10.1038/nmeth.1785\n21. Delaneau, O., Zagury, J.-F., Marchini, J.: Improved whole-chromosome\nphasing for disease and population genetic studies. Nature methods\n10(1), 5–6 (2013). doi:10.1038/nmeth.2307\n22. Browning, B.L., Browning, S.R.: A fast, powerful method for detecting\nidentity by descent. American journal of human genetics 88(2), 173–82\n(2011). doi:10.1016/j.ajhg.2011.01.010\n23. Browning, S.R., Browning, B.L.: High-resolution detection of identity\nby descent in unrelated individuals. American journal of human\ngenetics 86(4), 526–39 (2010). doi:10.1016/j.ajhg.2010.02.021\n24. ”1000 Genomes Project Consortium”, Abecasis, G.R., Altshuler, D.,\nAuton, A., Brooks, L.D., Durbin, R., Gibbs, R., Hurles, M., McVean,\nG.: An integrated map of genetic variation from 1,092 human\ngenomes. Nature 491(7422), 56–65 (2012). doi:10.1038/nature11632\n25. Brewer, C.A., Harrower, M.: ColorBrewer. On-line color brewing tool\navailable from URL http://www.colorbrewer.org 6, (2002)\n26. Mackinlay, J.: Automating the design of graphical presentations of\nrelational information. ACM Trans. Graph. 5(2), 110–141 (1986).\ndoi:10.1145/22949.22950\n27. Chambers, J.C., Zhang, W., Lord, G.M., van der Harst, P., Lawlor,\nD.A., Sehmi, J.S., Gale, D.P., Wass, M.N., Ahmadi, K.R., Bakker,\nS.J.L., Beckmann, J., Bilo, H.J.G., Bochud, M., Brown, M.J.,\nCaulfield, M.J., Connell, J.M.C., Cook, H.T., Cotlarciuc, I., Davey\nSmith, G., de Silva, R., Deng, G., Devuyst, O., Dikkeschei, L.D.,\nDimkovic, N., Dockrell, M., Dominiczak, A., Ebrahim, S., Eggermann,\nT., Farrall, M., Ferrucci, L., Floege, J., Forouhi, N.G., Gansevoort,\nR.T., Han, X., Hedblad, B., Homan van der Heide, J.J., Hepkema,\nB.G., Hernandez-Fuentes, M., Hypponen, E., Johnson, T., de Jong,\nP.E., Kleefstra, N., Lagou, V., Lapsley, M., Li, Y., Loos, R.J.F., Luan,\nJ., Luttropp, K., Maréchal, C., Melander, O., Munroe, P.B., Nordfors,\nL., Parsa, A., Peltonen, L., Penninx, B.W., Perucha, E., Pouta, A.,\nProkopenko, I., Roderick, P.J., Ruokonen, A., Samani, N.J., Sanna, S.,\nSchalling, M., Schlessinger, D., Schlieper, G., Seelen, M.A.J.,\nShuldiner, A.R., Sjögren, M., Smit, J.H., Snieder, H., Soranzo, N.,\nSpector, T.D., Stenvinkel, P., Sternberg, M.J.E., Swaminathan, R.,\nTanaka, T., Ubink-Veltmaat, L.J., Uda, M., Vollenweider, P., Wallace,\nC., Waterworth, D., Zerres, K., Waeber, G., Wareham, N.J., Maxwell,\nP.H., McCarthy, M.I., Jarvelin, M.-R., Mooser, V., Abecasis, G.R.,\nLightstone, L., Scott, J., Navis, G., Elliott, P., Kooner, J.S.: Genetic\nloci influencing kidney function and chronic kidney disease. Nature\ngenetics 42, 373–375 (2010). doi:10.1038/ng.566\n28. Le, T., Bhushan, V.: First Aid for the USMLE Step 1 2013. McGraw\nHill Professional, Louisville, USA (2012)\n29. Poeppel, P., Habetha, M., Marcão, A., Büssow, H., Berna, L.,\nGieselmann, V.: Missense mutations as a cause of metachromatic\nleukodystrophy. Degradation of arylsulfatase A in the endoplasmic\nreticulum. The FEBS journal 272, 1179–1188 (2005).\ndoi:10.1111/j.1742-4658.2005.04553.x\n30. Fry, B.: Computational information design. PhD thesis, Carnegie\nMellon University (April 2004)\n\nPage 16 of 16\n\n\f"
        ],
        [
         "22",
         "22",
         "cs.CE",
         "Computational Engineering",
         "1403.0307v1.pdf",
         "Isogeometric finite element analysis of laminated composite plates based on\na four variable refined plate theory.\nLoc V. Tran1, Chien H. Thai1, Buntara S. Gan 2 and H. Nguyen-Xuan1,3*\n1\n\nDivision of Computational Mechanics, Ton Duc Thang University Ho Chi Minh City, Vietnam\n\n2\n\nDepartment of Architecture, College of Engineering, Nihon University, Koriyama City, Fukushima\nPrefecture, Japan\n\n3\n\nDepartment of Mechanics, Faculty of Mathematics & Computer Science, University of Science Ho\nChi Minh City, Vietnam\n\nAbstract\nIn this paper, a novel and effective formulation based on isogeometric approach (IGA) and Refined\nPlate Theory (RPT) is proposed to study the behavior of laminated composite plates. Using many\nkinds of higher-order distributed functions, RPT model naturally satisfies the traction-free boundary\nconditions at plate surfaces and describes the non-linear distribution of shear stresses without requiring\nshear correction factor (SCF). IGA utilizes the basis functions, namely B-splines or non-uniform\nrational B-splines (NURBS), which achieve easily the smoothness of any arbitrary order. It hence\nsatisfies the C1 requirement of the RPT model. The static, dynamic and buckling analysis of\nrectangular plates is investigated for different boundary conditions. Numerical results show high\neffectiveness of the present formulation.\nKeywords: plate, composite, isogeometric analysis (IGA), refined plate theory (RPT).\n\n1. Introduction\nLaminated composite plates are being increasingly used in various fields of engineering such as\naircrafts, aerospace, vehicles, submarine, ships, buildings, etc, because they possess many favorable\nmechanical properties such as high stiffness to weight and low density.\nIn order to use them efficiently, a clear understanding of their behaviors such as: deformable\ncharacteristic, stress distribution, natural frequency and critical buckling load under various conditions\nare required. Hence, investigation on property of composite structure has been addressed since long\ntime. Pagano [1] initially investigated the analytical three-dimensional (3D) elasticity method to\n*\n\nCorresponding author. Email address: nxhung@hcmus.edu.vn (H. Nguyen-Xuan)\n\n1\n\n\fpredict the exact solution of simple static problems. Noor et al. [2,3] have further developed an 3D\nelasticity solution formulation for stress analysis of composite structures. It is well known that such an\nexact 3D approach is the most potential tool to obtain the true solution of plates. However, there is not\neasy to solve practical problems in which complex (or even slightly complicated) geometries and\nboundary conditions are required. In addition, each layer in the 3D elasticity theory is modeled as a 3D\nsolid so that the computational cost of laminated composite plate analyses will be increased\nsignificantly. Hence, many equivalent single layer (ESL) plate theories with suitable assumptions [4]\nhave been then proposed to transform the 3D problem to 2D one. Among of the ESL plate theories, the\nClassical Laminate Plate Theory (CLPT) based on the Love-Kirchoff assumptions is first proposed.\nDue to ignoring the transverse shear deformation, CLPT merely to provide acceptable results for the\nthin plate. The First Order Shear Deformation Theory (FSDT) based on Reissner [5] and Mindlin [6],\nwhich takes into account the shear effect, was therefore developed. In FSDT model, with the linear inplane displacement assumption through plate thickness, obtained shear strain/stress distributes\ninaccurately and does not satisfy the traction free boundary conditions at the plate surfaces. The shear\ncorrection factors (SCF) are hence required to rectify the unrealistic shear strain energy part. The\nvalues of SCF are quite dispersed through each problem and may be difficult to determine [7]. To\nbypass the limitations of the FSDT, many kind of Higher-Order Shear Deformable Theories (HSDT),\nwhich include higher-order terms in the approximation of the displacement field, have then been\ndevised such as Third-Order Shear Deformation Theory (TSDT) [8-12], trigonometric shear\ndeformation theory [13-17], exponential shear deformation theory (ESDT) [18-20], Refined Plate\nTheory (RPT) and so on. The RPT model was found in Ref [21] of Senthilnathan et al with one\nvariable lower than that of TSDT of Reddy, and then extended by Shimpi et al [22-24], Thai et al [2526]. It is worth mentioning that the HSDT models provide better results and yield more accurate and\nstable solutions (e.g. inter-laminar stresses and displacements) [27,28] than the FSDT ones without\nrequirement the SCF. However, the HSDT requires the C1-continuity of generalized displacement field\nleading to the second-order derivative of the stiffness formulation and it causes the obstacles in the\nstandard finite element formulations. Several C0 continuous elements [29-32] were then proposed or\nHermite interpolation function with the C1-continuity was added for just specific approximation of\ntransverse displacement [8]. It may produce extra unknown variables including derivative of\ndeflection w,x , w, y , w,xy [4] leading to increase in the computational cost. In this paper, we show that C1continuous elements will be easily achieved by IGA without any additional variables.\n\n2\n\n\fIsogeometric approach (IGA) has been recently proposed by Hughes et al. [33] to closely link the gap\nbetween Computer Aided Design (CAD) and Finite Element Analysis (FEA). The basic idea is that the\nIGA uses the same non-uniform rational B-Spline (NURBS) functions in describing the exact\ngeometry of problem and constructing finite approximation for analysis. It is well known that NURBS\nfunctions provide a flexible way to make refinement, de-refinement, and degree elevation [34]. They\nenable us to easily achieve the smoothness of arbitrary continuity order in comparison with the\ntraditional FEM. Hence, IGA naturally verifies the C1-continuity of plates based on the HSDT\nassumption, which is interested in this study. The IGA has been well known and widely applied to\nvarious practical problems [35-42] and so on.\nIn this paper, a combination between Isogeometric Approach and the RPT model (RPT-IGA) for\nstatic, free vibration and buckling analysis of laminated composite plates is studied. Herein, some\nhigher-order distributed functions [8,18,19,22,43] are utilized to multiply to higher-order term in\ndisplacement field. Several numerical examples are given to show the performance of the proposed\nmethod and results obtained are compared to other published methods in the literature.\nThe paper is outlined as follows. Next section introduces the RPT for composite plates. In section 3,\nthe formulation of plate theory based on IGA is described. The numerical results and discussions are\nprovided in section 4. Finally, this article is closed with some concluding remarks.\n\n2. The refined plate theory\n2.1. Displacement field\nTo consider the effect of shear deformation directly, the higher-order terms are incorporated into the\ndisplacement field. A simple and famous theory for bending plate has also given by JN. Reddy based\non TSDT [4]:\n\nu ( x, y, z )  u0  z  x  g ( z )   x  w, x \n\nh\n h\nv( x, y, z )  v0  z  y  g ( z )   y  w, y  , \nz \n2\n2\n\nw( x, y )  w0\n\nwhere g ( z )  \n\n(1)\n\nT\n4z3\nT\nand the variables u0  u0 v0  , w0 and    x  y  are the membrane\n2\n3h\n\ndisplacements, the deflection of the mid-plane and the rotations in the y-z, x-z planes, respectively. By\nmaking additional assumptions given in Eq. (2), Senthilnathan et al [21] proposed the refined plate\ntheory model with one reduced variable.\n\n3\n\n\fw0  wb  ws ; n  wb\n\n(2)\n\nwhere wb and ws are defined as the bending and shear components of deflection. Eq. (1) is taken in the\nsimpler form\n\nu ( x, y, z )  u0  zwb , x  g ( z ) ws , x\nv( x, y, z )  v0  zwb , y  g ( z ) ws , y\n\n(3)\n\nw( x, y )  wb  ws\nThe relationship between strains and displacements is described by\n  [ xx  yy  xy ]T  0  z b  g ( z)κ s\n\n(4)\n\nT\n\nγ   xz  yz   f ( z )ε s\n\nf ( z )  g ( z)  1\n\nwhere\n\n u0, x \n wb , xx \n ws , xx \n ws , x \n\n\n\n\n\n\n 0   v0, y  , κ b    wb , yy  , κ s   ws , yy  , ε s  \n\n ws , y \nu0, y  v0, x \n 2 ws , xy \n 2wb , xy \n\n\n\n\n\n\n\n(5)\n\nIt is seen that f ( z )  0 at z  h / 2 . It means that traction-free boundary condition at the top and\nbottom plate surfaces is automatically satisfied. Based on this condition, many kinds of distributed\nfunctions f ( z ) in forms: third-order polynomials by Reddy [8] and Shimpi [22], exponential function\nby Karama [18], sinusoidal function by Arya [19] and firth-order polynomial by Nguyen-Xuan [43]\nare listed in Table 1.\nTable 1: The various forms of shape function.\nModel\nReddy [8]\n\nf ( z )\n\nf ( z)\nz z /h\n4\n3\n\n3\n\n5\n4\n\nKarama [18]\n\nze2( z / h)\n\nArya [19]\n\nsin( h z )\n7\n8\n\n5\n4\n\nz  53 z 3 / h2\n\nShimpi [22]\n\nNguyen-Xuan [43]\n\n1  4 z 2 / h2\n\n2\n\n1  4 z\n\n(1 \n\n2\n\n\n\nz  h22 z 3  h24 z 5\n\n4\n\n4\nh2\n\n2\n\n/ h2 \n\nz 2 )e2( z / h )\n\nh\n\ncos( h z )\n\n7\n8\n\n h62 z 2  10\nz4\nh4\n\n2\n\n\f2.1. Weak form equations for plate problems\nA weak form of the static model for the plates under transverse loading q0 can be briefly expressed as:\n\n  D d   \nT\n\nb\n\n\n\nT\n\n\n\nDs d \n\n  wq d\n\n(6)\n\n0\n\n\n\nwhere\nA B E \nD =  B D F \n E F H \nb\n\n(7)\n\nand the material matrices are given as\nAij , Bij , Dij , Eij , Fij , H ij  \n\nh /2\n\n h /2\n\n(1, z, z 2 , g ( z ), zg ( z ), g 2 ( z ))Qij dz\n\n(i, j  1, 2, 6)\n\n(8)\nDijs  \n\nh /2\n\n f ( z)\n\n2\n\n h /2\n\nQij dz\n\n(i, j  4, 5)\n\nin which Qij are transformed material constants of the kth lamina (see [4] for more detail).\nFor the free vibration analysis of the plates, weak form can be derived from the following dynamic\nequation\n\n  D d   \nT\n\nb\n\n\n\n\n\nT\n\nDs d \n\n  u mud\nT\n\n\n\n(9)\n\nwhere m - the mass matrix is calculated according to the consistent form\nI 0\nm   0\n 0\n\n0\nI0\n0\n\n0\n I1\n\n0  where I 0   I 2\n I 4\nI 0 \n\nI2\nI3\nI5\n\nI4 \nI 5 \nI 6 \n\n(10)\n\nh /2\n\n I1 , I 2 , I3 , I 4 , I5 , I 6     ( z) 1, z, z 2 , g ( z), zg ( z), g 2 ( z) dz .\n h /2\n\n(11)\n\nand\n\n v0 \n u0 \nu1 \n w\n\n\n \n\n\n \nu  u 2  , u1   wb, x  ; u 2   wb, y  ; u3   0 \nu \nw \nw \n0\n \n 3\n s,x \n s, y \nFor the buckling analysis, a weak form of the plate under the in-plane forces can be expressed as:\n\n5\n\n(12)\n\n\f  D d   \nT\n\nb\n\n\n\n\n\nT\n\nDs d \n\n\n\n\n\nT  wN0wd  0\n\n N x0\nwhere T  [ / x  / y]T is the gradient operator and N0   0\n N xy\n\n(13)\n\n0\n\nN xy\n is a matrix related to the pre0\nN y \n\nbuckling loads.\n\n3. The composite plate formulation based on NURBS basis functions\n3.1. A brief of NURBS functions\nA knot vector Ξ  1 , 2 ,..., n  p 1 is defined as a sequence of knot value i  R , i  1,...n  p . If the\nfirst and the last knots are repeated p+1 times, the knot vector is called open knot. A B-spline basis\nfunction is C continuous inside a knot span and Cp-1 continuous at a single knot. Thus, as p  2 the\npresent approach always satisfies C1-requirement in approximate formulations of RPT.\nThe B-spline basis functions Ni , p   are defined by the following recursion formula\nNi , p   \n\ni  p 1  \n  i\nNi , p 1   \nNi 1, p 1  \ni  p  i\ni  p 1  i 1\n\n1 if i    i 1 \nas p = 0, Ni ,0    \n\notherwise \n0\n\n(14)\n\nBy the tensor product of basis functions in two parametric dimensions  and  with two knot\nvectors Ξ  1 , 2 ,..., n  p 1 and Η  1 ,2 ,...,m q 1 , the two-dimensional B-spline basis functions\nare obtained\nN A  ,   Ni , p   M j ,q  \n\n(15)\n\nFigure 1 illustrates the set of one-dimensional and two-dimensional B-spline basis functions\naccording to open uniform knot vector Ξ  0, 0, 0, 0, 0.5, 1, 1, 1, 1 .\n\n6\n\n\fFigure 1. 1D and 2D B-spline basis functions.\nTo present exactly some curved geometries (e.g. circles, cylinders, spheres, etc.) the non-uniform\nrational B-splines (NURBS) functions are used. Be different from B-spline, each control point of\nNURBS has additional value called an individual weight wA [33]. Then the NURBS functions can be\nexpressed as\nRA  ,  \n\nN A wA\nmn\n\n N  ,  w\nA\n\n(16)\n\nA\n\nA\n\nIt can be noted that B-spline function is only the special case of the NURBS function when the\nindividual weight of control point is constant.\n3.2. A novel RPT formulation based on NURBS approximation\nUsing the NURBS basis functions above, the displacement field u of the plate is approximated as\nmn\n\nu h  ,    RA  , q A\n\n(17)\n\nA\n\nwhere q A  u0 A v0 A wb A wsA  is the vector of nodal degrees of freedom associated with the control\nT\n\npoint A.\nSubstituting Eq. (17) into Eq. (5), the in-plane and shear strains can be rewritten as:\nmn\n\nT\nT\nT\nT\nT\nT0 Tb κTs Ts     BmA   BbA1   BbA2   B sA   q A\n\n\nT\n\nA1\n\nin which\n\n7\n\n(18)\n\n\f RA , x\n\nB  0\n RA , y\n\nm\nA\n\n0 0 RA, xx\n0 0\n b1\n\n0 0  , B A   0 0 RA, yy\n0 0 2 RA, xy\n0 0 \n\n\n0\nRA , y\nRA , x\n\n0\n\n0 ,\n0 \n(19)\n\n0 0 0 RA, xx \n 0 0 0 RA , x \n\n\nb2\nB A  0 0 0 RA, yy  , B sA  \n\n 0 0 0 RA , y \n0 0 0 2 RA, xy \n\n\nSubstituting Eq. (18) into Eqs. (6), (9) and (13), the formulations of static, free vibration and buckling\nproblem are rewritten in the following form\n\nKq = F\n\n(20)\n\nK   Mq  0\n\n(21)\n\nK  \n\n(22)\n\n2\n\ncr\n\nKg q  0\n\nwhere the global stiffness matrix K is given by\nT\n\nB m \n \nK   Bb1 \n\n b2 \nB \n\nm\n A B E  B \n B D F  Bb1   B sT Ds B s d\n\n \n E F H  Bb 2 \n \n\n(23)\n\nand the load vector is computed by\nF   q0 Rd\n\n(24)\n\n\n\nwhere\nR  0 0 RA\n\nRA \n\n(25)\n\nthe global mass matrix M is expressed as\nM   RT mRd\n\n\nwhere\n\n8\n\n(26)\n\n\f RA\nR1 \n \n\nR  R 2  , R1   0\nR \n0\n 3\n\n 0 RA\n\nR 2  0 0\n0 0\n\n\n0\n0\n0  RA , x\n0\n0\n\n0 \n\n0 ;\nRA, x \n\n0 \n 0 0 RA\n\n0  ; R 3  0 0 0\n0 0 0\nRA, y \n\n0\n RA , y\n0\n\nRA \n0 \n0 \n\n(27)\n\nthe geometric stiffness matrix is\n\n \n\nK g   Bg\n\n\nT\n\nN0B g d\n\n(28)\n\nwhere\n\n0 0 RA, x\nB gA  \n0 0 RA, y\n\nRA, x \nRA, y \n\n(29)\n\nin which , cr  R  are the natural frequency and the critical buckling value, respectively.\nIt is observed from Eq. (23) that the SCF is no longer required in the stiffness formulation. Herein, BbA1\nand BbA2 contain the second-order derivative of the shape functions. Hence, it requires C1-continuous\nelement in approximate formulations. It is now interesting to note that our present formulation based\non IGA naturally satisfies C1-continuity from the theoretical/mechanical viewpoint of plates [40, 28].\nIn our work, the basis functions are Cp-1 continuous. Therefore, as p  2 , the present approach always\nsatisfies C1-requirement in approximate formulations based on the proposed RPT.\n3.2. Essential boundary conditions\nHerein, many kinds of boundary condition are applied for an arbitrary edge with simply supported (S)\nand clamped (C) conditions including:\nSimply supported cross-ply:\nv0  wb  ws  0 at x  0, a\nu0  wb  ws  0 at y  0, b\n\n(30)\n\nSimply supported angle-ply:\nu0  wb  ws  0 at x  0, a\nv0  wb  ws  0 at y  0, b\n\n9\n\n(31)\n\n\fClamped (C):\n\nu0  v0  wb  ws  wb,n  ws ,n  0\n\n(32)\n\nThe Dirichlet boundary condition on u0 , v0 , wb and ws is easily treated as in the standard FEM.\nHowever, for the derivatives wb,n , ws ,n the enforcement of Dirichlet BCs can be solved in a simple and\neffective way [44]. The idea is as follows. The derivatives can be included in a compact form of the\nnormal slope at the boundary:\nw(n(xC )  n)  w(n(xC ))\nw\n lim\n0\n\nn\n\n0\nn\nn\n\n(33)\n\nAs w(n(xC ))  0 according to Eq. (32), Eq. (33) leads to impose the same boundary values, i.e, zero\nvalues, on the deﬂection variable at control points xA which is adjacent to the boundary control points\nxC. It can be observed that, implementing the essential boundary condition using this method is very\nsimple in IGA compare to other numerical methods.\n\n4. Results and discussions\nIn this section, we show the performance of the present method – RPT-IGA with various distributed\nfunctions given in Table 1 in analyzing the laminated composite plates. We illustrate the method using\nthe cubic basis functions with full ( p  1)  (q  1) Gauss points.\n4.1 Static analysis\nIn this sub-section, material set I is used with parameters given as:\nMaterial I:\n\nE1  25E2 , G12  G13  0.5E2 , G23  0.2E2 , 12  0.25,   1.\nFor convenience, the following normalized transverse displacement, in-plane stresses and shear\nstresses are expressed as:\n\nw\n\n102 wE2 h3\n h2\nh\n\n\n,\n, \n2\n4\nq0 a\nq0 a\nq0 a\n\nLet us consider the simply supported square plate shown in Figure 2a. The plates stacked by two layer\n[0/90] subjected to a sinusoidal pressure defined as q0 sin(\n\n10\n\nx\na\n\n)sin(\n\ny\na\n\n) at the top surface.\n\n\fWe first investigate the convergence of the normalization displacement and stresses with length to\nthickness ratio a/h =10. The plate is modeled with 77, 1111 and 1515 cubic elements as shown in\nFigure 2. The comparisons between present results using RPT and analytical solution using HSDT\ngiven by Khdeir and Reddy [47] are tabulated in Table 2. The relative error is given in the parentheses.\nIt can be seen that the obtained results agree well with the exact values. It is observed that model using\nthird-order polynomials gains the most accuracy displacement while that using fifth-order one\n(FiSDT) obtains the closest axial stress. IGA, moreover, gains high-convergence with coarse mesh\nusing 1111 cubic elements. Therefore, in the next problems, the meshing of 11x11 cubic NURBS\nelements shown in Figure 2c is used.\n\n(a)\n\n(b)\n\n(c)\n(d)\nFigure 2. Square plate: (a) The plate geometry; (b), (c), (d): meshing of 7x7, 11x11, 15x15 cubic\nelements, respectively.\n\n11\n\n\fTable 2: The convergence of the normalized displacement and stresses of the simply supported [0/90]\ncomposite plate (a/h =10) under sinusoidal load.\nPlate model\nReddy [8]\nShimpi [22]\nRPT-IGA Arya [19]\nKarama [18]\nFiSDT [43]\nAnalytical Solution [47]\nReddy [8]\nShimpi [22]\nRPT-IGA Arya [19]\nKarama [18]\nFiSDT [43]\nAnalytical Solution [47]\n(*)\nThe error in parentheses\n\nNor. Sol.\na\n2\n\nb\n2\n\nw( , )\n\n x ( a2 , b2 ,  h2 )\n\nMesh\n7x7\n\n11x11\n\n15x15\n\n1.2159 (0.01)\n1.2159 (0.01)\n1.2129 (0.25)\n1.2093 (0.55)\n1.2041 (0.98)\n\n1.2161 (-0.01)\n1.2161 (-0.01)\n1.2131 (0.24)\n1.2096 (0.53)\n1.2043 (0.96)\n\n1.2161 (-0.01)\n1.2161 (-0.01)\n1.2131 (0.24)\n1.2096 (0.53)\n1.2043 (0.96)\n1.216\n\n-0.7351 (1.57)\n-0.7351 (1.57)\n-0.7366 (1.37)\n-0.7379 (1.19)\n-0.7397 (0.95)\n\n-0.7421 (0.63)\n-0.7421 (0.63)\n-0.7436 (0.43)\n-0.7449 (0.25)\n-0.7467 (0.01)\n\n-0.7443 (0.33)\n-0.7443 (0.33)\n-0.7458 (0.13)\n-0.7471 (-0.04)\n-0.7489 (-0.28)\n-0.7468\n\nNext, the behavior of two-layer [0/90] square composite plate under two types of boundary condition\n(SSSS and SFSF) are considered; here, S = simply supported and F = free edge. The obtained results\nof present model are compared with those published one from 3D model of Vel and Batra [46]; HSDT,\nFSDT, CLPT using analytical solution by Khdeir and Reddy [47] and HOSNDPT using mesh free\nwith 18DOFs/node by Xiao et al [45]. The comparison is provided in Table 3. The normalized\ndisplacement and stresses of the present approach are in good agreement with the 3D exact solution\n[46]. Among present models, the third-order distributed functions by Reddy [8] and Shimpi [22]\ncombined in RPT-IGA archives the same results which are closest to 3D exact solution. The transverse\ndisplacement of plates is illustrated in Figure 3 according to SFSF and SSSS boundary conditions,\nrespectively. Figure 4 plots the stress distribution through the thickness of composite plate under full\nsimply supported condition with a/h = 10. Herein, the NURBS functions are used to model the RPT\nassumption with various f(z) functions such as: TSDT of Reddy [8], HSDT of Shimpi [22], ESDT of\n\n12\n\n\fKarama [18], SSDT of Arya [19] and FiSDT of Nguyen-Xuan [43]. Using RPT model, the in-plane\nstresses is plotted in the same path while there is a slight difference observed for shear stress\ndistribution. And, all of them satisfy the traction-free boundary conditions at the plate surfaces.\n\n(a)\n(b)\nFigure 3. Deflection profile of Al/ZrO2-1 FGM plates: (a) SFSF; (b) SSSS.\n\nTable 3: The non-dimensional deflection and stresses of two-layer [0/90] square composite plate under\nsinusoidal load.\nBC\nSSSS\n\nPlate model\n3D model [46]\nHSDT[47]\nExact\nFEM\nFSDT[47]\nCLPT [47]\nHOSNDPT MQ-MLPG\n[45]\nTPS-MLPG\nReddy [8]\nShimpi [22]\nRPT-IGA\nArya [19]\nKarama [18]\nFiSDT [43]\nSFSF 3D model [46]\nHSDT[47] Exact\nFEM\nFSDT[47]\nCLPT [47]\nHOSNDPT MQ-MLPG\n\nw( a2 , b2 )\n\n x ( a2 , b2 ,  h2 )\n\n y ( a2 , b2 , h2 )\n\n-0.7304\n-0.7468\n-0.6829\n-0.7157\n-0.7157\n-0.726\n-0.723\n-0.7421\n-0.7421\n-0.7436\n-0.7449\n-0.7467\n0.2503\n0.2624\n0.2212\n0.2469\n0.2403\n0.249\n\n0.7309\n0.7468\n0.6829\n0.7157\n0.7157\n0.727\n0.724\n0.7421\n0.7421\n0.7436\n0.7449\n0.7467\n1.210\n1.2295\n1.189\n1.1907\n1.1849\n1.21\n\n1.227\n1.216\n1.214\n1.237\n1.064\n1.22\n1.213\n1.2161\n1.2161\n1.2131\n1.2096\n1.2043\n2.026\n1.992\n2.002\n2.028\n1.777\n2.028\n\n13\n\n xy (0,0, h2 )\n\n0.0497\n0.0494\n0.0491\n0.053\n0.053\n0.053\n0.0531\n0.0531\n0.0119\n0.0118\n\n yz ( a2 , 0, 0)\n\n0.319\n0.2729\n0\n0.298\n0.278\n0.3181\n0.3181\n0.3252\n0.3319\n0.3369\n0.4489\n0.3882\n0\n0.488\n\n\f[45]\n\nRPT-IGA\n\nTPS-MLPG\nReddy [8]\nShimpi [22]\nArya [19]\nKarama [18]\nFiSDT [43]\n\n2.028\n1.990\n1.990\n1.9851\n1.9794\n1.9712\n\n0.249\n0.25472\n0.25472\n0.2555\n0.2562\n0.2572\n\n1.21\n1.2192\n1.2192\n1.221\n1.2225\n1.2247\n\n0.0119\n0.0121\n0.0121\n0.0121\n0.0122\n0.0122\n\n0.499\n0.4507\n0.4507\n0.46\n0.4686\n0.4744\n\nFigure 4. The stresses through thickness of laminate composite plate under full simply supported\ncondition with a/h=10 via different refined plate models.\nThe relation between non-dimensional deflection and length to thickness ratio is depicted in Table 4. It\nis observed that the present results using isogeometric finite element approach gain the identical\nsolutions compared to PRT [26] and TSDT [49] one using analytical solution, especially that of\nReddy’s and Shimpi’s models. Moreover, as plate becomes thinner (a/h increases) the difference\nbetween those solutions is not significant.\nTable 4: The non-dimensional deflection of simply supported two-layer (0/90) square plate under\nsinusoidal load.\n\n14\n\n\fPlate model\n3D model [1]\nTSDT [49]\nFSDT [48]\nRPT - analytical [26]\nReddy [8]\nShimpi [22]\nRPT-IGA Arya [19]\nKarama [18]\nFiSDT [43]\n\na/h\n5\n1.7287\n1.667\n1.667\n1.667\n1.6669\n1.6669\n1.6538\n1.6382\n1.6154\n\n10\n1.2318\n1.2161\n1.2416\n1.2161\n1.2161\n1.2161\n1.2131\n1.2096\n1.2043\n\n20\n1.106\n1.1018\n1.1113\n1.1018\n1.1018\n1.1018\n1.1011\n1.1002\n1.0989\n\n100\n1.0742\n1.0651\n1.0653\n1.0651\n1.0651\n1.0651\n1.065\n1.065\n1.065\n\n4.2 Free vibration analysis\nLet us consider the cross-ply [0/90]N composite plate with a/h=5 and simply supported boundary\nconditions. Herein, used material set II is defined as:\n\nE1 / E2  open, G12  G13  0.6E2 , G23  0.5E2 , 12  0.25,   1.\nThe effects of the number of layers N and elastic modulus ratios E1/E2 are tabulated in Table 5. A good\nagreement is found for present models in comparison with three-dimensional elastic solutions\nproposed by Noor [2] and the analytical solution given by Kant [50]. It is depicted that, among present\nmodel, FiSDT archives the highest results which is closest to 3D solution as E1/E2 10. As E1/E2 ratio\nranges from 20 to 40, the present results are asymptotic to analytical solutions for 2D plate model\nusing HSDT and RPT [50], especially, the model using third-order functions.\nTable 5: The natural frequency  =a 2 / h  / E2 of simply supported [0/90]N composite plate.\nE1/E2\nN\n1\n\n2\n\nModel\n3D elasticity [2]\nHSDT-12DOFs\nAnal.\nHSDT-9DOFs\nSol. [50]\nRPT\nFSDT\nReddy [8]\nShimpi [22]\nRPT-IGA Arya [19]\nKarama [18]\nFiSDT [43]\n3D elasticity [2]\nHSDT-12DOFs\nAnal.\nHSDT-9DOFs\nSol. [50]\nTSDT, RPT\n\n3\n6.2578\n6.2336\n6.1566\n6.2169\n6.149\n6.2169\n6.2169\n6.2189\n6.2224\n6.2296\n6.5455\n6.5146\n6.4319\n6.5008\n\n10\n6.9845\n6.9741\n6.9363\n6.9887\n6.9156\n6.9887\n6.9887\n6.9965\n7.0066\n7.0231\n8.1445\n8.1482\n8.101\n9.1954\n\n15\n\n20\n7.6745\n7.714\n7.6883\n7.821\n7.6922\n7.8211\n7.8211\n7.838\n7.8585\n7.8892\n9.4055\n9.4675\n9.4338\n9.6265\n\n30\n8.1763\n8.2775\n8.257\n8.505\n8.3112\n8.5051\n8.5051\n8.5317\n8.563\n8.6089\n10.165\n10.2733\n10.2463\n10.5348\n\n40\n8.5625\n8.7272\n8.7097\n9.0871\n8.8255\n9.0872\n9.0872\n9.1237\n9.1662\n9.2275\n10.6798\n10.8221\n10.7993\n11.1716\n\n\f3\n\n5\n\nFSDT\nReddy [8]\nShimpi [22]\nRPT-IGA Arya [19]\nKarama [18]\nFiSDT [43]\n3D elasticity [2]\nHSDT-12DOFs\nAnal.\nHSDT-9DOFs\nSol. [50]\nTSDT, RPT\nFSDT\nReddy [8]\nShimpi [22]\nRPT-IGA Arya [19]\nKarama [18]\nFiSDT [43]\n3D elasticity [2]\nHSDT-12DOFs\nAnal.\nHSDT-9DOFs\nSol. [50]\nTSDT, RPT\nFSDT\nReddy [8]\nShimpi [22]\nRPT-IGA Arya [19]\nKarama [18]\nFiSDT [43]\n\n6.4402\n6.5008\n6.5008\n6.5012\n6.5034\n6.5094\n6.61\n6.5711\n6.4873\n6.5552\n6.4916\n6.5558\n6.5558\n6.5567\n6.5596\n6.5663\n6.6458\n6.6019\n6.5177\n6.5842\n6.5185\n6.5842\n6.5842\n6.5854\n6.5885\n6.5957\n\n8.1963\n8.1954\n8.1954\n8.193\n8.1939\n8.2021\n8.4143\n8.3852\n8.3372\n8.4041\n8.3883\n8.4052\n8.4052\n8.4066\n8.4122\n8.4259\n8.5625\n8.5163\n8.468\n8.5126\n8.4842\n8.5126\n8.5126\n8.5156\n8.5229\n8.5394\n\n9.6729\n9.6265\n9.6265\n9.6205\n9.6201\n9.6316\n9.8398\n9.8346\n9.8012\n9.9175\n9.9266\n9.9181\n9.9181\n9.9211\n9.9313\n9.9555\n10.0843\n10.0438\n10.0107\n10.0674\n10.0483\n10.0674\n10.0674\n10.0741\n10.0882\n10.1185\n\n10.6095\n10.5348\n10.5348\n10.5268\n10.5261\n10.5418\n10.6958\n10.7113\n10.6853\n10.8542\n10.8723\n10.8547\n10.8547\n10.8604\n10.8758\n10.9106\n11.0027\n10.9699\n10.9445\n11.0197\n10.9959\n11.0197\n11.0197\n11.031\n11.0523\n11.0964\n\n11.2635\n11.1716\n11.1716\n11.1628\n11.1629\n11.1832\n11.2728\n11.3051\n11.2838\n11.5007\n11.5189\n11.5012\n11.5012\n11.5103\n11.5314\n11.5768\n11.6245\n11.5993\n11.5789\n11.673\n11.6374\n11.673\n11.673\n11.6894\n11.7182\n11.7757\n\nNext, with constant E1/E2 ratio (equal 40), the variation of natural frequency of two-layer laminate\ncomposite plate via length to thickness ratio are listed in Table 6. It is again seen that the obtained\nresults match well with analytical one using 12DOFs published by Kant [50]. The difference reduces\nvia the increase in the ratio of a/h (from approximate 8% to 0.02% according to a/h = 4 and 100,\nrespectively). The first three mode shapes of thick plate (a/h=10) is then plotted in Figure 5. It is clear\nthat beside the full mode shape of deflection (above), the mode shapes of all unknown parameters\nalong line y=a/2 are illustrated below.\nTable 6: The natural frequency  =a 2 / h  / E2 of simply supported [0/90] composite plate with\nE1/E2 = 40.\na/h\nPlate model\n\nAnal.\n\nHSDT-12DOFs\nHSDT-9DOFs\nTSDT\n\n4\n7.9081\n7.8904\n8.3546\n\n10\n10.4319\n10.4156\n10.568\n\n16\n\n20\n11.0663\n11.0509\n11.1052\n\n50\n11.2688\n11.2537\n11.2751\n\n100\n11.2988\n11.2837\n11.3002\n\n\fSol [50]\n\nRPT\nFSDT\nReddy [8]\nShimpi [22]\nArya [19]\nKarama [18]\nFiSDT [43]\n\nRPT-IGA\n\n1  10.5681\n\n8.3546\n8.0889\n8.3547\n8.3547\n8.4018\n8.4564\n8.5355\n\n10.568\n10.461\n10.5681\n10.5681\n10.5812\n10.5965\n10.6186\n\n11.1052\n11.0639\n11.1053\n11.1053\n11.109\n11.1133\n11.1196\n\n2  26.5015\n\n11.2751\n11.2558\n11.2752\n11.2752\n11.2758\n11.2758\n11.2776\n\n11.3002\n11.2842\n11.3003\n11.3003\n11.3004\n11.3006\n11.3009\n\n3  26.5015\n\n: ws\n: u0\n: v0\n: wb\nFigure 5. Vibration mode shapes: full plate (upper) and line y = a/2 (lower) of simply supported\n[0/90] composite plate with E1/E2 = 40, a/h=10.\nTo close this sub-section, the effect of boundary condition on normalized frequency of ten-layer crossply composite plate is investigated in Table 7. Compared with those reported by Reddy and Khdeir\n[51], the present models gain the good agreement. It can be seen that, present models using RPT gain\nthe closest results to analytical solution using TSDT with slightly higher results. In addition, when the\nconstrained edge changes from F to S and C, the structural stiffness increases, the magnitudes of free\nvibration thus increase, respectively. The mode shapes according to various boundary conditions are\nillustrated in Figure 6.\n\nTable 7: The natural frequency of ten-layer [0/90]5 composite plate with a/h=5 and E1/E2 = 40\nPlate model\n\nBoundary conditions\n\n17\n\n\fAnal.\nSol. [51]\n\nRPT-IGA\n\nTSDT\nFSDT\nCLPT\nReddy [8]\nShimpi [22]\nArya [19]\nKarama [18]\nFiSDT [43]\n\n(a) SFSF\n\nSFSF\nSFSC\nSSSS\nSSSC\nSCSC\nCCCC\n8.155\n8.966\n11.673\n12.514\n13.568\n8.139\n8.919\n11.644\n12.197\n12.923\n11.459\n13.618\n12.167\n23.348\n30.855\n8.1554\n9.0832\n11.673 13.0041 14.1566 15.2991\n8.1554\n9.0832\n11.673 13.0041 14.1566 15.2991\n8.1661\n9.0971 11.6894 13.0463 14.2418 15.4558\n8.1853\n9.1201 11.7182 13.1062 14.3513 15.6438\n8.2238\n9.1646 11.7757 13.2122 14.5318 15.9367\n\nb) SFSC\n\nc) SSSS\n\nd) SSSC\ne) SCSC\nf) CCCC\nFigure 6. Mode shape profile of ten layers [0/90]5 composite plate under various boundary conditions\n\n4.3 Buckling analysis\nA simply supported two-layer angle-ply  /   square plate is subjected to uniaxial compressive load\nalong x-direction shown in Figure 7a. Material set II is used. The results are compared with that of Ren\n[20] and the analytical solution [26] using FSDT, HSDT and RPT assumptions. For all values of a/h\nratio and fiber orientation, present model with third-order functions give the closest buckling load to\nthat of RPT predicted by Thai et al [26]. It can be again seen that all models give the same results for\nthin plates (a/h=100). Figure 8 illustrates the buckling mode of two-layer angle-ply composite plate in\ncase of  = 45. It can be seen that as plate thickness reduces, the non-dimension buckling value\n\ncr  cr a 2 / E2 h3 increases according to changing of mode shape from two halves sine wave (a/h=4)\n\n18\n\n\fto a half sine wave (a/h =10; 100, respectively). Furthermore, the portion of shear deflection\ncomponents ws in transverse displacement reduces and tends to zero as a/h=100. The present models,\nhence, reduce to CLPT model.\n\nFigure 7. Geometry of laminated composite plates under axial (a) and biaxial (b) compression.\nTable 8: The normalized uniaxial buckling load cr of simply supported two layer  /   composite\nplate with E1/E2 = 40.\na/h \n4 30\n45\n10 30\n45\n100 30\n45\n\nAnalytical solution [26]\nRPT-IGA\nRen\n[20]\nHSDT\nFSDT\nRPT\nReddy\nShimpi\nArya\nKarama FiSDT\n9.5368\n9.3391\n7.545\n9.3518\n9.3522\n9.3522\n9.6731\n9.9211 10.2046\n9.82\n8.2377\n6.7858\n8.3963\n8.3966\n8.3966\n8.6472\n8.9414\n9.3869\n15.7517\n17.1269 16.6132 17.2795 17.2797 17.2797 17.3495 17.4311 17.5489\n16.4558\n18.1544 17.5522 18.1544 18.1545 18.1545 18.2383 18.3354 18.4737\n20.4793\n20.5017 20.4944\n20.504 20.5042 20.5042 20.5052 20.5063 20.5078\n21.6384\n21.6663 21.6576 21.6663 21.6664 21.6664 21.6676 21.6689 21.6707\n\n19\n\n\f(a) cr  8.3966\n\n(b)\n\n(c) cr  21.6663\n\ncr  18.1545\n\n: v0\n: wb\n: ws\n: u0\nFigure 8. Buckling mode shapes: full plate (upper) and line y = a/2 (lower) of simply supported [45/45] composite plate with E1/E2 = 40 with various length to thickness ratios: (a) a/h=4; (b) a/h=10; (c)\na/h=100.\nTable 9: The normalized biaxial buckling load cr of simply supported three-layer [0/90/0] composite\nplate with a/h =10 and various E1/E2 ratios.\nE1/E2\nPlate model\nFSDT-FEM [52]\nHSDT-FEM [54]\nReddy\nShimpi\nRPT-IGA Arya\nKarama\nFiSDT\n\n10\n4.963\n4.963\n5.1067\n5.1067\n5.1077\n5.1105\n5.1171\n\n20\n7.588\n7.516\n7.8382\n7.8382\n7.8288\n7.8228\n7.8238\n\n40\n10.202\n10.259\n10.8825\n10.8825\n10.8549\n10.8336\n10.8247\n\nTable 10: The normalized biaxial buckling load cr of simply supported three-layer [0/90/0] composite\nplate under various a/h =10 ratios.\na/h\nPlate model\nHSDT- RPIM [53]\nFSDT- RPIM [53]\nHSDT- FEM [54]\nReddy\n\n5\n5.519\n5.484\n5.526\n6.1752\n\n10\n10.251\n10.189\n10.259\n10.8825\n\n15\n12.239\n12.213\n12.226\n12.714\n\n20\n\n20\n13.164\n13.132\n13.185\n13.5135\n\n\fRPT-IGA\n\nShimpi\nArya\nKarama\nFiSDT\n\n6.1752\n6.1571\n6.150\n6.166\n\n10.8825\n10.8549\n10.8336\n10.8247\n\n12.714\n12.6957\n12.6808\n12.6728\n\n13.5135\n13.5014\n13.4916\n13.4858\n\nFinally, we consider a three-layer symmetric cross-ply [0/90/0] simply supported plate subjected to the\nbiaxial buckling load as shown in Figure 7b. Various length-to-thickness a/h and elastic modulus E1/E2\nratios are studied in this example. Table 9 and Table 10 show the critical buckling parameter cr\nrespect to various modulus and length-to-thickness ratios. The obtained results are compared with\nthose of the ﬁnite element formulation based on FSDT [52], the ﬁnite element method based on HSDT\n[54], the mesh free method based on both FSDT and HSDT [53]. The present method shows a good\nperformance compared to other methods for various modulus ratios and length to thickness ratios. The\nnormalized critical biaxial buckling loads are increased with respect to increasing the modulus ratio\nE1/E2. Figure 9 reveals the buckling mode shapes of there-layer [0/90/0] composite plate with full plate\nmodel and along line x = a/2.\n\n: u0\n: v0\n: wb\n: ws\nFigure 9. Buckling mode shapes: full plate and line x = a/2 of simply supported [0/90/0] composite\nplate with E1/E2 = 40, a/h=10 (note that u0  v0).\n\n5. Conclusions\nIn this paper, the present results by RPT-IGA are compared with the analytical solutions and that of\nHSDT or 3D elastic models and the excellent agreement in static, free vibration and buckling analysis\nof laminated composite plates are observed. Using many kinds of higher-order distributed functions,\nRPT model satisfied naturally the traction-free conditions at the top and bottom plate surfaces. With\none variable lower than that of TSDT by Reddy, the present model also ensured the non-linear\n\n21\n\n\fdistribution of the shear stresses/strains through the plate thickness without using SCF. Furthermore,\nthe shear strains/stresses are obtained independently on the bending component. RPT has been thus\nstrongly similar to CLPT. As a consequence, the present results are asymptotic to CLPT ones as plates\nbecome thin.\n\nReferences\n1.\n\nPagano NJ. Exact solutions for rectangular bidirectional composites and sandwich plates.\nJournal of Composite Materials 1970;4(1):20–34.\n\n2.\n\nNoor AK. Free vibration of multilayered composite plates. AIAA J. 1973;11(7): 1038–1039.\n\n3.\n\nNoor AK. Stability of multilayered composite plates. Fiber. Sci. Tech. 1975; 8(2):81-89.\n\n4.\n\nReddy JN. Mechanics of laminated composite plates-theory and analysis. NewYork: CRC\nPress; 1997.\n\n5.\n\nReissner E. The effect of transverse shear deformation on the bending of elastic plates. J Appl\nMech Trans ASME 1945; 12(2):69–77.\n\n6.\n\nMindlin RD. Inﬂuence of rotary inertia and shear on ﬂexural motions of isotropic, elastic plates.\nJ Appl Mech Trans ASME 1951; 18(1):31–38.\n\n7.\n\nFerreira AJM, Castro LMS and Bertoluzza S. A high order collocation method for the static and\nvibration analysis of composite plates using a ﬁrst-order theory. Compos Struct 2003;\n34(7):627-636.\n\n8.\n\nReddy JN. Analysis of functionally graded plates. Int J Numer Methods Eng 2000;47:663–684.\n\n9.\n\nAmbartsumian SA. On the theory of bending plates. Izv Otd Tech Nauk ANSSSR 1958;5:269–\n277.\n\n10.\n\nReissner E. On transverse bending of plates including the effects of transverse shear\ndeformation. IntJ Solids and Structures 1975;25:495–502.\n\n11.\n\nLevinson M. An accurate simple theory of statics and dynamics of elastic plates. Mechanics\nResearch Communications 1980;7:343-350.\n\n12.\n\nReddy JN. A simple higher-order theory for laminated composite plates. Journal of Applied\nMechanics 1984; 51:745–752.\n\n13.\n\nMantari JL, Oktem AS and Guedes Soares C. Bending response of functionally graded plates by\nusing a new higher order shear deformation theory. Composite Structures 2012;94:714–723.\n\n14.\n\nZenkour AM. The reﬁned sinusoidal theory for FGM plates on elastic foundations. Int J Mech\nSci 2009; 51(11–12): 869–880.\n\n15.\n\nAit Atmane H, Tounsi A, Mechab I and Adda Bedia EA. Free vibration analysis of functionally\ngraded plates resting on Winkler-Pasternak elastic foundations using a new shear deformation\ntheory. Int J Mech Mater Des 2010;6(2):113–121.\n\n16.\n\nSoldatos KP. A transverse shear deformation theory for homogenous monoclinic plates. Acta\nMechanica 1992;94:195–220.\n\n22\n\n\f17.\n\nTouratier M. An efﬁcient standard plate theory. Int J Eng Sci 1991;29(8):901–916.\n\n18.\n\nKarama M, Afaq KS, and Mistou S. Mechanical behavior of laminated composite beam by new\nmulti-layered laminated composite structures model with transverse shear stress continuity. Int J\nSolids and Structures 2003;40:1525-1546.\n\n19.\n\nArya H, Shimpi RP, and Naik NK. A zigzag model for laminated composite beams. Composite\nStructures 2002; 56(1):21-24.\n\n20.\n\nAydogdu M. A new shear deformation theory for laminated composite plates. Composite\nStructures 2009; 89(1):94-101.\n\n21.\n\nSenthilnathan NR, Lim SP, Lee KH, and Chow ST. Buckling of Shear-Deformable Plates,\nAIAA Journal 1987;25(9):1268-1271.\n\n22.\n\nShimpi RP. Refined plate theory and its variants. AIAA Journal 2002; 40(1):137-146.\n\n23.\n\nShimpi RP and Patel HG. A two variable refined plate theory for orthotropic plate analysis, Int J\nSolids and Structures 2006; 43(22–23); 6783-6799.\n\n24.\n\nShimpi RP and Patel HG. Free vibrations of plate using two variable refined plate theory, J\nSound Vib. 2006; 296(4–5): 979-999.\n\n25.\n\nThai HT and Choi DH. A refined plate theory for functionally graded plates resting on elastic\nfoundation. Composites Science and Technology 2011;71(16):1850-1858.\n\n26.\n\nKim SE, Thai HT and Lee J. A two variable refined plate theory for laminated composite plates.\nComposite Structures 2009;89(2):197-205.\n\n27.\n\nThai HC, Nguyen-Xuan H, Bordas SPA, Nguyen-Thanh N and Rabczuk T. Isogeometric\nanalysis of laminated composite plates using the higher-order shear deformation theory.\nMechanics of Advanced Materials and Structures 2012, (in press).\n\n28.\n\nTran V Loc, Ferreira AJM and Nguyen-Xuan H. Isogeometric analysis of functionally graded\nplates using higher-order shear deformation theory, Composites Part B: Engineering 2013;51:\n368-383.\n\n29.\n\nThai H Chien, Tran V Loc, Tran TD, Nguyen-Thoi T and Nguyen-Xuan H. Analysis of\nlaminated composite plates using higher-order shear deformation plate theory and node-based\nsmoothed discrete shear gap method. Applied Mathematical Modelling 2012; 36:5657-5677.\n\n30.\n\nTran V Loc, Nguyen-Thoi T, Thai H Chien, Nguyen-Xuan H. An edge-based smoothed discrete\nshear gap method (ES-DSG) using the C0-type higher-order shear deformation theory for\nanalysis of laminated composite plates. Mechanics of Advanced Materials and Structures 2012,\n(in press).\n\n31.\n\nSankara CA and Igengar NGR. A C0 element for free vibration analysis of laminated composite\nplates. J. Sound and Vib.1996;191:721–738.\n\n32.\n\nKant T and Swaminathan K. Analytical solutions for the static analysis of laminated composite\nand sandwich plates based on a higher order refined theory. Composite Structures 1996;56(4);\n329-344.\n\n33.\n\nHughes TJR, Cottrell JA, and Bazilevs Y. Isogeometric analysis: CAD, finite elements,\nNURBS, exact geometry and mesh refinement. Comput Methods Appl Mech Eng 2005;\n194(39-41):4135–4195.\n\n23\n\n\f34.\n\nCottrell JA, Hughes TJR, Bazilevs Y. Isogeometric Analysis, Towards Integration of CAD and\nFEA. Wiley, 2009.\n\n35.\n\nElguedj T, Bazilevs Y, Calo V, and Hughes T. B and F projection methods for nearly\nincompressible linear and non-linear elasticity and plasticity using higher-order NURBS\nelements. Comput Methods Appl Mech Eng 2008;197:2732-2762.\n\n36.\n\nCottrell JA, Reali A, Bazilevs Y and Hughes TJR. Isogeometric analysis of structural vibrations.\nComput Methods Appl Mech Eng 2006;195(41-43):5257–5296.\n\n37.\n\nBenson DJ, Bazilevs Y, Hsu MC and Hughes TJR. Isogeometric shell analysis: The Reissner–\nMindlin shell. Comput Methods Appl Mech Eng 2006;199(5-8): 276–289.\n\n38.\n\nKiendl J and Bletzinger KU, Linhard J and Wchner R. Isogeometric shell analysis with\nKirchhoff-Love elements. Comput Methods Appl Mech Eng 2006;198(49-52):3902–3914.\n\n39.\n\nWall WA, Frenzel MA and Cyron C. Isogeometric structural shape optimization. Comput\nMethods Appl Mech Eng 2008;197(33-40): 2976–2988.\n\n40.\n\nThai HC, Nguyen-Xuan H, Nguyen-Thanh N, Le T-H, Nguyen-Thoi T and Rabczuk T. Static,\nfree vibration, and buckling analysis of laminated composite Reissner–Mindlin plates using\nNURBS-based isogeometric approach. Int J Numer Meth Engng 2012; 91(6):571–603.\n\n41.\n\nTran V Loc, Thai H Chien, Nguyen-Xuan H, An isogeometric finite element formulation for\nthermal buckling analysis of functionally graded plates, Finite Element in Analysis and Design\n2013;73: 65-76.\n\n42.\n\nNguyen-Thanh N, Kiendl J, Nguyen-Xuan H, Wüchner R, Bletzinger KU, Bazilevs Y and\nRabczuk T. Rotation free isogeometric thin shell analysis using PHT-splines. Comput Methods\nAppl Mech Eng 2011;200(47-48): 3410-3424.\n\n43.\n\nNguyen-Xuan H, Thai HC and Nguyen-Thoi T. Isogeometric ﬁnite element analysis of\ncomposite sandwich plates using a new higher order shear deformation theory. Composite Part\nB 2013; 55: 558–574.\n\n44.\n\nAuricchio F, Beiraoda Veiga F, Buffa A, Lovadina C, Reali A and Sangalli G. A fully lockingfree isogeometric approach for plane linear elasticity problems: a stream function formulation.\nComput Methods Appl Mech Eng 2007;197:160–172.\n\n45.\n\nXiao JR, Gilhooley DF, Batra RC, Gillespie JW and McCarthy MA. Analysis of thick composite\nlaminates using a higher-order shear and normal deformable plate theory (HOSNDPT) and a\nmeshless method. Composites Part B: Engineering 2008;39(2):414-427.\n\n46.\n\nVel SS and Batra RC. Analytical solutions for rectangular thick laminated plates subjected to\narbitrary boundary conditions. AIAA J 1999;37:1464–73.\n\n47.\n\nKhdeir AA and Reddy JN. Analytical solutions of reﬁned plate theories of cross-ply composite\nlaminates. J Pressures Vessel Tech 1991; 113(4):570–8.\n\n48.\n\nWhitney JM and Pagano NJ. Shear deformation inheterogeneous anisotropic plates. J Appl\nMech, Trans ASME1970;37(4):1031–6.\n\n49.\n\nReddy JN. A simple higher-order theory for laminated composite plates. J Appl Mech,Trans\nASME 1984;51:745–52.\n\n24\n\n\f50.\n\nKant T and Swaminathan K. Analytical solutions for free vibration of laminated composite and\nsandwich plates based on a higher-order refined theory. Composite Structures 2001; 53(1):7385.\n\n51.\n\nReddy JN and Khdeir A. Buckling and vibration of laminated composite plates using various\nplate theories. AIAA Journal 1989;27(12):1808-1817.\n\n52.\n\nFares ME and Zenkour AZ. Buckling and free vibration of non-homogeneous composite crossply laminated plates with various plate theories. Composite Structures 1999;44:279–287.\n\n53.\n\nLiu L, Chua LP, and Ghista DN. Mesh-free radial basis function method for static, free vibration\nand buckling analysis of shear deformable composite laminates. Composite Structures 2007;\n78:58–69.\n\n54.\n\nKhdeir AA and Librescu L. Analysis of symmetric cross-ply elastic plates using a higher-order\ntheory: PartII: buckling and free vibration. Composite Structures 1988; 9:259–277.\n\ncomputers and structures (SCI, IF = 1.5)\n\n25\n\n\f"
        ],
        [
         "23",
         "23",
         "cs.CE",
         "Computational Engineering",
         "1403.2000v1.pdf",
         "arXiv:1403.2000v1 [cs.CE] 8 Mar 2014\n\nA Galois-Connection between Myers-Briggs’\nType Indicators and Szondi’s Personality Profiles\nSimon Kramer\nsimon.kramer@a3.epfl.ch\nMay 8, 2014\nAbstract\nWe propose a computable Galois-connection between Myers-Briggs’\nType Indicators (MBTIs), the most widely-used personality measure for\nnon-psychiatric populations (based on C.G. Jung’s personality types), and\nSzondi’s personality profiles (SPPs), a less well-known but, as we show,\nfiner personality measure for psychiatric as well as non-psychiatric populations (conceived as a unification of the depth psychology of S. Freud,\nC.G. Jung, and A. Adler). The practical significance of our result is that\nour Galois-connection provides a pair of computable, interpreting translations between the two personality spaces of MBTIs and SPPs: one concrete from MBTI-space to SPP-space (because SPPs are finer) and one\nabstract from SPP-space to MBTI-space (because MBTIs are coarser).\nThus Myers-Briggs’ and Szondi’s personality-test results are mutually interpretable and inter-translatable, even automatically by computers.\nKeywords: applied order theory, computational and mathematical psychology, depth psychology, machine translation, MBTI, personality tests.\n\n1\n\nIntroduction\n\nAccording to [8, Page xxi and 210], the Myers-Briggs Type Indicator (MBTI) [7],\nbased on C.G. Jung’s personality types [3], has become “the most widely-used\npersonality measure for non-psychiatric populations” and “the most extensively\nused personality instrument in history” with over two million tests taken per\nyear. In this paper, we propose a computable Galois-connection [2] between\nMBTIs and Szondi’s personality profiles (SPPs) [9], a less well-known but, as\nwe show, finer personality measure for psychiatric as well as non-psychiatric\npopulations, and conceived as a unification [10] of the depth psychology of S.\nFreud, C.G. Jung, and A. Adler.\nOur result is a contribution to mathematical psychology in the area of depth\npsychology, which does not yet seem to have been explored with mathematical\nmeans besides those of statistics (often not part of mathematics departments). It\nis also meant as a contribution towards practicing psychological research with\n1\n\n\fthe methods of the exact sciences, for obvious ethical reasons. The practical\nsignificance of our result is that our Galois-connection provides a pair of efficiently computable, interpreting translations between the two personality spaces\nof MBTIs and SPPs (and thus hopefully also between their respective academic\nand non-academic communities): one concrete translation from MBTI-space to\nSPP-space (because SPPs are finer than MBTIs) and one abstract translation\nfrom SPP-space to MBTI-space (because MBTIs are coarser than SPPs). Thus\nMyers-Briggs’ and Szondi’s personality-test results are mutually interpretable\nand inter-translatable, even automatically by computers. The only restriction to\nthis mutuality is the subjective interpretation of the faithfulness of these translations. In our interpretation, we intentionally restrict the translation from\nSPP-space to MBTI-space, and only that one, in order to preserve (our perception of) its faithfulness. More precisely, we choose to map some SPPs to the\nempty set in MBTI-space (but every MBTI to a non-empty set in SPP-space).\nOur readers can experiment with their own interpretations, as we explain below.\nWe stress that our Galois-connection between the spaces of MBTIs and SPPs\nis independent of their respective test, which evaluate their testees in terms of\nstructured result values—the MBTIs and SPPs—in the respective space. Both\ntests are preference-based, more precisely, test evaluation is based on choices of\npreferred questions in the case of the MBTI-test [7] and on choices of preferred\nportraits in the case of the Szondi-test [9, 6]. Due to the independence of our\nGalois-connection from these tests, their exact nature need not concern us here.\nAll what we need to be concerned about is the nature of the structured result\nvalues that these tests generate. (Other test forms can generate the same form\nof result values, e.g. [5].) We also stress that our proposed Galois-connection is\nwhat we believe to be an interesting candidate brain child for adoption by the\ncommunity, but that there are other possible candidates, which our readers are\nempowered to explore themselves. In fact, not only do we propose a candidate\nGalois-connection between MBTI-space and SPP-space, but also do we propose\na whole methodology for generating such candidates. All what readers interested\nin generating such connections themselves need to do is map their own intuition\nabout the meaning of MBTIs to a standard interlingua, called Logical Pivot\nLanguage (LPL) here, and check that their mapping has a single simple property,\nnamely the one stated as Fact 2.1 about our mapping i in Figure 1. Their desired\nGalois-connection is then automatically induced jointly by their chosen mapping\nand a mapping, called p here, from SPP-space to LPL that we choose once and\nfor all possible Galois-connections of interest. What is more, our methodology is\napplicable even more generally to the generation of Galois-connections between\npairs of result spaces of other personality tests. SPPs just happen to have a\nfiner structure than other personality-test values that we are aware of, and so\nare perhaps best suited to play the distinguished role of explanatory semantics\nfor result values of other personality tests. Of course our readers are still free\nto choose their own preferred semantic space.\nAn SPP can be conceived as a tuple of eight, so-called signed factors whose\nsignatures can in turn take twelve values. So SPPs live in an eight-dimensional\nspace. On the other hand, an MBTI can be conceived as a quadruple of two2\n\n\fFigure 1: Mappings between personality spaces and interlingua\n.\n\nMBT I\n\nSPP\n\n/\n\np\n\ni\n\nLPL\n\nvalued components, namely, first, extro-/introversion, second, perception, being\neither sensing or intuition, third, judgment, being either thinking or feeling, and\nfourth, a dominance flag, indicating either a dominance of perception or judgment. So MBTIs live in a coarser, four dimensional space. Hence the translation\nfrom SPPs to MBTIs must be a projection (and thus surjection) of SPP-space\nonto MBTI-space. An insight gained in the finer referential system of SPPs is\nthat MBTIs turn actually out to be non-orthogonal or not independent, contrary\nto common belief [7, 8]. Of course our readers are still free to disagree on the\nvalue of this insight by giving a convincing argument for why SPP-space would\nbe an inappropriate semantics for MBTI-space. After all, Szondi conceived his\ntheory of human personality as a unifying theory that also includes Jung’s theory, on which MBTI-theory is based. We now put forward our own argument\nfor why we believe SPP-space is indeed an appropriate—though surely not the\nonly—semantics for MBTI-space. In Section 2.1, we present the defining mathematical structures for each space, and in Section 2.2, the defining mathematical\nmappings for their translation. No prior knowledge of either MBTIs or SPPs is\nrequired to appreciate the results of this paper.\n\n2\n\nThe connection\n\nIn this section, we present the defining mathematical structures for MBTI-space,\nthe interlingua LPL, and SPP-space, as well as the defining mathematical mappings for the concrete translation of MBTI-space to SPP-space and the abstract\ntranslation of SPP-space back to MBTI-space, both via LPL, see Figure 1.\n\n2.1\n\nStructures\n\nIn this section, we present the defining mathematical structures for MBTI-space,\nthe interlingua LPL, and SPP-space. We start with defining MBTI-space.\nDefinition 1 (The Myers-Briggs Type Indicator Space). Let\nMB = {E, I, F, T, N, S, J, P}\nbe the set of basic type indicators, with E meaning “extroversion,” I “introversion,” F “feeling,” T “thinking,” N “intuition,” S “sensing,” J “judging,” and P\n3\n\n\f“perceiving.” Further let\nMBTI = { ISTJ, ISFJ, INFJ, INTJ, ISTP, ISFP, INFP, INTP,\nESTP, ESFP, ENFP, ENTP, ESTJ, ESFJ, ENFJ, ENTJ }\nbe the set of Myers-Briggs Type Indicators (MBTIs) [7, 8].\nThen,\nMBT I = h 2MBTI , ∅, ∩, ∪, MBTI, · , ⊆ i\ndefines our Myers-Briggs Type Indicator Space, that is, the (inclusion-ordered,\nBoolean) powerset algebra [2] on MBTI (the set of all subsets of MBTI).\nNote that we do need to define MBT I as the set of all subsets of MBTI and\nnot simply as the set of all elements of MBTI. The reason is the aforementioned\nfact that in the finer referential system of SPP-space (see Definition 2), MBTIs\nturn out to be non-orthogonal or not independent, and thus an MBTI may have\nto be mapped to a proper set of SPPs (see Table 2). So the proper setting for\nSPP-space is a set of subsets of SPPs, which in turn, via the backward translation from SPP-space to MBT I, means that the proper setting for MBT I,\nas the target of a mapping of subsets, is also a set of subsets. Further, notice\nthat the MBTI-test [7], which as previously mentioned requires answering to\nquestions, actually requires the P-faculty (perception of the question), the Nfaculty (intuition in the sense of textual, and thus symbolic understanding), and\nthe J-faculty (judgment in the sense of choice of and decision about an answer)\nas its own prerequisites. Incidentally, the concept of choice is the key concept\nin Szondi’s depth-psychological fate analysis [11, 10], which is the background\ntheory for his test [9] and the SPPs that it generates.\nWe continue to define SPP-space.\nDefinition 2 (The Szondi Personality Profile Space). Let us consider the Hassediagram [2] in Figure 2 of the partially ordered set of Szondi’s twelve signatures\n[9] of human reactions, which are:\n• approval: from strong +!!! , +!! , and +! to weak + ;\n• indifference/neutrality: 0 ;\n• rejection: from weak − , −! , and −!! to strong −!!! ; and\n• ambivalence: ±! (approval bias), ± (no bias), and ±! (rejection bias).\n(Szondi calls the exclamation marks in his signatures quanta.)\nFurther let us call this set of signatures S, that is,\nS = { −!!!, −!!, −!, −, 0, +, +!, +!!, +!!!, ±! , ±, ±! }.\nNow let us consider Szondi’s eight factors and four vectors of human personality [9] as summarised in Table 1. (Their names are of clinical origin and\nneed not concern us here.) And let us call the set of factors F, that is,\nF = { h, s, e, hy, k, p, d, m }.\n4\n\n\fFigure 2: Hasse-diagram of Szondi’s signatures\n+!!!\n+!!\n+!\n+\n\n±!\n\n0\n\n±\n\n−\n\n±!\n\n−!\n−!!\n−!!!\n\nThen,\nSPP = { ((h, s1 ), (s, s2 ), (e, s3 ), (hy, s4 ), (k, s5 ), (p, s6 ), (d, s7 ), (m, s8 )) |\ns1 , . . . , s8 ∈ S }\nis the set of Szondi’s personality profiles, and\nSPP = h 2SPP , ∅, ∩, ∪, SPP, · , ⊆ i\ndefines our Szondi Personality Profile Space, that is, the (inclusion-ordered,\nBoolean) powerset algebra [2] on SPP (the set of all subsets of SPP).\nAs an example of an SPP, consider the norm profile for the Szondi-test [9]:\n((h, +), (s, +), (e, −), (hy, −), (k, −), (p, −), (d, +), (m, +))\nSpelled out, this norm profile describes the personality of a human being who\napproves of physical love, has a proactive attitude, has unethical but moral\nbehaviour, wants to have and be less, and is unfaithful and dependent.\nWe conclude this subsection with the definition of our interlingua LPL.\nDefinition 3 (The Logical Pivot Language). Let\nA = { hs1 , ss2 , es3 , hys4 , ks5 , ps6 , ds7 , ms8 | s1 , . . . , s8 ∈ S }\n5\n\n\fTable 1: Szondi’s factors and vectors\nVector\nS (Id)\nP\n(SuperEgo)\nSch (Ego)\nC (Id)\n\nSignature\n\nFactor\nh (love)\ns (attitude)\ne (ethics)\nhy (morality)\nk (having)\np (being)\nd (relations)\nm (bindings)\n\n+\n\n−\n\nphysical love\n(proactive) activity\nethical behaviour\nimmoral behaviour\nhaving more\nbeing more\nunfaithfulness\ndependence\n\nplatonic love\n(receptive) passivity\nunethical behaviour\nmoral behaviour\nhaving less\nbeing less\nfaithfulness\nindependence\n\nbe our set of atomic logical formulas, and LPL(A) the classical propositional\nlanguage over A, that is, the set of sentences constructed from the elements in\nA and the classical propositional connectives ¬ (negation, pronounced “not”),\n∧ (conjunction, pronounced “and”), ∨ (disjunction, pronounced “or”), etc.\nThen,\nLPL = h LPL(A), ⇒ i\ndefines our logical pivot language, with ⇒ being logical consequence.\nLogical equivalence ≡ is defined in terms of ⇒ such that for every φ, ϕ ∈\nLPL(A), φ ≡ ϕ by definition if and only if φ ⇒ ϕ and ϕ ⇒ φ.\n\n2.2\n\nMappings between structures\n\nIn this section, we present the defining mathematical mappings for the concrete\ntranslation . of MBT I to SPP via LPL and the abstract translation / of SPP\nback to MBT I again via LPL by means of the auxiliary mappings i and p. We\nalso prove that the ordered pair ( . , / ) is a Galois-connection, as promised.\nDefinition 4 (Mappings). Let the mapping (total function)\n• i be defined in the function space (MBTI → LPL(A)) as in Table 2 and\nin the function space (2MBTI → LPL(A)) such that for every I ∈ 2MBTI ,\n^\ni(I) = { i(i) | i ∈ I } ;\n• p be defined in the function space (SPP → LPL(A)) such that\np(((h, s1 ), (s, s2 ), (e, s3 ), (hy, s4 ), (k, s5 ), (p, s6 ), (d, s7 ), (m, s8 ))) =\nhs1 ∧ ss2 ∧ es3 ∧ hys4 ∧ ks5 ∧ ps6 ∧ ds7 ∧ ms8\nand in the function space (2SPP → LPL(A)) such that for every P ∈ 2SPP ,\n_\np(P ) = { p(p) | p ∈ P } .\n\n6\n\n\fTable 2: Translating MB and MBTI to LPL(A)\n\ni(E) = hy+ ∨ hy+! ∨ hy+!! ∨ hy+!!! ∨ hy±!\ni(I) = hy− ∨ hy−! ∨ hy−!! ∨ hy−!!! ∨ hy±!\ni(F) = (h+ ∨ h± ∨ h±! ) ∧ (p− ∨ p± ∨ p±! )\ni! (F) = (h+! ∨ h+!! ∨ h+!!! ∨ h±! ) ∧\n(p−! ∨ p−!! ∨ p−!!! ∨ p±! )\ni(T) = k− ∨ k± ∨ k±!\ni! (T) = k−! ∨ k−!! ∨ k−!!! ∨ k±!\ni(N) = (k+ ∨ k± ∨ k±! ) ∧ (p+ ∨ p± ∨ p±! )\ni! (N) = (k+! ∨ k+!! ∨ k+!!! ∨ k±! ) ∧\n(p+! ∨ p+!! ∨ p+!!! ∨ p±! )\ni(S) = (k+ ∨ k± ∨ k±! ) ∧\n((h+ ∨ e− ∨ hy− ∨ d+ ∨ m+) ∨\n(h± ∨ e± ∨ hy± ∨ d± ∨ m±) ∨\n(h±! ∨ e±! ∨ hy±! ∨ d±! ∨ m±! ))\ni! (S) = (k+! ∨ k+!! ∨ k+!!! ∨ k±! ) ∧\n((h+! ∨ e−! ∨ hy−! ∨ d+! ∨ m+!) ∨\n(h+!! ∨ e−!! ∨ hy−!! ∨ d+!! ∨ m+!!) ∨\n(h+!!! ∨ e−!!! ∨ hy−!!! ∨ d+!!! ∨ m+!!!) ∨\n(h±! ∨ e±! ∨ hy±! ∨ d±! ∨ m±! ))\ni(ISTJ) = i(I) ∧ i! (S) ∧ i(T)\ni(ISFJ) = i(I) ∧ i! (S) ∧ i(F)\ni(INFJ) = i(I) ∧ i! (N) ∧ i(F)\ni(INTJ) = i(I) ∧ i! (N) ∧ i(T)\ni(ISTP) = i(I) ∧ i(S) ∧ i! (T)\ni(ISFP) = i(I) ∧ i(S) ∧ i! (F)\ni(INFP) = i(I) ∧ i(N) ∧ i! (F)\ni(INTP) = i(I) ∧ i(N) ∧ i! (T)\ni(ESTP) = i(E) ∧ i! (S) ∧ i(T)\ni(ESFP) = i(E) ∧ i! (S) ∧ i(F)\ni(ENFP) = i(E) ∧ i! (N) ∧ i(F)\ni(ENTP) = i(E) ∧ i! (N) ∧ i(T)\ni(ESTJ) = i(E) ∧ i(S) ∧ i! (T)\ni(ESFJ) = i(E) ∧ i(S) ∧ i! (F)\ni(ENFJ) = i(E) ∧ i(N) ∧ i! (F)\ni(ENTJ) = i(E) ∧ i(N) ∧ i! (T)\n\n7\n\n\fThen, the mapping\n•\n\n.\n\n: MBT I → SPP defined such that for every I ∈ 2MBTI ,\nI . = { p ∈ SPP | p(p) ⇒ i(I) }\n\nis the so-called right polarity and\n•\n\n/\n\n: SPP → MBT I defined such that for every P ∈ 2SPP ,\nP / = { i ∈ MBTI | p(P ) ⇒ i(i) }\n\nis the so-called left polarity of the ordered pair ( . , / ).\nSpelled out, (1) the result of applying the mapping i to a set I of MBTIs i as\ndefined in Definition 4 is the conjunction of the results of applying i to each\none of these i as defined in Table 2; (2) the result of applying the mapping p\nto a set P of SPPs p as defined in Definition 4 is the disjunction of the results\nof applying p to each one of these p, which simply is the conjunction of all\nsigned factors in p taken each one as an atomic proposition; (3) the result of\napplying the mapping . to a set I of MBTIs is the set of all those SPPs p whose\nmapping under p implies the mapping of I under i; (4) the result of applying\nthe mapping / to a set P of SPPs is the set of all those MBTIs i whose mapping\nunder i is implied by the mapping of P under p. Thus from a computer science\nperspective [2, Section 7.35], MBTIs are specifications of SPPs and SPPs are\nimplementations or refinements of MBTIs. The Galois-connection then connects\ncorrect implementations to their respective specification by stipulating\nthat a\nV\ncorrect\nimplementation\nimply\nits\nspecification.\nBy\nconvention,\n∅\n=\n>\nand\nW\n∅ = ⊥ , that is, the conjunction over the empty set ∅ is tautological truth > ,\nand the disjunction over ∅ is tautological falsehood ⊥ , respectively.\nNote that an example of an SPP that maps to the empty set under / happens\nto be the Szondi norm profile mentioned before, because its mapping under p\np(((h, +), (s, +), (e, −), (hy, −), (k, −), (p, −), (d, +), (m, +))) =\nh+ ∧ s+ ∧ e− ∧ hy− ∧ k− ∧ p− ∧ d+ ∧ m+ ,\ndoes not contain any (individually) dominant factor (all factors are simply either positive or negative, and thus are without quanta). So it does not imply\nthe mapping of any MBTI under i, which requires a dominant factor, as we\nhave alluded to in the introduction with the dominance flag, and are going to\nexplain now by spelling out the translation of MBTIs to SPPs, see Table 2.\nThere, dominance is indicated by a quantum subscript of the translation i. As\ncan be seen, J and P merely act as flag values, which indicate which one of the\njudgment or perception faculties is the dominant faculty for dealing with the\nouter world, (that is, the observably dominant faculty) and this as a function of\nextro-/introversion. The rule is that extroverts do show their dominant faculty\nfor dealing with the outer world, whereas introverts do not [8, Page 14]. So with\njudging (J) introverts (I), a perceptive faculty shows up as dominant, with perceiving (P) introverts (I), a judging faculty, with perceiving (P) extroverts (E), a\n8\n\n\fperceptive faculty, and with judging (J) extroverts (E), a judging faculty. As can\nbe seen in Table 2, our interpretation of extroversion is a positive tendency of\nSzondi’s factor hy (immoral behaviour: being seen, showing off). Whereas our\ninterpretation of introversion is a negative tendency thereof (moral behaviour:\nseeing, hiding). Our readers might want to stipulate additional constraints for\ntheir own interpretation, but must take care not to create conflicts. Consistency\nis a necessary condition for the faithfulness of the translation i ! (It is also one\nfor the faithfulness of the translation p, but there it is obvious.)\nFact 1 (Consistency of the translation i).\n1. For every b ∈ {E, I} and b0 ∈ MB \\ {E, I}, i(b) ∧ i(b0 ) 6≡ ⊥ .\n2. For every b ∈ {F, T},\n• i(b) ∧ i! (N ) 6≡ ⊥ and i(b) ∧ i! (S) 6≡ ⊥ ;\n• i! (b) ∧ i(N ) 6≡ ⊥ and i! (b) ∧ i(S) 6≡ ⊥ .\nProof. By inspection of Table 2.\nOur interpretations of the remaining faculties follow a simple generating pattern:\n• for a non-dominant positive factor f , the pattern is f + ∨ f ± ∨ f ±! ;\n• for a non-dominant negative factor f , it is f − ∨ f ± ∨ f ±! ;\n• for a dominant positive factor f , it is f +! ∨ f +!! ∨ f +!!! ∨ f ±! ;\n• for a dominant negative factor f , it is f −! ∨ f −!! ∨ f −!!! ∨ f ±! .\nAs can be seen in Table 2, our interpretation of feeling is the conjunction of\npersonal warmth (h+) and empathy (p−), which [7] stipulate as the characteristic properties of this faculty. Our interpretation of thinking is simply its\ncorresponding factor (k−) in Szondi’s system. Our interpretation of the two\nperceptive faculties contains as a conjunct the factor (k+), which corresponds\nto perception in Szondi’s system. Our interpretation of intuition then further\ncontains its corresponding factor (p+) in Szondi’s system. Finally, our interpretation of sensing further contains the disjunction of all those factors that\ncorrespond to the human senses in Szondi’s system, namely: touching (h+),\nhearing (e−), seeing (hy−), smelling (d+), and tasting (m+).\nOur readers might be interested in comparing our interpreting mapping of\nMBTIs with D.W. Keirsey’s [4]: for him, ISTJ maps to the Inspector, ISFJ to\nthe Protector, INFJ to the Counselor, INTJ to the Mastermind, ISTP to the\nCrafter, ISFP to the Composer, INFP to the Healer, INTP to the Architect,\nESTP to the Promoter, ESFP to the Performer, ENFP to the Champion, ENTP\nto the Inventor, ESTJ to the Supervisor, ESFJ to the Provider, ENFJ to the\nTeacher, and ENTJ to the Fieldmarshal.\nWe now prove in two intermediate steps that the ordered pair ( . , / ) is indeed a Galois-connection. The first step is the following announced fact, from\n\n9\n\n\fwhich the second step, Lemma 1, follows, from which in turn the desired result,\nTheorem 1, then follows—easily. As announced, all that our readers need to\ncheck on their own analog of our mapping i is that it has the property stated\nas Fact 2.1. Their own Galois-connection is then automatically induced.\nFact 2 (Some facts about i and p).\n1. if I ⊆ I 0 then i(I 0 ) ⇒ i(I)\n2. if P ⊆ P 0 then p(P ) ⇒ p(P 0 )\n3. The functions i and p are injective but not surjective.\nProof. By inspection of Definition 4 and Table 2.\nWe need Fact 2.1 and 2.2 but not Fact 2.3 in the following development. Therefor, note the two macro-definitions ./ := . ◦ / and /. := / ◦ . with ◦ being\nfunction composition, as usual (from right to left, as usual too).\nLemma 1 (Some useful properties of\n1. if I ⊆ I 0 then I 0. ⊆ I .\n2. if P ⊆ P 0 then P 0/ ⊆ P /\n3. P ⊆ (P / ).\n4. I ⊆ (I . )/\n\n.\n\nand / ).\n\n( . is antitone)\n( / is antitone)\n\n( ./ is inflationary)\n( /. is inflationary)\n\nProof. For (1), let I, I 0 ∈ 2MBTI and suppose that I ⊆ I 0 . Hence i(I 0 ) ⇒ i(I) by\nFact 2.1. Further suppose that p ∈ I 0. . By definition, I 0. = { p ∈ SPP | p(p) ⇒\ni(I 0 ) }. Hence p(p) ⇒ i(I 0 ). Hence p(p) ⇒ i(I) by transitivity. By definition,\nI . = { p ∈ SPP | p(p) ⇒ i(I) }. Hence p ∈ I . . Thus I 0. ⊆ I . .\nFor (2), let P, P 0 ∈ 2SPP and suppose that P ⊆ P 0 . Hence p(P ) ⇒ p(P 0 )\nby Fact 2.2. Further suppose that i ∈ P 0/ . By definition, P 0/ = { i ∈ MBTI |\np(P 0 ) ⇒ i(i) }. Hence p(P 0 ) ⇒ i(i). Hence p(P ) ⇒ i(i) by transitivity. By\ndefinition, P / = { i ∈ MBTI | p(P ) ⇒ i(i) }. Hence i ∈ P / . Thus P 0/ ⊆ P / .\nFor (3), consider:\n1.\n\np∈P\n\n2.\n\n{p} ⊆ P\n\n3.\n\np(p) ⇒ p(P )\n\nhypothesis\n1\n2, Fact 2.2\n\n4.\n\np(p) is true\n\nhypothesis\n\n5.\n\np(P ) is true\n\n3, 4\n\n6.\n\nφ ∈ { i(i) | p(P ) ⇒ i(i) }\n\n7.\n\nthere is i s.t. φ = i(i) and p(P ) ⇒ i(i)\n\nhypothesis\n\n8.\n\nφ = i(i) and p(P ) ⇒ i(i)\n\n9.\n\np(P ) ⇒ i(i)\n\n6\nhypothesis\n8\n\n10\n\n\f10.\n\ni(i) is true\n\n11.\n\nφ = i(i)\n\n12.\n\nφ is true\n\n13.\n14.\n15.\n16.\n17.\n18.\n\n5, 9\n8\n10, 11\n\nφ is true\n\n7, 8–12\n\nfor every φ ∈ { i(i) | p(P ) ⇒ i(i) }, φ is true\nV\n{ i(i) | p(P ) ⇒ i(i) } is true\nV\n{ i(i) | i ∈ { i ∈ MBTI | p(P ) ⇒ i(i) }} is true\nV\n{ i(i) | i ∈ P / } is true\n/\n\ni(P ) is true\n\n14\n15\n16\n17\n\n/\n\n19.\n\np(p) ⇒ i(P )\n\n20.\n\np ∈ { p ∈ SPP | p(p) ⇒ i(P / ) }\n\n21.\n\n6–13\n\n4–18\n19\n\n/ .\n\np ∈ (P )\n\n20\n\n/ .\n\n1–21.\n\n22. P ⊆ (P )\n\nFor (4), consider:\n1.\n\ni∈I\n\n2.\n\n{i} ⊆ I\n\n3.\n\ni(I) ⇒ i(i)\n\nhypothesis\n1\n2, Fact 2.1\n\n7.\n\np(I . ) is true\nW\n{ p(p) | p ∈ I . } is true\nW\n{ p(p) | p ∈ { p ∈ SPP | p(p) ⇒ i(I) } } is true\nW\n{ p(p) | p(p) ⇒ i(I) } is true\n\n8.\n\nthere is p s.t. p(p) ⇒ i(I) and p(p) is true\n\n4.\n5.\n6.\n\np(p) ⇒ i(I) and p(p) is true\n\n9.\n\nhypothesis\n4\n5\n6\n7\nhypothesis\n\n10.\n\ni(I) is true\n\n9\n\n11.\n\ni(i) is true\n\n3, 10\n\n12.\n13.\n14.\n15.\n\ni(i) is true\n\n8, 9–11\n\n.\n\np(I ) ⇒ i(i)\n\n4–12\n.\n\ni ∈ { i ∈ MBTI | p(I ) ⇒ i(i) }\n. /\n\ni ∈ (I )\n\n13\n14\n\n16. I ⊆ (I . )/\n\n1–15.\n\nWe are ready for making the final step.\n11\n\n\fTheorem 1 (The Galois-connection property of ( . , / )). The ordered pair ( . , / )\nis an antitone or order-reversing Galois-connection between MBT I and SPP.\nThat is, for every I ∈ 2MBTI and P ∈ 2SPP ,\nP ⊆ I . if and only if I ⊆ P / .\nProof. Let I ∈ 2MBTI and P ∈ 2SPP . Suppose that P ⊆ I . . Hence (I . )/ ⊆ P /\nby Lemma 1.2. Further, I ⊆ (I . )/ by Lemma 1.4. Hence I ⊆ P / by transitivity.\nConversely suppose that I ⊆ P / . Hence (P / ). ⊆ I . by Lemma 1.1. Further,\nP ⊆ (P / ). by Lemma 1.3. Hence P ⊆ I . .\nThus from a computer science perspective [2, Section 7.35], smaller (larger) sets\nof MBTIs and thus less (more) restrictive specifications correspond to larger\n(smaller) sets of SPPs and thus more (fewer) possible implementations.\nNote that Galois-connections are connected to residuated mappings [1]. Further, natural notions of equivalence on MBT I and SPP are given by the kernels\nof . and / , respectively, which are, by definition:\nI ≡ I0\n\nif and only if I . = I 0. ;\n\nP ≡ P0\n\nif and only if P / = P 0/ .\n\nProposition 1 (The efficient computability of ( . , / )).\n1. Given I ∈ 2MBTI , I . is efficiently computable.\n2. Given P ∈ 2SPP , P / is efficiently computable.\nProof. Even a relatively brute-force approach is feasible with today’s laptop\ncomputers, which have a (high-speed) random access memory of several giga\n(109 ) bytes and a processor power of several giga instructions per second. Moreover in psychological practice, the sets I and P will usually be singletons of either\nonly one type indicator or only one personality profile, respectively.\n• Given I ∈ 2MBTI (thus 0 ≤ |I| ≤ 16), we precompute the list of all\nSPPs, which contains 168 entries, once and for all I, and store the list\non a computer hard disk and then load it into the (fast) random access\nmemory of the computer. Then we model-check the formula i(I), which\ncan also be precomputed, against each entry p in the list. That is, we\ncheck whether a given model p satisfies (makes true) the given i(I), and\ncollect up into the result set all those p that do satisfy i(I). It is wellknown that model-checking a propositional formula, such as i(I), takes\nonly a polynomial number of computation steps in the size of the formula,\nand thus only a polynomial number in the size of I (being at most 16).\nOf course, this computation can be done once and for all of the 216 possible\nsets I, and the results stored on a hard disk for faster, later look-up.\n• Given P ∈ 2SPP (thus 0 ≤ |P | ≤ 168 ), we model-check each p in P against\nthe (pre-computable) mapping i(i) of each i of the 16 MBTIs, and collect\nup into the result set all those i whose mapping i(i) satisfies p.\n12\n\n\fOf course, optimisations of the computation procedure given in the previous\nproof are possible, but we consider them as not sufficiently interesting implementation details.\n\n3\n\nConclusion\n\nWe have proposed a computable Galois-connection between Myers-Briggs Type\nIndicators and Szondi’s personality profiles as promised in the abstract. In\naddition, we have proposed a simple methodology for generating other such\nGalois-connections, including Galois-connections not only between this pair of\nspaces of personality-test result values but also between other such pairs.\nAcknowledgements I thank Danilo Diedrichs for proof-reading this article.\nThe LATEX-package TikZ was helpful for graph drawing.\n\nReferences\n[1] T.S. Blyth. Lattices and Ordered Algebraic Structures. Springer, 2005.\n[2] B.A. Davey and H.A. Priestley. Introduction to Lattices and Order. Cambridge University Press, 2nd edition, 1990 (2002).\n[3] C.G. Jung. Psychological Types, volume 6 of The Collected Works of C.G.\nJung. Princeton University Press, 1971.\n[4] D. Keirsey. Please Understand Me II. Prometheus Nemesis Book Company,\n1984 (1998).\n[5] R. Kenmo. Let the Personality Bloom. Humankonsult, 2009.\n[6] S. Kramer. www.szondi-test.ch, 2014. forthcoming.\n[7] I. Briggs Myers and M.H. McCaulley. Manual: A Guide to the Development and Use of the Myers-Briggs Type Indicator. Consulting Psychologists\nPress, 2nd edition, 1985.\n[8] I. Briggs Myers and P.B. Myers. Gifts Differing: Understanding Personality\nType. Consulting Psychologists Press, 1980 (1995).\n[9] L. Szondi. Lehrbuch der Experimentellen Triebdiagnostik, volume I: TextBand. Hans Huber, 3rd edition, 1972.\n[10] L. Szondi. Ich-Analyse: Die Grundlage zur Vereinigung der Tiefenpsychologie. Hans Huber, 1999. English translation: https://sites.google.\ncom/site/ajohnstontranslationsofszondi/.\n\n13\n\n\f[11] L. Szondi.\nSchicksalsanalyse: Wahl in Liebe, Freundschaft, Beruf,\nKrankheit und Tod. Schwabe, 4th edition, 2004.\n\n14\n\n\f"
        ],
        [
         "24",
         "24",
         "cs.CE",
         "Computational Engineering",
         "1610.03809v1.pdf",
         "A Continuous Model of Cortical Connectivity\n\narXiv:1610.03809v1 [q-bio.NC] 12 Oct 2016\n\nDaniel Moyer, Boris A. Gutman, Joshua Faskowitz, Neda Jahanshad, and\nPaul M. Thompson\nImaging Genetics Center, University of Southern California\nmoyerd@usc.edu\n\nAbstract. We present a continuous model for structural brain connectivity based on the Poisson point process. The model treats each streamline curve in a tractography as an observed event in connectome space,\nhere a product space of cortical white matter boundaries. We approximate the model parameter via kernel density estimation. To deal with\nthe heavy computational burden, we develop a fast parameter estimation method by pre-computing associated Legendre products of the data,\nleveraging properties of the spherical heat kernel. We show how our approach can be used to assess the quality of cortical parcellations with\nrespect to connectivty. We further present empirical results that suggest\nthe “discrete” connectomes derived from our model have substantially\nhigher test-retest reliability compared to standard methods.\nKeywords: Human Connectome, Diffusion MRI, Non-Parametric Estimation\n\n1\n\nIntroduction\n\nIn recent years the study of structural and functional brain connectivity has\nexpanded rapidly. Following the rise of diffusion and functional MRI connectomics has unlocked a wealth of knowledge to be explored. Almost synonymous\nwith the connectome is the network-theory based representation of the brain.\nIn much of the recent literature the quantitative analysis of connectomes has\nfocused on region-to-region connectivity. This paradigm equates physical brain\nregions with nodes in a graph, and uses observed structural measurements or\nfunctional correlations as a proxy for edge strengths between nodes.\nCritical to this representation of connectivity is the delineation of brain regions, the parcellation. Multiple studies have shown that the choice of parcellation influences the graph statistics of both structural and functional networks\n[15,17,18]. It remains an open question which of the proposed parcellations is\nthe optimal representation, or even if such a parcellation exists [14].\nIt is thus useful to construct a more general framework for cortical connectivity, one in which any particular parcellation of the cortex may be expressed and\nits connectivity matrix derived, and one in which the variability of connectivity\nEarlier forms of this paper included erroneous results due to a software bug. These\nerrors, which artificially raised ICC scores, have been corrected in this version.\n\n\fmeasures can be modeled and assessed statistically. It is also important that\nthis framework allow comparisons between parcellations, and representations in\nthis framework must be both analytically and computationally tractable. Since\nseveral brain parcellations at the macroscopic scale are possible, a representation\nof connectivity that is independent of parcellation is particularly appealing.\nIn this paper, we develop such a general framework for a parcellation independent connectivity representation, building on the work of [8]. We describe a\ncontinuous point process model for the generation of observed tract1 (streamline)\nintersections with the cortical surface, from which we may recover a distribution\nof edge strengths for any pair of cortical regions, as measured by the inter-region\ntract count. Our model is an intensity function over the product space of the\ncortical surface with itself, assigning to every pair of points on the surface a\npoint connectivity. We describe an efficient method to estimate the parameter\nof the model, as well as a method to recover the region-to-region edge strength.\nWe then demonstrate the estimation of the model on a Test-Retest dataset. We\nprovide reproducibility estimates for our method and the standard direct count\nmethods [10] for comparison. We also compare the representational power of\ncommon cortical parcellations with respect to a variety of measures.\n\n2\n\nContinuous Connectivity Model\n\nThe key theoretical component of our work is the use of point process theory\nto describe estimated cortical tract projections. A point process is a random\nprocess where any realization consists of a collection of discrete points on a\nmeasurable space. The most basic of these processes is the Poisson process, in\nwhich events occur independently at a specific asymptotic intensity (rate) λ over\nthe chosen domain [12]. λ completely characterizes each particular process, and\nis often defined as a function λ : Domain → R+ , which allows the process to\nvary in intensity by location. The expected count of any sub-region (subset) of\nthe domain is its total intensity, the integral of λ over the sub-region. In this\npaper, our domain is the connectivity space of the cortex, the set of all pairs of\npoints on the surface, and the events are estimated tract intersections with the\ncortical surface.\n2.1\n\nModel Definition\n\nLet Ω be union of two disjoint subspaces each diffeomorphic to the 2-sphere\nrepresenting the white matter boundaries in each hemisphere. Further consider\nthe space Ω × Ω, which here represents all possible end point pairs for tracts\nthat reach the white matter boundary. We treat the observation of such tracts as\nevents generated by an inhomogeneous (symmetric) Poisson process on Ω × Ω;\nin our case, for every event (x, y) we have a symmetric event (y, x).\n1\n\nIt is critical to distinguish between white matter fibers (fascicles) and observed\n“tracts.” Here, “tracts” denotes the 3d-curves recovered from Diffusion Weighted\nImaging via tractography algorithms.\n\n\fAssuming that each event is independent of all other events except for its\nsymmetric event (i.e., each tract is recovered independently), we model connectivity as a intensity function λ : Ω × Ω → R+ , such that for any regions\nE1 , E2 ⊂ Ω, the number of events is Poisson distributed with parameter\nZZ\nC(E1 , E2 ) =\nλ(x, y)dxdy.\n(1)\nE1 ,E2\n\nDue to properties of the Poisson distribution, the expected number of tracts is\nexactly C(E1 , E2 ). For any collection of regions {Ei }N\ni=1 = P , we can compute a\nweighted graph G(P, λ) by computing each C(Ei , Ej ) for pairs (Ei , Ej ) ∈ P × P .\nEach node in this graph represents a region, and each weighted edge represents\nthe pairwise connectivity\nof the pair\nS\nT of nodes (regions) it connects. We call P a\nparcellation of Ω if i Ei = Ω and i Ei has measure 0 ({Ei } is almost disjoint).\n2.2\n\nRecovery of the Intensity Function\n\nA sufficient statistic for Poisson process models is the intensity function λ(x, y).\nEstimation of the function is non-trivial, and has been the subject of much\nstudy in the spatial statistics community [3]. We choose to use a non-parametric\nKernel Density Estimation (KDE) approach due to an efficient closed form for\nbandwidth estimation described below. This process is self-tuning up to a choice\nof desiderata for the bandwidth parameter.\nWe first inflate each surface to a sphere and register them using a spherical\nregistration (See section 3.1); however this entire method can be undertaken\nwithout group registration. We treat each hemisphere as disjoint from the other,\nallowing us to treat Ω × Ω as the product of spheres (S1 ∪ S2 ) × (S1 ∪ S2 ).\nThroughout the rest of the paper D denotes a dataset containing observed tract\nendpoints (x, y)i , and λ̂ denotes our estimation of λ.\nThe unit normalized spherical heat kernel is a natural choice of kernel for S2 .\nWe use its truncated spherical harmonic representation [1], defined as follows for\nany two unit vectors p and q on the 2-sphere:\nKσ (p, q) =\n\nH\nX\n2h + 1\nh\n\nPh0\n\nth\n\n4π\n\nexp{−h(h + 1)σ}Ph0 (p · q)\n\nHere,\nis the h degree associated Legendre polynomial of order 0. Note\nthat the non-zero order polynomials have coefficient zero due to the radial\nsymmetry of the spherical heat kernel [1]. However, since we are estimating\na function on Ω × Ω, we use the product of two heat kernels as our KDE\nkernel κ. For any two points p and q, the kernel value associated to a end\npoint\npair (x, y) is κ((p, q)|(x, y)) = Kσ (x, p)Kσ (y, q). It is easy to show that\nR\nK\nσ (x, p)Kσ (y, q)dpdq = 1.\nΩ×Ω\nThe spherical heat kernel has a single shape parameter σ which corresponds\nto its bandwidth. While in general tuning this parameter requires the re-estimation\nof λ̂ at every iteration, by rewriting our kernel we can memoize part of the computation so that we only need to store the sum of the outer products of the\n\n\fharmonics. Writing out κ((p, q)|D) =\nthe following:\nκ((p, q)|D) =\n\nP\n\n(xi ,yi )∈D\n\n\u0013\u0012\n\u0013\nH X\nH \u0014\u0012\nX\n2k + 1\n2h + 1\nh\n\nk\n\n4π\n|\n\n4π\n\nKσ (xi , p)Kσ (yi , q), we have\n\nexp{−σ(h2 + h + k 2 + k)}\n{z\n}\n\nIndependent of D, evaluated every iteration\n\nX\n\n×\n\nPh0 (xi · p)Pk0 (yi · q)\n\n\u0015\n\n(xi ,yi )∈D\n\n|\n\n{z\n\n}\n\nIndependent of σ, evaluated once\n\nThus, evaluations of the kernel at any point (p, q) can be done quickly for sequences of values of σ. We then are left with the choice of loss function. Denoting\nthe true intensity function λ, the estimated intensity λ̂, and the leave-one-out\nestimate λ̂i (leaving out observation i), Integrated Squared Error (ISE) is defined:\nZ\nISE(σ|D) =\n(λ̂(x, y|σ) − λ(x, y))2 dxdy\nZΩ×Ω\nX\n2\nλ̂i (xi , yi ) + Constant.\n≈ λ̂(x, y|σ)2 dxdy −\n|D|\n(xi ,yi )∈D\n\nHall and Marron [9] suggest tuning bandwidth parameters using ISE. In practice,\nwe find that replacing each leave-one-out estimate with its logarithm log λ̂i (xi , yi )\nyields more consistent and stable results.\n2.3\n\nSelecting a parcellation\n\nGiven an estimated intensity λ̂ and two or more parcellations P1 , P2 , . . . , we\nwould like to know which parcellation and associated graph G(P, λ̂) best represents the underlying connectivity function. There are at least two perspectives\nto consider.\nApproximation Error: Because each Pi covers Ω (and Pi × Pi = Ω × Ω),\neach G(P1 , λ̂) can be viewed as a piece-wise function g : Ω × Ω → R+ , where\n1\ng(x, y) = |Ei ||E\nC(Ei , Ej ) such that x ∈ Ei and y ∈ Ej . In other words, g is\nj|\nthe constant approximation to λ over every pair of regions. A natural measure\nof error is another form of Integrated Squared Error:\nZZ\nErr(λ̂, G(P1 , λ̂)) =\n(g(x, y) − λ(x, y))2 dxdy.\n(2)\nΩ×Ω\n\nThis is analogous to squared loss (`2 -loss).\nLikelihood: An alternative viewpoint leverages the point process model to\nmeasure a likelihood:\nX\nlog L(P ) =\nlog Poisson(|{(x, y)i ∈ D : x ∈ Ei , y ∈ Ej }|; C(Ei , Ej )). (3)\nEi ,Ej ∈P\n\n\fHere, the independence assumption plays a critical role, allowing pairs of regions\nto be evaluated separately. Unfortunately this is biased toward parcellations with\nmore, smaller regions, as the Poisson distribution has tied variance and mean\nin one parameter. A popular likelihood-based option that somewhat counterbalances this is Akaike’s Information Criterion (AIC),\n\u0012\n\u0013\n|P |\nlog |D|.\n(4)\nAIC(P ) = −2 log L(P ) +\n2\nAIC balances accuracy with parsimony, penalizing overly parameterized models\n- in our case, parcellations with too many regions.\n\n3\n\nApplication to CoRR Test-Retest Data\n\nWe demonstrate the use of our framework on a test-retest dataset. We measure\nconnectome reproducibility using Intraclass Correlation (ICC) [13], and compare\nthree parcellations using multiple criteria (See Equations 2,3, and 4).\n3.1\n\nProcedure, Connectome Generation, and Evaluation\n\nOur data are comprised of 29 subjects from the Institute of Psychology, Chinese\nAcademy of Sciences sub-dataset of the larger Consortium for Reliability and\nReproducibility (CoRR) dataset [19]. T1-weighted (T1w) and diffusion weighted\n(DWI) images were obtained on 3T Siemens TrioTim using an 8-channel head\ncoil and 60 directions. Each subject was scanned twice, roughly two weeks apart.\nT1w images were processed with Freesufer’s [4] recon-all pipeline to obtain a\ntriangle mesh of the grey-white matter boundary registered to a shared spherical\nspace [5], as well as corresponding vertex labels per subject for three atlas-based\ncortical parcellations, the Destrieux atlas [6], the Desikan-Killiany (DK) atlas [2],\nand the Desikan-Killiany-Tourville (DKT31) atlas [11]. Probabilistic streamline\ntractography was conducted using the DWI in 2mm isotropic MNI 152 space,\nusing Dipy’s [7] implementation of constrained spherical deconvolution (CSD)\n[16] with a harmonic order of 6. As per Dipy’s ACT, we retained only tracts\nlonger than 5mm with endpoints in likely grey matter.\nWe provide the mean ICC score computed both with and without entries\nthat are zero for all subjects. When estimating λ̂ the kernels are divided by\nthe number of tracks, and we use a sphere with unit surface area instead of\nunit radius for ease of computation. We threshold each of the kernel integrated\nconnectomes at 10−5 , which is approximately one half of one unit track density.\nWe then compute three measures of parcellation representation accuracy, namely\nISE, Negative Log Likelihood, and AIC scores.\n3.2\n\nResults & Discussion\n\nTable 1 shows a surprisingly low mean ICC scores for regular count matrices. This\nmay be because ICC normalizes each measure by its s2 statistic, meaning that\n\n\fAtlas\nNumber of Regions\nCount ICC\nIntensity ICC (Full)\nIntensity ICC (w/Threshold)\n\nDK\n68\n0.2093\n0.4868\n0.5613\n\nDestrieux\n148\n0.1722\n0.4535\n0.6481\n\nDKT31\n62\n0.2266\n0.4388\n0.4645\n\nTable 1. This table shows mean ICC scores for each connectome generation method.\nThe count method - the standard approach - defines edge strength by the fiber endpoint\ncount. The integrated intensity method is our proposed method; in general it returns a\ndense matrix. However, many of the values are extremely low, and so we include results\nboth with and without thresholding. Highest ICC scores for each atlas are bolded.\n\nType\nDK\nDestrieux\nDKT31\nISE\n1.8526 × 10−5 2.1005 × 10−5 2.1258 × 10−5\nNegative LogLik\n85062.5\n355769.4\n88444.5\nAIC Score\n174680.95\n733294.8\n185253.5\nRetest ISE\n1.0517 × 10−5 1.0257 × 10−5 1.1262 × 10−5\nRetest Negative LogLik\n85256.0\n357292.9\n88434.9\nRetest AIC Score\n175068.1\n736341.9\n185234.3\nTable 2. This table shows the means over all subjects of three measures of parcellation\n“goodness”. The retest versions are the mean of the measure using the parcellation’s\nregional connectivity matrix (or the count matrix) from one scan, and the estimated\nintensity function from the other scan.\n\nentries in the adjacency matrices that should be zero but that are subject to a\nsmall amount of noise – a few erroneous tracks – have very low ICC. Our method\nin effect smooths tracts endpoints into a density; end points near the region\nboundaries are in effect shared with the adjacent regions. Thus, even without\nthresholding we dampen noise effects as measured by ICC. With thresholding,\nour method’s performance is further improved, handily beating the counting\nmethod with respect to ICC score. It is important to note that for many graph\nstatistics, changing graph topology can greatly affect the measured value [18].\nWhile it is important to have consistent non-zero measurements, the difference\nbetween zero and small but non-zero in the graph context is also non-trivial. The\nconsistency of zero-valued measurements is thus very important in connectomics.\nTable 2 suggests that all three measures, while clearly different, are consistent\nin their selection at least with respect to these three parcellations. It is somewhat\nsurprising that the Destrieux atlas has quite low likelihood criteria, but this may\nbe due to the (quadratically) larger number of region pairs. Both likelihood based\nretest statistics also choose the DK parcellation, while ISE chooses the Destrieux\nparcellation by a small margin. It should be noted that these results must be\nconditioned on the use of a probabilistic CSD tractography model. Different\nmodels may lead to different intensity functions and resulting matrices. The\n\n\fFig. 1. A visualization of the ICC scores for connectivity to Brodmann Area 45 (Destrieux region 14) for the Count connectomes (left) and the proposed Integrated Intensity connectomes (right). Blue denotes a higher score.\n\nR\nFig. 2. A visualization of the marginal connectivity M (x) = E λ̂(x, y)dy for the Left\ni\nPost-central Gyrus region of the DK atlas (Region 57). The region is shown in blue on\nthe inset. Red denotes higher connectivity regions with the blue region.\n\nbiases and merits the different models and methods (e.g. gray matter dilation\nfor fiber counting vs streamline projection) remain important open questions.\n\n4\n\nConclusion\n\nWe have presented a general framework for structural brain connectivity. This\nframework provides a representation for cortical connectivity that is independent\nof the choice of regions, and thus may be used to compare the accuracy of\na given set of regions’ connectivity matrix. We provide one possible estimation\nmethod for this representation, leveraging spherical harmonics for fast parameter\nestimation. We have demonstrated this framework’s viability, as well as provided\na preliminary comparison of regions using several measures of accuracy.\nThe results presented here lead us to conjecture that our connectome estimates are more reliable compared to standard fiber counting, though we stress\nthat a much larger study is required for strong conclusions to be made. Further adaptations of our method are possible, such as using FA-weighted fiber\ncounting. Our future work will explore these options, conduct tests on larger\ndatasets, and investigate the relative differences between tracking methods and\nparcellations more rigorously.\n\n\fAcknowledgments\nThis work was supported by NIH Grant U54 EB020403, as well as the Rose Hills\nFellowship at the University of Southern California. The authors would like to\nthank the reviewers as well as Greg Ver Steeg for multiple helpful conversations.\n\nReferences\n1. Chung, M.K.: Heat kernel smoothing on unit sphere. In: Biomedical Imaging: Nano\nto Macro, 2006. 3rd IEEE International Symposium on. pp. 992–995. IEEE (2006)\n2. Desikan, R.S., et al.: An automated labeling system for subdividing the human\ncerebral cortex on MRI scans into gyral based regions of interest. NeuroImage\n31(3), 968–980 (2006)\n3. Diggle, P.: A kernel method for smoothing point process data. Applied Statistics\npp. 138–147 (1985)\n4. Fischl, B.: Freesurfer. NeuroImage 2(62), 774–781 (2012)\n5. Fischl, B., et al.: High-resolution intersubject averaging and a coordinate system\nfor the cortical surface. Human Brain Mapping 8(4), 272–284 (1999)\n6. Fischl, B., et al.: Automatically parcellating the human cerebral cortex. Cerebral\nCortex 14(1), 11–22 (2004)\n7. Garyfallidis, E., et al.: Dipy, a library for the analysis of diffusion MRI data. Front.\nNeuroinform 8(8) (2014)\n8. Gutman, B., et al.: Registering cortical surfaces based on whole-brain structural\nconnectivity and continuous connectivity analysis. In: MICCAI 2014, pp. 161–168.\nSpringer (2014)\n9. Hall, P., Marron, J.S.: Extent to which least-squares cross-validation minimises\nintegrated square error in nonparametric density estimation. Probability Theory\nand Related Fields 74(4), 567–581 (1987)\n10. Jahanshad, N., et al.: Alzheimer’s Disease Neuroimaging I (2013) genome-wide scan\nof healthy human connectome discovers SPON1 gene variant influencing dementia\nseverity. Proc Natl Acad Sci USA 110(12), 4768–4773\n11. Klein, A., Tourville, J., et al.: 101 labeled brain images and a consistent human\ncortical labeling protocol. Front. Neurosci 6(171), 10–3389 (2012)\n12. Moller, J., Waagepetersen, R.P.: Statistical inference and simulation for spatial\npoint processes. CRC Press (2003)\n13. Portney, L.G., Watkins, M.P.: Statistical measures of reliability. Foundations of\nClinical Research: Applications to Practice 2, 557–586 (2000)\n14. de Reus, M.A., Van den Heuvel, M.P.: The parcellation-based connectome: limitations and extensions. NeuroImage 80, 397–404 (2013)\n15. Satterthwaite, T.D., Davatzikos, C.: Towards an individualized delineation of functional neuroanatomy. Neuron 87(3), 471–473 (2015)\n16. Tournier, J.D., Yeh, C.H., Calamante, F., Cho, K.H., Connelly, A., Lin, C.P.: Resolving crossing fibres using constrained spherical deconvolution: validation using\ndiffusion-weighted imaging phantom data. NeuroImage 42(2), 617–625 (2008)\n17. Wang, J., et al.: Parcellation-dependent small-world brain functional networks: A\nresting-state fMRI study. Human Brain Mapping 30(5), 1511–1523 (2009)\n18. Zalesky, A., et al.: Whole-brain anatomical networks: does the choice of nodes\nmatter? NeuroImage 50(3), 970–983 (2010)\n19. Zuo, X.N., et al.: An open science resource for establishing reliability and reproducibility in functional connectomics. Scientific Data 1 (2014)\n\n\f"
        ],
        [
         "25",
         "25",
         "cs.CE",
         "Computational Engineering",
         "1609.01689v3.pdf",
         "Accelerating Nuclear Configuration Interaction Calculations\nthrough a Preconditioned Block Iterative Eigensolver\n\narXiv:1609.01689v3 [cs.NA] 8 Sep 2017\n\nMeiyue Shao1 , H. Metin Aktulga2 , Chao Yang1 , Esmond G. Ng1 , Pieter Maris3 , and\nJames P. Vary3\n1\n\nComputational Research Division, Lawrence Berkeley National Laboratory, Berkeley, CA\n94720, United States\n2\nDepartment of Computer Science and Engineering, Michigan State University, East\nLansing, MI 48824, United States\n3\nDepartment of Physics and Astronomy, Iowa State University, Ames, IA 50011, United\nStates\nAugust 25, 2017\nAbstract\nWe describe a number of recently developed techniques for improving the performance of\nlarge-scale nuclear configuration interaction calculations on high performance parallel computers. We show the benefit of using a preconditioned block iterative method to replace the Lanczos\nalgorithm that has traditionally been used to perform this type of computation. The rapid convergence of the block iterative method is achieved by a proper choice of starting guesses of the\neigenvectors and the construction of an effective preconditioner. These acceleration techniques\ntake advantage of special structure of the nuclear configuration interaction problem which we\ndiscuss in detail. The use of a block method also allows us to improve the concurrency of the\ncomputation, and take advantage of the memory hierarchy of modern microprocessors to increase the arithmetic intensity of the computation relative to data movement. We also discuss\nimplementation details that are critical to achieving high performance on massively parallel\nmulti-core supercomputers, and demonstrate that the new block iterative solver is two to three\ntimes faster than the Lanczos based algorithm for problems of moderate sizes on a Cray XC30\nsystem.\n\n1\n\nIntroduction\n\nThe structure of an atomic nucleus with A nucleons can be elucidated by solutions of the many-body\nSchrödinger equation\nHΨ(r1 , r2 , . . . , rA ) = λΨ(r1 , r2 , . . . , rA ),\n(1)\nwhere H is the nuclear Hamiltonian acting on the A-body wavefunction Ψ(r1 , r2 , . . . , rA ) (with\nrj ∈ R3 , j = 1, 2, . . . , A), and λ the energy. For stable nuclei, the low-lying spectrum is discrete.\nThe solution associated with the lowest eigenvalue is the ground state. The nuclear Hamiltonian\nH contains the kinetic energy operator K, and the potential term V which describes the strong\ninteractions between nucleons as well as the Coulomb interactions between protons.\nNuclear structure presents a unique set of challenges for achieving robust simulations within\nquantum many-body theory. First, the underlying strong interactions span scales from low energies\n1\n\n\f(few MeV) to high energies (hundreds of MeV) as observed in the measured nucleon–nucleon scattering cross section. Second, many phenomena of interest, such as halo nuclei, collective rotation,\nfission and radioactive decay, involve the delicate interplay of binding contributions across those\nenergy scales leading to these important low-energy and long-range phenomena. Third, in order to\nachieve accurate results from nuclear theory that are competitive with accurate experimental data,\nit is now recognized that we require three-nucleon (3N) interactions whose contributions represent\none to two orders of magnitude increase in computational burden over calculations with two-nucleon\n(NN) interactions alone.\nOne approach to obtaining approximate solutions of (1) is to use the No-Core Configuration\nInteraction (NCCI) method [7] which projects the nuclear many-body Hamiltonian operator in (1)\nonto a finite-dimensional subspace spanned by Slater determinants of the form\n\n\nφa1 (r1 ) φa2 (r1 ) . . . φaA (r1 )\n φa (r2 ) φa (r2 ) . . . φa (r2 ) \n1\n2\nA\n\n 1\nΦa (r1 , r2 , . . . , rA ) = √ det \n(2)\n,\n..\n..\n..\n.\n.\n\n\n.\nA!\n.\n.\n.\nφa1 (rA ) φa2 (rA ) . . . φaA (rA )\n\nwhere φa ’s are orthonormal single-particle wavefunctions indexed by a generic label a. (Quantum\nmechanics dictates that wavefunctions of A identical spin- 12 particles are completely anti-symmetric,\na condition satisfied by a Slater determinant. In practice, we often form our nuclear many-body\nwavefunctions as products of separate Slater determinants for Z protons and N neutrons, with\nA = Z + N .) 1\nThe lowest eigenvalues and associated eigenvectors of the finite dimensional Hamiltonian Ĥ,\nwhich is large and sparse, are then computed. The dimension of Ĥ depends on (1) the number of\nnucleons A; (2) the number of single-particle states; and (3) the (optional) many-body truncation.\nIn the NCCI approach, typically one works in a basis of harmonic oscillator single-particle states\nwhere the number of single-particle states is implicitly determined by the many-body truncation\nNmax , which imposes a limit on the sum of the single-particle energies (oscillator quanta) included\nin each Slater determinant of A nucleons. In the limit of a complete (but infinite-dimensional)\nbasis, this approach would give the exact bound state wave functions; in practice, increasingly\naccurate approximations to both the ground state and the narrow (low-lying) excited states of a\ngiven nucleus often require increasing Nmax to the largest value where the eigenvalue problem is\nstill solvable with available methods and resources. The dimension of Ĥ increases rapidly both\nwith the number of nucleons, A, and with the truncation parameter, Nmax . This large sparse\nmatrix eigenvalue problem has been demonstrated to be a computationally challenging problem on\nhigh-performance computers.\nAn efficient way to construct and diagonalize the projected Hamiltonian Ĥ has been implemented in the software package MFDn (Many Fermion Dynamics for nuclear structure) developed\nby Vary et al. [22, 23, 36]. MFDn uses the Lanczos algorithm [21, 28] to compute the desired\neigenpairs. Over the last several years, we have developed a number of techniques to improve the\ncomputational efficiency of MFDn. These techniques include\n1. an efficient scalable parallel scheme for constructing the matrix Hamiltonian [35];\n2. efficient data distribution schemes that take into account the topology of the interconnect [3];\n1\nEach Slater determinant corresponds to a quantum many-body configuration. The configurations interact with\neach other through the Hamiltonian (which contains both the kinetic and potential energy contributions), hence the\nname “Configuration Interaction”. Since we include all nucleons of the nucleus in these configurations, we use the\nterm “No-Core”.\n\n2\n\n\f3. a technique to overlap communication with computation in a hybrid MPI/OpenMP programming model [4];\n4. an efficient scheme to multiply the sparse matrix Hamiltonian with a number of vectors [2].\nThe last technique is extremely useful in block iterative algorithms for computing eigenvalues and\neigenvectors of a sparse matrix. However, additional techniques are required in order to develop an\nefficient block eigensolver.\nIn this paper, we present implementation details of a preconditioned block iterative eigensolver\nto accelerate the NCCI computations in MFDn. Specifically, we adopt the Locally Optimal Block\nPreconditioned Gradient (LOBPCG) algorithm [17] as our eigensolver. The LOBPCG algorithm\nand its variants have been used to solve eigenvalue problems arising from a number of applications,\nincluding material science [6, 15, 20, 37] and machine learning [18, 19, 24]. The use of a block\niterative method allows us to improve the memory access pattern of the computation and make\nuse of approximations to several eigenvectors at the same time. To make this algorithm efficient,\nwe identified an effective preconditioner coupled with techniques to generate good initial guesses\nthat significantly accelerates the convergence of the LOBPCG algorithm. We describe in detail\nour techniques to construct the preconditioner and generate initial guesses, as well as efficient\nimplementation of these techniques on large scale distributed memory clusters.\nAlthough our main focus is to develop, implement and test an efficient LOBPCG approach\nwithin MFDn, many of the features of our implementation may be adopted straightforwardly in\nother configuration interaction (CI) codes. For example, the codes Antoine [10], BIGSTICK [16,\n30], KSHELL [31], and NuShellX [8] represent popular CI codes that also solve the large sparse\nmatrix eigenvalue problem arising in nuclear structure calculations. Of these codes, BIGSTICK is\nalso capable of treating three-body interactions and is specifically designed for high-performance\ncomputing. The most important difference between BIGSTICK and MFDn is that in MFDn the\nsparse matrix is stored in core during the diagonalization phase, whereas it is recomputed-on-the-fly\nin BIGSTICK. As the number of Lanczos iterations, each with a recomputed matrix, can then be\ntime-consuming, BIGSTICK may especially benefit from some of the key features of our LOBPCG\napproach.\nThe paper is organized as follows. In Section 2, we review the general formulation of the\nnuclear many-body problem and the structure of the Hamiltonian matrix to be diagonalized. We\ndescribe the LOBPCG algorithm for computing the lowest few eigenvalues of Ĥ in Section 3 along\nwith a discussion of our recent improvements, which are the main contribution of this paper. In\nSection 4, we discuss a number of implementation issues. In Section 5, we report the computational\nperformance of the resulting LOBPCG implementation and compare it with the existing Lanczos\neigensolver in MFDn.\n\n2\n\nThe Structure of the Hamiltonian\n\nThe efficiency of the block iterative LOBPCG eigensolver hinges, to a large extent, on how efficient\nthe multiplication of Ĥ with a block of vectors is carried out. Due to its large dimension and\nthe relatively large number of nonzero matrix elements it contains, the matrix Ĥ is generated in\nparallel, and stored over a set of distributed memory multiprocessors in a two dimensional fashion,\nsee Section 4.1. The sparsity structures of the local submatrices are not known in advance, and will\nin general depend on how the many-body basis functions Φa (r1 , r2 , . . . , rA ) are generated, ordered,\nand distributed. To achieve good performance, we generate these basis functions in a way to\n1. ensure that the parallel sparse matrix vector multiplication is well load-balanced;\n3\n\n\f2. allow the sparsity structure of Ĥ to be quickly determined without exhaustively evaluating\nhΦa |Ĥ|Φb i for every (a, b) pair [22, 35], where hΦa |H|Φb i denotes the inner product between\nΦa and HΦb defined in terms of integration with respect to r1 , r2 , . . . , rA ;\n3. enhance data locality by keeping the matrix in block sparse format [1, 2, 9].\nAll these issues are closely related, and depend on how the many-body basis functions are enumerated, which we will describe in Section 2.1. In Section 2.2, we describe a mechanism for grouping\nmany-body basis states to create matrix tiles (a tile is a rectangular block, i.e., a submatrix, of the\nmatrix Ĥ) that allow us to reduce the amount of work required to probe the sparsity structure of\nĤ and to enhance data locality when Ĥ is multiplied with a number of vectors. While the general\nmatrix tiling strategy was presented in [2], our previous approach did not leverage the Hamiltonian structure efficiently during matrix construction; it rather focused on exploring the benefits of\nusing sparse matrix multiple vector multiplication (SpMM) over a series of sparse matrix vector\nmultiplies (SpMV). In this paper, we present a novel grouping strategy to enhance data locality\nand incorporate this strategy directly during the construction of Ĥ.\n\n2.1\n\nWavefunction Indexing and Enumeration\n\nEach many-body basis function Φa (r1 , r2 , . . . , rA ) that we use to expand the nuclear wavefunctions\nand express the Hamiltonian can be indexed by an A-tuple of single-particle quantum numbers,\ni.e., a = (a1 , a2 , . . . , aA ). We often refer to Φa (r1 , r2 , . . . , rA ) or simply a as a many-body basis state.\nWe will call each component of a that appears in (2), i.e., φai (r), a single-particle state. In nuclear\nphysics we have two types of fermions, protons and neutrons, so that each many-body basis state\ncan actually be written as a product of two Slater determinants of the form (2), one for Z protons\nand one for N neutrons. Since we are dealing with spin- 12 particles, each single-particle state φai (r)\ncan either be occupied or unoccupied, but we cannot have more than one particle of the same type\nin any given single-particle state due to the Pauli exclusion principle.\nIn MFDn, we use a spherically-symmetric basis, for which the angular part of the single-particle\nwavefunctions can be expanded in spherical harmonics Yl (a spherical tensor of rank l, with 2l + 1\nspherical components). These spherical harmonics are multiplied with radial wavefunctions Rnl (r2 )\nto form the single-particle basis function. Using this expansion, we can label each single-particle basis function φai (r) by their quantum numbers (n, l, j, mj ): the radial quantum number n, counting\nthe number of nodes (excluding the nodes at the origin and the infinity) in the radial wavefunction\nRnl (r2 ); the orbital angular momentum quantum number l; the total single-particle angular momentum j = |l ± 21 | which is the intrinsic nucleon spin s = 12 coupled to l; and mj , the magnetic\nprojection of j, with mj = −j, −j + 1, . . . , j − 1, j. For a spherically-symmetric one-body Hamiltonian, its single-particle states that differ only in the mj component of the quantum numbers are\ndegenerate. We refer to the set of single-particle states with the same (n, l, j) but different values\nof m as orbitals; each orbital can be occupied by at most 2j + 1 identical particles.\nThe nuclear Hamiltonian preserves (among other things) the total angular momentum J, the\ntotal angular momentum projection M , and the parity P = ±1. MFDn is a so-called M -scheme\ncode, in which the many-body basis states (i.e., configuration space) have a specific value of M , as\nwell as a specific value of P , but not a specific value of J. The angular momentum projection is a\nsimple additive quantum number\nA\nX\nM=\nmj (ai ),\ni=1\n\n4\n\n\fand the parity is a multiplicative quantum number\nP =\n\nA\nY\n\np(ai )\n\ni=1\n\nwith p(ai ) = (−1)l(ai ) .2 In an M -scheme basis, the Hamiltonian is block-diagonal with each block\ncharacterized by its M and P values. In practice, we perform our calculation for one block at a\ntime, that is, for fixed M and P . Using M = 0 ( 12 ) for even (odd) A, one needs only two runs, one\nfor each parity, to determine the (low-lying) spectrum.\nWhen the single-particle states are chosen to be eigenfunctions of a 3D quantum harmonic\noscillators, they can be ordered by their energy levels (or eigenvalues), which are given by ( 32 +N )~ω\nfor some energy scale ω with N = 2n + l starting with N = 0. Note the eigenvalues are actually\nmultiple eigenvalues, because each N > 0, often referred to as a shell, corresponds to multiple\ncombinations of n and l. It is also important to keep in mind that each (n, l, j) orbital has a\nmultiplicity of 2j + 1, and thus each harmonic oscillator shell has a multiplicity of (N + 1)(N + 2)\nfor each type of nucleon, i.e., for protons and neutrons separately.\nThus, using the harmonic oscillator basis in combination with the Nmax truncation, the set of\nmany-body basis states (configurations) can be given by the following set of parameters\n1. the number of protons and neutrons, Z and N ;\n2. the quantum numbers M and parity P , discussed above;\n3. a many-body basis truncation parameter Nmax , defined below;\n4. the energy scale ω of the harmonic oscillator.\nThe truncation parameter Nmax sets an upper bound for the sum of 2n(ai ) + l(ai ) over all nucleons\ni = 1, 2, . . . , A, i.e.,\nA\nX\n\u0001\n2n(ai ) + l(ai ) ≤ N0 + Nmax ,\n(3)\ni=1\n\nwhere N0 is the minimum number of quanta for that nucleus given the Pauli exclusion principle.\nThe inequality (3) limits how large each n(ai ) and l(ai ) can become. Implicitly the limit on Nmax\nalso limits the underlying single-particle basis. Furthermore, since\n\u0001 or negative\nP parity is positive\ndepending on l being even or odd, even values and odd values of\n2n(ai ) + l(ai ) correspond to\nconfigurations with opposite parity. Thus, implicitly Nmax also selects parity when basis states are\nretained in decrements of two from Nmax .\nWe denote the set of configurations (many-body basis states) {a} by A. The size of A gives the\ndimension of the (sparse) matrix to be diagonalized; this dimension grows rapidly with increasing\nnumber of nucleons and with increasing Nmax ; see Fig. 1. In MFDn, we order single-particle states\nfirst by their energy levels (or equivalently, harmonic oscillator shell) as given by n and l. States\nin the same shell are further ordered by their angular momentum j, starting from the lowest j;\nand within the same orbital, labeled by (n, l, j), we order the single-particle states according to\nmj = −j, −j + 1, . . . , j − 1, j. The many-body basis states are first ordered by the values of\n\u0001\nPA\ni=1 2n(ai ) + l(ai ) , and then enumerated in lexicographical order based on this ordering of the\nsingle-particle states.\n2\n\nHere mj (ai ) and p(ai ) are used, respectively, to denote the mj and the parity p of the single-particle state with\nthe quantum numbers ai , and we will use a similar notation for the quantum numbers n, l, and j as well.\n\n5\n\n\f10\n\nnumber of nonzero matrix elements\n\nM-scheme dimension\n\n10\n\n10\n\n10\n\n10\n\n8\n\n6\n\n10\n10\n10\n\nLi\nBe\n10\nB\n12\nC\n14\nN\n16\nO\n18\nF\n20\nNe\n8\n\n6\n\n4\n\n2\n\n10\n\n14\n\n12\n\n10\n\n10\n\n6\n\n3-body\npotentials\n\n10\n\nLi\nBe\n10\nB\n12\nC\n14\nN\n16\nO\n18\nF\n20\nNe\n8\n\n2-body\npotentials\n\n8\n\n6\n\n0\n\n10 0\n\n2\n\n4\n\n6\n\n8\n\n10\nNmax\n\n12\n\n14\n\n16\n\n18\n\n10\n\n4\n\n10\n\n5\n\n6\n\n7\n\n10\n10\n10\nmatrix dimension\n\n8\n\n10\n\n9\n\n10\n\n10\n\nFigure 1: Dimensionality (left) and number of nonzero matrix elements in the lower triangular\nportion of the symmetric matrix (right) for light nuclei with the same number of protons and\nneutrons (i.e., Z = N ), with angular momentum projection M = 0 and fixed parity P .\n\n2.2\n\nTile and Block Structure of Ĥ\n\nLet us assume that H is an A-body Hamiltonian that contains d-body interactions, with d ≤ A.\nGiven two many-body basis states a and b belonging to A, it is evident that\nhΦa |H|Φb i = 0,\nif a and b differ by more than d single-particle states [11] after summing over the neutron and the\nproton differences. Therefore, the sparsity of the discretized matrix Hamiltonian Ĥ can in principle\nbe determined by examining the difference between each pair of (a, b) in A, once all configurations\nin A have been generated.\nHowever, going through all pairs of many-body basis states in an exhaustive manner is prohibitively expensive, given the cardinality of A; see Fig. 1. A more efficient scheme used in MFDn\nto determine the sparsity of Ĥ, is to first consider groups of many-body states based on their orbital\nquantum numbers (n, l, j) only, without considering mj . That is, many-body basis states that only\ndiffer in the mj values of each single-particle state are placed in the same group labeled by\n\b\ng(a) ≡ b ∈ A : n(ai ) = n(bi ), l(ai ) = l(bi ), j(ai ) = j(bi ), for i = 1, 2, . . . , A .\n(4)\n\nThis particular choice of grouping is physically natural because the set of many-body states belonging to the same g(a) have the same total single-particle energy when the many-body Hamiltonian\ncontains (or can be approximated by) the one-body Hamiltonian that generates the single-particle\nstates.\nThe size of these groups g(a)’s varies greatly, and can range from just a single M -scheme manybody state to many thousands of M -scheme many-body states. In order to improve load-balancing,\nit may be necessary to divide some groups labeled by g(a) into smaller groups, in particular for\nlarger nuclei, with A > 8. We can do this by subdividing the orbitals, that is, for example,\n\b\nmax\ng(a) ≡ b ∈ A : n(ai ) = n(bi ), l(ai ) = l(bi ), j(ai ) = j(bi ), mmin\n(ai )\n(5)\nj (ai ) ≤ mj (bi ) ≤ mj\nand consider these “split orbitals” as the building blocks for the groups of many-body states. Thus,\nwe may control the size of the largest groups.\n6\n\n\f0\n\n120\n100\n\n130\n200\n\n140\n300\n\n150\n\n400\n\n160\n\n170\n\n500\n\n180\n600\n\n190\n700\n\n200\n800\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n20\n\n800\n\n(a)\n\n30\n\n40\n\n50\n\n60\n\n70\n\n80\n\n90\n\n100\n\n110\n\n(b)\n\nFigure 2: On the left, sparsity structure of Ĥ for 6 Li at Nmax = 2, with 800 many-body basis\nstates, 92 groups of states, 1,826 nonzero tiles and 33,476 nonzero matrix elements in the lower\ntriangular portion of the symmetric matrix. On the right a more detailed plot of one CSB block\n(the (3,2)-block) of this matrix. Boundaries of CSB blocks and tiles are indicated by the solid and\ndashed lines, respectively.\nThe grouping of many-body basis states leads to a partitioning of Ĥ into many tiles. Each\ntile can be indexed by two group identification labels g(a) ≡ (g(a1 ), g(a2 ), . . . , g(aA )) and g(a0 ) =\n(g(a01 ), g(a02 ), . . . , g(a0A )), where g(ai ) denotes the (n, l, j) quantum numbers associated with the\nsingle-particle state ai . The dimension of the tile is determined by the sizes of g(a) and g(a0 ). In\nMFDn, we first perform pairwise comparisons of g(a) and g(a0 ) to determine (potentially) nonzero\ntiles. If g(ai ) 6= g(a0j ) for more than d single-particle states i and j, the entire tile indexed by g(a)\nand g(a0 ) is zero. If g(ai ) 6= g(a0j ) for up to d single-particle states, then the tile generally contains\nnonzero matrix elements.\nNote that the sparsity of these nonzero tiles depends on the number of single-particle states\nthat differ between g(a) and g(a0 ). The diagonal tiles, with g(a) = g(a0 ), are generally the least\nsparse (most dense) tiles, whereas tiles for which g(a) and g(a0 ) differ by exactly d particles are\nthe most sparse (least dense). In principle, additional blocking techniques such as those described\nin [35] can be used to identify potentially nonzero tiles, or nonzero matrix elements within the tiles,\nand further reduce the number of pairwise comparisons of (groups of) many-body basis states.\nIn Fig. 2, we show the sparsity pattern for a small case, 6 Li, with three protons and three\nneutrons, truncated to Nmax = 2. The M = 0 basis space dimension is 800, and the number\nof groups is 92. Thus, there are in principle 4,186 tiles in half of the symmetric matrix, and\n320,400 matrix elements. The number of (potentially) nonzero tiles is 1,826, corresponding to a\n“tile-sparsity” of 0.44, and the total number of nonzero matrix elements within these nonzero tiles\nis 33,476, corresponding to a sparsity of 0.104.\nA well known technique for improving the performance of sparse matrix computation on modern\nprocessors is blocking, i.e., partitioning the matrix into a number of smaller blocks that can be\nloaded into fast memory (e.g., level-2 cache) and processed one block at a time. It is tempting to\nuse the tile structure of Ĥ to naturally block the matrix. However, this is not the best strategy.\n7\n\n\fFrom Fig. 2(b), it is evident that the tiles are not uniform in size. For the small problem size\ndemonstrated in that figure, some tiles are as small as one by one (i.e., containing a single matrix\nelement) whereas the largest one is 52 by 52. The average group size is 8.7 for this particular\nexample. For the problems of interest, which are much larger in basis dimension, the smallest tiles\nwill also be as small as one by one, but the largest tiles can have dimensions of thousands or even\nmore. Keeping track of blocks of small sizes introduces significant overhead and the large variation\nin block sizes can make load balancing highly nontrivial. Therefore, we introduce an additional\nlevel of blocking to group g(a)’s into “sets of groups” containing up to β many-body basis states\nfor some modest β value (e.g., β = 4000). We shall refer to the resulting blocks as CSB blocks,\nnamed after the sparse matrix format we adopt as described in Section 4.2. Such a block structure\nis indicated by the solid vertical and horizontal lines in Fig. 2(a) for β = 100. Although it is not\nobvious from the figure, all block boundaries do coincide with tile boundaries. This is ensured by\nnot enforcing a constant CSB block size, and allowing the aggregate size of the sets of groups in\neach block to be slightly less than β. In addition, we impose block boundaries between subspaces\ncorresponding to different Nmax truncations. In the particular example shown in Fig. 2, β is set\nto 100, and the average size of obtained CSB blocks is 80 by 80. There is also a block boundary\nbetween the Nmax = 0 and Nmax = 2 subspaces (in this case the Nmax = 0 subspace has a dimension\nof 10, the left-most block column).\nThe ideal value of β and corresponding ideal largest group size strongly depend on both the\nnuclear physics problem (number of nucleons and the specific basis space truncation) and on the\ncomputer hardware (available cache sizes, memory bandwidth, number of OpenMP threads etc.),\nand has been discussed in detail in our previous work [1, 2]. The block partitioned matrix is stored\nin a compressed sparse block (CSB) format proposed in [9]. We will discuss the CSB data structure\nin Section 4.\n\n3\n\nThe LOBPCG Algorithm\n\nMost CI codes used in nuclear physics, including MFDn, use a Lanczos based eigensolver with\nfull orthogonalization to ensure numerical stability. There are three main advantages of using the\nLOBPCG algorithm over the existing Lanczos solver. First, the LOBPCG algorithm allows us to\nmake use of preconditioners to be discussed below. With good preconditioners, the convergence\nof the algorithm can be accelerated significantly as we demonstrate in Section 5. Second, the\nLOBPCG algorithm can make use of good approximations to different eigenvectors as the starting\nguess. In contrast, the Lanczos algorithm can only use one of these vectors as the starting vector.\nFinally, the implementation of the LOBPCG algorithm naturally depends on kernels with high\narithmetic intensities, i.e., multiplication of a sparse matrix with multiple vectors (SpMM) and\nlevel-3 BLAS operations to update the block vectors.\nIn this section, we first review the basic LOBPCG algorithm and then discuss how to make use\nof the special properties of the Hamiltonian in MFDn to accelerate the eigensolver.\n\n3.1\n\nThe basic LOBPCG algorithm\n\nWe denote the eigenvalues of an D × D discretized Hamiltonian Ĥ by λ1 ≤ λ2 ≤ · · · ≤ λD and the\ncorresponding eigenvectors by x1 , x2 , . . . , xD . It is well known that X∗ = [x1 , x2 , . . . , xk ] is the\nsolution to the trace minimization problem\nmin trace(X T ĤX).\n\nX T X=Ik\n\n8\n\n(6)\n\n\fThe locally optimal block preconditioned conjugate gradient (LOBPCG) algorithm developed by\nKnyazev [17] seeks the solution of (6) by updating the eigenvector approximation X (i) according\nto\nX (i+1) = X (i) G1 + R(i) G2 + P (i) G3 ,\n(7)\nwhere R(i) is the projected gradient\nh\n\u0001T i\nR(i) = I − X (i) X (i)\nĤX (i) ,\n\n(8)\n\nand P (i) is the previous search direction defined recursively as\n\nP (i) = R(i−1) G2 + P (i−1) G3 ,\nwith P (0) = 0. The superscript (i) denotes the iteration number. The k × k matrices G1 , G2 , and\nG3 are chosen in each iteration i to minimize the objective function in (6) within the subspace\nspanned by columns of\n\u0002\n\u0003\nY ≡ X (i) R(i) P (i) .\nThe solution to this subspace minimization problem can be obtained from the solution of the\nfollowing generalized eigenvalue problem\n(Y T ĤY )G = (Y T Y )GΘ,\nwhere\n\n(9)\n\n \nG1\nG = G2 \nG3\n\nconsists of eigenvectors associated with the smallest eigenvalues of the matrix pencil (Y T ĤY, Y T Y ).\nThese eigenvalues appear in the diagonal matrix Θ.\nTo accelerate the convergence of the algorithm, preconditioning is adopted in practice. More\nprecisely, let W (i) = T −1 R(i) , where T is the preconditioner. Then\n\u0002\n\u0003\nY ≡ X (i) W (i) P (i)\nis used to construct the projection subspace. We also need to correspondingly replace R(i) by\nW (i) in (7). The main steps of the LOBPCG algorithm are outlined in Algorithm 1. A practical implementation of the LOBPCG algorithm requires a more careful treatment of the subspace\nspan(Y ). For instance, it is advisable to choose k somewhat larger than the actual number of\ndesired eigenpairs. Orthogonalization on Y is also important for numerical stability. We refer the\nreaders to [12, 14] for techniques on a robust implementation.\nIn the rest of this section, we discuss how to make use of the special properties of the nuclear\nHamiltonian in the context of MFDn to accelerate the eigensolver.\n\n3.2\n\nPreconditioner\n\nThe convergence rate of the LOBPCG algorithm is related to the condition number of Ĥ. The\nmethod can be accelerated when a good preconditioner is chosen in step 6 of Algorithm 1. From an\noptimization point of view, the use of a symmetric positive definite preconditioner T = LLT where\nL is a lower triangular Cholesky factor of T , can be considered as a change of variable Y = L−1 X\napplied to (6) to yield a preconditioned minimization problem\nmin\n\nY T (LT L)Y =Ik\n\ntrace(Y T L−T ĤL−1 Y ).\n9\n\n(10)\n\n\fAlgorithm 1 The locally optimal block preconditioned conjugate gradient (LOBPCG) algorithm.\nInput: Ĥ = Ĥ T ∈ RD×D , X (0) ∈ RD×k satisfying (X (0) )T X (0) = Ik .\nOutput: The k smallest eigenpairs of Ĥ, i.e., X ∈ RD×k and Θ ∈ Rk×k such that ĤX = XΘ.\n1: Set Θ(0) to the Rayleigh quotient of X (0) .\n2: Set P (0) = [].\n3: for i = 1, 2, . . . do\n4:\nSet R(i) = ĤX (i) − X (i) Θ(i) .\n5:\nCheck convergence.\n6:\nSet W (i) = T −1\bR(i) .\n7:\nOrthogonalize X (i) , W (i) , P (i) and solve the projected eigenvalue problem on the subspace\n\b\nspan X (i) , W (i) , P (i) .\n8:\nSet Θ(i+1) to the k × k diagonal matrix with the k smallest Ritz values as its diagonals.\n9:\nChoose the Ritz vectors correspond to the k smallest Ritz values as X (i+1) .\n10:\nOrthogonalize X (i+1) − X (i) against X (i) as P (i) .\n11: end for\nIf the condition number of L−T ĤL−1 is smaller than that of Ĥ while T = LLT itself is not too\nill-conditioned, then the search directions generated in LOBPCG are likely to point more directly\ntowards the global minimizer of (10) [13].\nBecause the LOBPCG algorithm is closely related to the inexact Newton type methods such as\nthe Jacobi–Davidson algorithm [32], the use of a preconditioner can be viewed as a way to include\ninexact Newton correction in a search space from which approximate eigenpairs of Ĥ are extracted.\nSuch a viewpoint allows the choice of T to be made more flexibly. In particular, we can choose\ndifferent T ’s for different columns of R in step 6 of Algorithm 1. We use Tj to denote these different\nchoices of T . Each Tj can be chosen to approximate Ĥ − µj I, where µj is an approximation to jth\neigenvalue λj . Theoretical analysis for this type of shift-and-invert based preconditioners can be\nfound in [5, 25, 26].\nIn practical computations the preconditioning strategy is often problem dependent. When\nchoosing the preconditioner T , we need to balance the objectives of convergence acceleration with\nthat of keeping the cost of step 6 of Algorithm 1 low. We would like to choose a preconditioner that\ncan significantly reduce the number of LOBPCG iterations required to reach convergence without\nintroducing significant additional computational cost per iteration. The types of preconditioners\nwe have examined include the following:\n• Tj = K − µj I, where K is the kinetic energy part of Ĥ. To apply such a preconditioner,\nwe need to solve linear equations of the form (K − µj I)zj = rj , for j = 1, . . . , k. Because\nK − µj I cannot be efficiently factored, these linear equations need to be solved iteratively.\nEach step of the iterative solution requires multiplying K with multiple vectors. Because\nthe cost of such a multiplication is not significantly lower than that of multiplying Ĥ with\nmultiple vectors, the cost of applying the preconditioner is relatively high. As a result, the use\nof such a preconditioner typically does not result in an overall reduction in computational time\nalthough the number of LOBPCG iterations required to reach convergence can be reduced\nslightly. It is possible to drop small entries of K to make the preconditioner sparser, thereby\nreducing the cost of applying the preconditioner. However, it is not easy to determine an\nappropriate threshold for dropping entries of K. We found that dropping too many entries\nof K can actually result in an increase in the number of LOBPCG iterations.\n\n10\n\n\fo\nn\n• Tj = diag ĤNmax −2 − µj I, I where ĤNmax −2 is the Hamiltonian constructed from a smaller\nconfiguration space. Although this type of preconditioner makes intuitive sense, our numerical\nexperiments indicate that it is not effective, and often results in an increase in the number of\nLOBPCG iterations.\n• Tj = H̃ − µj I, where H̃ is a block diagonal approximation of Ĥ. Because the diagonal blocks\nof Ĥ contains matrix elements that correspond to Slater determinants that interact strongly\nthrough the many-body Hamiltonian, we expect H̃ to capture main spectral properties of Ĥ.\nWhen the sizes of the diagonal blocks are reasonably small, this preconditioner is cheap to\napply. Several block partitioning strategies can be used to create the diagonal preconditioner.\nFor example, we can use the diagonal tiles, diagonal CSB blocks, or the local Hamiltonian\nassigned to each diagonal processor (see Section 4.1) to construct this preconditioner. In\nthe extreme case, we can also use just\nn the diagonal\no elements of Ĥ to construct a diagonal\npreconditioner. If we use H̃ = diag H̃[1] , . . . , H̃[m] to denote the block diagonal preconditioner, where m is the number of diagonal blocks and varies based on the particular strategy\nused, then applying the preconditioner amounts to solving m independent linear systems of\nequations of the form\n(H̃[i] − µI)W[i] = R[i]\n(11)\nfor i = 1, . . . , m, where W[i] and R[i] are, respectively, the ith chunk of W and R partitioned\nconformally with the partitions H̃[i] . When the block size of H̃[i] is very small, (11) can be\nsolved by a direct method. Otherwise, we can use an iterative method such as the full orthogonal method (FOM) [27] or the generalized minimal residual method (GMRES) [29]. Our\nexperiments show that two or three iterations are typically sufficient to accelerate the convergence of LOBPCG. Using larger diagonal H̃[i] blocks may results in slightly fewer LOBPCG\niterations, but this clearly comes at the expense of increased computational costs during the\nFOM or GMRES iterations which require multiplications of H̃[i] with multiple vectors. We\nfound that a good compromise between reducing the number of LOBPCG iterations and\nkeeping the cost of applying the preconditioner low per iteration is to use the diagonal tiles\nassociated with many-body groups in the block diagonal approximation. To achieve faster\nconvergence for LOBPCG, we allow different shifts µj for different right hand sides in (11).\n\n3.3\n\nThe initial guess for LOBPCG\n\nIn principle, almost any linearly independent set of vectors can be used as the initial guess of\nLOBPCG. However, when good approximations to several eigenvectors are available, the LOBPCG\nalgorithm allows us to use all these approximate eigenvectors as the initial guess. Therefore it is\nbeneficial to find good approximate eigenvectors before applying the eigensolver. Fortunately, in\nMFDn, there are a number of efficient ways to construct reasonably good approximate eigenvectors.\nIn the following we discuss several choices, all based on approximations to the Hamiltonian. Some\ntechniques can be used not only for MFDn, but also for more general configuration interaction\ncalculations.\n3.3.1\n\nConstructing from a smaller configuration space\n\nLet Ĥ be partitioned as\n\n\u0015\nĤ11 Ĥ12\nĤ =\n,\nĤ21 Ĥ22\n\u0014\n\n11\n\n\fwhere\nHamiltonian corresponding to a smaller configuration space obtained\n\u0001\nPAĤ11 is the discretized\nby i=1 2n(ai ) + l(ai ) ≤ N0 + Nmax − 2. As the dimension of Ĥ11 is much smaller compared to\nthat of Ĥ, computing the k smallest eigenpairs of Ĥ11 requires relatively low cost. Suppose that we\nhave already computed Ĥ11 X1 = X1 Θ1 , for instance, by using the LOBPCG algorithm described\nabove. Approximate eigenvectors of Ĥ can then be obtained by padding X1 with zeros, i.e.,\n\u0014 \u0015\nX1\n(0)\nX =\n.\n(12)\n0\nThen\n\n(0)\n\nkrj k\n\n(0)\nkĤkkxj k\n\n(0)\n\n=\n\n(0)\n\nkĤxj − xj θj k\n(0)\nkĤkkxj k\n\n(0)\n\n=\n\nkĤ21 xj k\n\n(0)\nkĤkkxj k\n\n≤\n\nkĤ21 k\nkĤk\n\n=: \u000f.3\n\nThe ratio \u000f is much less than one when the truncation error of the smaller configuration space is\nsmall. Thus (12) provides a good initial guess for LOBPCG.\nAn alternative but computationally more expensive approach is to set\n\u0014 \u0015\nX1\n(0)\nX =\n,\n(13)\nY1\nwhere the jth column of Y1 is chosen as the jth column of (Ĥ22 − θj I)−1 X1 . The relative residual\nnorm is then bounded by\n(0)\n\nkrj k\n\n(0)\nkĤkkxj k\n\n(0)\n\n=\n\nT (Ĥ − θ I)−1 Ĥ x k\nkĤ21\n22\nj\n21 j\n(0)\nkĤkkxj k\n\n≤\n\nkĤk 2\n\u000f = O(\u000f2 ),\ngapj\n\nwhere gapj := k(Ĥ22 − θj I)−1 k−1 . Therefore, (13) is in general better than (12). The price to pay\nis that (13) requires solving linear systems to obtain Y1 .\nIn practice, we adopt (12) as the initial guess for its lower computational cost. However,\nwe remark that the block diagonal preconditioning\nprovides\nn\no us an opportunity to achieve better\napproximation quality. Suppose that Tj = diag Ĥ11 , Ĥ22 − θj I is used as the preconditioner for\n(0)\n\nrj\n\nwith the initial guess (12). Then it can be verified that\nW\n\n(0)\n\n\u0014\n\n\u0015\n0\n=−\n.\nY1\n\n\b\nAs the alternative initial guess (13) belongs to span X (0) , W (0) , one step of LOBPCG achieves\nthe O(\u000f2 ) accuracy. Although, compared to Tj , the tile-partitioned preconditioner leaves out more\noff-diagonal blocks from Ĥ − θj I, we still expect some accuracy improvement of the approximate\neigenvectors after one step of LOBPCG.\nFinally, we remark that when performing LOBPCG on a smaller configuration subspace, we\ncan actually utilize LOBPCG recursively on an even smaller configuration space to produce an\ninitial guess. Such a recursive use leads to a multi-level LOBPCG algorithm for CI calculations. In\nMFDn, we choose to perform three LOBPCG runs, corresponding to configuration spaces specified\nby Nmax − 4, Nmax − 2, and Nmax . For the smallest configuration space specified by Nmax − 4 we\nfind that LOBPCG with random initial guess is adequate for our purpose.\n3\n\nWe use the lower case variables, e.g., xj , to denote the jth column of X.\n\n12\n\n\fP1,4 P1,5\n\nP1,1\n\nP2,5\n\nP2,1 P2,2\nP3,1 P3,2 P3,3\nP4,2 P4,3 P4,4\n\nP5,3 P5,4 P5,5\nFigure 3: A triangular processor grid with nd = 5 using the BCM mapping in the current implementation of MFDn. Highlighted matrix blocks are stored on corresponding processors.\n3.3.2\n\nUse the eigenvectors of a closely related Hamiltonian\n\nIn addition to the multi-level LOBPCG method discussed above, MFDn also allows other types of\ninitial guesses to be used for LOBPCG. These types of initial guesses include:\n• eigenvectors of a simplified Hamiltonian that includes an external field;\n• eigenvectors of a Hamiltonian with a different type of nuclear potential or a different set of\nparameters.\nWe shall discuss them in more detail in Section 5.\n\n4\n\nA High Performance LOBPCG Implementation\n\nIn this section, we briefly describe a number of implementation issues that are critical for achieving\nhigh performance. In Section 4.1 we summarize the partitioning of the Hamiltonian and LOBPCG\nvectors, as well as the collective communication operations involved in a distributed memory setting.\nThe algorithms used for this purpose are similar to our Lanczos based implementation which is\ndescribed in more detail in our previous work [3, 4]. Then in Sections 4.2 through 4.4, we describe\ndetails specific to our LOBPCG implementation.\n\n4.1\n\nThe distribution of Ĥ and vector segments\n\nSince Ĥ is symmetric and the number of nonzero elements in Ĥ increases rapidly as the dimension\nof Ĥ increases, we store only the lower triangular part of the Hamiltonian. We would like to\ndistribute Ĥ among different processing units in such a way that each processing unit will perform\nroughly the same number of operations in parallel sparse matrix computations. In [3] we developed\na grouping and mapping scheme, which we refer to as balanced column major (BCM) mapping.\nFig. 3 shows the placement of 15 processors on a 5 × 5 processor grid using the BCM mapping.\nThe BCM scheme limits the number of processors in each row or column communication group to\n(nd + 1)/2.4 We refer to [3] for detailed discussions.\nThe distribution of dense blocks of LOBPCG vectors requires two different schemes, depending\non the type of operations applied to these vectors. When performing the SpMM operation U = ĤW ,\n4\n\nHere we require nd to be an odd number so that (nd + 1)/2 is an integer.\n\n13\n\n\fw1,4 w1,5\n\nw1,1\n\nw1\n\nw2,5\n\nw2,1 w2,2\n\nw2\nscatter\n\nw3\n\nw3,1 w3,2 w3,3\nw4,2 w4,3 w4,4\n\nw4\n\nw5,3 w5,4 w5,5\n\nw5\n\nFigure 4: Scatter a distributed vector w from nd diagonal processors (nd = 5 here) to all processors.\nEach diagonal processor scatters its portion of w to (nd + 1)/2 processors belonging to the same\ncolumn processor group.\nwe partition the dense blocks of vectors W and U conformal to the partition of Ĥ, and distribute\nthem among diagonal processors. Under the BCM scheme, we denote the submatrix of Ĥ assigned\nto the processor Pi,j by Ĥi,j . Since we store only half of the symmetric Hamiltonian, each processor\nis responsible for sparse matrix computations related to both the submatrix assigned to it, as well\nas the transpose of this submatrix. Then each processor needs to performs two local SpMM’s of the\nT W . Once local SpMM operations are completed, two reductions\nform Ui = Ĥi,j Wj and Uj = Ĥi,j\ni\nare required to aggregate partial products Ui and Uj into the global result vector U .\nAlthough the multiplication of Ĥ with a block of vectors constitutes the major computational\nwork in each LOBPCG iteration, the cost of other linear algebra operations, such as the projected\ngradient computation defined in (8), cannot be ignored in a high performance implementation. If\nW and U = ĤW are partitioned by rows and distributed among nd diagonal processors only, the\nnumber of processors that can effectively be used to perform the dense matrix–matrix multiplications required in (8) is limited to nd . To maximize the level of concurrency, we further divide\neach vector block of Wi and Ui on Pi,i and scatter these vector segments among (nd + 1)/2 processors in the column communication group of Pi,i ; see Fig. 4. As a result, the dense matrix–matrix\nmultiplications of (7), (8), and (9) can be carried out using all nd (nd + 1)/2 processors available.\nTo summarize, the data distribution scheme discussed above means that each LOBPCG step\ninvolves several distinct collective communication patterns, as illustrated in Fig. 5, as well as local\ncomputation. Schematically, we have\n1. Before performing U = ĤW , the segments of W which are distributed among column processor groups are gathered onto the diagonal processor within each column communication\ngroup;\n2. The diagonal processors broadcast the gathered submatrices across their row and column\ncommunication groups in preparation for the distributed SpMM computations. As a result,\neach processor (with the exception of the diagonal processors) receives two submatrices of W .\n3. After processor Pi,j performs two SpMM’s, one product is reduced among its column communication group onto the diagonal processor Pj,j , and the other one is reduced among the\nrow communication group onto the diagonal processor Pi,i .\n4. Each diagonal scatters the final result among (nd +1)/2 processors within its column communication group in preparation for the parallel dense matrix–matrix multiplications of LOBPCG\nshown in Eqs. (7), (8), and (9).\n14\n\n\fWe remark that in practice we can perform step 1 and the broadcast among the column communication group of step 2 by a single collective MPI call. Next, we can overlap the communication\nacross the row communication group in steps 2 and 3 with the local SpMM and transpose SpMM\noperations. And finally we can perform the reduction operation among the column communication\ngroup of step 3 and the subsequent scatter by a single collective MPI call.\n\n4.2\n\nData structures for storing submatrices of Ĥ and local vector blocks\n\nAs described above, the distribution of the Ĥ matrix, block vectors and collective communication\npatterns in our LOBPCG implementation are similar to those of the Lanczos solver from the earlier\nversions of MFDn. A major difference between the two eigensolvers is that LOBPCG requires\nworking with multiple vectors, as opposed to the single vector iterations performed in the Lanczos\nalgorithm. In fact, this difference leads to the third advantage of the LOBPCG over Lanczos\ndiscussed in Section 3.1—with LOBPCG, significant performance improvements can potentially be\nachieved by exploiting the high arithmetic intensity of the main computational kernels, namely\nthe sparse matrix computations and the update of LOBPCG vector blocks. As we shall show in\nSection 5, despite the use of an effective preconditioner and good initial guesses, the total number\nof sparse matrix vector operations required by LOBPCG can still be significantly higher than that\nof Lanczos. Therefore the choice of data structures that will allow us to effectively exploit this\nthird advantage is critical for the overall performance of LOBPCG.\nTo store the local block of vectors in LOBPCG, we adopt the row major storage. Compared\nto the column major alternative, row major storage is more convenient for the collective communications described in the previous subsection because in this scheme, data to be transmitted to\nother processes are stored contiguously in memory. More importantly, row major storage of vector\nblocks provides good data locality for performing SpMM—for each nonzero matrix element, several required entries of the vector blocks can be loaded into the cache with a single load. This is\nespecially important for modern architectures with limited cache space per core, and a widening\nperformance gap between the processor and the memory system.\nCompressed sparse row (CSR) format [33] is one of the most commonly used data structures\nfor sparse matrices, largely due to its simplicity and good performance in general. However, for\nMFDn, it has two important pitfalls. First, the CSR format cannot exploit the tiled sparsity\nstructure in local MFDn matrices, which leads to poor cache utilization. The second and more\nT W in parallel within a single MPI task\nsignificant issue is that performing the multiplication Ĥi,j\ni\nrequires accumulating the elementwise products in an array that must be shared and synchronized\namong different threads. The synchronization overhead severely degrades the performance of this\nmultiplication [2].\nTo achieve high performance while retaining a small memory footprint, we use the compressed\nsparse block (CSB) format proposed by Buluç et al. [9] to store Ĥi,j submatrices locally. Each\nCSB block is independently addressable through a 2D array of pointers. The block partitioning\nin CSB is an excellent match with the tiled structure of MFDn matrices that results from the\ngrouping of many-body states. The CSB format does not specify how nonzeros within each block\nare stored—we use the sparse coordinate format where the local row and column offsets for each\nnonzero element are stored along with the nonzero value. An important requirement to ensure a\nsmall memory footprint in this case is to keep β < 216 , so that 16-bit (instead of 32-bit) indexing\ncan be used for row and column offsets. For MFDn matrices, we empirically determined the ideal\nvalue of β to be from 2,000 to 4,000 depending on the size of the vector blocks in LOBPCG.\n\n15\n\n\fw1,4 w1,5\n\nw1,1\n\nw1\n\nw2,5\n\nw2,1 w2,2\n\nw2\ngather\n\nw3,1 w3,2 w3,3\n\nw3\n\nw4,2 w4,3 w4,4\n\nw4\n\nw5,3 w5,4 w5,5\n\nw5\n(column)\n\n(row)\n\nbroadcast\nw4\n\nw1\nw1\n\nw2\n\nw1\n\nw2\n\nw3\n\nw2\n\nw3\n\nw4\n\nw3\n\nw4\n\nw1\n\nw5\n\nw1\n\nw5\n\nw2\n\nw2\n\nw3\n\nw3\n\nw3\n\nw4\n\nw4\n\nw4\n\nw5\n\nw5\n\nw5\n\nw1\nw2\n\nw5\n\nSpMV/SpMM\n\nT w\nĤ1,1\n1\n\nĤ1,4 w4 Ĥ1,5 w5\n\nĤ1,1 w1\n\nT w Ĥ T w\nĤ2,1\n2 2,2 2\n\nĤ2,5 w5\n\nĤ2,1 w1 Ĥ2,2 w2\n\nT w Ĥ T w\nĤ1,4\n1 1,5 1\n\nT w\nĤ2,5\n2\n\nT w Ĥ T w Ĥ T w\nĤ3,1\n3 3,2 3 3,3 3\n\nĤ3,1 w1 Ĥ3,2 w2 Ĥ3,3 w3\n\nT w Ĥ T w Ĥ T w\nĤ4,2\n4 4,3 4 4,4 4\n\nĤ4,2 w2 Ĥ4,3 w3 Ĥ4,4 w4\n\nT w Ĥ T w Ĥ T w\nĤ5,3\n5 5,4 5 5,5 5\n\nĤ5,3 w3 Ĥ5,4 w4 Ĥ5,5 w5\n\n(row)\n\n(column)\n\nreduce\n\nu1,4 u1,5\n\nu1,1\n\nu1\n\nu2,5\n\nu2,1 u2,2\n\nu2\nscatter\n\nu3\n\nu3,1 u3,2 u3,3\nu4,2 u4,3 u4,4\n\nu4\n\nu5,3 u5,4 u5,5\n\nu5\n\nFigure 5: Collective communication in SpMV/SpMM operations.\n\n16\n\n\f4.3\n\nThe construction and application of the preconditioner\n\nAs discussed in Section 3.2, we prefer tile-partitioned block diagonal preconditioners in MFDn in\nwhich the diagonal tiles of the Ĥ matrix are used. Hence, when constructing Ĥ, we set up a\nseparate sparse matrix H̃ that consists of these diagonal tiles, and distribute it across the processor\ngrid conformal to the distribution of the vector blocks described in Section 4.1. Consequently, all\nlinear systems in the preconditioning stage are completely local, and can be solved without MPI\ncommunication.\nWithin each MPI process, the local linear system can be further decoupled according to the\nblock diagonal matrix structure of the preconditioner. These decoupled linear systems are assigned\nto different threads and solved in parallel in our hybrid MPI/OpenMP parallel implementation.\nFurthermore, since each linear system contains multiple right hand sides, multiple calls to the FOM\nsolver are needed to obtain the approximate solution. To increase arithmetic intensity, we fuse these\nFOM calls together so that in each iteration we can use a single SpMM instead of a number of\nSpMV’s to construct different Krylov subspaces associated with different right-hand sides at the\nsame time. Up to three FOM iterations are performed for all right-hand sides.\nFinally, we remark that we make several conservative choices for the preconditioning to enhance\nthe robustness:\n1. No preconditioning is applied for the first three LOBPCG iterates in any case;\n2. We do not apply the preconditioners if the relative residual norm of (λ1 , x1 ) is above 10−1 ;\n3. If (λi , xi ) is nearly converged, we set µi = θi − 2kri k/kxi k;\n4. If (λi , xi ) is far from converged while (λ1 , x1 ), . . ., (λi−1 , xi−1 ) are not, we set µi = · · · = µk =\nµi−1 .\n\n4.4\n\nA hybrid MPI/OpenMP parallel implementation\n\nThere are mainly four types of matrix operations in the LOBPCG algorithm:5\n1. Preconditioning W ← ĤR;\n2. Sparse matrix–matrix multiplication U ← ĤW ;\n3. Block inner product X T Y ;\n4. Linear combination X ← Y G.\nIn addition to the concurrency introduced by distributing the ingredients of the eigensolver among\ndifferent processors, thread level parallelism available through OpenMP can be added to all computations performed within each MPI process. Exploiting shared memory parallelism reduces the\ninternode communication volume [4], as well as the total memory footprint of MFDn [34].\n5\n\nAs a remark, in MFDn we perform the first two types of operations in single precision because Ĥ is stored using\nsingle precision floating point numbers, while the latter two types of operations are performed in double precision to\nenhance the numerical stability of LOBPCG.\n\n17\n\n\fPreconditioning In the preconditioning stage, the computational work involves solving a set of\nsmall linear systems approximately. As these small linear systems are completely independent, they\ncan be assigned to different threads and solved in parallel. In our implementation, we exploit this\nthread level parallelism, and let each thread perform a sequential (approximate) linear solve with\nmultiple right-hand sides. We adopt OpenMP’s dynamic scheduling option to resolve potential\nload imbalances due to different sizes of the linear systems involved.\nSparse Matrix Multiple Vector Multiplication (SpMM) The SpMM operation requires\nT W . As mentioned above, each CSB block that stores nonzero maperforming both Ĥi,j Wj and Ĥi,j\ni\ntrix elements is independently addressed through a 2D array of pointers. Hence, the multiplication\nT W can\nĤi,j Wj can be performed by processing this 2D array by rows, while the multiplication Ĥi,j\ni\nbe performed by processing the same 2D array by columns. The CSB format automatically enables\ncache blocking for each tile, thereby allowing us to achieve excellent performance [2].\nFor thread parallelism, we parallelize the loops over the 2D pointer array to perform Ĥi,j Wj\nT W with OpenMP. Let us assume that the local matrix Ĥ\nand Ĥi,j\ni\ni,j has been partitioned into `i\nrow blocks, corresponding to `i groups. Each row block contains `j column blocks, also conformal\nto the tile partition along the columns. When performing Ĥi,j Wj , different threads handle different\nrow blocks in parallel. As the size of row blocks and the number of nonzeros contained within\neach block vary, we adopt dynamic scheduling to improve load balance among different threads.\nT W , thread level parallelism is exploited at\nSimilarly, when performing the transpose operation Ĥi,j\ni\nthe level of column blocks. The ability to easily traverse the CSB blocks in column major order\nallows us to avoid any race conditions while performing the transpose SpMM operation. Unlike\nT W with a\nthe case with CSR storage, we achieve almost identical performance for Ĥi,j Wj and Ĥi,j\ni\nthread-parallel CSB implementation [2].\nBlock inner product and linear combination The dense matrix–matrix operations of LOBPCG\ngiven in Eqs. (7), (8), and (9) that involve vector blocks W (i) , R(i) , and P (i) can be described as\nblock inner products and linear combinations. For the purposes of this subsection, we refer to these\nvector blocks as X and Y , generically.\nThe block inner products X T Y are parallelized across MPI processes as local block inner products XiT Yi followed by a reduce operation. The local block inner product operation involves two\n(extremely) tall-and-skinny matrices Xi and Yi . While high performance BLAS libraries can be used\nfor this purpose, Intel’s MKL and Cray’s LibSci libraries exhibit poor performance, most likely due\nto the unusual shape of the matrices involved. Therefore we implemented a custom thread-parallel\nkernel for block inner products, where we further partition the local tall-and-skinny matrices into\nrow blocks, and make several sequential library GEMM calls with these small rectangular matrices.\nEach small sequential GEMM call is assigned to a different thread using dynamic scheduling, and\npartial results from each thread is aggregated at the end, again using thread-parallelism. By doing\nso, we achieve 1.7× to 10× speedup over performing the block inner products as direct calls to the\nlibrary GEMM implementations [1].\nThe operation of linear combination Y = XG, where G is a small square matrix whose rank is\nequal to the number of columns in X, is simpler than the block inner product because no interprocess\nMPI communication is needed. Similar to the local block inner product, we implemented a custom\nthread-parallel linear combination kernel that partitions the local tall-and-skinny matrix Xi into\nrow blocks and assigns the small linear combination operations to different threads. We observed\nonly small gains in this case over the default GEMM implementation in MKL and LibSci [1].\n\n18\n\n\fTable 1: List of the test cases. Top four cases are chosen with the two-body interaction; bottom\nfour cases include a three-body interaction.\nNuclei Nmax interaction dimension of Ĥ # of nonzeros in Ĥ # of cores\n9 Li\n8\n2-body\n37,145,993\n24,354,119,738\n28 × 6\n10 B\n8\n2-body\n159,953,284\n123,638,482,768\n120 × 6\n8 Be\n10\n2-body\n187,304,858\n206,509,001,984\n276 × 6\n7 Li\n12\n2-body\n252,281,462\n400,190,275,118\n496 × 6\n6 Li\n8\n3-body\n1,578,624\n23,402,203,878\n28 × 6\n8 He\n8\n3-body\n7,463,678\n125,380,883,295\n120 × 6\n10 He\n8\n3-body\n15,631,854\n247,561,956,281\n276 × 6\n8 Li\n8\n3-body\n17,684,028\n399,084,871,767\n496 × 6\n\n5\n\nNumerical Results and Computational Performance\n\nIn the following, we present computational examples performed on Edison, a Cray XC30 system\nwith 5576 computational nodes, at the National Energy Research Scientific Computing Center\n(NERSC).6 Each computational node has 24 cores, located on two twelve-core Intel “Ivy Bridge”\n2.4 GHz processors, and has 64 GB DDR3 1866 MHz memory. Each physical core has its own L1\nand L2 caches, with 64 KB (32 KB instruction cache, 32 KB data) and 256 KB, respectively, and\ncan run one or two user threads (i.e., hyperthreading). A 30 MB L3 cache is shared by twelve cores\non the “Ivy Bridge” processor. The computational nodes on Edison are connected by Cray Aries\nnetwork with Dragonfly topology with 23.7 TB/s global bandwidth. The MFDn code is compiled\nwith Intel Compiler version 15.0.1, and linked with the Cray Scientific Libraries package (LibSci)\nversion 13.3.0 and Cray’s MPI library version 7.3.1. We run the code using four MPI processes per\nnode, and six OpenMP threads per MPI process—no hyperthreading is utilized.\nWe run MFDn on eight test cases as shown in Table 1, using both Lanczos and LOBPCG\nsolvers. As indicated in the third column of Table 1, we select four test cases with the two-body\nnucleon–nucleon interaction alone and four test cases that include a three-body interaction. Each of\nthe four cases is paired in computational intensity as indicated by the number of nonzero elements\nin the lower triangular portion of Ĥ. The eight lowest eigenvalues and the associated eigenvectors\nare calculated for each test case. The convergence criterion for the eigensolvers is that the relative\nresidual norm drops below 10−6 .\n\n5.1\n\nImpact of preconditioning and initial guess in LOBPCG\n\nTo demonstrate how the techniques discussed in Section 3 help improve the efficiency of LOBPCG,\nwe perform several runs for the 7 Li example with different settings: with/without preconditioning,\nand with a good initial guess from a smaller configuration space or with a random initial guess.\nWhen adopting a good initial guess from a smaller configuration space, we run LOBPCG with\nNmax = 8 and Nmax = 10, and use the results as initial guesses for the LOBPCG method Nmax = 10\nand Nmax = 12, respectively. The iteration counts (for Nmax = 12) and execution times of the\ndiagonalization phase are summarized in Table 2.\nBoth preconditioning and a good initial guess can effectively reduce the number of iterations\nand the execution times for LOBPCG. If neither technique is adopted, LOBPCG becomes slower\nthan Lanczos for this example. Hence, an efficient SpMM implementation alone is not sufficient\n6\n\nhttp://www.nersc.gov/systems/edison-cray-xc30/\n\n19\n\n\fTable 2: Execution times (in seconds) for different settings of LOBPCG for the 7 Li example at\nNmax = 12 using 496 MPI processes. Execution times cover the overall diagonalization time and\nthe IO (i.e., writing the solution to files) time. The timing from the Lanczos algorithm is also listed\nfor comparison.\nLOBPCG\niterations\ntime\npreconditioning good initial guess\nNo\nNo\n154\n447.21\nNo\nYes\n81\n250.75\nYes\nNo\n105\n304.74\nYes\nYes\n53\n206.67\nLanczos\n400\n387.09\n\nrelative residual norm\n\n10 0\nno precond. & random init.\nprecond. & random init.\nno precond. & good init.\nprecond. & good init.\n\n10 -2\n\n10 -4\n\n10 -6\n\n0\n\n50\n\n100\niteration\n\n150\n\n200\n\nFigure 6: Convergence history of LOBPCG for the 7 Li example. Both a good initial guess and\npreconditioning largely improve the convergence rate of LOBPCG.\nfor an efficient block eigensolver. Note that the execution time for the LOBPCG runs with a good\ninitial guess includes the time for constructing the initial guess. But the iteration count refers to\nthe LOBPCG run for the Nmax = 12 case. In the run with preconditioning and good initial guess,\nit takes 38.2 seconds, about 18% of the overall diagonalization time, to perform two LOBPCG\nruns on smaller configuration spaces. The benefit of the runs in smaller configuration spaces is the\nreduced iteration counts and thereby reduced execution times when using the LOBPCG solver for\nthe largest configuration space with Nmax = 12.\nFig. 6 shows the convergence history of the LOBPCG algorithm for these different settings. It\ncan be clearly seen from the figure that the use of a random initial guess results in taking more than\n25 iterations just to bring the residual norm down to the same level satisfied by a good initial guess.\nAlso, the slopes of the curves with preconditioning are steeper than those without preconditioning,\nindicating that preconditioning improves the convergence rate. Hence both techniques are helpful\nfor reducing the total number of iterations. Finally, we remark that other examples have similar\nbehaviors using LOBPCG vs. Lanczos solvers, therefore we present these detailed convergence\npatterns only for this 7 Li example.\n20\n\n\fTable 3: Execution times of diagonalization phase (in seconds). Three different strategies of choosing initial guess in LOBPCG are tested: Column 3 (“smaller Nmax ”)—use solution from a smaller\nconfiguration space as the initial guess; Column 4 (“~ω 0 = 0.8~ω”)—use solution from a different\nbasis parameter ~ω as the initial guess; Column 5 (“different V ”)—use solution with two-body\npotential as the initial guess in a run with three-body potential. The execution times of Lanczos\nalgorithm are also listed for comparison. Speedup are measured as time ratio between Lanczos and\nLOBPCG.\nLanczos\nsmaller Nmax ~ω 0 = 0.8~ω different V\nNucleus\nspeedup\nnit 7 time nit\ntime nit\ntime nit time\n9 Li\n240 209.8\n36\n129.4\n37 124.3\n47 150.1 1.4 ∼ 1.7\n10 B\n280 344.9\n35\n187.3\n48 223.0\n40 203.3 1.6 ∼ 1.9\n8 Be\n480 531.3\n53\n194.1\n59 192.2\n62 203.1 2.6 ∼ 2.8\n7 Li\n400 389.7\n56\n201.3\n60 213.7\n60 177.7 1.8 ∼ 2.2\n6 Li\n240 156.1\n33\n46.5\n34\n58.7\n30\n51.3 2.7 ∼ 3.3\n8 He\n320 247.7\n66\n111.4\n59\n93.8\n56\n88.2 2.2 ∼ 2.8\n10 He\n480 319.4 122\n169.2 115 156.4 126 174.2 1.8 ∼ 2.0\n8 Li\n280 169.3\n38\n58.7\n37\n53.7\n35\n56.2 2.9 ∼ 3.2\n\n5.2\n\nComparison between LOBPCG and Lanczos\n\nIn the following we compare the LOBPCG method (using preconditioning and a good initial guess\nfrom smaller configuration spaces) with the Lanczos method. The execution times as well as the\niteration counts for all test cases are shown in Table 3. Though LOBPCG performs much more\ncomputation in SpMM (roughly speaking, each LOBPCG iteration requires applying SpMM to\ntwelve vectors as we use 50% more trial vectors compared to the number of desired eigenvectors to\nenhance the robustness), it consistently outperforms the Lanczos algorithm: up to and beyond 3×\nimprovements in the wallclock execution time is observed. (In terms of flop rate, the performance\nimprovement is even larger: with LOBPCG we perform more flops and consume less time than\nLanczos.)\nIn Table 3 we also show execution times for other choices for the initial guess. Since we often\nperform a scan of several basis parameters ~ω, it is straightforward to use solutions from a previous\nrun as an initial guess for the next run. In Table 3, we show numbers of iterations and times for the\ncase where the preconditioner is adopted from results at a basis parameter (called ~ω 0 ) that is 80%\nof the current value of basis parameter. This generally shows a similar reduction in execution time\nas using solutions from a smaller configuration space. Using a solution obtained with a different\nnuclear potential also shows similar improvements: in Table 3 we show in the column labeled\n“different V ” the iteration count and execution time using solution from a different NN potential\n(top four entries), and from the NN-only part of an NN-plus-3N potential (bottom four entries).\nThis option can be especially useful when fitting parameters in the NN and/or 3N potential. (Of\ncourse, the closer the initial guess is to the actual desired solutions the better.)\nTo analyze why LOBPCG outperforms Lanczos in all test cases, we illustrate in Fig. 7 a detailed profiling on execution times. Overall, SpMV (SpMM) dominates the execution time for\nLanczos (LOBPCG), especially in runs involving three-body interactions (results in the bottom\nrow of Fig. 7). The use of SpMM in LOBPCG is clearly more efficient than the SpMV operations\nperformed in Lanczos as SpMM performs more computational work but consumes less execution\n7\n\nThe convergence of the Lanczos algorithm is checked every 10 iterations at the beginning, and then every 20\niterations after 100 iterations, and then every 40 iterations after 200 iterations, etc.\n\n21\n\n\f200\n\n200\n\nwall time (sec)\n\n9\n\n10\n\n200\n\nB\n\n8\n\nBe\n\n7\n\n150\n\n150\n\n150\n\n150\n\n100\n\n100\n\n100\n\n100\n\n50\n\n50\n\n50\n\n50\n\n0\n\n200\n\ns\nzo\nnc\n\nLa\n\nG\nPC\nOB\n\nL\n\n6\n\nwall time (sec)\n\n200\n\nLi\n\n0\n\n200\n\ns\nzo\nnc\n\nLa\n\n0\n\nCG\nBP\n\nLO\n\nLi\n\n8\n\n200\n\ns\nzo\nnc\n\nLa\n\nHe\n\nG\nPC\n\nB\nLO\n\n0\n\n200\n\n10\n\nHe\n\n150\n\n150\n\n150\n\n100\n\n100\n\n100\n\n100\n\n50\n\n50\n\n50\n\n50\n\nLa\n\ns\nzo\nnc\n\nG\nPC\nOB\n\nL\n\n0\n\nLa\n\ns\nzo\nnc\n\n0\n\nCG\nBP\n\ns\nzo\nnc\n\nLa\n\nLO\n\nSpMV/SpMM Comput.\nSpMV/SpMM Comm.\nOrth. Comput.\n\nG\nPC\n\nB\nLO\n\nG\n\nC\nBP\nLO\n8\n\n150\n\n0\n\ns\nzo\n\nnc\n\nLa\n\n0\n\nOrth. Comm.\nPrecond.\n\ns\nzo\n\nnc\n\nLa\n\nLi\n\nLi\n\nG\n\nC\nBP\nLO\n\nFigure 7: Detailed profiling on execution times for Lanczos and LOBPCG (using initial guess from\na smaller Nmax ). LOBPCG outperforms Lanczos in both communication and computations. In\nLOBPCG, the time consumed by preconditioning is relatively small compared to other components.\ntime. For runs with two-body interactions (top row of panels in Fig. 7), the orthogonalization\ncost is also non-negligible. This is likely due to the relatively large amount of flops required in the\northogonalization process compared to three-body runs. The communication cost in orthogonalization for the Lanczos method is sometimes quite significant. As a comparison, orthogonalization\nin LOBPCG is performed using level-3 operations, hence uses more efficient communication. As\ndesired, the preconditioning phase in LOBPCG is cost effective, especially for large systems.\n\n6\n\nConclusions\n\nWe described recently developed techniques for improving the performance of nuclear configuration\ninteraction calculation. These techniques include\n1. using a block iterative method such as the LOBPCG algorithm that exposes a higher level of\nconcurrency, and can take advantage of the memory hierarchy on modern microprocessors to\nincrease arithmetic intensity relative to data movement;\n2. choosing an appropriate starting guess of the eigenvectors;\n3. constructing an effective preconditioner.\nThe latter two techniques exploit special structures of the nuclear many-body Hamiltonian which\nwe described in detail. Understanding these structures is also key to developing an effective data\n\n22\n\n\fdistribution and parallelization strategy to achieve high performance on massively parallel multicore supercomputers. The performance improvements achieved by using our developments are\ndemonstrated by several examples.\n\nAcknowledgments\nThis work was supported in part by the U. S. Department of Energy (DOE) under grants\nNo. DESC0008485 (SciDAC/NUCLEI) and DE-FG02-87ER40371. Computational resources were\nprovided by the National Energy Research Scientific Computing Center (NERSC), which is supported by the U. S. Department of Energy under Contract No. DE-AC02-05CH11231. H. M. Aktulga\nwas also supported by a research grant from Michigan State University.\n\nReferences\n[1] H. M. Aktulga, M. Afibuzzaman, S. Williams, A. Buluç, M. Shao, C. Yang, E. G. Ng, P. Maris,\nand J. P. Vary. A high performance block eigensolver for nuclear configuration interaction\ncalculations. IEEE Trans. Parallel Distrib. Syst., 28(6):1550–1563, 2017.\n[2] H. M. Aktulga, A. Buluç, S. Williams, and C. Yang. Optimizing sparse matrix-multiple\nvectors multiplication for nuclear configuration interaction calculations. In 2014 IEEE 28th\nInternational Parallel and Distributed Processing Symposium (IPDPS 2014), pages 1213–1222.\nIEEE, 2014.\n[3] H. M. Aktulga, C. Yang, E. G. Ng, P. Maris, and J. P. Vary. Topology-aware mappings for\nlarge-scale eigenvalue problems. In Euro-Par 2012 Parallel Processing, volume 7484 of Lecture\nNotes in Computer Science, pages 830–842. Springer, 2012.\n[4] H. M. Aktulga, C. Yang, E. G. Ng, P. Maris, and J. P. Vary. Improving the scalability of a\nsymmetric iterative eigensolver for multi-core platforms. Concurr. Comput., 26(16):2631–2651,\n2014.\n[5] M. E. Argentati, A. Knyazev, K. Neymeyr, E. E. Ovtchinnikov, and M. Zhou. Convergence\ntheory for preconditioned eigenvalue solvers in a nutshell. Found. Comput. Math., 17(3):713–\n727, 2017.\n[6] Z. Bai and R.-C. Li. Minimization principles for the linear response eigenvalue problem II:\nComputation. SIAM J. Matrix Anal. Appl., 34(2):392–416, 2013.\n[7] B. R. Barrett, P. Navratil, and J. P. Vary. Ab initio no core shell model. Prog. Part. Nucl.\nPhys., 69:131–181, 2013.\n[8] B. A. Brown and W. D. M. Rae. The shell-model code NuShellX@MSU. Nuclear Data Sheets,\n120:115–118, 2014.\n[9] A. Buluç, J. T. Fineman, M. Frigo, J. R. Gilbert, and C. E. Leiserson. Parallel sparse matrixvector and matrix-transpose-vector multiplication using compressed sparse blocks. In Proceedings of the twenty-first annual symposium on Parallelism in algorithms and architectures,\npages 233–244. ACM, 2009.\n[10] E. Caurier and F. Nowacki. Present Status of Shell Model Techniques. Acta Physica Polonica\nB, 30:705, Mar. 1999.\n23\n\n\f[11] E. U. Condon and G. H. Shortley. The Theory of Atomic Spectra. Cambridge University Press,\n1967.\n[12] J. A. Duersch, M. Gu, M. Shao, and C. Yang. A robust and efficient implementation of\nLOBPCG, 2017.\n[13] M. R. Hestenes and E. Stiefel. Methods of conjugate gradients for solving linear systems. J.\nRes. Nat’l Bur. Std., 49(6):409–436, 1952.\n[14] U. Hetmaniuk and R. Lehoucq. Basis selection in LOBPCG. J. Chem. Phys., 218:324–332,\n2006.\n[15] W. Hu, L. Lin, A. S. Banerjee, E. Vecharynski, and C. Yang. Adaptively compressed exchange operator for large-scale hybrid density functional calculations with applications to the\nadsorption of water on silicene. J. Chem. Theory Comput., 13(3):1188–1198, 2017.\n[16] C. W. Johnson, W. E. Ormand, and P. G. Krastev. Factorization in large-scale many-body\ncalculations. Comput. Phys. Commun., 184(12):2761–2774, 2013.\n[17] A. V. Knyazev. Toward the optimal preconditioned eigensolver: Locally optimal block preconditioned conjugate gradient method. SIAM J. Sci. Comput., 23(2):517–541, 2001.\n[18] A. V. Knyazev. Modern preconditioned eigensolvers for spectral image segmentation and graph\nbisection. In Boley, Dhillon, Ghosh, and Kogan, editors, Clustering Large Data Sets; Third\nIEEE International Conference on Data Mining (ICDM 2003), pages 59–62. IEEE, 2003.\n[19] A. V. Knyazev and A. Malyshev. Accelerated graph-based spectral polynomial filters. In 2015\nIEEE international workshop on machine learning for signal processing, pages 1–6. IEEE,\n2015.\n[20] D. Kressner, M. Miloloža Pandur, and M. Shao. An indefinite variant of LOBPCG for definite\nmatrix pencils. Numer. Algor., 66(4):681–703, 2014.\n[21] C. Lanczos. An iteration method for the solution of the eigenvalue problem of linear differential\nand integral operators. J. Res. Nat’l Bur. Std., 45(4):255–282, 1950.\n[22] P. Maris, H. M. Aktulga, S. Binder, A. Calci, Ü. V. Çatalyürek, J. Langhammer, E. Ng,\nE. Saule, R. Roth, J. P. Vary, et al. No-Core CI calculations for light nuclei with chiral 2-and\n3-body forces. J. Phys.: Conf. Ser., 454(1):012063, 2013.\n[23] P. Maris, H. M. Aktulga, M. A. Caprio, Ü. V. Çatalyürek, E. G. Ng, D. Oryspayev, H. Potter,\nE. Saule, M. Sosonkina, J. P. Vary, et al. Large-scale ab initio configuration interaction\ncalculations for light nuclei. J. Phys.: Conf. Ser., 403(1):012019, 2012.\n[24] J. McQueen, M. Meilǎ, J. VanderPlas, and Z. Zhang. Megaman: Scalable manifold learning\nin Python. J. Mach. Learn. Res., 17:1–5, 2016.\n[25] K. Neymeyr. A geometric theory for preconditioned inverse iteration. I. Extrema of the\nRayleigh quotient. Linear Algebra Appl., 322:61–85, 2001.\n[26] K. Neymeyr. A geometric theory for preconditioned inverse iteration. II. Convergence estimates. Linear Algebra Appl., 322:87–104, 2001.\n\n24\n\n\f[27] Y. Saad. Krylov subspace methods for solving large unsymmetric linear systems. Math. Comp.,\n37(155):105–126, 1981.\n[28] Y. Saad. Numerical Methods for Large Eigenvalue Problems. SIAM, Philadelphia, PA, USA,\n2nd edition, 2011.\n[29] Y. Saad and M. H. Schultz. GMRES: A generalized minimal residual algorithm for solving\nnonsymmetric linear systems. SIAM J. Sci. Stat. Comput., 7(3):856–869, 1986.\n[30] H. Shan, S. Williams, C. Johnson, K. McElvain, and W. E. Ormand. Parallel implementation\nand performance optimization of the configuration-interaction method. In Proceedings of the\nInternational Conference for High Performance Computing, Networking, Storage and Analysis,\nSC ’15, pages 9:1–9:12, New York, NY, USA, 2015. ACM.\n[31] N. Shimizu. Nuclear shell-model code for massive parallel computation, “KSHELL”. 2013.\n[32] G. L. G. Sleijpen and H. A. V. der Vorst. A Jacobi–Davidson iteration method for linear\neigenvalue problems. SIAM J. Matrix Anal. Appl., 17(2):401–425, 1996.\n[33] R. A. Snay. Reducing the profile of sparse symmetric matrices. Bull. Géodésique, 50(4):341–\n352, 1976.\n[34] A. Srinivasa and M. Sosonkina. Nonuniform memory affinity strategy in multithreaded sparse\nmatrix computations. In Proceedings of the 2012 Symposium on High Performance Computing\n(HPC ’12), pages 9:1–9:8. Society for Computer Simulation International, 2012.\n[35] P. Sternberg, E. G. Ng, C. Yang, P. Maris, J. P. Vary, M. Sosonkina, and H. V. Le. Accelerating configuration interaction calculations for nuclear structure. In Proceedings of the 2008\nACM/IEEE Conference on Supercomputing (SC08). IEEE, 2008.\n[36] J. P. Vary, P. Maris, H. Potter, M. A. Caprio, R. Smith, S. Binder, A. Calci, S. Fischer,\nJ. Langhammer, R. Roth, H. M. Aktulga, E. Ng, C. Yang, D. Oryspayev, Sosonkina, E. Saule,\nand Ü. V. Çatalyürek. Ab initio no core shell model—recent results and further prospects. In\nProceedings of the 2014 International Conference on Nuclear Theory in the Supercomputing\nEra (NTSE-2014), page 71. Pacific National University, 2015.\n[37] E. Vecharynski, J. Brabec, M. Shao, N. Govind, and C. Yang. Efficient block preconditioned\neigensolvers for linear response time-dependent density functional theory. Comput. Phys. Commun., to appear.\n\n25\n\n\f"
        ],
        [
         "26",
         "26",
         "cs.CE",
         "Computational Engineering",
         "1111.3127v1.pdf",
         "Tracing the temporal evolution of clusters\nin a financial stock market\nArgimiro Arratiaa,1,∗, Alejandra Cabañab,2\n\narXiv:1111.3127v1 [cs.CE] 14 Nov 2011\n\na\n\nLlenguatges i Sistemes Informàtics,\nUniversitat Politècnica de Catalunya, Barcelona, Spain\nb\nMatemàtiques,\nUniversitat Autónoma de Barcelona, Barcelona, Spain\n\nAbstract\nWe propose a methodology for clustering financial time series of stocks’ returns, and a graphical set-up to quantify and visualise the evolution of these\nclusters through time. The proposed graphical representation allows for the\napplication of well known algorithms for solving classical combinatorial graph\nproblems, which can be interpreted as problems relevant to portfolio design\nand investment strategies. We illustrate this graph representation of the\nevolution of clusters in time and its use on real data from the Madrid Stock\nExchange market.\nKeywords: financial time series, raw–data clustering, graph combinatorics\n1. Introduction\nThe problem of classifying financial time series by some measure of similarity has received a lot of attention, although the emphasis has been mostly\non what has been termed as model–based clustering (see [7], [8] and references\ntherein). In this type of clustering the similarity of the financial time series\n∗\n\nCorresponding author\nEmail addresses: argimiro@lsi.upc.edu (Argimiro Arratia),\nacabana@mat.uab.cat (Alejandra Cabaña)\n1\nResearch partially supported by Spanish Government MICINN projects: SESAAME\nTIN2008-06582-C03-02 and SINGACOM MTM2007-64007\n2\nResearch partially supported by Spanish Government MICINN projects: MTM200910893, and SESAAME TIN2008-06582-C03-02\n\nPreprint submitted to arXiv\n\nNovember 15, 2011\n\n\fis translated into the similarity of the models characterising them (usually\nan ARMA or GARCH process), and hence what is been classified is the\ndynamics of the variances or volatility of the series.\nThe present work can be framed into the category of raw-data based clustering [7], where the series are compared with respect to their history of\nprices, normally sampled at the same time interval. We are interested in\nidentifying stocks whose return history strongly resembles each other in order to use this information for designing investment strategies, such as “pairs\ntrading” [13].\nPairs trading is a stock trading strategy that attempts to capture the\nspread between two correlated stocks as they return to the mean price.\nRoughly speaking it consists on taking long and short positions on two stocks\nthat tend to move together. The first step in this strategy is to find such a\npair, but as correlation depends on the length of time span what the trader\nis really interested in is to identify short time correlations that are sustained\nthrough long periods of time, in order to capture the correlation in movement.\nIn this paper, we propose a graphical representation of the evolution\nin time of clusters of financial time series. The clustering is done through\nstandard procedures with respect to a metric based on pair wise correlation\ncoefficients. The temporal graph representation of the evolution of clusters\nthrough time is basically a weighted graph whose vertices are clusters of\nstocks, bound by a relation of similarity on their return history, and edges\namong clusters with non empty intersection are weighted by the cardinality\nof this intersection. The details of this construction are given in §2.\nAn added advantage of having this graph is that it allows to tackle certain\nquestions about the behaviour of financial time series as classical problems\non the combinatorics of graphs. To illustrate this last point, here is a couple\nof possible applications, which will be dealt with later:\n1. Finding a path (or a fixed number of paths) of heaviest weight in the\ngraph, translates to detecting the most stable clusters through time.\nIn the context of finance it helps to detect those stocks whose return\nhistory mirror each other (but certainly at different scales) through\nlong stretches of time.\n2. Finding a vertex cover [4], translates into finding a (minimum) set of\nstocks that intersects all clusters in the temporal graph. This set of\nstocks can be considered as a group of representatives for the market\n2\n\n\findex, in the sense of being the stocks that in conjunction best replicate\nthe overall combined market price in a given period of time; or, from\nanother point of view, it can be considered as a minimum portfolio\ncovering the market.\nWe expand on these ideas in §3. Finally, in §4 we present an illustration\nof the proposed methodology of clustering, its graphical representation and\ncombinatorial studies derived from it to real data from the Madrid Stock\nExchange market.\n2. Description of the temporal graph of clusters\nThis section outlines the algorithmic construction of the graph that describes the temporal evolution of clusters of a given set of financial time\nseries, in a given time span. References for notation and facts on the statistical tools implemented, and in particular on financial time series can be\nfound in the books by Tsay [12] and Brockwell and Davis [1].\n2.1. Adjusting to significant correlation\nWe assume that daily returns for each of the stocks in financial markets\nare random variables. For each stock X, we observe n values, considered as\nvectors x = (x1 , . . . , xn ) corresponding to the returns during n trading days;\nthat is, each xk corresponds to the variation of the daily closing price:\nxk =\n\nP (k)\n−1\nP (k − 1)\n\n(1)\n\nwhere P (k) is the closing price of the stock at time\nP instant k.\nGiven the observations x, denoting x̄ = n−1 ni=1 xi the sample mean,\nP\nthe sample autocovariance function is defined as γ̂(h) = n−1 n−|h|\nt=1 (xt+|h| −\nx̄)(xt − x̄) and the sample autocorrelation at lag h is γ̂(h)/γ̂(0) (see, for\ninstance, [1]). Ljung–Box test can be applied in order to check whether the\ndata are serially uncorrelated. Ljung–Box test is a so-called portmanteau\ntest, in the sense that takes into account all sample autocorrelations up to\na given lag. For n time periods a good upper bound for the choice of lag is\nlog(n) [1, 12].\nAs a first descriptive tool, we compute the sample correlation coefficients\nfor pairs of stocks returns. For pairs of observations, x and y, taken in\n\n3\n\n\fa common period of n time instants, the sample correlation coefficient is\ncomputed as\nPn\n(xi − x̄)(yi − ȳ)\nr(x, y) = pPn i=1\n.\nPn\n2\n2\ni=1 (xi − x̄)\ni=1 (yi − ȳ)\nThis quantity r = r(x, y) is a consistent estimator of the true correlation\nρ between the random returns of the stocks being considered, which is a\nmeasure of the linear association between them. Correlation is only a measure\nof association, and has little use in prediction; however, it proves to be useful\nfor detecting clusters of stocks that behave in association. It is often useful\nto test the null hypothesis H0 : ρ = 0; that is, that the correlation coefficient\nρ equals zero. An appropriate statistic for this test is\n√\nr n−2\nt0 = √\n1 − r2\nwhich is distributed as a t with n − 2 degrees of freedom under the null\nhypothesis ρ = 0. Thus, “big” values of t0 (or r) will lead us to reject the null\nhypothesis. In fact these kind of data are rather log-normally distributed, but\nin practice, the distinction between simple and log-returns is not substantial\n[12]. Hence, we use the bounds given by the t test critical points to determine\nsignificant correlations.\nTable 1 shows the critical points cα for the tests with critical region |r| >\ncα for testing H0 , for levels α = 0.05 and 0.01 for different values of n,\nthat is, the number of trading days considered. In other words, correlations\nequal or smaller than the critical point in absolute value will be statistically\nindistinguishable from zero at level α for the corresponding sample size n.\nn\nc0.05\nc0.01\n\n5\n0.8783\n0.9587\n\n10\n0.6319\n0.7646\n\n20\n0.4438\n0.5614\n\n40\n60\n0.3120 0.2542\n0.4026 0.3301\n\n240\n0.1267\n0.1660\n\nTable 1: Critical points for the contrast for H0 : ρ = 0 with critical region {|r| > cα }\n\n2.2. Clustering\nWe reason as follows for defining the clusters in the most appropriate way.\nAny strategy for clustering a set S of data, under some notion of similarity,\n4\n\n\fseeks to partition S in such a way that any two elements in the same subset\n(or cluster) are similar and two elements in different subsets are dissimilar. A\nsound procedure is to endow S with some metric d, related to the similarity\ncriteria, and treat each cluster as a ball, in topological space (S, d), centered\nat some element and of a certain radius. There are many ways of partition\nS in balls, and these depend on the chosen center and radius.\nFor two financial returns series the similarity is measured by their correlation coefficient (with the appropriate adjustment as explained in the previous\nsection). However, we are reluctant to use correlation values as a measure\nof proximity for a collection of returns since these do not constitute a metric. A second caveat is that once we choose a stock A and find the subset\nSA of stocks with series of returns correlated to the series of returns a of A\nobserved in the same time period, the succeeding groups (or balls) have to\nbe constructed from S \\ SA , thus failing from considering all possible correlations of returns of stocks in SA for the successive stocks’ returns fixed as\ncenter, and furthermore, the last stocks chosen will forcibly have few or none\ncorrelated match.\nTherefore, in order to overcome this “order of selection” dependency plus\nthe lack of metric to quantify the similarity distance, we do the following:\nLet τ be a fixed time period.\n• If it is the case that one wants to cluster stocks with positive correlated\nreturns series, taken in the span of τ , then for each stock A, with series\nof returns a in the time period τ , we group into GA all stocks X whose\nseries of returns x, on the same time span of a, have correlation with\na higher than a positive δ; that is,\nX ∈ GA ⇐⇒ r(a, x) > δ.\nThe δ is determined by our statistical test explained in previous section\n(cf. Table 1) and depends on the sample size. For example, for 40 days\nperiod we take δ = 0.65, a midpoint between 1 and the threshold of\nc0.05 = 0.3120, to ensure a significant correlation.\nFor the case of clustering stocks with negative correlated return time\nseries, consider a negative δ and define GA as X ∈ GA ⇐⇒ r(a, x) < δ.\n• Next, for each pair of stocks A and B define\nd(A, B) = 1 −\n5\n\n|GA ∩ GB |\n|GA ∪ GB |\n\n\f(the term\ndistance).\n\n|GA ∩GB |\n|GA ∪GB |\n\nis known as Jaccard measure and d(A, B) a Jaccard\n\nd is a metric: it is positive, symmetric, d(A, B) = 0 iff A = B, and\nverifies triangle inequality. The last property follows by straightforward\ncomputation:\n|GA ∪ GB \\ GA ∩ GB | |GB ∪ GC \\ GB ∩ GC |\n+\n≥\n|GA ∪ GB |\n|GB ∪ GC |\n|GA ∪ GB \\ GA ∩ GB | + |GB ∪ GC \\ GB ∩ GC |\n=\n|GA ∪ GB ∪ GC |\n|GA ∩ GB ∩ GC |\n|GA ∩ GC |\n≥ 1−\n.\n1−\n|GA ∪ GB ∪ GC |\n|GA ∪ GC |\nOne can see that if A and B are similar, in the sense that their respective groups GA and GB have almost the same members, then d(A, B)\nis close to 0; whilst if GA and GB greatly differ then d(A, B) is close to\n1. Note that d is a stronger measure of similarity than taking a direct\ncorrelation coefficient, since it says that two stocks are similar, not only\nif they are correlated but also correlated to almost all the same stocks.\n• Apply a clustering method based on the distance metric d. The method\nwe have chosen is the hierarchical clustering, which is an agglomerative\nalgorithm that stepwise builds clusters, beginning from singletons and\nsuccessively increasing their size by merging elements that are nearer\naccording to d (see [6]). The decision of the final number of clusters\nthat are to be considered is made by the standard procedure of cutting\nat mid level the tree representation, or dendogram, of the hierarchical\nclustering. After the cut, the clusters that contain only one element are\nmerged into one group and labelled as outliers (elements whose returns\nare not significantly correlated to any other).\nOther methods of clustering can be applied leading to different experimental results. For example, a most popular choice is the k–means\nalgorithm [6]. However with this algorithm the user must arbitrarily\nchoose before hand the number of clusters, or rather use a test like\ngap statistics [11] to make a more accurate selection of the number\nof clusters. In any case, with k–means every element is assigned to\nsome cluster to the expense of lowering the similarity measure within\n6\n\n\fclusters. Thus, depending on the application in mind the user should\nuse one method or the other. As mentioned in the Introduction one\nof our concerns is identifying groups of stocks with high similar return\nbehaviour in order to apply diverse investment strategies (e.g. pairs\ntrading). Hierarchical clustering serves well this purpose. On the other\nhand, the method of k-means might serve the purpose of producing\nclusters corresponding to the different industrial sectors of the market, which is a most common application of clustering of financial time\nseries under the model–based paradigm (see e.g. [8]).\nWe repeat the computations for further temporal intervals, and use combinatorial graphs algorithms to build a (weighted) representation of the evolution of clusters in time. This construction is explained in the next section.\n2.3. Temporal graph of clusters\nLet S be a set of market stocks. Formally, S is a set of financial time\nseries corresponding to these stocks. Let T be the time period selected for\nanalysis, which is partitioned into m successive sub-periods of time τ1 , τ2 ,\n. . . , τm . For example, in our experiments presented in §4 we take T to be\ntwo consecutive years and each τi two successive months in the span of T .\nFor each i = 1, . . . , m, let Ci be the collection of clusters obtained in each\ntemporal interval τi , constructed as describe in §2.2, and assuming we applied\nhierarchical clustering, and let ni = |Ci |. Let Si,j , 1 ≤ i ≤ m, 1 ≤ j ≤ ni , be\nthe clusters in Ci with at least two elements, and Qi be the subset of elements\nwith no significant correlation in time segment τi , for 1 ≤ i ≤ m.\nWe define a directed graph G with vertex set the collection of subsets\n{Si,j : 1 ≤ i ≤ m, 1 ≤ j ≤ ni } ∪ {Qi : 1 ≤ i ≤ m}\nand weighted edge set\n{(Si,j , Si+1,k , |Si,j ∩ Si+1,k |) : 1 ≤ i ≤ m − 1, 1 ≤ j ≤ ni , 1 ≤ k ≤ ni+1 }\nthat is, we link a cluster Si,j in the time segment τi with a cluster Si+1,k in\nthe next time segment τi+1 as long as their intersection is non empty, and\nthe cardinality of their intersection is the weight assigned to the link. The\nsets Qi remain as isolated vertices. Beware that these Qi ’s do not represent\nstocks whose returns have some significant (negative or not) correlations\nwith the returns of other stocks. These are just variables that do not pass\n7\n\n\four statistical test and hence can not be considered significantly correlated.\nWe call G the Temporal Graph\nS of Clusters (TGC) for the set of stocks S in\nthe time segmentation T = 1≤i≤m τi .\nBy considering the cardinality of the intersection of the clusters as the\nweight for the edges of G we seek to capture the most persistent or stable\nclusters through time. These would be located on the paths of heaviest weight\n(considering the weight of a path as the sum of the weights of the edges that\ncomprise it).\n2.4. The Algorithm\nLet S be a set of financial time series corresponding to some market\nstocks. Let T be the time period selected for analysis, which is partitioned\ninto m successive sub-periods of time τ1 , τ2 , . . . , τm .\nPre–processing: Depending on the provider of the data there will be some\npre–processing to be done. For instance, if the financial data is retrieved from\nyahoo.com, the file comes in columns labelled Date, Open, High, Low, Close\nand Volume. Then one has to produce, for each stock, a two columns table\nof Date and Return (equation 1). Denote by Ŝ the resulting pre-processed\ndata S.\nInput: Ŝ, a list of tables (with entries Date and Return) containing the\nreturn history of each stock in the time period T . The partition of T =\n{τ1 , . . . , τm }.\nPart I For each time segment τi , 1 ≤ i ≤ m, determined by its initial date\nand ending date, do:\n1. Build the matrix R of series of returns for the period τi of all\nstocks in Ŝ. R is the concatenation as columns of each of these\nreturn series taken on the same period τi .\n2. Compute clusters following the method explained in §2.2. This\npart consist of two steps.\nstep 1 For each column of R, containing the return series of some\nstock A in the time period τi , compute the set GA of all other\nstock X whose return series has correlation coefficient with\nthat of A above the threshold obtained from Table 1 (respectively, below the threshold if what is desired to measure is\nnegative correlation). Then define the dissimilarity matrix M\nwhere each entry is the value of the jaccard distance d(A, B),\ndefined in §2.2.\n8\n\n\fstep 2 Apply the hierarchical clustering algorithm to the matrix\nM . The output is a list of clusters which is formatted as a\ncolumn vector V Col.\n3. Define the vertex set V as the successive concatenations of column\nvectors V Col.\nPart II\n\n1. Remove from V all those entries (clusters) containing a single\nelement, and rename the result V 0 .\n2. Compute the edge relation E on V 0 by linking an element of column i in V 0 to an element in column i + 1, whenever the intersection is non zero. The weight of the edge is the cardinality of the\nintersection.\n\nOutput: V and E.\n3. Graph combinatorics applied to financial analytics\nThroughout this section G is the weighted graph representation of the\ntemporal evolution of clusters in a financial market, the TGC as constructed\nin §2.3, without the isolated vertices Qi . Applying graph combinatorial techniques to G we can solve problems relevant to financial analytics. We shall\naddress three problems:\n1. Stable clusters, consist on finding the clusters (or sub-clusters) of stocks\nthat appear more frequently in consecutive intervals of time. On the\nTGC G this amounts to finding the heaviest paths from the first time\ninterval to the last. The problem of deciding the heaviest weighted path\nin a graph is in general NP-hard; however, on acyclic directed graphs\npolynomial time solutions are known using dynamic programming ([4]\nproblem [ND29]). We shall make use of such algorithmic methodology\nfor producing our own solution to this problem on G. The relevance\nof this graph problem to financial analytics is of a large extent. For\nexample, as suggested in the Introduction it helps detect those stocks\nthat correlate through different by continuous time intervals, and hence\nbecome candidates for a pairs trading investment strategy [13].\n2. The trace of a given stock, consist on finding the sequence of clusters\nthrough consecutive intervals of time where a given stock is contained\nin all. On G this is simply to find a path from the first time interval\nto the last, subject to the restriction that a given stock must appear in\n9\n\n\fall vertices (clusters) conforming the path. This allows to identify the\nmarket associations of a given stock through time.\n3. Stock cover, consist on finding the smallest possible set of stocks that\nintersects every (non singleton) cluster in G. This is the Hitting Set\nproblem ([4] problem [SP8]), which is a form of the Vertex Cover problem on graphs, both NP-hard. Hence, the best we can aim for is an\napproximate solution, which we construct following a greedy strategy.\nThe interest of this computational query is to discover those stocks that\nessentially represent the market behaviour through a sequence of time\nperiods; thus, conforming a portfolio that covers the market.\nIn the next three sections we give the algorithmic details. As a reference\nfor the programming schemes and data structure that we implement in our\nalgorithms we recommend [2].\n3.1. Stable clusters\nGiven a positive integer k ≥ 1 and TGC G = hV, Ei, with V = {Si,j :\n1 ≤ i ≤ m, 1 ≤ j ≤ ni }, we want to find the k first heaviest paths starting\nat any cluster Si,j with no ingoing edge.\nLet {τ1 , . . . , τm } be the sequence of time segments which partition the\ntotal time period T . We view the edges in G as directed going from time\nsegment τi to time segment τi+1 . Then we add an extra (dummy) cluster\nSm+1,1 after time period τm , and which serves as a sink in the sense that\nevery cluster S ∈ V that do not have an outgoing edge (i.e. is terminal) is\nconnected by an edge of weight 1 to Sm+1,1 . Now, a brute force solution is to\nfind all paths that end in Sm+1,1 (say by Breadth First Search), order them by\nweight and keep the k heaviest. This, however, would incur in exponentially\nmany comparisons, as we can have a worst case containing O(nm ) many\npaths, where n is the maximum number of clusters per time segment and\nm is the number of time segments. Instead we apply a bottom-up approach\n(aka dynamic programming) where at step i, we look at time segment τi and\ndefine, for each cluster Si,j , a heap (i.e. a tree with nodes having a positive\nvalue or key) where the root, labelled Si,j , has key 0, its descendent will be\nall heaps defined in previous time segments τi0 , i0 < i, for which there is an\nedge from Si0 ,j 0 to Si,j , and the keys of all leaves of these descendent heaps are\nupdated by adding the weight of the edge (Si0 ,j 0 , Si,j ), denoted w(Si0 ,j 0 , Si,j ).3\n3\n\nBy a leaf of a heap we understand a node with no ingoing edges. By updating the\n\n10\n\n\fObserve that the total weight of a path in the heap of root Si,j is the key of\nthe leaf or initial node of such path. We order these leaves by key value and\nkeep the paths for the k leaves with highest key, removing the paths of the\nremaining leaves before proceeding to step i + 1 of algorithm.\nBelow we present our algorithmic solution written in pseudo–code style.\nWe use the following abbreviations for functions whose implementation we\ndo not specify but are obvious: for a vertex S, key[S] keeps the key value of\nS in a heap which should be clear from context; for a heap h, leaves(h) is\nthe set of leaves of heap h; for two heaps h and h0 , append(h0 , h), returns a\nheap made by drawing an edge from the root of heap h0 to the root of heap\nh.\nAlgorithm: k first heaviest paths\n1.\ninput: k > 0, V = {Si,j : 1 ≤ i ≤ m, 1 ≤ j ≤ ni }, E\n2.\n(preprocessing) Add a sink Sm+1,1 :\n3.\nfor each S ∈ V with no outgoing edge do\n4.\ndefine new edge (S, Sm+1,1 ) of weight 1;\n5.\nend for;\n6.\nfor each i ∈ {1, . . . , m + 1} do\n7.\nfor each j ∈ {1, . . . , ni } do\n8.\ndefine heap hi,j with unique element\n9.\nlabelled Si,j and key[Si,j ] = 0;\n10.\nfor each Si0 ,j 0 such that (Si0 ,j 0 , Si,j ) ∈ E do\n11.\nhi,j = append(hi0 ,j 0 , hi,j );\n12.\nfor each S ∈ leaves(hi0 ,j 0 ) do\n13.\nkey[S] = key[S] + w(Si0 ,j 0 , Si,j );\n14.\nend for;\n15.\nend for;\n16.\nremove from hi,j all but k paths to Si,j\n17.\nof heaviest weight; // The weight of each path\n18.\n// can be read-off from the key of each leaf.\n19.\nend for;\n20. end for;\n21. output: hm+1,1 ;\nkeys of the leaves only we significantly reduce computation time since we do not need to\ntraverse all the heap. Also the information about the key value of inner nodes is irrelevant\nfor our purposes. For further details on heaps see [2]\n\n11\n\n\fOne can see that for m time segments and a maximum of n many clusters per time segment, the algorithm has a worst-case running time bound\nof O(mn3 ), which is much better than the naive solution explained at the\nbeginning.\n3.2. The trace of a given stock\nGiven an specific stock (a company’s name) and the TGC G = hV, Ei, we\nwant to find the path in G comprise of clusters containing the stock. This is\na straightforward path finding algorithm (e.g. breath first search) with the\nrestriction that at each step we check that the given stock is in the currently\nvisited cluster.\n3.3. Stock cover\nGiven a TGC G = hV, Ei, with V = {Si,j : 1 ≤ i ≤ m, 1 ≤ j ≤ ni }, we\nwant to find a (minimum) set of stocks that intersects every (non singleton)\ncluster in G.\nWe apply the following greedy strategy: pick the stock that shows up more\noften in the collection of clusters V ; then remove the clusters containing this\nstock (i.e. covered by the stock). Repeat with remaining clusters.\nIn the pseudo-code implementation of the above strategy we define an\nincidence matrix M for the set of clusters V , where rows are labelled by\nthe companies identifier or ticker and columns labelled by the clusters. Π\nrepresents the set of tickers (which is given as input). Then there is a 1 in\nentry M(π, S), π ∈ Π and S ∈ V , if stock represented by ticker π is in cluster\nS, or a 0 otherwise. By dim(M) we denote the dimension of matrix M, which\nis the number of rows multiplied by the number of columns. Details follows.\nAlgorithm: stock cover\n1.\ninput: V = {Si,j : 1 ≤ i ≤ m, 1 ≤ j ≤ ni }, Π\n2.\n(initialize) Γ = ∅;\n3.\nDefine a matrix M with rows labelled by each ticker\nπ ∈ Π and columns labelled by each cluster S ∈ V ,\nand there is a 1 in entry M(π, S), if stock represented\nby π is in cluster S, or a 0 otherwise;\n4.\nwhile dim(M) 6= 0 do //dim(M) = (num. rows) × (num. columns)\n5.\norder the rows decreasingly with respect to\n12\n\n\f6.\n7.\n8.\n9.\n10.\n11.\n12.\n\nthe number of 1 appearing in it;\n// Thus the first row corresponds to the stock\n// appearing more often in the clusters.\nlet π1 be the ticker of the stock labelling the first row;\nΓ = Γ ∪ {π1 };\ndelete every column with label S ∈ V for which M(π1 , S) = 1;\ndelete row labelled π1 ;\nrename M the reduced matrix;\nend while;\noutput: Γ;\n\nThe running time of this algorithm is bounded by dim(M); that is by\n|Π| · |V |. More importantly is to see that in any case the size of the solution\nis not too big with respect to the size of the full set of companies involved.\nWe show that the size of the solution set Γ is at most one half of the size of\nthe set of companies Π.\nProposition 1. |Γ| ≤ 21 |Π| .\nProof. Each cluster contains at least two stocks. The worst situation is that\nany stock appears in at most one cluster. The algorithm selects a stock π1\n(of highest weight) and removes the cluster where it belongs together with at\nleast another stock that will not enter the solution set (otherwise its weight\nis higher than π1 ’s and would have been selected before). Thus, at most 1/2\nof the stocks enters Γ.\n\u0003\n4. An application: Exploring the Spanish market\nIn this section we describe the results obtained by applying our algorithms\non real data from the Madrid Stock Exchange during the years 2008 and 2009.\nWe have chosen 34 of the big cap companies contained in the main index of\nthis market, IBEX 354 . The IBEX 35 is the benchmark index composed of\nthe 35 most liquid securities listed on the Madrid Stock Exchange. It is a\nmarket capitalization weighted index, calculated by a recursive formula [10].\n4\n\nIBEX 35 is an index composed by 35 companies, subject to changes in its composition.\nIn the time span considered we found these 34 as the largest set of persistent companies.\n\n13\n\n\fThus, IBEX 35 is a geometric index (as opposed, for example, to Dow Jones\nwhich is arithmetic), with a value at a time instant highly correlated with the\nvalues of its components of higher market capitalization. Hence, according\nto the formula, an investor that wants to have a portfolio that replicates\nthis index should buy the shares of the “biggest of the big cap” companies.\nNonetheless, we shall show that other options are possible.\nThe companies we analyse are the following (the symbol in parenthesis\nis the market identifier or ticker, which we use in our graphical representation): Abertis (ABE), Abengoa (ABG), Actividades de Construcción y Servicios (ACS), Acerinox (ACX), Acciona (ANA), Bankinter (BKT), Banco\nBilbao Vizcaya Argentaria (BBVA ), Bolsas y Mercados Españoles (BME),\nBanesto (BTO), Criteria Caixa Corp. (CRI), Endesa (ELE), Enagas (ENG),\nFomento de Construcciones y Contratas (FCC), Ferrovial (FER), Gamesa\n(GAM), Gas Natural (GAS), Grifols (GRF), Iberdrola (IBE), Iberia (IBLA),\nIberdrola Renovables (IBR), Indra Sistemas (IDR), Inditex (ITX), Mapfre\n(MAP), Banco Popular (POP), Red Eléctrica (REE), Repsol YPF (REP),\nBanco Sabadell (SAB), Banco Santander (SAN), Arcelor Mittal (MTS),\nSacyr Vallehermoso (SYV), Telefónica (TEF), Telecinco (TL5), Técnicas Reunidas (TRE), Obrascón Huarte Lain (OHL). This set of companies will be\nreferred to as Ibex Big Cap (or IbexBC for short).\nThe data have been obtained from Yahoo Finance, and it is stored and\nmanaged with MySQL. The algorithms (see §2.4 and §3) were programmed\nin R, making use of some of the packages contained in this statistical software\n[9]. Although we test our clustering algorithm and graphical representation\non this particular market, they are applicable to any financial stock market.\nBefore computing clusters we test for serial autocorrelation of the 34\nseries, as indicated in §2.1. We have performed the Ljung–Box test for all the\nabove assets by means of the instruction Box.test(x,lag=6,type=\"Ljung\")\nin R. Applying Ljung–Box test up to lag 6 ≈ log(256), for daily data, Table\n2 shows the p-values obtained for IBEX companies in the years 2008 and\n2009. The boldface numbers correspond to p values smaller than 0.05. There\nis no statistically significant dependence in most assets returns series in the\nperiod 2009. There are few more companies that show serial autocorrelation\nin their prices during 2008.\nWe apply the algorithm as described in §2.4, with time period T from\n1-06-2008 to 1-08-2009, segmented bimonthly, thus T is partitioned into 7\n14\n\n\fTicker\n\n2008\np-value\nABE\n0.1087\nABG\n0.2771\nACS\n0.4305\nACX\n0.0271\nANA\n0.8196\nBKT\n0.5887\nBBVA 0.0159\nBME 0.5205\nBTO\n6 e-06\nCRI\n0.0369\nELE\n0.5658\nENG\n0.1697\nFCC\n0.3361\nFER\n1\nGAM 0.038\nGAS\n0.0094\nGRF\n0.0165\n\n2009\np-value\n0.4052\n0.8080\n0.9764\n0.1748\n0.4961\n0.4525\n0.0124\n0.0185\n0.2817\n0.0517\n0.7748\n0.9862\n0.5496\n0.001\n0.9006\n0.0141\n0.1639\n\nTicker\nIBE\nIBLA\nIBR\nIDR\nITX\nMAP\nPOP\nREE\nREP\nSAB\nSAN\nMTS\nSYV\nTEF\nTL5\nTRE\nOHL\n\n2008\np-value\n0.0055\n0.1525\n0.0004\n0.0051\n0.0039\n0.0016\n0.0329\n0.0004\n0.0112\n0.4162\n0.1390\n0.0114\n0.1213\n2e-06\n0.5294\n0.2801\n0.4200\n\n2009\np-value\n0.3469\n0.9290\n0.3875\n0.3095\n0.1386\n0.1612\n0.1492\n0.8557\n0.9884\n0.0043\n0.6888\n0.9009\n0.2185\n0.5921\n0.5735\n0.4134\n0.0295\n\nTable 2: p-values for IbexBC components in 2008 and 2009\n\n15\n\n\fperiods, τ1 , . . . , τ7 .\nFigure 15 presents the correlation matrix for the first period of our sample, 1-06-2008 to 1-08-2008 (the scale on the right represents the correlation\nvalues found). Next, Figure 2 is the dendogram representation of the result\nof applying the hierarchical clustering algorithm to the dissimilarity matrix\nassociated to the correlation matrix. Figure 3 is the graph of clusters formed\nthrough the different time periods in the selected years: the green boxes\n(boxes in first row from bottom) contain the time periods for each return\nseries; the pink boxes (boxes in second row from bottom) collect companies\nwith correlations below our established threshold (hence, no conclusion can\nbe drawn for these); and the blue boxes (all remaining boxes above second\nrow from bottom) represent non singleton clusters, which also contain a label ij to indicate its position in the adjacency matrix representation, so for\nexample cluster 13 is {ACX, ANA, SYV}; the edges are weighted by the\nintersection measure.\nObserve that in the time period τ3 = [01/10/2008, 01/12/2008] is where\nthe largest cluster appears (cluster 31). This time frame corresponds to a\nbullish period when all prices were rising on the market and thus align in\ncorrelation as expected. Also there is one title, GRF, that does not belong to\nany cluster in any time period. Incidentally the business of the corresponding company, Grifols, is far from the industrial sectors to which all other\ncompanies of IbexBC belong. Notice also that the resulting clustering is not\nsectorial. This is not surprising as we are comparing time series with respect\nto their first order moments, disregarding volatility which has been reported\nto discriminate industrial sectors (see, e.g. [8]).\nThe procedure for building the temporal graph of clusters for negatively\ncorrelated stocks is the same as in the positive case but considering as good\ncorrelation values those below a negative threshold given by our critical points\nshown in Table 1. For our particular subject of experimenting our clustering\nmethod, namely IbexBC, we found that there is no time segmentation for\nthe span of the years 2008 to 2009 that will produce clusters of stocks with\nsignificant negative correlations. On periods of time of 40 or 60 days of\nreturns there are very few stocks that have negative correlations below -0.2\n(see for example Figure 1), which is not a significant value. For four or\n5\n\nAll figures are in the Appendix.\n\n16\n\n\fsix months long time segments the lowest correlation coefficient we got was\nabout -0.25. For the full year 2008 the minimum correlation was -0.02, and\nfor the year 2009, -0.22. None of these values can be considered as significant\nnegative correlations, according to our criteria from §2.1.\n4.1. Graph combinatorics\n1. Applying the k first heaviest paths algorithm to IbexBC for the years\n2008-2009 and k = 3, we found the following three heaviest paths (the number\non top of the arrow is the weight of the corresponding edge):\n5\n\n6\n\n6\n\n3\n\n3\n\n4\n\n2\n\n4\n\n6\n\n3\n\n3\n\n4\n\n5\n\n6\n\n3\n\n2\n\n1\n\n4\n\n14 7→ 21 7→ 31 7→ 44 7→ 52 7→ 62 7→ 72\n14 7→ 23 7→ 31 7→ 44 7→ 52 7→ 62 7→ 72\n14 7→ 21 7→ 31 7→ 41 7→ 54 7→ 62 7→ 72\nOn these heaviest paths we find the subsets of companies with higher\ntendency to correlate through time (correlation in movement). We find that\nthese belong mostly to three sectors: banks, energy and construction. On\nthe second heaviest path we found the two most correlated pair, namely,\nthe two banks SAN and BBVA, thus making a good candidate for a pairs\ntrading strategy. To find more pairs to which apply the pairs trading we look\nfor paths where the edge weight is 2 and remain constant for consecutive\nperiods of time. This is the case for the path 16 7→ 24 7→ 33, with all\nclusters consisting of ENG and REE, two companies from the energy sector.\nInterestingly these two companies remain in the boxes of unclassified (pink\nboxes) of the graph through all 2009.\n2. Applying the stock cover algorithm to IbexBC for the years 2008-2009\nwe found the following cover: {BBVA, FCC, REP, REE, TRE, ACX, ABG,\nCRI, TEF, MAP, BME, ANA, MTS, TL5}\nThese fourteen companies represent a minimum set of stocks that intersects all non isolated clusters in the TGC for IbexBC through 2008 and 2009.\nIt constitute a portfolio that would have replicated the behaviour of the market index IBEX 35 throughout those years. It is not unique (for example,\nBBVA can be substituted by SAN), and its composition is a consequence of\nthe order of selection imposed by the algorithm. Another issue is that for\nthe sake of completeness we should add to the cover found by the algorithm\nthose stocks that are not in any cluster, such is the case of GRF.\n17\n\n\f5. Conclusions\nWe have proposed a graphical tool in order to monitor the temporal evolution of clusters of financial time series; that is, a representation of the\nclusters in movement. We have exploited some useful links between graph\ncombinatorics and financial applications, which shows how problems in the\nformer field translates to problems in the latter and used some known combinatorial methods from graph theory to produce sound answers for problems\nabout financial markets.\nThe finding of the heaviest paths in our model confirmed a popular observation known by many local brokers that the banks BBVA and SAN conform\nthe most stable cluster through any period of time, and further we see that\nthis duo tends to participate in the largest clusters of IBEX 35 components\nat different periods of time; hence being both together a driving force of the\nSpanish market. Additionally, our experiments revealed that pairs of stocks\nthat remain clustered together through long periods of time tend to belong\nto the same industrial sectors, as is the case of BBVA and SAN (banks),\nFCC and FER (construction), or ENG and REE (energy). These pairs can\nbe good candidates for applying a pairs trading strategy.\nThe stock cover algorithm produces an approximate solution with at most\nhalf of the stocks available, in the worst scenario. The optimal solution is\nnot algorithmically feasible and will certainly depend on the length and the\nnumber of time segments considered in the study. The approximate solution\ncan be taken as a starting group of stocks that can be shaped up to obtain an\nadequate portfolio (according to Benjamin Graham [5, ch. 5] “a minimum of\nten issues and a maximum of about thirty” is adequate). However, adequacy\nis not only a matter of size, but among other issues is about diversification\n(read [5]). From our experiments with negative correlation we conclude that\nthe big cap companies of IBEX 35 are, in general, always positively correlated. Thus, an investor can not have a balanced portfolio consisting only on\nthe big cap companies in the Madrid Stock Exchange.\nReferences\n[1] Brockwell, P. J., Davis, R. A., 2002. Introduction to Time Series and\nForecasting, 2nd ed., Springer.\n[2] Cormen, T., Leiserson, C., Rivest, R., Stein, C., 2001. Introduction to\nAlgorithms, 2nd. ed., The MIT Press.\n18\n\n\f[3] Campbell, J. Y., Lo, A. W., MacKinly, A. C., 1997. The Econometrics of\nFinancial Markets, Princeton University Press.\n[4] Garey, M. R., Johnson, D. S, 1979. Computers and Intractability. A Guide\nto the Theory of NP–Completeness, Freeman, San Francisco.\n[5] Graham, B., 2006. The Intelligent Investor – Revised Edition / updated\nwith new commentary by Jason Zeig, Collins Business Essentials.\n[6] Hastie, T., Tibshirani, R., Friedman, J., 2001. The Elements of Statistical\nLearning, Springer.\n[7] Liao,T., 2005. Clustering time series data: a survey. Pattern Recognition,\n38, 1857–1874.\n[8] Otranto, E., 2008. Clustering heteroskedastic time series by model-based\nprocedures. Comp. Stat. & Data Anal. 52, 10, 4685–4698.\n[9] R Development Core Team, 2009. R: A Language and Environment for\nStatistical Computing. R Foundation for Statistical Computing, Vienna,\nAustria. http://www.R-project.org\n[10] Sociedad de Bolsas Española, 2008. Technical Regulations for the Composition and Calculation of the Sociedad de Bolsas, S.A. Indexes. (PDF\nfrom http://www.sbolsas.com/data/normdef-in.pdf)\n[11] Tibshirani, R., Walther, G., Hastie, T., 2001. Estimating the number of\nclusters in a data set via the gap statistic. J. R. Statistic Soc. B, 63, part\n2, 411–423.\n[12] Tsay, Ruey S., 2010. Analysis of Financial Time Series, 3rd ed., Wiley.\n[13] Vidyamurthy, G., 2004. Pairs Trading, Quantitative Methods and Analysis, Wiley.\n\n19\n\n\fAppendix: Figures\n\nFigure 1: Correlation matrix for time segment 1-6-2008 to 1-8-2008\n\n20\n\n\fFigure 2: Dendogram for the correlation matrix of Figure 1\n\n21\n\n\fFigure 3: Temporal Cluster Graph for IBEX 35, years 2008-2009\n\n22\nACX ANA\nELE GAM\nGRF IBLA\nIBR IDR\nITX MTS\nSYV OHL\n20080801 20081001\n\n20080601 20080801\n\n24 ENG REE\n\nABE ELE\nGAS GRF\nIBE IBLA\nIBR IDR\nREP MTS\n\n1\n\nABG\nGAS IBE\n\n23\nACS\nBBVA MAP\nSAN TEF\n\n22\n\n25 REP TRE\n\n1\n\n2\n\n1\n\n2\n\n1\n2\n\n1\n\n5\n1\n\n1\n\n19 ITX TEF\n\n18 GAM TRE\n\n17 FCC FER\n\n16 ENG REE\n\n15 CRI MAP\n\n14\nBKT\nBBVA BME\nBTO POP\nSAB SAN\n\n13\nACX\nANA SYV\n\n12 ACS TL5\n\n11 ABG OHL\n\n21 ABE BKT\nBME BTO\nCRI FCC\nFER POP\nSAB TL5\n\n4\n\n3\n\n1\n\n1\n2\n\n6\n\n1\n\n20081001 20081201\n\nBKT BTO\nGRF IBLA\nIDR MAP\nSYV TL5\n\n34 ITX TRE\n\n33 ENG REE\n\n32 BME REP\n\n31\nABE\nABG ACS\nACX ANA\nBBVA CRI\nELE FCC\nFER GAM\nGAS IBE IBR\nPOP SAB\nSAN MTS\nTEF OHL\n\n1\n1\n\n1\n\n3\n\n3\n\n6\n\n1\n\n1\n\n1\n\n20081201 20090201\n\nABG\nACS BKT\nBME CRI\nELE ENG\nGAS GRF\nIBLA ITX\nSYV TL5\n\n47 POP REP\n\n46 IBR REE\n\n45 BTO MAP\n\n44 BBVA FER\nGAM SAB\nSAN TEF\n\n43 ANA IDR\n\n42\nACX\nMTS OHL\n\n41 ABE FCC\nIBE TRE\n\n21\n\n13\n\n21\n\n1\n\n20090201 20090401\n\nACX ANA\nELE ENG\nGAS GRF\nIBLA\nIBR IDR\nITX MAP\nREE MTS\nTEF TL5\nTRE OHL\n\n54 FCC FER\nIBE SYV\n\n53 BKT CRI\n\n52ACS BBVA\nGAM POP\nREP SAN\n\n51\nABE\nABG BME\nBTO SAB\n\n12\n\n1 1\n\n1\n\n31\n\n1\n\n1\n\n20090401 20090601\n\nABG ANA\nBKT BME\nCRI ELE\nENG GAS\nGRF IBLA\nIBR IDR\nITX MAP\nREE SYV\nTEF TL5\nTRE OHL\n\n65 POP REP\n\n64 GAM MTS\n\n63\nACX\nFER SAB\n\n62\nACS\nBBVA BTO\nIBE SAN\n\n61 ABE FCC\n\n1\n1\n\n1\n\n12\n1\n\n1\n\n4\n\n1\n\n20090601 20090801\n\nBKT CRI\nELE ENG\nGAM GRF\nIBR IDR ITX\nMAP REE\nSYV TL5\n\n76 FCC IBE\n\n75\nBME\nGAS IBLA\n\n74 ANA TRE\n\n73 ACX FER\nREP OHL\n\n72 ABG ACS\nBBVA BTO\nPOP SAB\nSAN MTS\n\n71 ABE TEF\n\n\f"
        ],
        [
         "27",
         "27",
         "cs.CE",
         "Computational Engineering",
         "1610.02258v1.pdf",
         "Parallel STEPS: Large Scale Stochastic Spatial Reaction-Diffusion\nSimulation with High Performance Computers\nWeiliang Chen1*, Erik De Schutter1\n1\n\nComputational Neuroscience Unit, Okinawa Institute of Science and Technology Graduate\nUniversity\n* Correspondence:\nWeiliang Chen\nw.chen@oist.jp\nKeywords: STEPS, parallel simulation, stochastic, spatial reaction-diffusion, HPC.\nAbstract\nStochastic, spatial reaction-diffusion simulations have been widely used in systems biology and\ncomputational neuroscience. However, the increasing scale and complexity of simulated models and\nmorphologies have exceeded the capacity of any serial implementation. This led to development of\nparallel solutions that benefit from the boost in performance of modern large-scale supercomputers.\nIn this paper, we describe an MPI-based, parallel Operator-Splitting implementation for stochastic\nspatial reaction-diffusion simulations with irregular tetrahedral meshes. The performance of our\nimplementation is first examined and analyzed with simulations of a simple model. We then\ndemonstrate its usage in real-world research by simulating the reaction-diffusion components of a\npublished calcium burst model in both Purkinje neuron sub-branch and full dendrite morphologies.\nSimulation results indicate that our implementation is capable of achieving super-linear speedup for\nbalanced loading simulations with reasonable molecule density and mesh quality. In the best scenario\na parallel simulation with 2000 processes achieves more than 3600 times of speedup relative to its\nserial SSA counterpart and more than 20 times of speedup relative to parallel simulation with 100\nprocesses. While simulation performance is affected by unbalanced loading, a substantial speedup\ncan still be observed without any special treatment.\n\n\fParallel STEPS\n1\n\nIntroduction\n\nActive research in the fields of systems biology and computational neuroscience, such as the study on\nPurkinje cell calcium dynamics (Anwar et al., 2014), has significantly boosted the development of\nspatial stochastic reaction-diffusion simulators in recent years. These simulators split into two major\ncategories according to the methods used, voxel-based and particle-based. Voxel-based simulators,\nsuch as STEPS (Hepburn et al., 2012), URDME (Drawert et al., 2012), MesoRD (Hattne et al.,\n2005), NeuroRD (Oliveira et al., 2010), divide the geometry into small voxels where different spatial\nvariants of the Gillespie Stochastic Simulation Algorithm (Gillespie SSA) (Gillespie, 1976) are\napplied. Particle-based simulators, for example Smoldyn (Andrews and Bray, 2004) and MCell (Kerr\net al., 2008), track the Brownian motion of individual molecules, and simulate molecule reactions\naccording to their collisions. Although achieving great success, both voxel-based and particle-based\napproaches are known to be computational expensive. Particle-based simulators suffer from the\nrequirement of tracking the position and movement of every individual molecule in the system.\nWhile tracking individual molecules is not required for voxel-based simulators, the exact solution of\nthe Gillespie exact SSA is known to be highly sequential and inefficient for large-scale simulation\nwith massive numbers of SSA events (Dematté and Mazza, 2008).\nThere is a high need for more efficient stochastic spatial reaction-diffusion simulation of large scale\nmodels. Over the years many efforts have been devoted and achieved considerable successes, both in\nalgorithm developments and software implementations, but the increasing simulation scale and\ncomplexity has significantly exceeded the speedup gained from these efforts.\nSince the introduction of the original Gillespie SSA, the performances of voxel-based simulators\nhave been substantially improved thanks to new algorithms and data structures. Giving N as the\nnumber of kinetic events (reactions and diffusions) in the system, the computational complexity of a\nsingle SSA iteration has been reduced from O(N) with the Direct method (Gillespie, 1976), to\nO(log2(N)) with Gibson and Bruck’s modification (Gibson and Bruck, 2000), to O(1) with the\ncomposition and rejection SSA (Slepoy et al., 2008). Approximate solutions for well-stirred systems\nsuch as the well-known tau-leaping method (Gillespie, 2001) can also be applied to spatial domain\n(Koh and Blackwell, 2011; Marquez-Lago and Burrage, 2007), which provide further speedups with\ncontrollable errors. It is clear, however, that the performance of a serial simulator is restricted by the\nclock speed of a single computing core, while multi-core CPU platforms have become main stream.\nOne possible way to bypass the clock speed limitation is parallelization, but the development of an\nefficient and scalable parallel solution has proven to be challenging. An optimistic Parallel Discrete\nEvent Simulation (PDES) solution has been applied to the exact Gillespie SSA, achieving maximum\n8 times of speedup with a 12 core cluster (Dematté and Mazza, 2008). This approach has been further\ninvestigated and tested with different synchronization algorithms available for PDES systems (Wang\net al., 2009), such as Time Warp (TW), Breathing Time Bucket (BTB) and Breathing Time Warp\n(BTW). Their results indicate that while considerable speedup can be achieved, for example 5 times\nof speedup with 8 cores using the BTW method, the speedup decays rapidly once inter-node\ncommunication is involved, due to significant network latency. Another optimization attempt of the\nPDES solution with thread-based implementation has been reported lately (Lin et al., 2015), which\nachieved 9 times of speedup with 32 processing threads. All above studies show problematic\nscalability, due to the dramatic increase of rollbacks triggered by conflicting diffusion events\nbetween partitions, even with the support from well-developed PDES algorithms.\n\n2\n\n\fParallel STEPS\nParallelization of approximate SSA methods has also been investigated. D'Agostino and colleagues\n(D'Agostino et al., 2014) introduced a parallel spatial tau-leaping solution with both Message Passing\nInterface (MPI) based and Graphics Processing Unit (GPU) based implementations, achieving 20\ntimes of speedup with a 32 node cluster, and about 50 times of speedup on a 192 core GTX-Titan.\nTwo variants of Operator-Splitting approach, originating from the serial Gillespie Multi-Particle\n(GMP) method (Rodriguez et al., 2006), have been independently introduced by Roberts (Roberts et\nal., 2013) and Vigelius (Vigelius et al., 2011) to their GPU implementations, both achieve above\nhundred times of speedup comparing to serial SSA implementation. It is worth noticing that all above\nparallel solutions divide the simulated geometries into sub-volumes using a cubic mesh grid, which\nmay not be suitable to accurately represent realistic morphologies.\nSeveral studies of parallel particle-based implementations have been reported. Balls and colleagues\n(Balls et al., 2004) demonstrate their early attempt of parallel MCell implementation under KeLP\ninfrastructure (Fink et al., 1998) with a 64 core cluster. Two GPU-based parallel implementations of\nSmoldyn have also been reported (Dematté, 2012; Gladkov et al., 2011), both show 100 ~ 200 times\nof speedup comparing to the CPU-based serial Smoldyn implementation.\nHere we introduce an MPI-based parallel implementation of the STochastic Engine for Pathway\nSimulation (STEPS) (Hepburn et al., 2012). STEPS is a GNU licensed, stochastic spatial reactiondiffusion simulator implemented in C++ with a Python user interface. The main solver of current\nserial STEPS simulates reaction and diffusion events by applying a spatial extension of the\ncomposition and rejection SSA (Slepoy et al., 2008) to sub-volumes of unstructured tetrahedral\nmeshes. Our parallel implementation aims to provide an efficient and scalable solution that can\nutilize state-of-art supercomputers to simulate large scale stochastic reaction-diffusion models with\ncomplex morphologies. In Section 2 we explain the main algorithm and details that are essential to\nour implementation. In Section 3, we then showcase two examples, from simple model to complex\nreal-world research model, and analyze the performance of our implementation with their results.\nFinally, we will discuss possible future developments of parallel STEPS in Section 4.\n2\n\nMethods\n\nWe choose the MPI protocol on CPU clusters as the development environment of our parallel\nimplementation, since it is currently the most well-supported parallel environment in academic\nresearch. Modern clusters allow us to explore the scalability of our implementation with massive\nnumber of computing nodes, and provide insightful information for further optimization towards\nsuper large scale simulations. The MPI-based implementation also serves as the foundation of future\nimplementations with other parallel protocols and hardware such as GPU and Intel Xeon Phi clusters.\nPrevious attempts (Dematté and Mazza, 2008; Lin et al., 2015; Wang et al., 2009) of the exact\nGillespie SSA parallelization have shown that system rollbacks triggered by straggler cross-process\ndiffusion events can critically negate any performance gained from parallelization. The issue is\nfurther intensified for MPI-based implementation due to the significance of network latency. To take\nfull advantage of parallelization, it is important to relax the exact time dependency of diffusion\nevents and take an approximate, time-window approach that minimizes data communication and\neliminates system rollbacks. Inspired by the GMP method, we developed a tetrahedral based\nOperator-Splitting algorithm as the fundamental algorithm of our parallel implementation. The serial\nimplementation of this algorithm and its accuracy have been discussed previously (Hepburn et al.,\n2016). Here we discuss implementation details of the parallel version.\n\n3\n\n\fParallel STEPS\n2.1 Initialization of a Parallel STEPS Simulation\nTo initialize a parallel STEPS simulation, the user is required to provide the biochemical model and\ngeometry to the parallel solver. For the purpose of user convenience, our parallel implementation\naccepts the same biochemical model and geometry data used in the serial SSA solver as inputs. In\naddition, partitioning information of the mesh is required so that tetrahedrons can be distributed and\nsimulated. The partitioning information is a simple list which can be generated automatically using\nthe grid based partitioning solution provided in the STEPS utility module, or more sophisticated,\nthird-party partitioning applications such as Metis (Coupez et al., 2000). The STEPS utility module\ncurrently provides necessary support functions for format conversions between STEPS and Metis\nfiles.\nAssuming that a list of tetrahedrons is hosted by an MPI process p, {tet | tet is hosted by p}, parallel\nSTEPS firstly creates a standard Gillespie SSA system for all reactions in each hosted tetrahedron.\nThis includes the population state of all species and reaction propensities. For each of the reaction\nRtet,p, it also creates an update dependency list deps(Rtet,p), that is, the list of reactions and diffusions\nthat require an update if Rtet,p is chosen and applied by the SSA. As a reaction only affects molecule\nstates and propensities of reactions and diffusions within its own tetrahedron, the above information\ncan be stored locally in p. The localized storage of SSA and dependency information significantly\nreduces the memory consumption for each process comparing to the serial SSA implementation,\nwhich is crucial to the simulator’s performance. We will further address its importance with\nsimulation results in section 3.\nThe simulation also stores the list of hosted diffusions {Dtet,p | Dtet,p is in tet hosted by p} and the\ndependency list deps(Dtet, p) for each diffusion Dtet,p. In addition, if a tetrahedron tet is a boundary\ntetrahedron of p, in other words, the species state of tet is affected by diffusions in tetrahedrons\nhosted by other MPI processes rather than p, a species update dependency list for every diffusive\nspecies Stet,p in tet is also created. The species update dependency list, deps(Stet,p), is defined as the\nlist of reactions and diffusions which are hosted by p, and require update if the count of Stet,p is\nmodified by cross-process diffusion. The species dependency list allows each MPI process to update\nhosted reactions and diffusions independently after receiving molecule change information from\nother processes, thus reducing the need for cross-process communication.\nFurthermore, a suitable diffusion time window is determined according to the biochemical model and\ngeometry being simulated (Hepburn et al., 2016). Given dS,tet as the local diffusion rate for diffusive\nspecies S in tetrahedron tet, each process p computes a local minimal time window 𝜏\" =\n𝑚𝑖𝑛',)*) (1.0/𝑑',)*) ) , over all diffusive species in every hosted tetrahedron. A collective\ncommunication is then performed to search for the global minimum, 𝜏 = 𝑚𝑖𝑛\" (𝜏\" ), which is set as\nthe diffusion time window for every process in the simulation. It is worth mentioning that 𝜏 is\ncompletely determined by the biochemical model and geometry and stays constant regardless the\nchanges in molecule population. Therefore no continuous update of 𝜏 is required during the\nsimulation.\nThe final step is to initialize the molecule population state of the simulation, which can be done using\nvarious API functions provided in parallel STEPS. Once this is completed, the simulation is ready to\nenter the runtime main loop described below.\n\n4\n\n\fParallel STEPS\n2.2 Runtime Main Loop\nThe runtime main loop for each MPI process is shown in Algorithm 1. When a process is asked to\nexecute the simulation from time t to tend, a remote change buffer for cross-process data\ncommunication is created for each of the neighboring processes of p. Details of the buffer will be\ndiscussed later.\nThe entire runtime [t, tend] is divided into iterations of constant time window 𝜏, whose value is\ncomputed during initialization. At the start of every time window, each process first executes the\nReaction SSA operator for the period of 𝜏. In addition to the standard exact SSA routines, the process\nalso keeps tracking the update time and occupancy for each reactant and product species of reactions\n(Hepburn et al., 2016). The mean occupancies of species during the period of 𝜏 are used later to\ncompute the number of molecules that diffuse to neighboring tetrahedrons in the Diffusion operator.\nThe parallel solver treats diffusion events differently based on the ownerships of tetrahedrons\ninvolved. If both the source and designation tetrahedrons of a diffusion event are in a single process,\nthe diffusion is applied directly. If a diffusion is cross-process, that is, the source tetrahedron and\ndesignation tetrahedron are hosted by different processes, the change to the source tetrahedron is\napplied directly, while the change to the designation tetrahedron is registered to the corresponding\nremote change buffer. Once all diffusion events are applied or registered, the buffers are sent to\ndesignation neighboring processes where molecule changes are applied remotely.\nThe algorithm is designed for optimal operation in a parallel environment. Most of its operations can\nbe performed independently without network communication. In fact, the only data communication\nrequired is the transfer of remote change buffers between neighboring processes. This leads to two\nimportant implications. First and foremost, the communication is strictly regional, meaning that each\nprocess only communicates to a small subset of processes that it shares geometry boundaries with,\nregardless of the overall scale of the simulation. Secondly, thanks to the non-blocking\ncommunication, each process can start the reaction SSA Operator for next iteration t1, as soon as it\nhas received remote change buffers for current iteration t0 from all its neighboring processes and\napplied those changes (Figure 1). Therefore, data communication can be hidden behind computation,\nwhich helps to reduce the impact of network latency.\nSince the remote change buffer is the only data being transferred across the network, it is important\nto limit its size so that communication time can be reduced. Furthermore, an efficient registering\nmethod is also required since all molecule changes applied to remotely hosted tetrahedrons need to be\nregistered. Instead of recording every cross-process diffusion event, the remote change buffer records\nthe accumulated change of a molecule species in a tetrahedron hosted remotely. Thus the size of the\nremote change buffer is upper-bounded by the number of remotely-hosted neighboring tetrahedrons\nand the number of diffusive species within those tetrahedrons. Each process also stores mirror images\nof remotely-hosted neighboring tetrahedrons. Each mirror image contains information of the\nneighboring tetrahedron’s host process 𝑝′, its index on the hosted process 𝑡𝑒𝑡 6 , indices of all diffusive\nspecies S within, and a location marker 𝐿𝑜𝑐)*) : ,' for each of the diffusive species. Correspondingly,\nthe remote change buffer stores entries of changes one by one in a vector, where each entry consists\nof three elements, 𝑡𝑒𝑡 6 , S, as well as the accumulated molecule change 𝑚)*) : ,' . When a new diffusion\nevent causes molecule change of species S in 𝑡𝑒𝑡 6 of 𝑝′, the host process of source tetrahedron first\nchecks the entry data at 𝐿𝑜𝑐)*) : ,' of the remote change buffer for 𝑝′. If the entry’s tetrahedron index\nas well as species index matches the information of 𝑡𝑒𝑡 6 and S, the accumulated change of this entry\nis increased according to the diffusion event. As each buffer is reset after its content has been sent to\n5\n\n\fParallel STEPS\ncorresponding process, a mismatch of entry information indicates such reset has taken place since\nprevious registration of the same diffusion event, in which case a new entry (𝑡𝑒𝑡 6 , S, 𝑚)*) : ,' ) is\nappended to the end of the buffer and the location of this entry is stored back to the location marker\n𝐿𝑜𝑐)*) : ,' for future reference. With a suitable data structure, both accessing entry data and appending\nnew entry can achieve constant complexity, providing efficient solution for registering remote\nmolecule changes.\n3\n\nResults\n\nAs the accuracy of the solution has been examined in a previous paper (Hepburn et al., 2016), here\nwe mainly focus on the performance and scalability of our implementation. Simulations reported in\nthis paper were run on OIST’s university-shared high performance cluster, “Sango”. Each computing\nnode on Sango has two 12-core 2.5GHz Intel Xeon E5-2680v3 processors, sharing 128GiB of system\nmemory. All nodes are interconnected using 56Gbit/s InfiniBand FDR. Due to the sharing policy\nonly a limited number of cores could be used for our tests, and cluster conditions were different for\neach test and may scatter across the entire cluster. It is clear to us that cluster condition inevitably\naffects simulation performance. To accommodate this impact and provide important insights of how\nour implementation performs under real-life cluster restrictions, we repeated the tests multiple times,\neach starting at a different time and date with varying cluster condition, and report the averaged\nresults.\nSimulation performance were measured by both speedup and efficiency. Each simulation was run for\na predefined period, and the wall-clock time was recorded. The averaged wall-clock time for a set of\nrepeated simulations is hereby noted as Tp, where p is the number of MPI processes used in each\nsimulation. The speedup of a parallel simulation with p processes relative to the one with q processes\nis defined as Sp/q = Tq / Tp. Specifically, the speedup of parallel simulation with p processes relative to\nits serial SSA counterpart is defined as Sp/SSA = TSSA / Tp, where TSSA is the wall-clock time for the\nsame simulation run by the serial SSA solver.\nCorrespondingly, we define the efficiency of a simulation with p processes relative to the one with q\n<\nprocesses as 𝐸\"/< = 𝑆\"/< ∙ . The efficiency measurement is commonly used to study the scalability\n\"\n\nof a parallel implementation. Specifically, the strong scalability of an implementation is measured by\ncalculating the efficiency of parallel simulations of a fixed size problem with increasing number of\ncomputing processes, while weak scalability measures the efficiency of simulation whose overall\nproblem size increases in direct proportion to the number of computing processes. Both scalabilities\nof our implementation were investigated here.\n3.1 Reaction-Diffusion Simulation with Simple Model and Geometry\nWe first examine the simulation results of a fixed size reaction-diffusion problem. The simulated\nmodel was used to benchmark our serial spatial SSA solver in previous paper (Hepburn et al., 2012)\nand to test accuracy of our serial Operator-Splitting solution (Hepburn et al 2016), which consists of\n10 diffusive species, each with differing diffusion coefficients and initial molecule counts, and 4\nrecursive reactions with various rate constants. The model was simulated in a 10×10×100𝜇𝑚A\ncuboid mesh with 3363 tetrahedrons. Tetrahedrons were linearly partitioned based on the y and z\ncoordinates of their barycenter, where the numbers of partitions of each axis for a simulation with p\nprocesses was arranged as [Partsx = 1, Partsy = 5, Partsz = 𝑝/5]. At the beginning of each simulation,\nspecies molecules were placed uniformly into the geometry, and the simulation was run for tend = 20\nseconds, after which the wall-clock time was recorded. We started each series of simulations from p\n6\n\n\fParallel STEPS\n= 5 and progressively increased the number of processes by 5 each time until p = 300. Each series\nwas repeated 30 times to produce an average result.\nSpeedup and efficiency are reported relative to the simulation result with 5 processes, in other words,\n𝑆\"/C and 𝐸\"/C . By increasing the number of processes, simulation performance of the fixed size\nproblem improves dramatically. In fact, the simulation maintains super-linear speedup up until p ≈\n250 (Figure 2A). While the efficiency decreases in general, it remains above 0.8 with p = 300 (Figure\n2B), where on average each process hosts approximately 10 tetrahedrons.\nIn addition to the overall wall-clock time, we also recorded the time cost of each algorithm segment\nto analyze the behavior of the implementation. The whole time cost for the simulation Ttotal is divided\ninto three portions. The computation time Tcomp includes the time cost for reaction SSA and the cost\nof diffusion operations within the process. The synchronous time Tsync includes the time cost for\nreceiving remote change buffers from neighboring processes, and the time cost for applying those\nchanges. The time spent on waiting for the buffer’s arrival, as well as the waiting time for all buffers\nto be sent after reaction SSA is recorded as the idle time, Tidle. In summary,\nTtotal = Tcomp + Tsync + Tidle\nA detailed look at the time cost distribution of a single series trial (Figure 2C) suggests that the\nmajority of speedup is contributed by Tcomp, which is consistently above the theoretical ideal (Figure\n2D), thanks to the improvement of memory caching performance caused by distributed storage of\nSSA and update dependency information mentioned above. The result shows that Tsync also decreases\nsignificantly as the number of processes increases, however, as the number of boundary tetrahedrons\nare limited in the simulations, Tsync has the smallest proportion in overall time consumption (Figure\n2C). Another important finding is that the change of Tidle becomes insignificant when p > 100. Since\nTcomp and Tsync decrease as p increases, Tidle progressively becomes critical in determining the\nsimulation performance. Other trials of simulations exhibited similar results but are not shown here.\nTo further study how molecule density affects simulation performance, we repeat the above test with\ntwo new settings, one reduces the initial count of each molecule species by 10 times, and the other\nincreases molecule counts by 10 times (Figure 3A). We name these tests “Default”, “0.1x” and “10x”\nrespectively. The speedups relative to the serial SSA counterparts 𝑆\"/''D are also reported for\ncomparison (Figure 3B). The 0.1x Simulations receive less performance boost from parallelization\ncomparing to the default and 10x simulations. This is because in the 0.1x simulations Tcomp quickly\ndecreases below Tidle, and the speedup becomes less significant as Tidle is mostly consistent\nthroughout the series (Figure 3C). In the 10x simulations Tcomp maintains its domination of the three\ntiming segments, thus simulations achieve similar speedup ratio as the default ones (Figure 3D). This\nresult also indicates that 𝑆\"/''D greatly depends on molecule density. In general, parallel simulation\nwith high molecule density and high number of processes can achieve higher speedup relative to the\nserial SSA counterpart (Figure 3B).\nMesh coarseness also affects simulation performance greatly. Figure 4 shows the results of\nsimulations with the same model and geometry dimensions, but different numbers of tetrahedrons\nwithin the meshes. Simulations with a finer mesh generally take longer to complete because there are\nmore diffusion events, however, the time increment is not directly proportional to the increment of\ntetrahedron count. In fact, increasing the tetrahedron count from 13,009 to 113,096 only increases the\nwall-clock time for approximate 24% with p = 300, while the simulation with 13,009 tetrahedrons\ntakes 233% more time than the 3,363 tetrahedrons case (Figure 4B). Figure 4C shows that both\n7\n\n\fParallel STEPS\n13,009 and 113,096 cases achieve dramatic relative speedups from parallelization, which are greatly\nabove the theoretical ideal. This is because simulation with fine mesh and low process count has a\nhigh memory footprint per process and is unfriendly to memory caching. Massive parallelization\ngreatly reduced the memory footprint for each process, thus improving the computational efficiency\nthanks to the caching effect.\nWeak scalability can be studied by measuring the efficiency of a simulation whose size increases in\ndirect proportion to the number of processes. We use the “Default” simulation with 300 processes as\nthe baseline, and increase the problem size by duplicating the geometry along a specific axis, as well\nas increasing the number of initial molecules proportionally. Table 1 gives a summary of all\nsimulation settings. As the problem size increases, the simulation efficiency progressively\ndeteriorates (Figure 5). While around 95% of efficiency is maintained for doubling the problem size,\ntripling the problem size reduces the efficiency to about 80%. This is an expected outcome of the\ncurrent implementation, because although the storage of reaction SSA and update dependency\ninformation are distributed, each process in the current implementation still keeps the complete\ninformation of the mesh geometry. Therefore the memory footprint per process increases as problem\nsize increases, reducing the simulation efficiency. Optimizing memory footprint for super large scale\nproblems is one of the main focuses in our next development iteration. This result also indicates that\ngeometry partitioning plays an important role in determining simulation performance, as extending\nthe mesh along the z axis gives better efficiency than along the y axis, even though they have similar\nnumbers of tetrahedrons. This can be explained by the increase of boundary tetrahedrons in the latter\ncases. Since the number of boundary tetrahedrons determines the upper-bound of the size of remote\nchange buffer and consequently the time for communication, reducing the number of boundary\ntetrahedrons is a general recommendation for geometry partitioning in parallel STEPS simulations.\n3.2 Large Scale Reaction-Diffusion Simulation with Real-World Model and Geometry\nSimulations from real-world research often consist of reaction-diffusion models and geometries that\nare notably more complex than the ones studied above. As a preliminary example, we extracted the\nreaction-diffusion components of a previously published spatial stochastic calcium burst model\n(Anwar et al., 2013) as our test model to investigate how our implementation performs with large\nscale real-world simulations. The extracted model consists of 15 molecule species, 8 of which are\ndiffusive, as well as 22 reactions. Initial molecule concentrations, reaction rate constants and\ndiffusion coefficients were kept the same as in the published model.\nThe Purkinje cell sub-branch morphology, published along with the model, was also used to generate\na tetrahedral mesh that is suitable for parallel simulation. The newly generated mesh has 111,664\ntetrahedrons, and was partitioned using Metis and STEPS supporting utilities. As discussed before,\nreducing boundary tetrahedrons is the general partitioning strategy for parallel STEPS simulations.\nThis is particularly important for simulations with a tree-like morphology since grid based\npartitioning approach used for the simple cuboid above cannot capture and utilize spatial features of\nsuch morphology. The sub-branch mesh for our simulation is partitioned based on the connectivity of\ntetrahedrons. Specifically, a connectivity graph of all tetrahedrons in the mesh was presented to Metis\nas input. Metis then produced a partitioning profile which met the following criteria. First of all, the\nnumber of tetrahedrons in each partition was similar. Secondly, tetrahedrons in the same partition\nwere all connected. Finally, the average degree of connections is minimum. Figure 6 shows the mesh\nitself as well as two partitioning profiles generated for p = 50 and p = 1000. As a preliminary test,\nthis partitioning procedure does not account for any size differences of tetrahedrons and the influence\nfrom the biochemical model and molecule concentrations, although their impacts can be significant\n8\n\n\fParallel STEPS\nin practice. At present, some of these factors can be abstracted as weights between elements in Metis,\nhowever, substantial manual scripting is required and the solution is project-dependent.\nTo mimic the calcium concentration changes caused by voltage-gated calcium influx simulated in the\npublished results (Anwar et al., 2013), we also extracted the region-dependent calcium influx profile\nfrom the result data, which can be applied to the parallel simulation periodically. Depending on\nwhether this profile is applied, the parallel simulation behaved differently. Without calcium influx,\nthe majority of simulation time was spent on diffusion events of mobile buffer molecules. As these\nbuffer molecules were homogeneously distributed within the mesh, the loading of each process was\nrelatively balanced throughout the simulation. When the calcium influx was applied and constantly\nupdated during the simulation, it triggered calcium burst activities that rapidly altered the gradient of\ncalcium concentration, consequently unbalancing the process loadings. It also activated calciumdependent pathways in the model and increased the simulation time for reaction SSA operations.\nTwo series were simulated, one without calcium influx and data recording, and the other one with the\ninflux enabled and data recorded periodically. Each series of simulations started from p = 50, and\nfinished at p = 1000, with an increment of 50 processes each time. Both series of simulations were\nrun for 30ms, and repeated 20 times to acquire the average wall-clock times. For the simulations with\ncalcium influx, the influx rate of each branch segment was adjusted according to the profile every\n1ms, and the calcium concentration of each branch was recorded to a file every 0.02ms, as in the\noriginal simulation. Figure 7A shows the recorded calcium activity of each branch segment over a\nsingle simulation trial period, which exhibits great spatial and temporal divergences. As a\nconsequence of the calcium influx changes, process loading of the series was mostly unbalanced so\nthat simulation speedup and efficiency were significantly affected. However, a substantial\nimprovement can still be observed (Figure 7B and C). To further improve the performance of\nsimulations with strong concentration gradients, a sophisticated and efficient dynamic load balancing\nalgorithm is required, which will be briefly discussed later.\nFinally, to test the capability of our implementation for full cell stochastic spatial simulation in the\nfuture, we generated a mesh of full Purkinje dendrite tree from a publically available surface\nreconstruction (3DModelDB (McDougal and Shepherd, 2015), ID: 156481) and applied the above\nmodel on it. To our best knowledge this is the first parallel simulation of mesoscopic level, stochastic,\nspatial reaction-diffusion system with full cell dendritic tree morphology. The mesh consisted of\n1,044,155 tetrahedrons. As the branch diameters of the original reconstruction have been modified\nfor other purposes, the mesh is not suitable for actual scientific study but only to evaluate\ncomputational performance. Because of this reason, and the fact that no calcium influx profile can be\nacquired for this reconstruction, we only simulated the one without calcium influx. The simulation\nseries started from p = 100, and progressively increased to p = 2000 by an increment of 100\nprocesses each time. The maximum number of processes (p = 2000) was determined by the fairsharing policy of the cluster. Figure 8A gives an overview of the full cell morphology as well as a\nzoom-in look of the mesh. Both speedup and efficiency relative to simulation with p = 100 (Figure 8,\nB and C) show super-linear scalability and reach a peak with p = 2000. This result suggests that\nsimulation performance may be further improved with a higher number of processes.\nIt is also worth mentioning that all parallel simulations above perform drastically better than their\nserial SSA counterparts. The speedups relative to the serial SSA simulations are shown in Figure 9.\nEven in the most realistic case with dynamically updated calcium influx as well as data recording,\nwithout any special load balancing treatment, the parallel simulation with 1000 processes is still 500\ntimes faster than the serial SSA simulation. The full cell parallel simulation without calcium influx\n9\n\n\fParallel STEPS\nachieves an unprecedented 3600 times speedup with 2000 processes. This means our implementation\nwas not only faster to finish than a serial simulation, but is also capable to compute 0.8 times more\nrealizations of the stochastic simulation with the same amount of computing resource and time\ncompared to a batch of serial SSA realizations.\n4\n\nDiscussion and Future Directions\n\nOur current parallel STEPS implementation achieves significant performance improvement and good\nscalability, as shown in our test results. However, as a preliminary implementation, it lacks or\nsimplifies several functionalities that could be important to real-world simulations. These\nfunctionalities require further investigation and development in future generations of parallel STEPS.\nComparing to the serial version of STEPS, one core functionality missing from our current\nimplementation is the ability to simulate membrane potential as well as voltage-dependent gating\nchannels (Hepburn et al., 2013). This is the main reason why we were unable to fully simulate the\nstochastic spatial calcium burst model with the Purkinje sub-branch morphology in our example, but\nrely on the calcium influx profile extracted from previous serial simulation instead. The combined\nsimulation of neuronal electrophysiology and molecular reaction-diffusion has recently raised\ninterests as it bridges the gap between computational neuroscience and systems biology, and is\nexpected to be in great use in the foreseeing future. To address such demand, we are actively\ncollaborating with the Human Brain Project (Markram, 2012) to develop a parallel implementation of\nthe corresponding electric field (E-Field) sub-system, which will be integrated into parallel STEPS\nupon its completion.\nAs analyzed in the results, the majority of performance speedup is contributed by the reduction of\nTcomp thanks to parallel computing. Eventually Tidle becomes the main bottleneck as it is mostly\nconstant relative to the process count, unlike Tcomp which decreases consistently. This observation\nsuggests two future investigation and development directions, maximizing the speedup gained from\nTcomp, and minimizing Tidle.\nMaximizing the speedup gained from Tcomp is important to real-world research because significant\nperformance improvement needs to be achieved with reasonable amount of computational resource.\nAdapting advanced algorithms and optimizing memory caching ability are two common approaches\nto achieve this goal. At present, we mainly focus on further optimizing memory footprint and caching\nability for super-large scale simulations. In the current implementation, although reaction SSA and\npropensity update information are distributed, each process still stores the complete information of\nthe biochemical model and the mesh. This noticeably affects the weak scalability of our\nimplementation, as shown in Figure 5. The redundant information is so far required for the purpose\nof interfacing with other non-parallel sub-systems such as serial E-Field, but we will investigate\nwhether the model and geometry information can be split based on the demand of individual\nprocesses.\nProcess load balancing plays a crucial role in determining the idle time of the simulation Tidle, and\nconsequently the maximum speedup the simulation can achieve. In an unbalanced-loading simulation,\nprocesses will always be idle until the slowest one finishes, thus dramatically increasing Tidle. This\nissue is essential to stochastic spatial reaction-diffusion simulations as high concentration gradients\nof molecules can be observed frequently in many real-world models similar to our calcium burst\nmodel. Because molecule concentrations change significantly during simulation due to reactions and\ndiffusion, the loading of each process may change rapidly. While adding model and initial molecule\n10\n\n\fParallel STEPS\nconcentration information to the partitioning procedure may help to balance the loading for early\nsimulation, the initial partitioning will eventually become inefficient as molecule concentrations\nchange. An efficient load balancing algorithm is required to solve this problem. The solution should\nbe able to redistribute tetrahedrons between processes automatically on the fly based on their current\nworkloads. Efficiency is the main focus of the solution, because constantly copying tetrahedron data\nbetween processes via network communication can be extremely time consuming, and may\novershadow any benefit gained from the rebalancing.\nIn its current status, our parallel STEPS implementation remains a great improvement on the serial\nSSA solution. The calcium burst simulation with Purkinje cell sub-branch morphology, dynamic\ncalcium influx and periodic data recording is representative of the simulation condition and\nrequirements of typical real-world research. Similar models that previously required years of\nsimulations can now be completed within days. The shortening of the simulation cycle is greatly\nbeneficial to research as it provides opportunities for further fine-tuning the model based on\nsimulation results.\nConflict of Interest\nThe authors declare that the research was conducted in the absence of any commercial or financial\nrelationships that could be construed as a potential conflict of interest.\nAuthor Contributions\nWC designed, implemented and tested the parallel STEPS described, as well as drafted the\nmanuscript. EDS conceived of and supervised the STEPS project and helped draft the manuscript.\nBoth authors contributed to the manuscript and read and approved the submission.\nAcknowledgments\nThis work was funded by the Okinawa Institute of Science and Technology Graduate University. All\nsimulations were run on the “Sango” cluster at the Okinawa Institute of Science and Technology. We\nare very grateful to Iain Hepburn of the Computational Neuroscience Unit, OIST, Japan, for\ndiscussion and critical review of the initial draft of this manuscript.\nReferences\nAndrews, S. S., and Bray, D. (2004). Stochastic simulation of chemical reactions with spatial\nresolution and single molecule detail. Phys. Biol. 1, 137–151. doi:10.1088/1478-3967/1/3/001.\nAnwar, H., Hepburn, I., Nedelescu, H., Chen, W., and De Schutter, E. (2013). Stochastic calcium\nmechanisms cause dendritic calcium spike variability. 33, 15848–15867.\ndoi:10.1523/JNEUROSCI.1722-13.2013.\nAnwar, H., Roome, C. J., Nedelescu, H., Chen, W., Kuhn, B., and De Schutter, E. (2014). Dendritic\ndiameters affect the spatial variability of intracellular calcium dynamics in computer models.\nFront Cell Neurosci 8, 168. doi:10.3389/fncel.2014.00168.\nBalls, G. T., Baden, S. B., Kispersky, T., Bartol, T. M., and Sejnowski, T. J. (2004). A large scale\nmonte carlo simulator for cellular microphysiology. Proceedings of 18th International Parallel\nand Distributed Processing Symposium, 42:26–30.\n11\n\n\fParallel STEPS\nCoupez, T., Digonnet, H., and Ducloux, R. (2000). Parallel meshing and remeshing. Applied\nMathematical Modelling 25, 153–175. doi:10.1016/S0307-904X(00)00045-7.\nD'Agostino, D., Pasquale, G., Clematis, A., Maj, C., Mosca, E., Milanesi, L., et al. (2014). Parallel\nsolutions for voxel-based simulations of reaction-diffusion systems. BioMed Research\nInternational 2014, 980501–10. doi:10.1155/2014/980501.\nDematté, L. (2012). Smoldyn on graphics processing units: massively parallel Brownian dynamics\nsimulations. IEEE/ACM Trans Comput Biol Bioinform 9, 655–667.\ndoi:10.1109/TCBB.2011.106.\nDematté, L., and Mazza, T. (2008). “On Parallel Stochastic Simulation of Diffusive Systems,” in\nComputational Methods in Systems Biology Lecture Notes in Computer Science. eds.M. Heiner\nand A. M. Uhrmacher (Berlin, Heidelberg: Springer Berlin Heidelberg), 191–210.\ndoi:10.1007/978-3-540-88562-7_16.\nDrawert, B., Engblom, S., and Hellander, A. (2012). URDME: a modular framework for stochastic\nsimulation of reaction-transport processes in complex geometries. BMC Syst Biol 6, 76.\ndoi:10.1186/1752-0509-6-76.\nFink, S. J., Baden, S. B., and Kohn, S. R. (1998). Efficient run-time support for irregular blockstructured applications. Journal of Parallel and Distributed Computing 50, 61–82.\nGibson, M. A., and Bruck, J. (2000). Efficient Exact Stochastic Simulation of Chemical Systems\nwith Many Species and Many Channels. J. Phys. Chem. A 104, 1876–1889.\ndoi:10.1021/jp993732q.\nGillespie, D. T. (1976). A general method for numerically simulating the stochastic time evolution of\ncoupled chemical reactions. Journal of Computational Physics 22, 403–434. doi:10.1016/00219991(76)90041-3.\nGillespie, D. T. (2001). Approximate accelerated stochastic simulation of chemically reacting\nsystems. J. Chem. Phys. 115, 1716. doi:10.1063/1.1378322.\nGladkov, D. V., Alberts, S., D'Souza, R. M., and Andrews, S. (2011). Accelerating the Smoldyn\nspatial stochastic biochemical reaction network simulator using GPUs. in (Society for Computer\nSimulation International), 151–158.\nHattne, J., Fange, D., and Elf, J. (2005). Stochastic reaction-diffusion simulation with MesoRD.\nBioinformatics 21, 2923–2924. doi:10.1093/bioinformatics/bti431.\nHepburn, I., Cannon, R., and De Schutter, E. (2013). Efficient calculation of the quasi-static electrical\npotential on a tetrahedral mesh and its implementation in STEPS. Front. Comput. Neurosci. 7.\ndoi:10.3389/fncom.2013.00129.\nHepburn, I., Chen, W., and De Schutter, E. (2016). Accurate reaction-diffusion operator splitting on\ntetrahedral meshes for parallel stochastic molecular simulations. J. Chem. Phys. 145, 054118–22.\ndoi:10.1063/1.4960034.\nHepburn, I., Chen, W., Wils, S., and De Schutter, E. (2012). STEPS: efficient simulation of\n12\n\n\fParallel STEPS\nstochastic reaction–diffusion models in realistic morphologies. BMC Systems Biology 6, 36.\ndoi:10.1186/1752-0509-6-36.\nKerr, R. A., Bartol, T. M., Kaminsky, B., Dittrich, M., Chang, J.-C. J., Baden, S. B., et al. (2008).\nFast Monte Carlo Simulation Methods for Biological Reaction-Diffusion Systems in Solution\nand on Surfaces. SIAM J. Sci. Comput. 30, 3126–3149. doi:10.1137/070692017.\nKoh, W., and Blackwell, K. T. (2011). An accelerated algorithm for discrete stochastic simulation of\nreaction–diffusion systems using gradient-based diffusion and tau-leaping. J. Chem. Phys. 134,\n154103. doi:10.1063/1.3572335.\nLin, Z., Tropper, C., Ishlam Patoary, M. N., McDougal, R. A., Lytton, W. W., and Hines, M. L.\n(2015). NTW-MT. in (New York, New York, USA: ACM Press), 157–167.\ndoi:10.1145/2769458.2769459.\nMarkram, H. (2012). The Human Brain Project. Scientific American 306, 50–55.\ndoi:10.1038/scientificamerican0612-50.\nMarquez-Lago, T. T., and Burrage, K. (2007). Binomial tau-leap spatial stochastic simulation\nalgorithm for applications in chemical kinetics. J. Chem. Phys. 127, 104101–10.\ndoi:10.1063/1.2771548.\nMcDougal, R. A., and Shepherd, G. M. (2015). 3D-printer visualization of neuron models. Front.\nNeuroinform. 9, 1–9. doi:10.3389/fninf.2015.00018.\nOliveira, R. F., Terrin, A., Di Benedetto, G., Cannon, R. C., Koh, W., Kim, M., et al. (2010). The\nRole of Type 4 Phosphodiesterases in Generating Microdomains of cAMP: Large Scale\nStochastic Simulations. PLoS ONE 5, e11725. doi:10.1371/journal.pone.0011725.\nRoberts, E., Stone, J. E., and Luthey-Schulten, Z. (2013). Lattice Microbes: high-performance\nstochastic simulation method for the reaction-diffusion master equation. J. Comput. Chem. 34,\n245–255. doi:10.1002/jcc.23130.\nRodriguez, J. V., Kaandorp, J. A., Dobrzynski, M., and Blom, J. G. (2006). Spatial stochastic\nmodelling of the phosphoenolpyruvate-dependent phosphotransferase (PTS) pathway in\nEscherichia coli. Bioinformatics 22, 1895–1901. doi:10.1093/bioinformatics/btl271.\nSlepoy, A., Thompson, A. P., and Plimpton, S. J. (2008). A constant-time kinetic Monte Carlo\nalgorithm for simulation of large biochemical reaction networks. J. Chem. Phys. 128, 205101.\ndoi:10.1063/1.2919546.\nVigelius, M., Lane, A., and Meyer, B. (2011). Accelerating reaction-diffusion simulations with\ngeneral-purpose graphics processing units. Bioinformatics 27, 288–290.\ndoi:10.1093/bioinformatics/btq622.\nWang, B., Yao, Y., Zhao, Y., Hou, B., and Peng, S. (2009). Experimental Analysis of Optimistic\nSynchronization Algorithms for Parallel Simulation of Reaction-Diffusion Systems. in (IEEE),\n91–100. doi:10.1109/HiBi.2009.22.\n\n13\n\n\fParallel STEPS\n\nAlgorithm 1: Parallel STEPS Runtime Main Loop\nGiven p as the running process, and neighs_p as the set of processes hosting tetrahedrons\nthat are neighbors to any tetrahedron hosted by p\nfor each 𝑝6 in neighs_p, do\ncreate remote_change_bufferS:\nend\ninitialize buffer_sent_complete = True\nwhile 𝑡 < 𝑡*UV , do\nif 𝑡 + 𝜏 > 𝑡*UV , then 𝜏 = 𝑡*UV − 𝑡\n# Reaction SSA Operator\ninitialize ∆𝑡 = 0.0\nfor each reactant and product S in each hosted reaction R, do\ninitialize 𝑡' = 0.0\ninitialize 𝑂' = 0.0\nend\nwhile ∆𝑡 ≤ 𝜏, do\ncompute the next reaction time 𝑡U*^) using SSA\nif ∆𝑡 + 𝑡_*`a > 𝜏 then break\nfind the next reaction event 𝑅U*^) using SSA\nif no 𝑅U*^) is found then break\n∆𝑡+= 𝑡_*`a\napply molecule changes caused by 𝑅U*^)\nfor each reactant and product species S of 𝑅U*^) , do\n𝑂' += 𝑁' (∆𝑡 − 𝑡' ), where 𝑁' is the previous molecule count of S at 𝑡'\n𝑡' = ∆𝑡\nend\n(cont. )\n\n14\n\n\fParallel STEPS\n\nAlgorithm 1: Parallel STEPS Runtime Main Loop (conc.)\n# Diffusion Operator\nif buffer_sent_complete == False, then\nwait for all buffers to be sent\nbuffer_sent_complete = True\nfor each 𝑝6 in neighs_p, do\nreset remote_change_bufferS:\nend\nfor each diffusive species S in tetrahedron tet hosted by p, do\nd = 𝑂' / 𝜏\n𝑁\nd , 𝜏, 𝑑',)*) )\ndistribute n molecules among neighbors of tet, where 𝑛 = 𝑏𝑖𝑛𝑜𝑚𝑖𝑎𝑙(𝑁\nfor each neighboring tetrahedron, neigh_tet, do\nif neigh_tet is hosted by p then\n#local diffusion\napply molecule changes for tet and neigh_tet\nelse if neibh_tet is hosted by 𝑝′, 𝑝′ ≠ 𝑝 then\n#cross-process diffusion\napply molecule changes for tet\nregister molecule changes for neigh_tet to remote_change_bufferS:\nend if\nend\nend\nupdate affected propensities of hosted reactions and diffusions in p\n# Cross-Process Communication and Update\nfor each p’ in neighs_p, do\nsend remote_change_bufferS: to 𝑝6 using non-blocking communication\nend\nbuffer_sent_complete = False\ninitialize n_buffer_received = 0\nwhile n_buffer_received ≠ sizeof(neighs_p), do\nwait until a buffer arrives\nreceive buffer\napply molecule changes and update propensities according to the buffer data\nn_buffer_received += 1\nend\n𝑡 += 𝜏\nend\nif buffer_sent_complete == False, then\nwait for all buffers to be sent\nfor each 𝑝6 in neighs_p, do\ndelete remote_change_bufferS:\nend\n\n15\n\n\fParallel STEPS\nTable 1. Simulation Settings for Weak Scalability Study\n\nGeometry Dimensions Initial Molecule Count Num. Processes\ndefault\n300\n10×10×100𝜇𝑚A\nA\n2x\n600\n10×10×200𝜇𝑚\nA\n2x\n600\n10×20×100𝜇𝑚\n3x\n900\n10×10×300𝜇𝑚A\nA\n3x\n900\n10×30×100𝜇𝑚\nFigures\n\nFigure 1. Schematic illustration of different runtime stages of two processes, p1 and p2, assuming that\np1 is the only neighboring process of p2. Once p2 receives and applies the remote change buffer from\np1 for iteration t0, it can immediately start the reaction SSA computation for iteration t1, without\nwaiting for p1 to complete iteration t0.\n\n16\n\n\fParallel STEPS\n\nFigure 2. Performance of parallel simulations with simple model and geometry. Each series start\nfrom p = 5 and progressively increase to p = 300. Both speedup and efficiency are measured relative\nto simulations with p = 5. (A) Simulations maintain super-linear speedup up until p ≈ 200. (B) In\ngeneral efficiency decreases as p increases, but remains above 0.8 in the worst case (p = 300). (C and\nD) TComp contributed the majority of the speedup as it is the most time consuming segment during\nsimulation, it maintains super-linear speedup throughout the whole series. However, as TComp\ndecreases, Tidle becomes a critical factor because its change is insignificant once p is above 100.\n\n17\n\n\fParallel STEPS\n\nFigure 3. Performances of simulations with different molecule density. (A) Speedups relative to\nsimulations with p = 5. Simulations with low molecule density (0.1x) achieve smaller speedups\ncompared to the default and high density (10x) cases. (B) In general, simulation with higher molecule\ndensity and larger scale of parallelization achieves higher speedup relative to the serial SSA\ncounterpart. (C) In the 0.1x cases, Tcomp rapidly decreases and eventually drops below Tidle, thus the\noverall speedup is less significant. (D) In the 10x cases, Tcomp remains above Tidle, therefore its\ncontribution to speedup is significant throughout the series.\n\n18\n\n\fParallel STEPS\n\nFigure 4. Performances of simulations with different mesh coarseness. (A) Meshes with the same\ngeometry dimensions but different number of tetrahedrons are simulated. (B) Simulation with finer\nmesh takes longer to complete, but the time increment is not directly proportional to the increment of\ntetrahedrons. (C) Speedups relative to T5. Simulation with finer mesh achieves much higher speedup\nin massive parallelization, thanks to the memory caching effect.\n\n19\n\n\fParallel STEPS\n\nFigure 5. Weak scalability of the implementation. (A) The default 10×10×100𝜇𝑚A mesh is\nexpended along either the y or z axis as problem size increases. (B) Efficiencies relative to the default\ncase (p = 300).\n\nFigure 6. (A) Tetrahedral mesh of a Purkinje cell sub-branch morphology. This mesh consists of\n111,664 tetrahedrons. (B) Partitioning generated by Metis for p = 50 and p = 1000. Each color\nsegment indicates a set of tetrahedrons hosted by a single process.\n20\n\n\fParallel STEPS\n\n21\n\n\fParallel STEPS\nFigure 7. Calcium burst simulations with a Purkinje cell sub-branch morphology. (A) Calcium\nactivity of each branch segment over a single trail period, visualized by the STEPS visualization\ntoolkit. The calcium activity shows large spatial and temporal divergences, which significantly\naffects the speedup (B) and efficiency (C) of the simulation.\n\nFigure 8. Performance of reaction-diffusion simulation with a mesh of a complete Purkinje dendrite\ntree. (A) Morphology of the mesh and a zoom-in look of its branches. The mesh consists of\n1,044,155 tetrahedrons. (B) Speedup relative to the simulation with p = 100 shows super-linear\nscalability. (C) Efficiency also increases as p increases, indicates that better efficiency may be\nachieved with more processes.\n\n22\n\n\fParallel STEPS\n\nFigure 9. Speedups of the parallel calcium burst simulations relative to their serial SSA counterparts.\nDashed line assumes that p processes are used to simulate a batch of p serial SSA realizations.\n\n23\n\n\f"
        ],
        [
         "28",
         "28",
         "cs.CE",
         "Computational Engineering",
         "1701.00557v2.pdf",
         "Discrete Optimal Global Convergence of an Evolutionary Algorithm for\nClusters under the Potential of Lennard Jones\n\narXiv:1701.00557v2 [cs.CE] 4 Jan 2017\n\nCarlos Barrón-Romero\ncbarron@correo.azc.uam.mx\nUniversidad Autónoma Metropolitana, Unidad Azcapotzalco\nAv. San Pablo No. 180, Col. Reynosa Tamaulipas, C.P. 02200,\nMEXICO\n2016\nAbstract\nA review of the properties that bond the particles under Lennard Jones Potential allow to states properties\nand conditions for building evolutive algorithms using the CB lattice with other different lattices. The new\nlattice is called CB lattice and it is based on small cubes, such the number of its vertices in a region is always\ngreater than the number of the particles of a cluster or a region of a lattice inside of the same size region\nof the CB lattice. Moreover, the estimation of a putative optimal cluster of the Lennard Jones can be done\ntheoretically in short time but, the proof, for such cluster to be the global optimal cannot be determining\nin\n\u0001\nefficient time. The proof of the global optimality for a cluster is related to the binomial coefficient m\n,\nwhich\nn\nit corresponds with the selection of n particles from a collection with m given particles. A set of propositions\nstates convergence and optimal conditions over the CB lattice for an evolutionary algorithm. The evolutionary\nalgorithm is a reload version of previous genetic algorithms based in phenotypes. The novelty using CB\nlattice, together with the other lattices, and ad-hoc cluster segmentation and enumeration, is to allow the\ncombination of genotype (DNA coding for cluster using their particle’s number) and phenotype (geometrical\nshapes using particle’s coordinates in 3D). A parallel version of an evolutionary algorithm for determining\nthe global optimality is depicted. The algorithm for determining global optimality (which it\u0001 is far from this\nresearch, and it is not included) is just a force brute searching algorithm with complexity m\n, where n is the\nn\nnumber of the cluster’s particles and m ≫ n is the number of particles of an appropriate CB lattice’s region.\nThe results presented are from a standalone program for a personal computer of the evolutionary algorithm,\nwhich can estimate all putative Optimal Lennard Jones Clusters from 13 to 1612 particles. The novelty are\nthe theoretical results for the evolutionary algorithm’s efficiency, the strategies with phenotype or genotype,\nand the classification of the clusters based in an ad-hoc geometric algorithm for segmenting a cluster into its\nnucleus and layers. Also, the standalone program is not only capable to replicate the optimal Lennard Jones\nclusters in The Cambridge Cluster Database (CCD), but to find new ones.\nKeywords: 02.60.Pn Numerical optimization, 21.60.Gx Cluster models, 31.15.Qg Molecular dynamics and\nother numerical methods, 36.40.Qv Stability and fragmentation of clusters, Lennard Jones Potential.\n\n1\n\nIntroduction\n\nThe problem for determining optimal clusters under Lennard Jones captures my attention for the possible implications for building an efficient algorithm for the class of NP. My techniques for the NP Class has an application\nfor building an appropriate algorithms for looking the optimal clusters under Lennard Jones Potential.\nOver a decade ago, I states the conjecture in [2] that IF lattice could contain all optimal clusters under the\nPotential of Lennard Jones (LJ). The title of the article: Minimum search space and efficient methods for structural\ncluster optimization was proposed as result of some inquiries from D. J. Wales, J. P. K. Doye, G.L. Xue and Bern\nHardke about the optimal LJ clusters.\n\n1\n\n\fFigure 1: MIF1739 contains the initial particles’ positions for the Cn∗ , n = 2, . . . , 1000.\n\na)\n\nb)\n\n∗\n∗\nFigure 2: a) C38\nand b) C664\ninside of a region of the IF lattice.\n\nIF lattice results from overlapping the positions of the IC lattice and FC lattice. The main result was a\nminimum region of IF, where all putative global optimal LJ clusters from 2 to 1000 can be found (see figures 1,\nand 2).\nFigure 1 was constructed by the selection of not repeated positions from where a given initial selection of\nparticles converges always by a local minimization process to its putative minimal LJ cluster. This set of positions\nis finite, and it can be enumerate, such each position corresponds with a unique id number.\nTherefore, it could be simple to locate a minimal LJ cluster by the set of its particles’ number of the cluster. I\nproposed a telephone algorithm, which is like make a phone call but, here n id particles corresponds to a cluster’s\nphone number from a set of an appropriate selection of m id numbers from a region of IF lattice. After the\nminimization if the value of the LJ potential is less than a previous cluster’s number, then it is the phone number\nof the cluster of n particles. Even, it is like more genotype, I did not introduce this type of DNA mechanics in\nmy previous genetic and evolutive algorithms in order to keep a phenotype representation (this means geometric\nshapes using the 3d particles’ coordinate). The main idea was to look for the putative optimal LJ cluster by\nan exhaustive searching.\nThis is a brute force algorithm with complexity related to the Newton binomio for\n\u0001\ncombinations, m\n.\nn\nEven with all putative optimal LJ clusters from 2 to 1000 in the lattice IF, I can not prove my conjecture.\nBut, reviewing my previous work and the mathematical properties of the LJ potential function, it is possible to\ndetermine from the cubic lattice (CB) all optimal LJ clusters in efficient time.\nThis paper presents an evolutive algorithm based in our previous Genetic algorithm. It is based on the partial\ngrowing sequence property that the optimal LJ clusters exhibe (To my knowledge, it was Northby [12] the pioneer\n\n2\n\n\fa)\n\nb)\n\n∗\n∗\nFigure 3: a) C37\n’s view with nucleus n7 and b) C37\n’s view with nucleus n1 IC.\n\na)\n\nb)\n\n∗\nFigure 4: a) C37\n’s view with nucleus n1 IR and b) non optimal, symmetric C37 with nucleus n7 .\n\na)\n\nb)\n\n∗\n∗\nFigure 5: a) C38\n’s view with nucleus n1, and b) C38\n’s classical view with nucleus n6\n\n3\n\n\fa)\n\nb)\n\n∗\n∗\nFigure 6: a) C107\nwith nucleus n7, b) C107\n’s shell\n\na)\n\nb)\n\n∗\n∗\nFigure 7: a) C13\nb) C38\nwith their initial points inside of CB lattice\n\n4\n\n\fto state the growing sequence property of the optimal LJ cluster over the IC lattice, also Hoare [8] pointed out\nthe morphology of the microclusters). It means that clusters with relative closed number of particles could have\nsimilar geometry or in other words, they belong to same lattice or they belong to the same geometrical family or\nthey shares some similar bricks or building blocks.\nSome ideas and techniques are difficult to replicate, therefore for this article, I added a simple Matlab programs\nto visualice my novel cluster partition and geometry, and to help for verifying my 8 categories of classification by\nsimilar number of particles considered the nucleus.\nThe corroboration of my results was possible because all the putative optimal LJ clusters are reported in The\nCambridge Cluster Database (CCD) [17].\nThe next subsection depicts the notation used. Section 2 has the properties and the proposition. The subsection 2.1 depicts the technique for the creation of a partition of the cluster’s particles into layers, and subsection 2.2\ndepicts an heuristic for determining a cluster’s nucleus (in the appendix a Matlab program of such heuristic is depicted). Section 3 describes my version of parallel evolutionary algorithm. The next section presents the numerical\nresults, and finally, the last section the conclusions and the future work.\n\n1.1\n\nNotation\n\nGiven a set S, |S| is the number of elements of the set. Also if A[·] is an array, |A[·]| is the number of elements\nof the array. ∅ is the empty set. || · || is the norm in R3 .\nParticularly, Cn∗ denotes an optimal LJ cluster with n particles, and Cn denotes an arbitrary cluster with n\nparticles.\nA cluster Cn or Cn∗ are sets of natural numbers, where each number correspond to a particle’s properties (pi ).\nFor this research the particle’s properties are the particle’s 3D coordinates. ||pi , pj || is the Euclidian distance\nbetween particles pi and pj .\n√\n∗\n∗\nBy example, Cn∗ = {1, 2}, p1 = (− d2 , 0, 0), and p2 = ( d2 , 0, 0) where d∗ = 6 2 is the optimal distance for two\nparticles under LJ potential:\nLJ(d) = d−12 − 2d−6\nSeveral references explain how to build IC and FC lattices [10, 11, 16, 18]. The CB lattice is very simply is the\nset of points that correspond to the intersection of the parallel lines to the axes with a separation of d∗ /2 from\nthe (0,0,0).\n\n2\n\nProperties of LJ\n\nThere are several articles about LJ potential function’s properties. The proposition 1 in [2] is repeated as\nproposition 2.1 in [3], together with proposition 2.2:\nProposition 2.1 Exist a discrete set, Ω, where ∀j ∈ N , j ≥ 2, the potential of SOCDXX(j) has the same\n(”close value”) optimal value of SOCCXX(j) for a potential function such that\n1. limri,j →0 VXX (ri,j ) = ∞.\n2. ∇2 VXX (x∗ ) semi-positive, k∇VXX (x∗ )k ≪ 1 and\n\nk∇VXX(x∗ )k\n< δ0 , where 0 < δ0 ≪ 1\n|VXX(x∗ )|\n\nwhere XX is BU or LJ.\nwhere SOCYXX means search for optimal cluster, D is discrete, C is continues and XX is LJ for Lennard Jones\nPotential or BU for Buckingham potential.\nProposition 2.2 Any shape of n particles with edges ≈ d∗ can be approximated from the CB lattice.\nThis means that with an appropriate region of CB lattice is sufficient to look for optimal clusters of size n. I\ndid not state the size of the appropriate region of CB. However, today, any optimal LJ cluster in the CCD has an\ninitial configuration in CB lattice, such that from this initial configuration converges by a minimization process\nto its corresponding putative optimal LJ cluster.\n\n5\n\n\fProposition 2.1. For any set of particles of a cluster’s CL or a set of particles of a region RL of any lattice\nbased on a unit u. Then corresponding region RB of the CB lattice such RB covers them under the || · ||∞ . Then\nparticles of CL or RL are less than the number of particles of RB, i.e., |CL| < |RB| or |RL| < |RB|, where | · |\nis the number of particles.\nProof. Under the || · ||∞ any region of CB is a 3D cube. By construction, it has a point at the center (0, 0, 0), the\nfirst cube with −u/2, 0, u/2 has 33 points, the second cube with −u, −u/2, 0, u/2, u has 53 points, . . . , the k cube\nhas (2k + 1)3 points. Any polyhedra or lattice based in the unit u can not have more than 12 neighbors at ratio\nu. The icosahedra has 13 points but the corresponding cube to cover it is the second cube, with 53 points, i.e.,\nno. particles of icosahedra ¡ no. particles of unit cube of CB lattice. In general, for a given cluster CL or region\nRL they can be divided and contained by a set of unit cubes of CB, which is a cube, let’s call RB. Therefore,\n|CL| < |RB| or |RL| < |RB|.\nIt follows that any region of the CB lattice has more points than the same region of a lattice.\nBut more important, the global continuos optimal LJ cluster can be approximated in a discrete set of 3D\npoints, Ω. Then a connection between RB an appropriate region and Ω will be provide a discrete set of 3D points\nwhere the continuous optimal global cluster is approximated by a discrete set of points! A local minimization\nprocedure is the connection to approximate the continuous optimal global in RB. On the other hand, for a cluster\nwith n particles, let’s suppose\nto have an appropriate region RB, m = |RB|. The number of the posible clusters\n\u0001\nn\nof size n in RB is M = m\n≫ 0.\nNote that M is a big number. It follows naturally from prop. 2.1 than for any region of a given lattice, the\nnumber of clusters with n particles is ≪ M.\nProposition\n2.2. RB is an appropriate search region of the CB lattice for a cluster with n particles, M =\n\u0001\nn\n,\nm\n=\n|RB|,\nM is a huge positive number. |RL| is a region of a lattice where there are different clusters with\nm\nn particles, and it is supposed that it contains the optimal LJ cluster. Then\n\u0013\n\u0012\n\u0013\n\u0012\nF2\nF1\n<P\nP\nRB\nRB ∪ RL\nwhere P(·) is a probability function, F1 is the set of the optimal candidates for being the global optimal LJ cluster\nin RB, and F2 is the set of the optimal candidates for being the global optimal LJ cluster in RB ∪ RL.\nProof. It follows from\n|RL|f1 < f ′ M\n\n′\nwhere f1 = |F1 |, f ′ > 0 is the number of candidates for being the global optimal\n√ LJ cluster in RL,\n√ f > 0, it\nis not zero because the assumption that RL contains the optimal LJ cluster, f1 ≪ M, and |RL| ≪ M. Then\n\n(M + |RL|)f1 < (f1 + f ′ )M,\n\u0012\n\u0013\n\u0013\n\u0012\nF1\nf1\nf1 + f ′\nF2\nP\n=\n<\n=P\nRB\nM\nM + |RL|\nRB ∪ RL\nIt is important to assume that F2 ∩ RL 6= ∅, to increase the probability for determining the global optimal\nclusters, otherwise the probability does not increase. Many of the ad-hoc, heuristic, genetic, and evolutionary\nalgorithms for determining the optimal LJ clusters have been used this property as previous knowledge to favorece\nsome candidates over others with success and speed to replicate the putative optimal LJ clusters.\nIn a personal communication, I suggested at 2004 to Shao, et al. to use different lattices from [16]. In [15]\nappears the acknowledge: ”The authors would like to thank Prof. Carlos Barrón Romero for his personal communications and collaborations with us in the studies on the lattice-based optimization methods, including also the\nwork published in J. Phys. Chem. A, 108, 3586-3592 (2004).”\nSo even, knowing that an appropriate region of the CB lattice has the optimal LJ clusters, to improve the\nefficiency for determining optimal LJ cluster is a good strategy to use other sources of candidates to favorece\ndiversity in the complex process for looking the unknown optimal LJ clusters.\n\n6\n\n\fFinally, the theoretical results point out that it is posible to increase the speed of any algorithm for determining\noptimal LJ clusters but without any proof that they are global optimal. The repeated putative optimal LJ clusters\nare stationary states, from where a criteria such of the number of times that the same cluster appears, then stop\nand accept it as the putative optimal global LJ cluster.\n\u0001\nn\nThe number of steps in these cases are clearly very less than M = m\n, m = |RB|. M is the huge number\nrelated to the numbers of candidates to compare for determining global optimality in RB. A force brute algorithm\nfor the estimation of the different combinations of a set can be found in [3], it is a version for determining the\ndifferent cycles of a complete graph, G = (V, A), |V | = n.\n\n2.1\n\nPartition technique for a cluster\n\nThe geometry of the LJ clusters have been strongly related to different geometric structures (see [9, 8, 12, 16])\nicosahedral, dodecahedral, cuboctahedral, and so on.\nMy segmentation’s technique provides different cluster’s views as an arbitrary polyhedron with its partitioning\ninto its core, layers and shell by using the particle’s neighbors. The advantages to segment a cluster with my\ntechnique are 1) to help for interpretation and interaction with other clusters and lattices, and 2) to build a cluster\nfrom lego or building blocks.\nThese properties are quite important because they support the previous research about the knowledge of the\nclusters’ morphology, properties, geometrical families, chemistry, or the well know grow sequence.\nA particle’s neighbor structure is defined as follow:\n1. Define a unit: u.\n2. Define a tolerance t as the porcentaje for accepting the expansion and the compression of u. (0 ≤ t ≤ 1.0).\n3. Neighbor’s criteria: Particles pi , pj are neighbor if an only if (1 − t)u < dist(pi − pj ) < (1 + t)u.\n4. for each particle, N vec(i) is the number of neighbor of the particle pi , and V ec(i, k) is the array for storing\nthe number of the particles pk that they satisfy the neighbor’s criteria with a given particle pi .\nwhere dist is an appropriate distance function between the particles. pi stands for particle’s representation in\na n dimensional space, a particle is represented by pi , which it could contain all the relevant particle’s attributes.\nFor this research, u = d∗ , pi is the particle’s 3D coordinates, dist = || · || is the Euclidian distance, and t = 0.1.\nOne characteristic of LJ clusters is the compression-expansion over the distance between particles with respect\nto the unit d∗ . The value of t = 0.1 allow to differentiate a diagonal from a expanded-compressed unit u and\nit works well to identify the ”hard LJ optimal clusters” (see [1], by example clusters with 38, 75, 98,75, 76,\n77, 102, and 103 particles). For any possible LJ cluster, the upper limit of 12 neighbors over its particles, i.e.,\nN vec(i) ≤ 12, ∀i. comes from the upper limit inherited by the 3D twelve kissing spheres geometrical property.\nWith the cluster’s neighbors information, the next algorithm builds an arbitrary partition of a cluster’s particles\ninto a set of layers:\nAlgorithm 2.3. Partitioning a cluster Cn\ninput: Cn : array of int, for the set of particles’ numbers;\nN vec: array of int, with the particles’ number of neighbor;\nV ec[i, k] : array of [int, int] with the particles’ neighbors;\nN uc = {i0 , i1 , . . . , ik }: array of int, for a given set of particles’s number to be the nucleus, with |N uc| ≤ n;\noutput: capa: array of int, for the corresponding layer of a particle;\nncapa: int, for the cluster’s number of layers;\nmemory: f mk: int;\nfor i := 1 to n do\ncapa[i] := 0;\nend for\nncapa := 1;\nfor i := 1 to |N uc| do\ncapa[N uc[i]] := ncapa\n7\n\n\f∗\nFigure 8: C37\nwith its layers for an arbitrary selection of its nucleus\n\nend for\nf mk := 0;\nwhile (1) do\nfor i:=1 to n do\nif (capa[i] == ncapa) then\nfor jv := 1 to N vec[i] do\nkv := V ec[i][jv];\nif (capa[kv] == 0) then\ncapa[kv] := ncapa + 1;\nf mk := 1;\nend if\nend for\nncapa := ncapa + 1;\nend while\nncapa := ncapa - 1;\nreturn;\nHereafter, layer number 1 is the core or nucleus, and the last layer ncapa is the shell.\nIt is easy to verify that the set capa of the particles’ numbers is a partition of Cn , i.e,:\nSncapa\n• i=1 capa[i] = Cn\n• capa[i] ∩ capa[j] = ∅, i 6= j.\n\nThe previous algorithm gives an arbitrary cluster partition that it could not correspond to the standard accepted\ngeometric structures but it can be used for genetic cuts for creating a new offspring as playing with set of figures\nof lego. In particular, the results could be similar to the Hoare’s (see [8]) morphology of simple microclusters,\n∗\npolyhedral structures (PT), and an arbitrary representation of cluster’s isomeros. Figure 8 depicts C37\nwith its\n∗\nlayers for an arbitrary selection of its nucleus. Figure 6 depicts C107 with nucleus n7 and its shell.\n\n2.2\n\nHeuristic for determining a nucleus for a cluster\n\nThe proposed heuristic is simple and it is based in previous knowledge of the LJ cluster structures that other\nauthors have been point out.\nThe main concept is to look for a set of cluster’s particles inside of an sphere with center at the cluster’s center\nof mass with ratio 1.1d∗ .\nThe particles inside of the sphere are natural candidates for being considering the cluster’s nucleus. There\nmany cases, for the selection of the cluster’s nucleus. Let P N be the set of particles inside of the sphere, and cm\nthe cluster’s coordinates of its center of mass:\n1. IC or IR when ∃ pk ∈ P N , such that arg k = mink ||pi − cm||, and ||pk − cm|| < 0.35d∗. By example,\nC1∗ 3, C5∗ 5, . . . are IC (n1 IC), and C7∗ 5, C7∗ 6, . . . are IR (n1 IR).\n\n8\n\n\f2. IC without a particle as a center when |P N | = 12 and these 12 are closed to the sphere’s shell. This is the\ncase for IC nucleus with 12 particles (n0 IC). By example, C5∗ 21, C5∗ 33, . . ..\n3. When 3 ≤ |P N | and |P N ≤ 7, P N is considering the cluster’s nucleus. This criteria gives a nucleus with 3\nto 7 particles. By example, n3: 665, 668, 672, 673, 728, . . .; n4: 26, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 98,\n. . .; n5: 22, 23, 24, 25, 28, 29, 33, 34, 78, . . .; n6: 31, 32, 38, 43, 44, 99, 121, . . .; n7: 18, 19, 20, 21, 27, 30,\n35, 36, 37, 39, . . .\n4. when |P N | ≥ 8, adjust the center of the sphere to the center of mass of mass of the the cluster with the P N ’s\nparticles with 12 neighbors. The new sphere’s ratio is set to 0.9d∗ , then take as the nucleus the particles\ninside of this new sphere.\nIt is showed in the appendix the Matlab routine ”S plot geCl LJ.m”. It is a version of an algorithm using this\n∗\nheuristic. Figure 6 depicts the results of the C107\nand its shell with algorithm 2.3 with a nucleus defined by the\nheuristic of this section.\nThe next algorithm builds a set of coordinates or give a set of number that they correspond to a cluster inside\nof a region. A region could be and arbitrary set of points, or points of a lattice, or the points of other big cluster.\nAlgorithm 2.4. Matching a cluster Cn with a given region R\ninput:\npn : array of 3D, for the particles’ coordinates of the cluster Cn ;\nRk : array of 3D, with the points’ coordinates of the region R;\nM : int, with the number of points of the region R;\noutput:\nr[·]: array of int, for the corresponding particles’ numbers in the R;\ns[·]: array of 3D, for the closed corresponding particles’ coordinates of the points of R;\nmemory:\ndmin : real; // minimum distance imin : int; // particle’ number mk[·] := 0: array of int, set all to 0;\nif M < n then\nprint(”Error, it is insufficient the number of points of R for the cluster”);\nreturn;\nend if\nfor i := 1 to n do\ndmin := 108 ;\nfor k := 1 to M do\nif (ml[k] == 0) then\nd := dist(p[i], R[k]);\nif (d < dmin ) then\ndmin := d;\nimin := k;\nend if\nend if\nend for\nmk[imin ] := 1;\nr[i] := imin ;\ns[i] := R[imin ];\nend for\nreturn;\nThe previous algorithm always answers with a set of points and with a set on particles’ numbers that they\ncorrespond to a cluster, but the original and the output cluster from the region R could have very different shapes.\nBy example, when a cluster is centered at (0,0,0), and the region is box [20, 50] × [20, 50] × [20, 50] of the cb lattice.\n∗\n∗\nOn the other hand, figure 7 depicts C13\nand C38\ninside of a CB region, where the light blue small boxes are their\ncorresponding points of CB lattice.\n\n9\n\n\fa)\n\nb)\n\nFigure 9: Examples of numeration from the nucleus to its shell a) region of the IC lattice, and b) region of the\nCB lattice\n\n3\n\nParallel evolutionary algorithm for searching optimal LJ clusters\n\nThe novelty of the Parallel evolutionary algorithm of this section is not a new complete paradigm, as I mentioned\nbefore, it is a reload version of previous ad-hoc and genetic algorithm (see [7, 4, 5, 6, 14, 13, 2, 3]).\nThe term evolutionary algorithm for my approach is justified by the fact that the results are converted into\ninput data, and this cause a change of the expected behavior of the program beyond of its programming. What, I\nprecisely mean is that the efficacy and efficiency of the program for determining optimal LJ clusters is improved.\nAlso, I added new routines based in phenotype and genotype strategies that my previous algorithms have not.\nBut in the essence, it is an evolutionary program improved by his creator to increase its efficiency and efficacy\nwith adding changes into its old routines by hand. One of the aspect to point out, is that previous version was\nonly based in elitism, here the diversity is favored and it will come from the data of the optimal clusters and for\nthe data of the CB lattice, particularly.\nThe algorithm [14]) now includes the following genotype and phenotype mechanisms.\nFor the genotype mechanisms, a telephone model for the clusters consist to get a set of number to represent a\ncluster. This can be done with a region of a lattice and a cluster by using the algorithm 2.4.\nAn enumeration from the nucleus to its shell can be done by ordering the particles of a region by its ratio, y\ncoordinate and its angle on the XY plane. Figure 9 depict a IC and CB regions with this numeration. This helps\nbecause for a cluster, lower number are in the core, larger number are in the shell whatever it is respect to a lattice\nor to other Cn∗ .\nWith the telephone model for a cluster, a mutation is like to dial the cluster’s telephone with one or more\nmistakes. The mistake numbers can be replaced by any number not in the cluster’s telephone with the condition\nthat such numbers are the indices of particles’ coordinates in a given region.\nChildren can be created by replacing sequences of the clusters’ telephones of 2 or more parent clusters.\nAfter the creation of the children by any genetic mechanism, a minimization procedure is applied for the\ncorresponding coordinates of the particles’ numbers of the children to get a local minimal LJ cluster for elitism\nselection.\nOn the other hand, the previous genetic algorithm adds mechanisms to use the algorithm 2.3 with or without\nthe heuristic given in subsection 2.2. New kind of mutations are incorporate by using the matching algorithm 2.4\nto transform a cluster into a CB, IC, IF or any lattice before crossover and make up. Previously, the current\npopulation include the optimal LJ cluster, the cluster with more 12 neighbors, and the worst LJ cluster. The\nchange is to include the lower and as many clusters as possible of the 8 categories of the heuristic for determining\nthe nucleus with the current optimal LJ cluster.\nThe parrallel algorithm defines a player main routine, which consists in two main routines: Cerberus and\nPrometheus.\n\n10\n\n\fAlgorithm 3.1. Player\ninput:\ntimer: set an interval of time for sending a stop signal.\nIn : int parameter of the initial cluster (≥ 13);\nFn : int parameter of final cluster (≥ 13 and (≥ In ));\nPsz : int parameter of the population size (≥ 9).\nRCB : set of 3D points of the CB lattice (≫ Fn );\nRL1 , . . . RLk : set of 3D points of other lattices (≫ Fn );\noutput:\nmemory:\nCn∗ : private data of the current putative optimal LJ clusters;\nwhile (1) do\nexecute Cerberus;\nexecute Prometheus(In , Fn , Psz , RCB , RL1 , . . . RLk );\na timer or the user send a signal to stop;\nend while\nCerberus is an elitism routine for communicating and keeping the best putative optimal LJ clusters. It take\ncare of the communication but never interrupt the process of Prometheus.\nAlgorithm 3.2. Cerberus\ninput:\nPc : input pile of messages for Ck (LJ clusters);\nstP : int, exclusive variable to communicate with Prometheus’s state, 1: Prometheus is searching or 0:Prometheus\nis not searching;\noutput:\nsignal: semaphore command for waiting or executing;\nmemory:\nCn∗ : private data of the current putative optimal LJ clusters;\nsignal Prometheous goes;\nwhile (1) do\nif (stP == 0) then\nwhile pile(Pc ) is not empty do\nsignal Prometheous to wait;\nCk := pop(Pc );\nif LJ(Ck ) < LJ(Ck∗ ) then\n(Ck∗ ) = (Ck );\nsend message Ck∗ to others players;\nend if\nend while\nsignal Prometheous to continue;\nend while\nPrometheus is the implementation of the previous evolutionary algorithm. It has exclusive access to the best\nLJ clusters during the evolutionary process.\nAlgorithm 3.3. Prometheus\ninput:\nIn : int parameter of the initial cluster (≥ 13);\nFn : int parameter of final cluster (≥ 13 and (≥ In ));\nPsz : int parameter of the population size (≥ 9).\nRCB : set of 3D points of the CB lattice (≫ Fn );\nRL1 , . . . RLk : set of 3D points of other lattices (≫ Fn );\nsignal: semaphore command for waiting or executing;\noutput:\nstP : int, exclusive variable to communicate its state, 1: busy or 0:waiting;\n11\n\n\fFigure 10: Histogram of nucleus type for Cn∗ , n = 13, . . . , 1612 particles\n\nmemory:\nCn∗ : private data of the current putative optimal LJ clusters;\nwhile (1) do\nstP : = 0;\nfor n := In to Fn do\ndo Cerberus(signal) or player(signal);\nstP := 1;\nexecute: evolutionary algorithm for exploring Cn\nstP :=0;\nend for\nend while\nWhen Prometheus is executing the evolutionary algorithm, there is not access to the private memory even for\nothers clusters different of the current n. This is because the evolutionary algorithm could use any Cj∗ for creating\noffsprings at any time. Before or after, the process of the evolutionary algorithm, or when Prometheus is off, there\nis access to the best LJ optimal clusters.\nThe player routine is designed for working with copies of it. This could cause a bootle’s neck for the communications. Therefore, it is convenient to define a master player. In this case, only the master has the ability to send\nand receive messages, meanwhile the slave players can only send messages to it.\n\n4\n\nResults\n\nMy previous results [2] are in the figure 1. It depicts a set of particles MIF1739, which contains Cn∗ , n =\n2, . . . , 1000. I tried to use MIF1739 has a main lattice from where an algorithm could takes advantage of its\nbuilding property: ∃ Cn ∈ MIF1739, such that by a minimization process, Cn converges to Cn∗ . However, it is not\n∗\neasy task to locate a ”good” initial set of points closed to an optimal LJ cluster. Figure 2 depicts where C38\nand\n∗\nC664 are located into the IF lattice. It is possible to select points by using sphere in MIF1739. Two parameters\nare need, the ratio and the center of sphere.\nThe efficiency of the evolutionary algorithm changes dramatically with the incorporation of the CB lattice,\nand the phenotype and genotype strategies. The best results comes from starting with no optimal clusters but by\nusing CB lattice and IC, IR, FC, dodecahedral lattices, and 14 as the size of the population.\n\n12\n\n\fFigure 11: LJ potential, potential difference vs Cn∗ , n = 13, . . . , 332 particles\n\nFigure 12: LJ potential, potential difference vs Cn∗ , n = 333, . . . , 652 particles\n\n13\n\n\fFigure 13: LJ potential, potential difference vs cluster with Cn∗ , n = 653, . . . , 972 particles\n\nFigure 14: LJ potential, potential difference vs Cn∗ , n = 973, . . . , 1292 particles\n\n14\n\n\fFigure 15: LJ potential, potential difference vs Cn∗ , n = 1293, . . . , 1612 particles\n\nThe heuristic for determining a nucleus (see subsection 2.2) and the algorithm 2.3, helped to extend the selection\nand interaction between clusters to create offspring for mutation or phenotype crossover or genotype crossover.\nThis heuristic classifies into 8 categories by just using the numbers of particles in a nucleus. The nucleus types n4,\nn5, n6, and n7 are not geometrically equal. A refinement of heuristic is possible, but it has a computation cost.\nFigure 10 depicts the histogram of 8 categories resulting of this heuristic for the Cn∗ , n = 13, . . . , 1612. Tables 1, 2, 3, and 4 contains the classification of the clusters. It seems that many cases of Cn∗ are obtained rapidly\nby a the make up operation of its previous or next clusters, but also by considering to extend the diversity of the\ncurrent population to 8 categories.\nMy results could help to answer some old conjetures about the morphology of the microclusters. See Hoare [8]:\n”Werfelmer’s essential contribution was to point out the possibility of extremely compact fivefold symmetric\nstructures for N7, suggesting that the pentagonal bipyrimid (N=7) (fig.3(a)) and the icosahedron (N=13) (fig.3(b))\nmight be the dominant motifs in larger assemblies.” It is partially true, from table 4the type 7), a nucleus with 7\nparticles (a pentagonal bipyramid) is the dominant motif for Cn∗ , n = 18, . . . , 1530, but IC is not a dominant motif\nfrom table 1, type 1) and type2) without considering type 8) (a nucleus IC with 12 particles, which was unknown\nat 1983).\nFigures 11, 12, 13, 14, and 15 depict where the different nucleus type appears. At the bottom of each figure\nthe LJ potential difference bet the consecutive clusters is depicted. Some type of optimal clusters are isolated and\nthe LJ potential difference is highly variable from n = 13 to 1420. But after this cluster it seems to diminish its\nvariations for the IR Cn∗ , n ≥ 142. My results are 1600 optimal LJ clusters with n = 13, . . . , 1612 particles, (Most\nof them are posted in The Cambridge Cluster Database (CCD) [17]), a novel 8 categories of nucleus classification,\nand 65 new putative LJ Clusters, which are not reported at December of 2016. See table 5.\n\n5\n\nConclusions and future work\n\nThe advances of the technology and science of Physics and Chemistry are fantastic, together with the molecular\nand the nanostructures design. The results presented here have many implications for the computational molecular\ndesign and their models and algorithms.\n15\n\n\fI hope to witness, that it is quite possible to replicate and to improve these results by using one of the top\nworldwide supercomputer. Some of the definitions are broad, and this research can be easily extended and applied\nfor exploring geometries and interactions of clusters under other molecular potentials.\n\nAppendix\nMatlab Programs\nfunction S_plot_Cl_LJ(ncl,xcl,ycl,zcl,nv1,nv2)\n% This subroutine draws the geometry of\n% a minimal Lennard Jones Potential’s cluster\n% The input parameters are:\n% ncl : numbers of particles\n% xcl, ycl, zcl: arrays of numbers corresponding to the 3D\n% cluster’s coordinates\n% nv1, nv2 : integer number to determine the shells to draw\n% With nv1=0 and nv2=2, the cluster’s nucleus and the first shell\n% are depicted (layer 1 is the nucleus).\n% ====================================================================\n% More information of this subroutine is in the article:\n% Discrete Optimal Global Convergence of a Evolutionary Algorithm\n% for Clusters under the Potential of Lennard Jones\n% Author: Carlos Barron-Romero\n% Universidad Autonoma Metropolitana, campus Azcapotzalco\n% Mexico City.\n% This subroutine can be freely used, distributed or modified.\n% ====================================================================\n% Compute center of mass\nxclm=mean(xcl);\nyclm=mean(ycl);\nzclm=mean(zcl);\n% Arrays for particle’s neighbors and number of neighbors\nvec=zeros(ncl,12);\nnvc=zeros(ncl,1);\n% dmg is the optimal distance of a pair of particles under\n% Lennard Jones Potential\ndmg=2^(1/6);\n% 10% is the factor to define the lower and upper limits\n% for accepting a particle’s neighbor\ntol=0.1;\nd_inf = dmg * (1 - tol);\nd_sup = dmg * (1 + tol);\n%\n% This loop determines the particle’s neighbor\nfor i=1:ncl-1\nfor j=i+1:ncl\ndij=norm([(xcl(i)-xcl(j)),(ycl(i)-ycl(j)),(zcl(i)-zcl(j))]);\nif ((d_inf < dij) && (dij < d_sup))\nnvc(i)=nvc(i)+1;\nvec(i,nvc(i))=j;\nnvc(j)=nvc(j)+1;\nvec(j,nvc(j))=i;\nend\nend\n16\n\n\fend\n% Select a set of particles closed to the cluster’s center of mass\nra = dmg*1.1;\npnuc=zeros(13,1);\nnnuc=0;\nfnuc = 0;\nfor i=1:ncl\ndcl=norm([(xcl(i)-xclm),(ycl(i)-yclm),(zcl(i)-zclm)]);\nif (dcl < ra)\nnnuc=nnuc+1;\npnuc(nnuc)=i;\nend\nend\n% Analyze the set of particles\nif (nnuc >0)\n% First case.\n% Look for a particle with 12 neighbors\n% closed to the CM\ninuc = -1;\nd12nuc = 99999.999;\nfor k=1:nnuc\ni=pnuc(k);\nif (nvc(i) == 12)\ndcl=norm([(xcl(i)-xclm),(ycl(i)-yclm),(zcl(i)-zclm)]);\nif (dcl < d12nuc)\nd12nuc = dcl;\ninuc=i;\nend\nend\nend\nra_nuc = dmg * 0.35;\nif ((inuc ~= - 1) & (d12nuc < ra_nuc))\n% There is a particle with 12 neighbors\npnuc(1) = inuc;\nnnuc = 1;\nra = d12nuc;\nfnuc = 1;\nend\n% After verify that there is no center,\n% Then it is the nucleus with 12 particles\nif ((inuc == - 1) & (nnuc == 12))\nfnuc = 1;\nend\nend\n% Final case.\n% Skip four (tetrahedron, nnuc=4),\n% five (Trigonal bipyramid, nnuc=5),\n% six particles (octahedron, nnuc=6),\n% (pentagonal polyhedron bipyramide, nnuc=7)\n%\nif ((fnuc == 0) & (nnuc >= 8))\n% Adjust the cluster’s center of mass\n% considering only particles with 12 neighbors\n% of the selected set pnuc\n17\n\n\fnclm = 1;\nfor k=1:nnuc\ni=pnuc(k);\nif (nvc(i) == 12)\nxclm = xclm + xcl(i);\nyclm = yclm + ycl(i);\nzclm = zclm + zcl(i);\nnclm = nclm + 1;\nend\nend\nxclm = xclm / nclm;\nyclm = yclm / nclm;\nzclm = zclm / nclm;\n% Adjust the selected set, keeping the closed\n% to the adjusted center\nra = dmg*0.9;\npnuc_nw=zeros(13,1);\nnnuc_nw=0;\nfor k=1:nnuc\ni=pnuc(k);\ndcl=norm([(xcl(i)-xclm),(ycl(i)-yclm),(zcl(i)-zclm)]);\nif (dcl < ra)\nnnuc_nw=nnuc_nw+1;\npnuc_nw(nnuc_nw)=i;\nend\nend\n% Take this new set as the nucleus\npnuc = pnuc_nw;\nnnuc = nnuc_nw;\nfnuc = 1;\nend\n% Determine the cluster’s layers\n% with the nucleus particles\ncapa=zeros(ncl,1);\nncapa=1;\nfor jp=1:nnuc\ncapa(pnuc(jp))=1;\nend\nwhile (1)\nfmk=0;\nfor i=1:ncl\nif (capa(i) == ncapa)\nfor jv=1:nvc(i)\npvc=vec(i,jv);\nif (capa(pvc)== 0)\ncapa(pvc) = ncapa+1;\nfmk=1;\nend\nend\nend\nend\nif (fmk == 0)\nbreak;\nend\n18\n\n\fncapa = ncapa + 1;\nend\n% Color table\ntclr = [ 1,0,0; ...\n0,1,0; ...\n0,0,1; ...\n1,1,0; ...\n51/255, 153/255, 1; ...\n1,0,1; ...\n0.5,0.5,1; ...\n0.5,0.5,0.5; ...\n1,0.5,1; ...\n1,0.5,0.5; ...\n1,0.5,0.75; ...\n1,0.75,0; ...\n0,0,0];\nhold on;\nfor i=1:ncl\nif (capa(i) < nv1)\ncontinue;\nend\nif (capa(i) > nv2)\ncontinue;\nend\n% cluster’s lines\nfor jv=1:nvc(i)\npvc=vec(i,jv);\nif (capa(i) == capa (pvc))\nline([xcl(i),xcl(pvc)], ...\n[ycl(i),ycl(pvc)], ...\n[zcl(i),zcl(pvc)], ...\n’Color’,tclr(capa(i),:), ...\n’LineWidth’,1);\nend\nend\n% particles\nplot3(xcl(i), ...\nycl(i), ...\nzcl(i), ’-ko’,...\n’LineWidth’,1,...\n’MarkerEdgeColor’,’k’, ...\n’MarkerFaceColor’,tclr(capa(i),:),...\n’MarkerSize’,8);\nend\naxis equal;\ngrid on;\nlinp=sprintf(’Cluster of %d particles’,ncl);\ntitle(linp);\nview(45,45);\nhold off;\nfunction [n,x,y,z] = S_read_Cl_LJ(file_name)\n% This subroutine reads in the format of the file from the\n\n19\n\n\f% The Cambridge Energy Landscape Database (\\protect\\vrule width0pt\\protect\\href{http://www-wales.ch.cam.\n% of a minimal cluster under Lennard Jones Potential\n%\n% The input parameters is the file name\n% Is is ####.txt, where #### is the cluster’s number of particles\n%\n% ====================================================================\n% This subroutine is in the article:\n% Discrete Optimal Global Convergence of a Evolutionary Algorithm\n% for Clusters under the Potential of Lennard Jones\n% Author: Carlos Barron-Romero\n% Universidad Autonoma Metropolitana, campus Azcapotzalco\n% Mexico City.\n% This subroutine can be freely used, distributed or modified under\n% your own responsibility.\n% ====================================================================\n%\nfid=fopen(file_name);\np = fscanf(fid,’%g %g %g’, [3 inf]);\nfclose(fid);\n% x, y and z are the cluster’s coordinates\nx=p(1,:);\ny=p(2,:);\nz=p(3,:);\n% n is the cluster’s number of particles\nn=length(x);\n\n% This program calls the\n% subroutine S_plot_geCl_LJ_cl to draw the geometry of\n% an optimal Lennard Jones cluster\n% ====================================================================\n% More information of this subroutine is in the article:\n% Discrete Optimal Global Convergence of a Evolutionary Algorithm\n% for Clusters under the Potential of Lennard Jones\n% Author: Carlos Barron-Romero\n% Universidad Autonoma Metropolitana, campus Azcapotzalco\n% Mexico City.\n% This subroutine can be freely used, distributed or modified under\n% your own responsibility.\n% ====================================================================\nfeature(’UseGenericOpenGL’,0);\n[filename,pname] = uigetfile(’*.TXT’);\n% Define your own routine to read the particles’ coordinates\n% of a optimal cluster under the Leenard Jones Potential\n% or S_read_cl_LJ is set to read the files in the\n% The Cambridge Energy Landscape Database (\\protect\\vrule width0pt\\protect\\href{http://www-wales.ch.cam.\n%\n% The input parameters is the file name\n% Is is ####.txt, where #### is the cluster’s number of particles\n[ncl, xcl,ycl,zcl] = S_read_Cl_LJ([pname,filename]);\nnv1=0;\nnv2=2;\nclf;\nS_plot_geCl_LJ(ncl,xcl,ycl,zcl,nv1,nv2);\n\n20\n\n\fn1 IC 231\n13 14 15 16 17 45 46 47 48 49\n50 51 52 53 54 55 56 57 58 59\n60 61 62 63 64 65 66 67 126\n127 128 129 130 131 132 133\n134 135 136 137 138 139 140\n141 142 143 144 145 146 147\n148 149 150 151 152 153 154\n155 156 157 158 159 160 161\n162 163 164 165 166 168 272\n273 274 275 276 277 278 279\n280 281 282 283 284 285 286\n287 288 289 290 291 292 293\n294 295 296 297 298 299 300\n301 302 303 304 305 306 307\n308 309 310 311 312 313 314\n315 316 317 318 319 320 321\n322 323 324 325 326 327 328\n329 330 331 332 333 334 335\n336 337 338 339 340 495 498\n499 503 504 505 507 508 509\n510 511 512 513 514 515 516\n517 518 519 520 522 523 524\n525 526 527 528 529 530 531\n532 534 535 539 540 544 545\n549 550 551 552 553 554 555\n556 557 558 559 560 561 562\n563 564 565 566 567 568 569\n570 571 572 573 574 575 576\n577 578 579 580 581 582 583\n584 586 587 588 589 590 591\n592 593 594 595 596 598 599\n600 601 602 603 604 923\n\nn1 IR 313\n75 76 77 188 189 190 191 192 650 651 652 653 654 655\n656 657 658 659 660 661 662 663 664 682 683 684 685\n686 687 688 689 691 1027 1029 1031 1033 1035 1036\n1037 1038 1039 1040 1041 1042 1043 1044 1045 1046\n1047 1048 1049 1050 1051 1052 1053 1054 1055 1056\n1057 1058 1059 1060 1061 1062 1063 1064 1065 1066\n1067 1068 1069 1070 1071 1072 1073 1074 1075 1076\n1077 1078 1079 1080 1081 1082 1083 1084 1085 1086\n1087 1088 1089 1090 1091 1092 1093 1094 1095 1096\n1097 1098 1099 1100 1101 1102 1103 1104 1105 1106\n1107 1108 1109 1110 1111 1112 1113 1114 1115 1116\n1117 1118 1119 1120 1121 1122 1123 1124 1125 1126\n1127 1128 1129 1130 1131 1132 1133 1134 1135 1136\n1137 1138 1139 1140 1141 1142 1143 1144 1145 1146\n1147 1148 1149 1150 1151 1152 1153 1154 1155 1156\n1157 1158 1159 1160 1161 1162 1163 1164 1165 1166\n1167 1168 1169 1170 1171 1172 1173 1174 1175 1176\n1177 1178 1179 1180 1181 1182 1183 1184 1185 1186\n1187 1188 1189 1190 1191 1192 1193 1194 1195 1196\n1197 1198 1199 1200 1201 1202 1203 1204 1205 1212\n1213 1214 1215 1216 1217 1218 1219 1220 1515 1516\n1517 1518 1519 1520 1521 1522 1523 1524 1525 1526\n1527 1528 1529 1531 1532 1533 1534 1535 1536 1537\n1538 1539 1540 1541 1542 1543 1544 1545 1546 1547\n1548 1549 1550 1551 1552 1553 1554 1555 1556 1557\n1558 1559 1560 1561 1562 1563 1564 1565 1566 1567\n1568 1569 1570 1571 1572 1573 1574 1575 1576 1577\n1578 1579 1580 1581 1582 1583 1584 1585 1586 1587\n1588 1589 1590 1591 1592 1593 1594 1595 1596 1597\n1598 1599 1600 1601 1602 1603 1604 1605 1606 1607\n1608 1609 1610 1611 1612\n\nTable 1: Type 1 and 2 of the Cn∗ , n = 13, . . . , 1612\n\nReferences\n[1] Conquering the hard cases of lennard-jones clusters with simple recipes. Computational and Theoretical\nChemistry.\n[2] C. Barrón-Romero. Minimum search space and efficient methods for structural cluster optimization. arXiv,\nMath-ph:0504030-v4, 2005. To honor the CIMAT’s XXV Anniversary.\n[3] C. Barrón-Romero. The Complexity of the NP-Class. arXiv, arxiv.org/abs/ 1006.2218, 2010.\n[4] C. Barrón-Romero, S. Gómez, and D. Romero. Archimedean Polyhedron Structure Yields a Lower Energy\nAtomic Cluster. Applied Mathematics Letters, 9(5):75–78, 1996.\n[5] C. Barrón-Romero, S. Gómez, and D. Romero. Lower Energy Icosahedral Atomic Cluster with Incomplete\nCore. Applied Mathematics Letters, 10(5):25–28, 1997.\n[6] C. Barrón-Romero, S. Gómez, D. Romero, and A. Saavedra. A Genetic Algorithm for Lennard-Jones Atomic\nclusters. Applied Mathematics Letters, 12:85–90, 1999.\n21\n\n\fn3 29\n665 668 669 672 673\n728 729 730 731 732\n733 734 735 736 737\n738 739 740 741 742\n743 744 745 746 747\n748 749 751 753\n\nn4 91\n26 86 87 88 89 90 91 92 93 94 95 98 125 167 201 203 204 205\n207 208 209 210 211 212 213 214 215 216 217 218 219 393\n395 396 397 398 399 400 401 402 403 404 405 406 407 408\n410 411 412 413 414 415 416 417 418 419 420 421 506 706\n710 711 712 713 715 717 842 843 844 846 847 848 849 850\n852 853 854 855 856 857 858 859 860 861 862 863\n\n206\n394\n409\n709\n851\n\nTable 2: Type 3 and 4 of the Cn∗ , n = 13, . . . , 1612\n\nn5 163\n22 23 24 25 28 29 33 34 78 79 80 81\n82 83 84 96 97 100 101 105 122 185\n187 193 194 195 196 197 198 199 200\n202 221 222 223 224 225 226 227 228\n229 230 231 232 233 234 235 243 270\n369 370 371 373 374 375 376 377 378\n379 380 381 382 383 384 385 386 387\n388 389 390 391 392 422 423 424 425\n426 427 428 429 430 431 432 433 434\n435 436 437 438 439 440 441 442 443\n444 445 446 447 448 449 451 585 647\n649 666 667 670 671 674 675 676 677\n678 679 680 681 727 750 752 763 764\n765 766 767 768 769 770 771 772 773\n824 825 829 830 831 832 833 834 835\n836 837 838 839 840 841 845 956 957\n958 959 960 961 962 963 964 965 966\n967 968 969 970 971 972\n\nn6 185\n31 32 38 43 44 99 121 123 124 220 264 265 266\n267 268 269 271 486 487 488 489 490 491 492 493\n494 496 497 597 606 612 614 616 619 620 623 624\n634 636 690 692 693 694 695 696 697 698 699 700\n701 702 703 704 705 707 708 714 716 718 719 720\n721 722 723 724 725 726 777 778 779 780 781 782\n783 784 785 786 787 788 789 790 791 792 793 794\n795 796 797 798 799 800 801 802 803 804 805 806\n807 808 809 810 811 812 813 814 817 826 827 828\n973 974 975 976 977 978 979 980 981 982 983 984\n985 986 987 988 989 992 993 994 995 996 997 998\n999 1000 1001 1002 1003 1004 1005 1006 1007\n1008 1009 1010 1011 1012 1013 1014 1015 1016\n1017 1018 1019 1020 1021 1022 1023 1025 1028\n1030 1032 1034 1315 1316 1317 1318 1319 1320\n1321 1322 1323 1324 1325 1326 1327 1328 1329\n1330 1331 1332 1333 1335 1336 1347\n\nTable 3: Type 5 and 6 of the Cn∗ , n = 13, . . . , 1612\n\n22\n\n\fn7 430\n18 19 20 21 27 30 35 36 37 39 40 41 42 68 69 70 71 72 73\n74 85 102 103 104 106 107 108 109 110 111 112 113 114 115\n116 117 118 119 120 169 170 171 172 173 174 175 176 177 178\n179 180 181 182 183 184 186 236 237 238 239 240 241 242 244\n245 246 247 248 249 250 251 252 253 254 255 256 257 258 259\n260 261 262 263 341 342 343 344 345 346 347 348 349 350 351\n352 353 354 355 356 357 358 359 360 361 362 363 364 365 366\n367 368 372 450 452 453 454 455 456 457 458 459 460 461 462\n463 464 465 466 467 468 469 470 471 472 473 474 475 476 477\n478 479 480 481 482 483 484 485 500 501 502 605 607 608 609\n610 611 613 615 617 618 621 622 625 626 627 628 629 630 631\n632 633 635 637 638 639 640 641 642 643 644 645 646 648 754\n755 756 757 758 759 760 761 762 774 775 776 815 816 818 819\n820 821 822 823 990 991 1024 1026 1206 1207 1208 1209 1210\n1211 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230\n1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241\n1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252\n1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263\n1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274\n1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285\n1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296\n1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307\n1308 1309 1310 1311 1312 1313 1314 1334 1337 1338 1339\n1340 1341 1342 1343 1344 1345 1346 1348 1349 1350 1351\n1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362\n1363 1364 1365 1366 1423 1424 1425 1426 1427 1428 1429\n1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440\n1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451\n1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462\n1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473\n1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484\n1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495\n1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506\n1507 1508 1509 1510 1511 1512 1513 1514 1530\n\nn0 IC 158\n521 533 536 537 538 541\n542 543 546 547 548 864\n865 866 867 868 869 870\n871 872 873 874 875 876\n877 878 879 880 881 882\n883 884 885 886 887 888\n889 890 891 892 893 894\n895 896 897 898 899 900\n901 902 903 904 905 906\n907 908 909 910 911 912\n913 914 915 916 917 918\n919 920 921 922 924 925\n926 927 928 929 930 931\n932 933 934 935 936 937\n938 939 940 941 942 943\n944 945 946 947 948 949\n950 951 952 953 954 955\n1367 1368 1369 1370\n1371 1372 1373 1374\n1375 1376 1377 1378\n1379 1380 1381 1382\n1383 1384 1385 1386\n1387 1388 1389 1390\n1391 1392 1393 1394\n1395 1396 1397 1398\n1399 1400 1401 1402\n1403 1404 1405 1406\n1407 1408 1409 1410\n1411 1412 1413 1414\n1415 1416 1417 1418\n1419 1420 1421 1422\n\nTable 4: Type 7 and 8 of the Cn∗ , n = 13, . . . , 1612\n\n23\n\n\f(n, LJold , LJnew ) 65\n(293,-1888.4271,-1888.4274) (506,-3427.6212,-3427.6875) (521,-3539.3314,-3539.5098)\n(533,-3628.2529,-3629.2999) (662,-4581.2049,-4581.2058) (664,-4596.1971,-4596.1978)\n(813,-5712.2507,-5712.2517) (974,-6928.5630,-6928.6305) (1064,-7616.1673,-7616.1680)\n(1075,-7700.8750,-7700.8755) (1102,-7905.8577,-7905.8651) (1103,-7913.5579,-7913.5631)\n(1106,-7935.9638,-7935.9689) (1115,-8004.9485,-8004.9868) (1125,-8081.1711,-8081.1859)\n(1126,-8088.8631,-8088.8764) (1143,-8218.2614,-8218.2690) (1144,-8225.9422,-8225.9630)\n(1146,-8240.8643,-8240.8662) (1147,-8248.5659,-8248.5706) (1148,-8256.2554,-8256.2609)\n(1158,-8333.0767,-8333.0809) (1161,-8355.7255,-8355.7298) (1162,-8363.4124,-8363.4313)\n(1163,-8371.1178,-8371.1218) (1166,-8393.5692,-8393.5838) (1167,-8401.2616,-8401.2743)\n(1179,-8493.0215,-8493.0221) (1184,-8530.6521,-8530.6600) (1185,-8538.3320,-8538.3532)\n(1187,-8553.2573,-8553.2634) (1189,-8568.6507,-8568.6566) (1225,-8844.5625,-8844.5758)\n(1243,-8982.2245,-8982.2304) (1244,-8989.9194,-8989.9244) (1275,-9229.3690,-9229.3694)\n(1287,-9322.0063,-9322.0069) (1289,-9337.6149,-9337.6157) (1292,-9360.2960,-9360.2969)\n(1294,-9375.7081,-9375.7091) (1312,-9515.0285,-9515.0327) (1315,-9537.6838,-9537.6880)\n(1317,-9553.0747,-9553.0788) (1324,-9606.7794,-9606.7798) (1336,-9699.4110,-9699.4116)\n(1338,-9715.0285,-9715.0314) (1341,-9737.7061,-9737.7094) (1343,-9753.1169,-9753.1200)\n(1366,-9930.4757,-9930.4764) (1445,-10538.9878,-10538.9883)\n(1457,-10631.4452,-10631.4458) (1483,-10831.4787,-10831.4791)\n(1488,-10869.6845,-10869.6917) (1489,-10877.3862,-10877.4015)\n(1490,-10885.0944,-10885.1024) (1523,-11139.7180,-11139.7228)\n(1526,-11162.5614,-11162.5647) (1528,-11178.2283,-11178.2291)\n(1552,-11364.4027,-11364.4088) (1554,-11380.0717,-11380.0735)\n(1558,-11410.7180,-11410.7193) (1585,-11620.2304,-11620.2305)\n(1595,-11698.0815,-11698.0820) (1611,0.0000,-11821.7581) (1612,0.0000,-11829.4494)\n\nTable 5: New Cn∗\n\n[7] S. Gómez and C. Barrón-Romero. The Exponential Tunneling Method. Technical Report Research Report\n3(1), IIMAS-UNAM, Julio 1991 1991.\n[8] M. R. Hoare and J. A. McInnes. Morphology and statistical statics of simple microclusters. Advances in\nPhysics, 32(5):791–821, 1983.\n[9] M. R. Hoare and P. Pal. Physical cluster mechanics: statistical thermodynamics and nucleation theory for\nmonatomic systems. Advances in Physics, 24(5):645–678, 1975.\n[10] R. H. Leary. Global Optima of Lennard-Jones Clusters. Journal of Global Optimization, 11(1):35–53, 1997.\n[11] R. Maier, J. Rosen, and G. Xue. A discrete-continuous algorithm for molecular energy minimization. In\nProceedings. Supercomputing ’92. (Cat. No.92CH3216-9), 16-20 Nov. 1992, Proceedings. Supercomputing\n’92. (Cat. No.92CH3216-9), pages 778–786, Minneapolis, MN, USA, 1992. IEEE Comput. Soc. Press.\n[12] J. A. Northby. Structure and binding of Lennard-Jones clusters: 13 ≤ n ≤ 147. Journal of Chemical Physics,\n87(10):6166–6177, 1987.\n[13] D. Romero, C. Barrón-Romero, and S. Gómez. The optimal configurations of LJ atomic clusters: 148-309.\nIn Siam Annual Meeting, Atlanta, GA, USA, 1999.\n[14] D. Romero, C. Barrón-Romero, and S. Gómez. The optimal geometry of Lennard-Jones clusters: 148-309.\nComputer Physics Communications, 123:87–96, 1999.\n[15] X. Shao, Y. Xiang, and W. Cai. Structural Transition from Icosahedra to Decahedra of Large Lennard-Jones\nClusters. Personal Communication, 2005.\n[16] I. A. Solov’yov, A. V. Solov’yov, and W. Greiner. Fusion process of Lennard-Jones clusters: global minima\nand magic numbers formation. ArXiv Physics e-prints, pages 1–47, 2003.\n\n24\n\n\f[17] D. J. Wales, J. P. K. Doye, A. Dullweber, M. P. Hodges, F. Y. Naumkin, F. Calvo,\nJ. Hernández-Rojas, and T. F. Middleton. The cambridge cluster database, lennard-jones clusters,\nhttp://www-doye.ch.cam.ac.uk/jon/structures/lj.html.\n[18] Y. Xiang, H. Jiang, W. Cai, and X. Shao. An Efficient Method Based on Lattice Construction and the\nGenetic Algorithm for Optimization of Large Lennard-Jones Clusters. Journal of Physical Chemistry A,\n108(16):3586–92, 2004.\n\n25\n\n\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f\f"
        ],
        [
         "29",
         "29",
         "cs.CE",
         "Computational Engineering",
         "0110047v1.pdf",
         "1 Lenwood S. Heath\n\nThe Expresso Microarray Experiment Management System:\nThe Functional Genomics of Stress Responses in Loblolly Pine\nLenwood S. Heath1, Naren Ramakrishnan1, Ronald R. Sederoff2, Ross W. Whetten2,\nBoris I. Chevone3, Craig A. Struble1, Vincent Y. Jouenne1, Dawei Chen1, Leonel van\nZyl2, and Ruth G. Alscher3\n\nAbstract:\nConception, design, and implementation of cDNA microarray experiments present a\nvariety of bioinformatics challenges for biologists and computational scientists. The\nmultiple stages of data acquisition and analysis have motivated the design of Expresso, a\nsystem for microarray experiment management. Salient aspects of Expresso include\nsupport for clone replication and randomized placement; automatic gridding, extraction\nof expression data from each spot, and quality monitoring; flexible methods of combining\ndata from individual spots into information about clones and functional categories; and\nthe use of inductive logic programming for higher-level data analysis and mining. The\ndevelopment of Expresso is occurring in parallel with several generations of microarray\nexperiments aimed at elucidating genomic responses to drought stress in loblolly pine\nseedlings. The current experimental design incorporates 384 pine cDNAs replicated and\nrandomly placed in two specific microarray layouts. We describe the design of Expresso\nas well as results of analysis with Expresso that suggest the importance of molecular\nchaperones and membrane transport proteins in mechanisms conferring successful\nadaptation to long-term drought stress.\n\nIntroduction\nMicroarray technology makes possible the measurement of levels and patterns of gene\nexpression important in growth, metabolism, development, behavior, and adaptation of\nliving systems. A single microarray hybridization can provide information about the\nexpression of hundreds or thousands of genes. The complexity of devising an appropriate\nmicroarray design, as well as the complexity of analyzing large amounts of resulting data,\nare well beyond human capabilities. The design of a microarray must address several\ninterrelated issues: selection of cDNAs; replication of each cloned cDNA fragment; and\nplacement of each replicate. A sophisticated computational approach to design is clearly\nneeded. A typical hybridization yields two images with thousands of visible spots\nconveying information about relative levels of expression. However, the reliable capture\nof the expression information from images is an ill-specified problem that must be\naddressed with flexible techniques that adapt to the particular parameters of the\nexperiment, as well as the presence of noise and artifacts in the images.\n1\n\nDepartment of Computer Science, Virginia Tech, Blacksburg, VA 24061.\nForest Biotechnology, College of Natural Resources, North Carolina State University, Raleigh, NC\n27695.\n3\nDepartment of Plant Pathology, Physiology, and Weed Science, Virginia Tech, Blacksburg, VA 24061.\n2\n\n\f2 Lenwood S. Heath\n\nChoreography of Gene Expression Patterns\nMicroarrays provide a means of determining the transcript profiles of entire genomes\nunder a given set of experimental conditions, where an entire genomic sequence is\nknown. When sequencing is still in progress (e.g., maize, loblolly pine), there is the\nadded potential for gene discovery. A cDNA microarray experiment increases our ability\nto find genes that are expressed in similar patterns over time or under particular\nconditions and to find patterns in expression data. Powerful tools for inferring function\nalso come from the assignment of common responses to multiple environmental stresses.\nMuch functional genomic information remains buried in the data already accumulated\n[Bard, 1999]. New computational tools for mining this hidden information are needed, as\nare tools for analyzing gene expression patterns to reveal unknown cellular regulatory\nnetworks and signal transduction pathways. Accomplishments brought about through the\nuse of microarrays are already considerable [Epstein and Butow, 2000, Somerville and\nSomerville, 1999, Cushman and Bohnert, 2000]. Microarrays have been used to identify\ntranscript profiles characteristic of particular human tumor types [Khan et al., 1999,Yang\net al., 1999, Perou et al., 1999, Golub et al., 1999, Monni et al., 2001, Kannan et al.,\n2001]; cell cycle regulatory mechanisms [Cho et al., 1998, Chu et al., 1998]; salt stress\n[Kawasaki et al., 2001]; hypoxia [Gracey et al., 2001]; apoptosis [Brachat et al., 2000];\nand oxidative stress responses [Jelinsky et al., 2000, Alexandre et al., 2001].\nIn plants, microarray experiments with Arabidopsis cDNAs [Schaffer et al., 2000] have\nrevealed differential effects of wounding compared to insect feeding [Reymond et al.,\n2000]; induction of novel potential regulatory genes in response to nitrate [Wang et al.,\n2000]; differential responses to cold and drought stress [Seki et al., 2001]; potential\nregulatory sequences in developing seeds [White et al., 2000]; and differential expression\namong leaves, flowers, and roots [Ruan et al., 1998]. Aharoni at al. [Aharoni et al., 2000]\ndiscovered a novel strawberry gene that plays a crucial role in flavor biogenesis in\nripening fruit.\nComplex gene expression patterns are being revealed with this new technology, involving\nlarge numbers of genes and unexpected components of cellular function in regulation and\nmetabolism. As the complexity of microarray experiments increases, more sophisticated\nkinds of biological information can be extracted from microarray data. Enhanced\nexploitation of microarray technology requires more powerful and subtle data analysis\nand mining technology.\n\nMicroarray Data Analysis\nSeveral algorithms from data mining, machine learning, and parametric and nonparametric statistics have entered microarray data analysis [Aharoni et al., 2000,Seki\net al., 2001,Wang et al., 1999,White et al., 2000]. Analysis techniques such as k-means\nclustering, clustering by principal components, average linkage clustering [Jain and\nDubes, 1988], self-organizing maps [Golub et al., 1999], agglomerative and hierarchical\nalgorithms [Eisen et al., 1998,Reymond et al., 2000], Bayesian methods [Smyth, 1996],\nplaid models [Lazzeroni and Owen, 2000], and support vector machines [Brown et al.,\n\n\f3 Lenwood S. Heath\n2000] have been featured in a majority of published research. A commonly accepted\ndichotomy of analysis techniques distinguishes between supervised and unsupervised\nmethods.\n\nUnsupervised Methods\nFrom a probabilistic viewpoint, unsupervised methods model the unconditional\ndistribution (e.g., via densities) of gene expression data, as revealed by microarray\nexperiments [Jordan and Bishop, 1997]. For example, the set of genes responding\npositively can be modeled by a functional form such as a Gaussian or as a mixture of\nfunctional forms (called a mixture model). The mixture model, in particular, uses\nsuperpositions of simpler densities (such as Gaussians defined with known means and\ncovariance) to represent high-dimensional and high-variability regions of gene\nexpression [Yeung et al., 2001]. The distributions modeled by these simpler densities can\nbe viewed as clusters, which are categorically homogeneous subsets. Thus, clustering\nlooks for regularities in the training data (the data resulting from microarray\nexperiments) and can be used to provide a compact representation of the input problem\ndomain. Different clustering methods have been proposed that represent clusters in\ndifferent ways, using, for example, a representative exemplar of a cluster; a probability\ndistribution over a space of attribute values; or necessary and sufficient conditions for\ncluster membership. To represent a cluster by a collection of training data and to `assign'\nnew samples to existing clusters, some form of a utility measure is used. This is normally\nbased on one or more mathematical properties -- such as distance, angle, curvature,\nsymmetry, and intensity -- exhibited by the members of the cluster. k-means clustering,\naverage linkage clustering, clustering by principal components, self-organizing maps,\nagglomerative and hierarchical algorithms are examples of this unsupervised mode of\ninvestigation.\n\nSupervised Methods\nSupervised methods, on the other hand, can be viewed as modeling conditional\ndistributions. Jordan and Bishop [Jordan and Bishop, 1997] demonstrate the value of\nviewing these techniques as modeling the dependence of a set of output variables on a set\nof input variables. For example, in addition to capturing the similarity between a set of\npositively-responding genes, supervised techniques can relate this gene expression to\nputative functional categorizations of the genes or other a priori knowledge. Thus, gene\nexpression is an output variable, conditionally dependent on the input (such as functional\ncategory). Modeling the conditional distribution is important if we are interested in\nassigning (predicting) output values for new (untested) input value, as well as if we are\ninterested in deriving phenomenological models that serve as initial explanations of the\nobservations. Support vector machines [Brown et al., 2000], decision trees [Garofalakis\net al., 2000], regression [Vapnik, 1998], discriminant analysis [Sullivan, 2001], and\nbackpropagation neural networks [Gallant, 1993] are examples of supervised techniques.\n\nModel-based vs. Model-Free Methods\n\n\f4 Lenwood S. Heath\nIn addition, analysis techniques can be characterized as model-free (also called nonparametric, memory-based, or lazy) or as model-based (also called parametric or eager)\ntechniques. A model-free technique makes no assumption concerning the underlying\n(conditional or unconditional) density function. Examples are nearest neighbor\ntechniques and voting schemes. A model-based technique attempts to generalize from\ndata by capturing a more succinct representation of data -- certainly more succinct than\nexhaustively enumerating the data. Formal methodologies such as the minimum\ndescription length (MDL) principle emphasize this view of learning as\ncompression [Rissanen, 1978].\nThe importance of analysis techniques in finding coordinated sets of gene expression\npatterns is widely accepted [Schaffer et al., 2000]. As a result, researchers have spent\nconsiderable effort improving and tuning these techniques. Many clustering techniques\nare sensitive to the choice of the similarity metric used to distinguish intra-cluster\nvariation from inter-cluster variation. Approaches such as permutation testing have been\nproposed to address this sensitivity for agglomerative and hierarchical clustering\n(K. Schlauch, personal communication).\nWhile many of these techniques permit extremely efficient implementations, they suffer\nfrom two fundamental drawbacks:\n1. An inability to incorporate prior biological knowledge. For example, clustering\nresults are far from self-explanatory and a manual inspection is often required to\ndecipher the biological implications of a set of genes being assigned to the same\ncluster, if this can be done at all. Supervised techniques studied in the microarray\nliterature alleviate this problem to some extent but are limited by the second\ndrawback.\n2. The lack of expressiveness of the mined patterns; that is, each of these techniques\ncan represent only a limited set of facts using its patterns. Furthermore, they can\nexploit prior knowledge in only a limited form, typically a priori functional\ncategorizations.\nThe dichotomy between unsupervised and supervised methods has prompted several\ninteresting observations. One particular strategy, studied by Sherlock [Sherlock, 2000]\nand Golub et al. [Golub et al., 1999] addresses the question “Is the overlap between the\ngenes in a functional class and the genes in a particular expression cluster greater than\nwould be expected by chance?” It is instructive to note that this has been viewed as a\nproblem of correlating expression data to other information; thus many researchers [Cho\net al., 1998, Kannan et al., 2001] have approached this problem by conducting supervised\nand unsupervised analyses as separate stages and later using statistical tests to determine\nthe significance of such correlations.\n\nEnvironmental Effects on Tree Growth and Wood Properties\nForest trees are in the earliest stages of domestication. Both tree breeding and\nfundamental genetic studies have been greatly hindered by the long generation times and\n\n\f5 Lenwood S. Heath\nlarge size of forest trees. The technology of genomics and microarrays allows insights\ninto the molecular basis of growth and specific wood properties without the need for\nextensive breeding experiments over many generations, and can be carried out on\nexisting material or on young seedlings. Genomics allows insights into the physiology of\notherwise intractable systems and allows us to benefit greatly from comparisons to model\nsystems based on sequence information. The identification, functional analysis, and\nlocation of expressed genes in forest trees has many applications. One area of specific\ninterest is the identification of the effects of abiotic stress on wood formation, particularly\nwater stress, because of the effect of water stress on tree growth and on wood\nproperties [Lev-Yadun and Sederoff, 2000,Costa et al., 1998,Costa and Plomion, 1999].\nGrowth and wood properties are important commercial factors because they affect the\nyield and quality of commercial forests.\nA major motivation for the development of microarrays for forest trees is the potential for\nthe analysis of environmental effects on trees in natural populations. The microarray\nsystem is potentially able to monitor acute and chronic environmental effects based upon\nthe specificity and level of expression observed for a large suite of genes. The microarray\nmethod has great inherent accuracy and precision, but there is a great deal of\ndevelopment needed for this potential to be realized. One of the major barriers at present\nis the lack of appropriate computational tools for analyzing and interpreting the specific\nand the correlated effects on gene expression during development of adaptation to abiotic\nor biotic stress. Microarray technology can contribute greatly to studies of adaptation and\nresponse to climate change.\n\nShort- and Long-Term Adaptational Responses of Plants to\nEnvironmental Stress\nThe ability of a plant to protect itself against environmental stress is essential to its\nsurvival [Alscher et al., 1997]. Acclimation of plants to extreme environmental\nconditions or to rapid changes in growth conditions requires a global cellular response\nand changes in the expression of many genes. Exposure to extremes of light intensity and\ntemperature, drought, and some herbicides can cause the downstream formation of\n–\nreactive oxygen species (ROS). ROS may be present in the form of superoxide (O2 ),\n–\n–\nhydrogen peroxide (H2O2), or the hydroxyl ion (OH ). ROS, especially OH , are toxic\nbecause they can oxidize any macromolecule in the cell [Scandalios (ed.), 1997]. This\npotential threat to cellular function can cause protein unfolding, the inactivation of\nenzymes, DNA damage, mutation, lipid peroxidation, and consequent disruption of cell\nmembrane function.\nIn animals, injury due to unchecked ROS damage has been linked to cancer and aging\n[Gilchrest and Bohr, 1997]. ROS have also been implicated as links in stress-responsive\nsignal transduction mechanisms [May et al., 1998]. There is a suggested role for\nhydrogen peroxide in the signaling events leading to the activation of the defenseresponse [Mullineaux et al., 2000]. In some instances, the plant successfully adapts to its\nchanged environment. Both short- and long-term effects occur. Shinozaki et al.\n[Shinozaki and Yamaguchi-Shinozaki, 2000] distinguish between rapid, emergency\n\n\f6 Lenwood S. Heath\nresponses, and slower, adaptive responses associated with successfully attaining a new\nsteady state under stress conditions.\nOur understanding of genome-wide mechanisms contributing to successful adaptation in\nplant cells is incomplete. A coordinated global shift in gene expression in plant cells is\nexpected to be involved in adjustment to unfavorable conditions. There is growing\nevidence that while the mechanism whereby a plant perceives and transduces a particular\nadverse environmental change is specific to that change, that there is considerable\noverlap among the mechanisms that respond to the class of adverse environmental\nchanges [Uno et al., 2000] (see also Fig. 2). Genes controlling osmotic adjustment,\nprotein stabilization, ROS detoxification, ion transport, membrane fluidity, gene\nactivation, and signal transduction have all been implicated in stress adjustment responses\nin higher plants in separate experimental systems [Cushman and Bohnert, 2000,\nShinozaki and Yamaguchi-Shinozaki, 1997]. Specifically, dehydrins have been shown to\nbe associated with dehydration tolerance [Zhu et al., 2000]; proteases with signal\nperception and transduction [Callis and Vierstra, 2000]; calmodulin-mediated processes\nand membrane transport with salt tolerance [Geisler et al., 2000]; cell wall extensibility\nevents (extensins, proline-rich proteins) with adaptation to drought stress [Wu and\nCosgrove, 2000]; and lignin production (laccases, phenylpropanoid pathway enzymes)\nwith growth under multiple stress conditions, including salt stress [Degenhardt and\nGimmler, 2000] (see Fig. 6 and 7).\n\nA Search for Molecular Adaptation Mechanisms in Plants\nLittle information has been gathered for any experimental system that documents global\nchanges in gene expression associated with successful, long-term adaptation to stress.\nThis is in sharp contrast to the available data that documents response to short-term\nexposures. Although there are several molecular defense systems that respond to\nenvironmental stress in plants (Fig. 2), their relative importance for long-term stress\nresistance is not known. It is likely that the coordination, identity, timing, and level of\ninduction or suppression of stress-responsive genes is critical for effective and sustained\nremoval of toxic ROS. Redox sensing appears to play a central role in environmental\nstress responses. The effective repair and renewal of individual, stress-susceptible,\nmacromolecules and associated cellular and physiological processes is essential for cell,\ntissue, and organism survival (see Fig. 2). Gasch et al. [Gasch et al., 2000] used\nmicroarray technology to investigate adaptive responses of the entire yeast genome to a\nseries of abiotic stresses. They interpreted increases in transcript abundance in genes\nassociated with signal transduction, chaperones, ROS detoxification, and bioenergetics as\nevents involving essential processes to ensure cellular survival in the face of long-term\noxidative stress.\n\nChoice of Experimental System\nComparison of the transcriptional responses of higher plants to environmental stresses is\na powerful tool for understanding the functions of individual genes in the responses as\nwell as the adaptative response of the genome as a system. The use of comparative\n\n\f7 Lenwood S. Heath\nfunctional genomics to study responses to environmental stress in forest trees enables\nnew and different investigations of mechanisms of adaptation. Information on gene\nfunction related to environmental adaptation is far from complete even in the model plant\nArabidopsis. Many genes have important interactions that may not be apparent from their\nprimary function. Even when a homolog has been identified by comparative sequence\nanalysis and a specific function has been implicated, many genes may have additional\nfunctions and interactions in a woody plant that might not be inferred from studies of\nArabidopsis.\n\nExpresso: A Microarray Experiment Management System\nOur research group has recognized the need to address all phases of a microarray\nexperiment as a coherent whole and to fashion a computational system that integrates the\ndesign, analysis, and data management tasks as well as the laboratory and computational\ncomponents. The Expresso system [Alscher et al., 2001] is designed to support all\nmicroarray activities including experiment design, data acquisition, image processing,\nstatistical analysis, and data mining. Currently, the latter three stages are completely\nautomated and integrated within our implementation. The data and physical flows\nrealized using Expresso are shown in Fig. 1.\nThe design of Expresso underscores the importance of modeling both physical and\ncomputational flows through a pipeline to aid in biological model refinement and\nhypothesis generation. It provides for a constantly changing scenario (in terms of data,\nschema, and the nature of experiments conducted). The design, analysis, and data mining\nactivities in microarray analysis are strongly interactive and iterative. Expresso thus\nutilizes a lightweight data model to intelligently “close the loop” and address both\nexperiment design and data analysis. Expresso also uses inductive logic programming\n(ILP), a relational data mining technique to model interactions among genes and to\nevaluate and refine hypothesized gene regulatory networks. We refer the reader to\n[Alscher et al., 2001] for a more detailed exposition of the computational, algorithmic,\nand system implementation issues underlying the design of Expresso.\n\nModeling With Expresso\nThe first step in modeling with Expresso entails defining semi-structured data records\ncorresponding to information about material selection, PCR, randomization, spotting,\nhybridization, gridding, data extraction, and data analysis. An instance of all of these\nrecords thus represents a pipeline of stages involved in a single microarray experiment (a\npartial example is given in Fig. 3).\nThese self-describing descriptions serve several useful purposes. First, since they can be\nstored in a database and queried, programmatic descriptions of new experiments can be\nautomatically created by writing queries. For example, a high-level specification such as\n“Design a new experiment using the layouts from 1999, the dye concentrations used by\nMark, and the conditions of mild drought stress,” or “Use the same experimental setup as\nEXPT-99-Pine-Drought, but with a signal threshold of 0.60” can be defined by writing\n\n\f8 Lenwood S. Heath\ndatabase queries in languages like SQL and XML. In addition, biologists are able to\ninteract with Expresso using abstractions such as stress experiments and expression\nlevels, instead of the current emphasis on tedious details such as wells in microtitre plates\nor measured fluorescence in a tiff image. This can be achieved by providing an interface\nthat masks how individual records are composed to arrive at full-fledged descriptions of\nexperiments. Second, such descriptions can (optionally) be then used to manage the\nphysical and computational execution environment (e.g., pipetting robots, image readers,\nand data mining software). For instance, it is possible to transform an Expresso\ndescription into the low-level programming code for controlling and driving laboratory\ninstruments that have programmable interfaces supporting laboratory automation and\nmanagement. Together, these features help us store descriptions, `run' the descriptions to\nobtain data, record the data back in the database, and associate the data with the\ndescription that corresponds to its experimental setup. Finally, having descriptions of\nexperiments allows us to provide sophisticated services such as change management. For\nexample, consider that two students configure Expresso independently with different\nchoices for various stages in the pipeline and arrive at contradictory results. They could\nthen query the database for “What is different between the experiments that produced\ndata in directory A from the ones in directory B?” -- providing responses such as “The\nonly difference is that a calibration threshold of 0.84 was used in B instead of 0.96 for\nA,” which are obtained by automatically analyzing the descriptions.\nIn contrast to the variety of standards (many, XML-based) available for describing\nmicroarray data, our data model is thus aimed at capturing representations of\nexperiments, not just experimental data. We posit that the description of an experiment is\na more persistent representation of the data (it produces) than the data itself. As\ntechnology matures and evolves, recording how specific data was obtained is important\nfor the purposes of ensuring repeatability and reliability. For example, if gridding\ntechnology improves, then `running' the same (stored) description with the new setup can\nbe used to obtain new results.\nOne of the hallmarks of Expresso and its semi-structured representation is that the data\nmodel is lightweight and can elegantly adapt to changes in schema over time. There is\nnothing in our language design that commits us, for instance, to describing DYEs by two\nattributes (ref. Fig. 3). As new forms of DYEs are introduced into Expresso, the data\nmodel can `expand' to accommodate new fields and attributes, that were not applicable in\nolder records. The design of the semi-structured language is beyond the scope of this\narticle; a preliminary description is available in [Alscher et al., 2001]. Here, we\nspecifically concentrate on using Expresso to understand stress responses in Loblolly\nPine.\nIn this paper, we explicitly concentrate on the use of Expresso to mine global patterns of\ngene expression in order to uncover regulatory mechanisms that are essential for longterm adaptation to stress in woody species. In particular, our main focus is on gene\nexpression associated with adaptation to drought stress over one growing season in\nloblolly pine.\n\n\f9 Lenwood S. Heath\n\nResults\nIn June 1999, drought stress was developed by withholding water from rooted cuttings of\ntwo unrelated loblolly genotypes from the Atlantic Coastal Plain, while control plants\nwere watered normally. Mild and severe drought stress constituted pre-dawn water\npotentials (pressure bomb technique) of –10 to –12 bars, and –16 to –18 bars,\nrespectively. Rooted cuttings were subjected to mild or severe drought stress for four\n(mild) or three (severe) cycles Plants were watered to field capacity once these pressure\npotentials were attained. Needles were harvested after the drought cycles were completed\n(adaptation). Mild stress produced little effect on growth with new flushes as in control\ntrees. Imposition of severe stress resulted in growth retardation with markedly fewer new\nflushes compared to controls. Using RNA harvested from individual trees and different\ntreatments we have determined global patterns of gene expression on microarrays. With\nalgorithms incorporated into Expresso, we have identified genes and groups of genes\ninvolved in stress responses.\n\nEffect of Mild and Severe Stress on Gene Expression\nData were obtained for the control versus mild stress condition and for the control versus\nsevere, non-adaptive, condition for Genotype D and for the control versus mild condition\nalone for Genotype C. We performed preliminary statistical analysis on both genotypes\nbut applied the inductive logic programming technique only to data from Genotype D.\nThe reason for this restriction involves the theoretical model of machine learning\nassumed by typical ILP implementations. All expression data are distilled within\nExpresso into a single metric of up-expressed, down-expressed, or unchanged. Inclusion\nof Genotype C into our study would imply that the probability distribution from which\nexamples (instances of gene expression data) are picked is nonuniform (ref. the control\nversus mild condition). However, currently available ILP systems do not provide\nsystematic ways to incorporate this prior knowledge (of the distribution of examples) as\nlearning parameters. While theoretical analyses are definitely feasible, our goal was to\nensure the biological validity of the hypotheses generated. A total of 37 rules were mined\nby ILP for Genotype D.\nUsing Expresso, we determined that 72 of the 384 cDNAs present on the microarray\nshowed an increase in transcript abundance relative to controls at the end of four repeated\ncycles of exposure to mild stress in Genotype D. Of those 72 cDNAs, 69 showed either a\ndecrease or no change in the control versus severe comparison. Expresso mined this\nobservation as the rule:\n~level(A,CvsS,positive) :- level(A,CvsM,positive).\n\n(1)\n\nIn rules, the notation `~' means logical negation or `not'. Hence, this rule means that if a\nclone (A) is up-expressed in the CvsM (control versus mild drought stress) comparison,\nthen it was not up-expressed in the CvsS (control versus severe drought stress)\n\n\f10 Lenwood S. Heath\ncomparison. This rule is supported by observations of transcript abundance of 69 out of\n72 relevant cDNAs. This gives a confidence level of 69/72 (about 96%).\n\nThese clones, and their associated functional categories, are candidates for drought stress\nadaptation genes and for participation in associated mechanisms. The functional\ncategories of the positive responders in the `protective processes' grouping are shown in\nFig. 7. The remaining 257 cDNAs were either unaffected (204), or showed a decrease\nrelative to the controls (43). Among the positive responders were genes already\nassociated with water stress responses such as the dehydrins and water channel proteins\nor aquaporins. The class of transport proteins, into which the aquaporins fall, showed a\nnegative response in the mild versus severe stress comparison, and a positive in the\ncontrol versus mild contrast. This was indicated by the next two rules (confidences\n63.63% and 66.67% respectively):\nlevel(A,CvsM,positive) :- category(A,membranetransportprotein).\nlevel(A,MvsS,negative) :- category(A,membranetransportprotein).\n\n(2)\n(3)\n\nIn the case of both dehydrins and aquaporins, different cDNAs responded to probes from\nthe two genotypes. Aquaporins associated with both the tonoplast and the plasma\nmembrane were present on the array. cDNAs representing both subcellular locations\nresponded positively in the control versus mild stress comparison, suggesting the\nimportance of water channel proteins in the tonoplast and the plasma membrane for\nadaptation to mild drought stress. Genes encoding heat shock proteins (HSPs) -- HSP70\n(chloroplast-associated chaperone function [Rial et al., 2000]), HSP23 (LEA-like genes\n[Dong and Dunstan, 1996]), and HSP100 (thermotolerance [Hong and Vierling, 2000]) -also responded positively to mild drought stress (confidence 83.33%):\nlevel(A,CvsM,positive) :- category(A,heat).\n\n(4)\n\nIn contrast, HSP80s (thought to be involved in chromatin organization [Schnaider et al.,\n1999]) did not respond in either genotype. In some cases, different cDNAs responded to\nprobes from the two genotypes (see Fig. 4 and Table 1 for a summary of results obtained\nfor HSPs). Several HSPs (Clone 226, an HSP 101; clone 228, an HSP 23.5; and clone\n296, an HSP 70) showed a positive response in the control versus mild stress comparison\nand a negative in the control versus severe stress comparison, making them strong\ncandidates for drought stress adaptation genes. Rubisco-binding proteins were also\namong the positive responders in the control versus mild drought stress comparison. In\ncontrast, Rubisco-binding proteins were unchanged in the mild versus severe comparison,\nsuggesting that these genes may not be among the class of candidates for stress\nadaptation. LP-3, an established water-stress inducible gene in loblolly pine, responded\npositively to probes from both genotypes. No difference in expression level of LP-3 was\ndetected in the control versus severe contrast in Genotype D. LP-3, a protein with a\nchaperone function, therefore falls into the class of candidate genes associated with\n\n\f11 Lenwood S. Heath\nresistance to mild drought stress. There was no detectable difference among the thiolutilizing enzymes for the control versus mild, or the control versus severe stress\ncomparisons. There is much documentation demonstrating the involvement of thiolutilizing enzymes in short term responses to oxidative stress, but little to document events\nassociated with long-term adaptation.\nThe class of genes categorized very loosely as ``isoflavone reductases,'' of which four\nseparate ESTs were included on the array, exhibited positive responses in both genotypes\nin the control versus mild drought stress comparison, with two ESTs with greatest\nresemblance to phenylcoumarinbenzylic ether reductases responding in Genotype C, and\ntwo ESTs corresponding to the closely related pinoresinol-lariciresinol reductases in\nGenotype D. On the other hand, the IFR homologs showed no detectable change in the\nmild versus severe comparison, suggesting a response to stress, but no correlation with\nsuccessful adaptation to mild drought conditions. Genes associated with lignin\nbiosynthesis also responded positively, as did GST, proteases, and receptor-like protein\nkinases. In the case of cell wall associated genes, positive change was detected in the\ncontrol versus mild comparison (confidence: 81.25%)\nlevel(A,CvsM,positive) :- category(A,cellwallrelated).\n\n(5)\n\nand a negative response for lignin biosynthesis genes in the control versus severe contrast\n(confidence: 81.81%)\nlevel(A,CvsS,negative) :- category(A,ligninbiosynthesis).\n\n(6)\n\nsuggesting a role for lignin biosynthesis in drought stress adaptation.\n\nDiscussion\nUsing cDNA microarrays, we have investigated expression patterns of genes in needles\nof loblolly rooted cuttings (equivalent in size and development to one-year-old seedlings,\nbut of identical genotype) from two different unrelated genotypes from the Atlantic Coast\nPlain that had successfully adapted to cycles of mild drought conditions over a growing\nseason. We have compared those results with results obtained from rooted cuttings of one\nof the genotypes exposed to more severe, non-adaptive, conditions over the same time\nperiod.\nThe expression data reported here reflect the adaptational adjustments made by loblolly\npine needles to long-term and intermittent drought stress. The control versus mild stress\ncomparison for two unrelated genotypes identifies candidate functional categories for\ndrought stress tolerance and resistance. The positive response of LP3 in the control versus\nmild stress comparison, a known water-stress inducible gene in Pinus taeda, serves as a\npositive control for our data. Dehydrins and aquaporins are among the responders, as\nwould be expected from their established physiological roles. There were many\n\n\f12 Lenwood S. Heath\naquaporin ESTs in our microarray group. However, we cannot definitively distinguish\nbetween ESTs from one gene or from closely related members of a multi-gene family.\nThe aquaporins were divided among tonoplast and cell membrane-associated groups; thus\nwe are dealing with at least two different genes. The genes from the heat shock proteins\nthat responded fell into three groups. Of these three, two -- the HSP70s and the HSP23s -have known chaperone functions, not necessarily related to heat shock responses, and can\nperhaps be regarded as fulfilling a maintenance or repair role in cells that are coping with\nmild drought stress on an ongoing basis. The positive response of the HSP70s, which\nfulfill a chaperone or targeting function for proteins synthesized in the cytosol and\ndestined for the chloroplast, is in agreement with the increases in transcript abundance of\nphotosynthesis-associated genes. These HSPs showed no difference, or were negative in\nthe control versus severe stress comparison. Thus, the HSPs are candidate genes for\ndrought resistance.\nThe HSP100s, which also showed a positive response in the control versus mild stress\ncomparison, are more definitively associated with heat shock itself. Their response may\nindicate the existence of a common core of genes that respond to a range of different\nstresses; a result that is in agreement with many others in the literature. The IFR\nhomologs that responded positively in both genotypes, as well as the GSTs, may also fall\ninto the class of a core of stress responsive genes, although not in the class of stress\nresistance genes per se. Both the IFR homologs and the GSTs are most commonly\nassociated with responses to biotic and xenobiotic stress and not to the abiotic challenge\npresented by drought stress. Response to increased ROS levels may be the common\ndenominator for these changes in the various functional categories.\nOur results present a snapshot of the state of gene expression in loblolly needle tissue that\nhas adapted to mild drought stress. A detailed time course study is needed to identify\nevents in gene expression that lead to adaptation. Many, transitory, stages in signal\nperception and transduction can only be captured by sampling early on in the adaptation\nprocess. We plan to set up a sampling scheme to glean evidence for the physiological\nchanges that underlie short-term emergency adjustments and to identify those changes\nthat are essential for subsequent, long-term adaptation.\nThese requirements point to the future directions in the development of Expresso. We are\nnow extending Expresso to intelligently integrate experiment design and data analysis.\nThis will provide us the ability to use run-time information from the results of data\nmining to make recommendations about the earlier stages in experimentation, such as\nlayout, randomization, and choice of clones for the next iteration of studies.\nA long-term biological goal is the modeling of the dynamics of adaptation to\nenvironmental changes. Understanding the qualitative and quantitative responses of\nmetabolic pathways to external and internal signals implies the need to integrate\nbiological knowledge drawn from gene expression studies together with information from\nproteomics and metabolic profiling. The data management architecture of Expresso is\ndesigned to have the flexibility to support this aspect of inquiry.\n\n\f13 Lenwood S. Heath\n\nDetection of Gene Expression Changes\nThe inevitable presence of experimental errors complicates the determination, for each\nclone (cDNA) represented on a set of microarrays, whether that clone shows clear\nchanges in transcript levels under the experimental conditions considered. Various\nstatistical tests suggested in the literature do not fully utilize the available information or\nmake assumptions that are probably too strong and unrealistic. Chen et al. [Chen et al.,\n1997] derive a probability density of transcription ratios under strong (and highly\nunrealistic) distributional assumptions. They investigate neither the effects of deviations\nfrom their strong assumptions nor all possible sources of variation (e.g., due to\nbackground estimation). Some authors (e.g., Claverie [Claverie, 1999]) have suggested\nthe use of t-tests applied to intensity values. This approach requires replication of the\nsame gene on one or more arrays and the use of paired t-tests or non-parametric paired\ntests such as the sign test. These tests are expected to have poor efficiency with few\nreplications. Hilsenbeck et al. [Hilsenbeck et al., 1999] uses prediction regions from\nprincipal components, while Greller and Tobin [Greller and Tobin, 1999] describe a\ndecision function using a statistical discordancy test. Several approaches involving\nBayesian methods have also been proposed. One is the Hierarchical Generalized Linear\nModel (see Daniels et al. [Daniels and Gatsonis, 1999] for general methodology and Lee\net al. [Lee et al., 2000] for application to activity data), which can model and estimate\nnoise variance components if replication is available.\nThe methodology followed in Expresso is qualitatively different; we obtain multiple\n(typically 16) log-calibrated-ratios for a single replicated clone; by observation, we find\nthat the log-calibrated-ratios for a single clone do not follow a normal distribution. Far\nfrom it, each distribution is spread relatively evenly over a large range. Statistical\nanalysis based on mean and standard deviation will thus be overly pessimistic in\nidentifying clones that are up- or down-expressed. Given this observation, we make a\nmuch weaker probabilistic assumption on the distribution; we assume that a clone whose\nexpression is not different between a probe pair will show a distribution centered at a\nmean log ratio of 0.0. Our assumption of a zero-centered distribution is more general than\nthe assumption of a particular distribution, such as a normal distribution, and hence is\nmore likely to hold in a real experiment. In a zero-centered distribution, the probability\nthat any particular log ratio is positive (or negative) is 0.5. The number of positive (or\nnegative) log ratios follows a binomial distribution with parameters 16 and 0.5. The\nprobability of 12 positive log ratios (or 12 negative log ratios), out of 16, for a clone\nwhose expression was unaffected by drought stress is 0.0384064. Consequently, a clone\nwith 12 or more positive log ratios is up-expressed with a probability of 0.96. Our more\ngeneral assumption avoids the trap of having to classify the response of each spot; rather\nwe classify the response of each EST as one of: up-regulated, down-regulated; or no\nclear change. Our three-way response classification allows us to develop meaningful\nrelationships among genes and among treatments and also provides sufficient results for\nthe use of sophisticated data mining techniques (see below).\n\nInductive Logic Programming\n\n\f14 Lenwood S. Heath\nOur analysis methodology is motivated by the need to connect functional categorizations\nof genes with systematic variations in expression levels. In his review of expression data\nanalysis, Sherlock [Sherlock, 2000] discusses two techniques for correlating biological\ninformation with expression data. The first technique builds a two-way classification\npredictor based on weighted votes provided by gene expression from tissues in each of\nthe two classifications. Golub, et al. [Golub et al., 1999] successfully apply the technique\nto classifying leukemia type from human tissues. The second technique first organizes\ngenes into clusters using k-means clustering of expression data and then computes\nstatistical correlations between each cluster and each of a set of functional categories. Our\nanalysis methodology is fundamentally different from the techniques discussed by\nSherlock [Sherlock, 2000]. We use the inductive logic programming (ILP) approach\n[Muggleton and Feng, 1990, Muggleton, 1999, Dzeroski, 1996] as an aid in data mining\nand formation of biological hypotheses. ILP is a technique that provides, in one\nintegrated procedure,\n1. A way to correlate output variables (gene expression) with input variables\n(functional categorizations, for instance);\n2. A richer representational basis (allowing the incorporation of expressive a priori\nbiological knowledge, not limited to functional categories); and\n3. A methodology of abduction, a process of hypothesis formation (that can involve\nfinding coordinated sets of gene expression data).\nILP takes input data expressed as gene expression levels in particular experiments,\nrelationships between experiments, functional categories, and any biological knowledge\nthat is available. As output, it provides rules of the form\nlevel(A,CvsS,negative) :- level(A,CvsM,positive).\n\nwhere C, M, and S represent control, mild drought stress, and severe drought stress\nconditions, respectively. This rule states: ``If a clone (represented in the rule as A) was\npositively expressed in the control versus mild drought stress comparison (CvsM), then it\nwas negatively expressed in the control versus severe drought stress (CvsS) comparison.''\nThe restated rule is easily understood and can be used in later diagnostics and what-if\nanalyses. ILP algorithms do not require explicit invocation or instructions to mine rules\nacross comparisons. Rules are produced as the result of a process of systematic search for\nsuccinct, conceptual clusters of data. ILP can thus be used to find patterns within a given\ncomparison, across comparisons, and across functional categories. In contrast, a purely\nunsupervised method may recognize a particular group of clones in the control versus\nmild drought stress comparison that exhibit a positive response and also recognize\nanother group of clones in the control versus severe drought stress comparison that\nexhibit a negative response. However, it cannot model the connection between these two\ncomparisons (unless we know beforehand that this kind of connection is what we are\nlooking for). A purely supervised technique can make such a connection only after the\nabove clusters are modeled (recognized and given as input). ILP subsumes both these\nmodes of analysis in rule formation. In addition, ILP rules can be recursive, a feature\n\n\f15 Lenwood S. Heath\nthat makes them amenable to discovery of complex relationships involving hierarchies of\nfunctional categories [Muggleton, 1999] (see results below).\n\nData preparation for ILP\nILP systems typically take a database of positive examples (correct gene expression\ndata), negative examples (information known not to represent correct gene expression\ndata) and background knowledge (functional categories, for example) and attempt to\nconstruct a predicate logic formula (such as level(A,B,C)) so that all (most) positive\nexamples can be logically derived from the background knowledge and no (few) negative\nexamples can be logically derived. The need for negative examples can be seen by\nobserving that ILP algorithms conduct mining by searching through a space of possible\npatterns. Such a space is typically organized as a specialization-generalization hierarchy.\nThe more specific patterns are at the bottom of the subsumption lattice and the most\ngeneral patterns are at the top of the lattice. While ILP algorithms differ in how they\nnavigate, prune, or focus on this space of patterns, all of them require a way to evaluate\nany specific pattern encountered in such a search. One useful form of such evaluation\npertains to how accurately the pattern fits the positive examples and how accurately it\nfails to fit the negative examples. The more negative examples that are covered by a\npattern, the less likely that it will be a good representation (or predictor) for the\nunderlying data distribution. Hence negative examples are important to ensure that the\nmining does not produce overly general patterns. For instance, suppose that the only\nclones presented to ILP are from the `heat' category and that they all responded positively\nin a certain comparison. Mining that all clones respond positively in that comparison\nwould certainly be a valid (from the data distribution viewpoint) but dangerous (from the\nbiological viewpoint) conclusion to make. If additional clones (from other categories) are\npresented that do not respond positively in the given comparison, then data mining can\ncorrectly infer that it is the membership in the `heat' category that co-occurs with the\npositive expression. Similarly, negative examples help produce valid patterns by defining\nthe boundaries of generalization without any truly ``additional'' information.\nA partial listing of our database tuples is shown in Fig. 5. Negative examples are\nautomatically generated by invoking the closed-world assumption, which states that all\nrelevant facts are stored in the tables and facts not recorded can be taken to be false. For\ntables that have a large number of columns (high arity), this might cause us to generate a\nhuge number of negative examples. The typical solutions are to (i) place restrictions on\nhow variables are coupled in an ILP rule (allowing us to use them to generate negative\nexamples), (ii) perform a probabilistic analysis by constructing so-called stochastic logic\nprograms. In our experiments, negative examples are easy to generate since the only\nvariability allowed in the level table is in the Expression column. Thus, if a clone was\npositive for a particular comparison, we can declare two negative examples, namely that\nit was negative and unchanged for the (same) comparison. For our experiments, we made\nuse of the Aleph ILP software [Srinivasan, 2001] from the Oxford University Machine\nLearning Laboratory.\n\nAttribute-value versus Relational Learning Techniques\n\n\f16 Lenwood S. Heath\nILP is a relational learning technique, distinguishing its representational basis from those\nof attribute-value based techniques (discussed in the beginning of this section). Formally,\nILP uses a representation of (a proper subset of) first-order predicate logic, whereas\nattribute-value techniques work at the level of propositional logic. Its expressiveness\nmakes it a highly desirable tool in structured domains (such as microarray data analysis)\nwhere comprehension and interpretation of patterns is important. Notice that to achieve\nsimilar results with attribute-value techniques requires (i) explicit enumeration of all\npossible types of rules; (ii) data preparation for each type of rule; and (iii) recasting the\nrules in biological terms. For even three comparisons and 100 functional categories of\nESTs, there are potentially 2100 x 23 possible types of rules. ILP techniques use pruning\nalgorithms that focus on the most promising scenarios. Obtaining the same effect\nmanually with attribute-value techniques is practically impossible because of the huge\nnumber of types of rules.\nILP techniques can incorporate prior biological knowledge; for example, if the biologist\nknows that there is a possible connection between protective processes such as ROS\ndetoxification and protected processes such as the reductive pentose phosphate pathway,\nan ILP execution can be modeled to exploit such knowledge. In many cases, such prior\nknowledge is also helpful in speeding up ILP.\n\nMethods\nPlant Material\nRooted cuttings of loblolly pine equivalent in size to one-year-old seedlings were\nobtained from Dr. Barry Goldfarb at NCSU and were cloned from two unrelated\ngenotypes (clones C and D) from the Atlantic Coastal Plain provenance.\n\nChoice of Target cDNAs\nA substantial number of expressed genes (approximately 15,000) have now been\nidentified from loblolly pine as part of the Pine Genome Sequencing Project [NCSU\nForest Biotechnology Group, 2001]. The predominant source of these expressed genes is\nfrom wood forming tissues. These tissues are rich in expressed genes involved in cell\nwall biosynthesis and in intracellular signalling. Microarrays have been used with a small\nsubset of these expressed genes to examine gene expression during development and\nunder environmental stress. The long term goals of this project are to identify the genes\nexpressed during wood formation, to identify the time and place of their expression at the\ncellular level, and to correlate the effects of their expression with variation in wood\nproperties. This approach depends on genetic mapping of expressed genes and the\ncorrelation of the map positions of loci affecting quantitative variation in wood properties\nwith the location of specific expressed genes.\nOf the ESTs sequenced by the Pine Genome Sequencing Project, many have a proposed\nfunctional annotation derived from a BLAST search of protein databases. From this\n\n\f17 Lenwood S. Heath\nannotation, we selected 384 ESTs from differentiating xylem or shoot tips representing\ngenes of annotated function. We grouped the 384 pine ESTs into four major functional\ncategories (as shown in Fig. 6) -- gene expression, signal transduction, protective\nprocesses, and protected processes. A complete list of our functional categories and\ngroupings is available at\nhttp://bioinformatics.cs.vt.edu/~ralscher/functional_categories.html,\n\nand a detailed list, including annotations for individual clones, at\nhttp://bioinformatics.cs.vt.edu/~ralscher/clones_annotation.html.\n\nOur selection of clones includes genes involved in proposed resistance processes and\nsignal transduction mechanisms, as well as genes associated with processes expected to\nbe vulnerable to drought stress such as those associated with carbon metabolism,\nphotosynthesis, and respiration. Genes associated with ROS detoxification, cell wall\nextension, lignin biosynthesis, and chaperone function as well as stress specific genes,\nsuch as aquaporins and dehydrins were included in the protective processes category (see\nFig. 7). We also included genes known to respond to other stresses, such as UV\nirradiation, pathogen invasion, and sulfur stress (`isoflavone reductase-like') [Gang et al.,\n1999] and xenobiotic stress (glutathione-S-transferases) (see Figs. 6 and 7).\n\nPCR Amplification\n384 ESTs from differentiating xylem or shoot tips [NCSU Forest Biotechnology Group,\n2001] with putative functions of interest were selected and PCR amplified using M13\nforward and reverse universal primers. PCR was performed in a 50 µl reaction containing\n39.1 µl ddH2O, 5 µl 10x PCR buffer, 1.5 µl MgCl2 (50mM), 1 µl dNTPs (lOmM each), 1\nµl M13 forward primer (10 µM), 1 µl M13 reverse primer (10 µM), 0.4 µl TAQ\npolymerase (5U/µl), and 1 µl cDNA diluent. Amplifications were carried out in a MJ\nResearch thermocycler (Waltham, MA, USA). Denaturation was performed at 94° C for\n30 sec, followed by primer annealing at 57° C for 1 min. Chain elongation took place at\n72° C for 4 min. These steps were repeated for 35 cycles. Final chain elongation took\nplace at 72° C for 10 min. PCR products were then electrophoresed in 1.5% agarose gels,\nstained with ethidium bromide, and visualized using UV light. This step was necessary to\nconfirm both quantity and quality of the PCR reactants.\n\nMicroarray Design and Layout\nThe 384 ESTs were organized in 4 microtitre source plates after PCR amplification.\nExpresso generated a design for printing two types of microarrays that replicated each\nclone four times in each microarray type and that placed the replicates at random\nlocations in a microarray so that any systematic errors due to location can be analyzed\nand corrected (see Fig. 8). Implementing a replicated, randomized design required the use\nof two robots. First, the contents of the 4 source plates were re-pipetted into 8 sets of 4\nmicrotitre printing plates, using a TECAN Genesis 2 robot; each set was an independent\nrandomization of the 4 source plates. Second, slides were printed using a Stanford-type\n\n\f18 Lenwood S. Heath\narrayer (see http://cmgm.stanford.edu/pbrown/mguide) built in-house at NCSU. The\nrandomized design guaranteed that every EST was represented once in each set of 4\nmicrotitre plates and that each set was a different physical arrangement (permutation) of\nthe 384 ESTs. Two types of microarrays, A and B, were printed; each had 4 replicates of\nthe 384 ESTs, but the random arrangement of clones differed in the two microarrays.\nConsequently, each array type has 4 replicates of each EST, randomly placed, and a total\nof 1536 spots. Each glass slide contained 2 identical arrays (either type A or type B);\ntherefore, each slide had a total of 8 replicates of each EST.\n\nSlide Preparation\nAfter printing, slides were processed according to the manufacturer's instructions\n(Telechem International, Sunnyvale, CA), with some modifications. In the first step,\nslides were first rinsed in 0.2% SDS twice for two minutes, with vigorous agitation; were\nthen rinsed in distilled water twice for two minutes; and were finally transferred to\nboiling water for two minutes and cooled to room temperature for five minutes. In the\nsecond step, 1.5 g sodium borohydride was dissolved in 450 ml phosphate buffered saline\nto which 133 ml of 100% ethanol was added immediately prior to use. In the third step,\nthe slides were first transferred to the sodium borohydride solution for five minutes; were\nthen rinsed in 0.2% SDS for one minute three times; and were finally rinsed once in\ndistilled water for one minute. Array boundaries were marked with a diamond-tipped pen.\nIn the final step, the slides were dried by centrifugation and stored in the dark in a\ndessicator at room temperature.\n\nHybridization\nEach comparison of treatments (control versus mild drought stress; control versus severe\ndrought stress; and mild versus severe drought stress) was done with hybridizations\ninvolving 4 microarrays on 2 slides of different types, comprising a total of 16 replicates\nof each EST. Total RNA was isolated from 11 separate needle samples harvested at the\nend of the growing season by the method of Chang et al. [Chang et al., 1993], and was\nthe source of cDNA to probe the microarrays, after processing using the Genisphere Gene\nMicroExpression Kit. The two arrays present on each slide were hybridized with probe\npairs labeled reciprocally with Cy3 and Cy5 dyes. A ScanArray 4000 was used to scan\nthe slides after hybridization (Packard BioScience). The resultant TIFF images were\nanalyzed initially using MicroArray Suite (Scanalytics, Fairfax, VA).\n\nImage Processing\nFor each cDNA spot, a calibrated ratio of intensities in the Cy3 and Cy5 channels was\ncalculated and subsequently analyzed. We used Microarray Suite by Scanalytics to\ncalculate these calibrated ratios. The techniques used by Microarray Suite are described\nin Chen et al. [Chen et al., 1997]. In general, we applied the default values used by\nMicroarray Suite during our processing. A grid was placed manually on the TIFF images,\nidentifying the locations of cDNA spots. For each grid location, the pixels comprising the\nspot contained within were determined using the Mann-Whitney statistical test at 99%\n\n\f19 Lenwood S. Heath\nconfidence level. All remaining pixels comprise the background surrounding the spot.\nThe mean intensities of the spot pixels and background pixels are calculated for each\nchannel, from which a background-corrected ratio is computed. Finally, a calibrated ratio\nfor each spot is computed used the iterative method described by Chen et al. [Chen et al.,\n1997].\n\nAcknowledgments\nThis work was supported in part by National Science Foundation Award number\n9975806 to Ronald R. Sederoff entitled Genomics of Wood Formation in Loblolly Pine.\n\nBibliography\nAharoni, A., Keizer, L., Bouwmeester, H., Sun, Z., Alvarez-Huerta, M.,\nVerhoeven, H., Blaas, J., van Houwelingen, A., De Vos, R., van der Voet, H.,\nJansen, R., Guis, M., Mol, J., Davis, R., Schena, M., van Tunen, A., and\nO'Connell, A. (2000). Identification of the SAAT gene involved in strawberry\nflavor biogenesis by use of DNA microarrays. Plant Cell, 12(5):647-662.\nAlexandre, H., Ansanay-Galeote, V., Dequin, S., and Blondin, B. (2001). Global\ngene expression during short-term ethanol stress in saccharomyces cerevisiae.\nFEBS Letters, 498(1):98-103.\nAlscher, R., Chevone, B., Heath, L., and Ramakrishnan, N. (2001). Expresso: A\nproblem solving environment for bioinformatics: Finding answers with\nmicroarray technology.\nIn Tentner, A., editor, Proceedings of the High\nPerformance Computing Symposium, Advanced Simulation Technologies\nConference, pages 64-69.\nAlscher, R. G., Donahue, J. L., and Cramer, C. L. (1997). Reactive oxygen\nspecies and antioxidants: Relationships in green cells.\nPhysiol Plantarum,\n100:224-233.\nBard, J. B. (1999). A bioinformatics approach to investigating developmental\npathways in the kidney and other tissues. Int J Dev Biol, 43:397-403.\nBrachat, A., Pierrat, B., Brungger, A., and Heim, J. (2000).\nComparative\nmicroarray analysis of gene expression during apoptosis-induction by growth\nfactor deprivation or protein kinase C inhibition. Oncogene, 19(44):5073-5082.\nBrown, M. P., Grundy, W. N., Lin, D., Cristianini, N., Sugnet, C. W., Furey,\nT. S., Ares, Jr., M., and Haussler, D. (2000). Knowledge-based analysis of\nmicroarray gene expression data by using support vector machines. Proc Natl\nAcad Sci U S A, 97(1):262-7.\n\n\f20 Lenwood S. Heath\nCallis, J. and Vierstra, R. (2000). Protein degradation in signaling. Curr. Opin.\nPlant Biol., 3:381-386.\nChang, S., Puryear, J., and Cairney, J. (1993). A simple and efficient method for\nisolating RNA from pine trees. Plant Molec. Biol. Reporter, 11:113-116.\nChen, Y., Dougherty, E., and Bittner, M. (1997). Ratio-based decisions and the\nquantitative analysis of cDNA microarray images. J. Biomed Optics, 2:364-374.\nCho, R., Campbell, M., Winzeler, E., Steinmetz, L., Conway, A., Wodicka, L.,\nWolfsberg, T., Gabrielian, A., Landsman, D., Lockhart, D., and Davis, R. (1998).\nA genome-wide transcriptional analysis of the mitotic cycle. Mol Cell, 2:65-73.\nChu, S., DeRisi, J., Eisen, M., Mulholland, J., Botstein, D., Brown, P., and\nHerskowitz, I. (1998). The transcriptional program of sporulation in budding\nyeast. Science, 282:699-705.\nClaverie, J. (1999). Computational methods for the identification of differential\nand coordinated gene expression. Hum. Mol. Genet., 8:1821-1832.\nCosta, P., Bahrman, N., Frigerio, J. M., Kremer, A., and Plomion, C. (1998).\nWater-deficit-responsive proteins in maritime pine. Plant Molecular Biology,\n38(4):587-96.\nCosta, P. and Plomion, C. (1999).\nGenetic analysis of needle proteins in\nmaritime pine. 2. Variation of protein accumulation.\nSilvae Genetica,\n48(3/4):146-50.\nCushman, J. and Bohnert, H. (2000).\nGenomic approaches to plant stress\ntolerance. Curr Opin Plant Biol, 3:117-124.\nDaniels, M. and Gatsonis, C. (1999). Hierarchical generalized linear models in\nthe analysis of variations in health care utilization. J Amer Stat Assoc, 94:29-42.\nDegenhardt, B. and Gimmler, H. (2000). Cell wall adaptations to multiple\nenvironmental stresses in maize roots. J. Exp. Bot., 51:595-603.\nDong, J. and Dunstan, D. (1996). Characterization of three heat-shock protein\ngenes and their developmental regulation during somatic embryogenesis in white\nspruce [Picea Glauca (Moench) Voss]. Planta, 200:85-91.\nDzeroski, S. (1996). Inductive Logic Programming and Knowledge Discovery in\nDatabases. In Fayyad, U., Piatetsky-Shapiro, G., Smyth, P., and Uthurusamy, R.,\neditors, Advances in Knowledge Discovery and Data Mining, pages 117-152.\nAAAI Press/MIT Press.\n\n\f21 Lenwood S. Heath\nEisen, M., Spellman, P., Brown, P., and Botstein, D. (1998). Cluster analysis and\ndisplay of genome-wide expression patterns genes and proteins. Proc. Natl.\nAcad. Sci. USA, 95:14863-14868.\nEpstein, C. B. and Butow, R. A. (2000). Microarray technology -- enhanced\nversatility, persistent challenge. Curr Opin Biotechnol, 11(1):36-41.\nGallant, S. I. (1993). Neural Network Learning and Expert Systems. MIT Press,\nCambridge, MA.\nGang, O. R., Kasahara, M., Xia, Z. Q., Mijnsbrugge, K. V., Bauw, G., Boerjan,\nW., Montagu, M. V., Davin, L. B., and Lewis, N. G. (1999). Evolution of plant\ndefense mechanisms. Relationships of phenylcoumaran benzylic ether reductases\nto pinoresinol-lariciresinol and isoflavone reductases.\nJ Biol Chem,\n274(11):7516-27.\nGarofalakis, M., Hyun, D., Rastogi, R., and Shim, K. (2000).\nEfficient\nalgorithms for constructing decision trees with constraints. In Proceedings of the\nSixth ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, pages 335-9.\nGasch, A., Spellman, P., Kao, C., Carmel-Harel, O., Eisen, M., Storz, G.,\nBotstein, D., and Brown, P. (2000).\nGenomic expression programs in the\nresponse of yeast cells to environmental changes. Mol. Biol. Cell, 11:4241-4257.\nGeisler, M., Frangne, N., Gomes, E., Martinoia, E., and Palmgren, M. (2000).\nThe ACA4 gene of arabidopsis encodes a vacuolar membrance calcium pump that\nimproves salt tolerance in yeast. Plant Physiol., 124:1814-1827.\nGilchrest, B. and Bohr, V. (1997).\nFASEB J., 11:322-330.\n\nAging processes, DNA damage, and repair.\n\nGolub, T., Slonim, D., Tamayo, P., Huard, C., Gaasenbeek, M., Mesirov, J.,\nColler, H., Loh, M., Downing, J., Caligiuri, M., Bloomfield, D., and Lander, E.\n(1999). Molecular classification of cancer: Class discovery and class prediction\nby gene expression monitoring. Science, 286:531-537.\nGracey, A., Troll, J., and Somero, G. (2001). Hypoxia-induced gene expression\nprofiling in the euryoxic fish Gillichthys Mirabilis. Proc. Natl. Acad. Sci., USA,\n98(4):1993-1998.\nGreller, L. and Tobin, F. (1999). Detecting selective expression of genes and\nproteins. Genome Research, 9:282-296.\nHilsenbeck, S., Friedrichs, W., O'Connell, P., Hansen, R., Osborne, C., and\nFuqua, S. (1999). Statistical analysis of array expression data as applied to the\n\n\f22 Lenwood S. Heath\nproblem of tamoxifen resistance.\n91:453-549.\n\nJournal of the National Cancer Institute,\n\nHong, S. and Vierling, E. (2000). Mutants of arabidopsis thaliana defective in\nthe acquisition of tolerance to high temperature stress. Proc. Natl. Acad. Sci.\nUSA, 97:4392-4397.\nJain, A. and Dubes, R. (1988). Algorithms for Clustering Data. Prentice Hall,\nEnglewood Cliffs NJ.\nJelinsky, S., Estep, P., Church, G., and Samson, L. (2000). Regulatory networks\nrevealed by transcriptional profiling of damaged Saccharomyces cerevisiae cells:\nrpn4 links base repair with proteasomes. Mol. Cell Biol., 20:8157-8167.\nJordan, M. and Bishop, C. (1997). Neural networks. In Tucker, A., editor, The\nComputer Science and Engineering Handbook, pages 536-556. CRC Press.\nKannan, K., Amariglio, N., Rechavi, G., Jakob-Hirsch, J., Kela, I., Kaminski, N.,\nGetz, G., Domany, E., and Givol, D. (2001). DNA microarrays identification of\nprimary and secondary target genes regulated by p53. Oncogene, 20(18):22252234.\nKawasaki, S., Borchert, C., Deyholos, M., Wang, H., Brazille, S., Kawai, K.,\nGalbraith, D., and Bohnert, H. (2001). Gene expression profiles during the initial\nphase of salt stress in rice. Plant Cell, 13(4):889-906.\nKhan, J., Bittner, M. L., Saal, L. M., Teichmann, U., Azorsa, D. O., Gooden,\nG. C., Pavan, W. J., Trent, J. M., and Meltzer, P. S. (1999). cDNA microarrays\ndetect activation of a myogenic transcription program by the PAX3-FKHR fusion\noncogene. Proc Natl Acad Sci U S A, 96(23):13264-13269.\nLazzeroni, L. and Owen, A. (2000). Plaid models for gene expression data.\nTechnical Report TR 2000-211, Stanford University.\nLee, J., Smith, L., Tanabe, L., and Weinstein, J. (2000). Analyzing genomic and\npharmacological data in the national cancer institute's drug discovery program: A\nBayesian hierarchical effects approach using Gibbs sampling. Bioinformatics.\nsubmitted.\nLev-Yadun, S. and Sederoff, R. (2000). Pines as model gymnosperms to study\nevolution, wood formation, and perennial growth. Journal of Plant Growth\nRegulation, 19:290-305.\nMay, M. J., Vernous, T., Leaver, C., Montagu, M. V., and Inzé, D. (1998).\nGlutathione homeostatis in plants: Implications for environmental sensing and\nplant development. Journal of Experimental Botany, 49(321):649-667.\n\n\f23 Lenwood S. Heath\n\nMonni, O., Barlund, M., Mousses, S., Kononen, J., Sauter, G., Heiskanen, M.,\nPaavola, P., Avela, K., Chen, Y., Bittner, M. L., and Kallioniemi, A. (2001).\nComprehensive copy number and gene expression profiling of the 17q23\namplicon in human breast cancer. Proc Natl Acad Sci U S A, 98(10):5711-6.\nMuggleton, S. (1999). Scientific knowledge discovery using inductive logic\nprogramming. Communications of the ACM, 42(11):42-46.\nMuggleton, S. and Feng, C. (1990). Efficient induction of logic programs. In\nArikawa, S., Goto, S., Ohsuga, S., and Yokomori, T., editors, Proceedings of the\nFirst International Conference on Algorithmic Learning Theory, pages 368-381.\nJapanese Society for Artificial Intelligence, Tokyo.\nMullineaux, P., Ball, L., Escobar, C., Karpinska, B., Creissen, G., and Karpinski,\nS. (2000).\nAre diverse signalling pathways integrated in the regulation of\narabidopsis antioxidant defence gene expression in response to excess excitation\nenergy? Philos Trans R Soc Lond B Biol Sci, 355(1402):1531-40.\nNCSU Forest Biotechnology Group (2001).\nProject.\n\nWeb site hosts NSF Pine Genome\nURL:\nhttp://www2.ncsu.edu/unity/lockers/project/forestbiotech/.\nPerou, C. M., Jeffrey, S. S., van de Rijn, M., Rees, C. A., Eisen, M. B., Ross,\nD. T., Pergamenschikov, A., Williams, C. F., Zhu, S. X., Lee, J. C., Lashkari, D.,\nShalon, D., Brown, P. O., and Botstein, D. (1999). Distinctive gene expression\npatterns in human mammary epithelial cells and breast cancers. Proc Natl Acad\nSci USA, 96:9212-9217.\nReymond, P., Weber, H., Damond, M., and Farmer, E. (2000). Differential gene\nexpression in response to mechanical wounding and insect feeding in arabidopsis.\nPlant Cell, 12(5):707-720.\nRial, D., Arakaki, A., and Ceccarelli, E. (2000). Interaction of the targeting\nsequence of chloroplast precursors with HSP70 molecular chaperones. Eur. J.\nBiochem., 267:6239-6248.\nRissanen, J. (1978). Modeling by shortest data description. Automatica, 14:46571.\nRuan, Y., Gilmore, J., and Conner, T. (1998). Towards arabidopsis genome\nanalysis: Monitoring expression profiles of 1400 genes using cDNA microarrays.\nPlant J., 15(2):821-833.\nScandalios (ed.), J. (1997).\nOxidative Stress and the Molecular Biology of\nAntioxidant Defenses. Cold Spring Harbor, Plainview.\n\n\f24 Lenwood S. Heath\n\nSchaffer, R., Landgraf, J., Perez-Amador, M., and Wisman, E. (2000).\nMonitoring genome-wide expression in plants.\nCurrent Opinion in\nBiotechnology, 11:162-167.\nSchnaider, T., Oikarinen, J., Ishiwatari-Hayasaka, H., Yahara, I., and Csermely, P.\n(1999). Interactions of HSP90 with histones and related peptides. Life Sci.,\n65:2417-2426.\nSeki, M., Narusaka, M., Abe, H., Kasuga, M., Yamaguchi-Shinozaki, K.,\nCarninci, P., Hayashizaki, Y., and Shinozaki, K. (2001).\nMonitoring the\nexpression pattern of 1300 arabidopsis genes under drought and cold stresses by\nusing a full-length cDNA microarray. Plant Cell, 13(1):61-72.\nSherlock, G. (2000). Analysis of large-scale gene expression data.\nOpinion in Immunology, 12(2):201-205.\n\nCurrent\n\nShinozaki, K. and Yamaguchi-Shinozaki, K. (1997). Gene expression and signal\ntransduction in water-stress response. Plant Physiol., 115:327-334.\nShinozaki, K. and Yamaguchi-Shinozaki, K. (2000). Molecular responses to\ndehydration and low temperature: Differences and cross-talk between two stress\nsignaling pathways. Curr. Opin. Plant Biol., 3:217-223.\nSmyth, P. (1996). Clustering using Monte-Carlo cross-validation.\nInt. Conf. on Knowledge Discovery and Data Mining.\n\nProc. 2nd\n\nSomerville, C. and Somerville, S. (1999). Plant functional genomics. Science,\n285:380-383.\nSrinivasan,\n\nA.\n\n(2001).\n\nThe\n\nAleph\n\nManual.\n\nURL:\n\nhttp://oldwww.comlab.ox.ac.uk/oucl/groups/machlearn/Aleph/aleph_t\noc.html.\n\nSullivan, T. (2001). Locating question difficulty through explorations in question\nspace. In Proceedings of the First ACM/IEEE-CS Joint Conference on Digital\nLibraries, pages 251-2.\nUno, Y., Furihata, T., Abe, H., Yoshida, R., Shinozaki, K., and YamaguchiShinozaki, K. (2000). Arabidopsis basic leucine zipper transcription factors\ninvolved in an abscisic acid-dependent signal transduction pathway under drought\nand high-salinity conditions. Proc. Natl. Acad. Sci. USA, 97(21):11632-11637.\nVapnik, V. N. (1998). Statistical Learning Theory.\nNew York. A Wiley-Interscience Publication.\n\nJohn Wiley & Sons Inc.,\n\n\f25 Lenwood S. Heath\nWang, A., Pierce, A., Judson-Kremer, K., Gaddis, S., Aldaz, C. M., Johnson,\nD. G., and MacLeod, M. C. (1999). Rapid analysis of gene expression (RAGE)\nfacilitates universal expression profiling. Nucleic Acids Res, 27(23):4609-18.\nWang, R., Guegler, K., LaBrie, S., and Crawford, N. (2000). Genomic analysis\nof a nutrient response in arabidopsis reveals diverse expression patterns and novel\nmetabolic and potential regulatory genes induced by nitrate.\nPlant Cell,\n12(8):1491-1509.\nWhite, J. A., Todd, J., Newman, T., Focks, N., Girke, T., de Ilarduya, O. M.,\nJaworski, J. G., Ohlrogge, J. B., and Benning, C. (2000).\nA new set of\narabidopsis expressed sequence tags from developing seeds. The metabolic\npathway from carbohydrates to seed oil. Plant Physiol, 124(4):1582-94.\nWu, Y. and Cosgrove, D. (2000). Adaptation of roots to low water potentials by\nchanges in cell wall extensibility and cell wall proteins. J. Exp. Bot., 51:15431553.\nYang, G. P., Ross, D. T., Kuang, W. W., Brown, P. O., and Weigel, R. J. (1999).\nCombining SSH and cDNA microarrays for rapid identification of differentially\nexpressed genes. Nucleic Acids Res, 27(6):1517-1523.\nYeung, K., Fraley, C., Murua, A., Raftery, A., and Ruzzo, W. (2001). Modelbased clustering and data transformations for gene expression data. Technical\nReport TR 2001-396, Statistics Department, University of Washington.\n40\npages.\nZhu, B., Choi, D., Fenton, R., and Close, T. (2000). Expression of the barley\ndehydrin multigene family and the development of freezing tolerance. Mol. Gen.\nGenetics, 264:145-153.\n\n\f26 Lenwood S. Heath\n\nFigure 1: Execution and flows in Expresso. The solid lines indicate computational flows;\nthe dashed lines indicate realizations of the pipeline in devices such as robots and image\nreaders.\n\nFigure 2: A Scenario for Specific and General Stress Responses in Plant Cells. Upon\nimposition of oxidative stress, ROS levels increase, and cellular redox-sensing\nmechanisms are activated. General downstream events include the activation of ROS\ndetoxification mechanisms, such as the ascorbate glutathione scavenging cycle. Events\nspecific to individual stresses include the activation of aquaporins in response to drought\nstress, and the activation of isoflavone reductase-like genes in response to pathogen\ninvasion or UV-irradiation.\n\n\f27 Lenwood S. Heath\n\nEXPERIMENT PINE_DROUGHT_GROWTH May-August,2000 \"384 clones\"\n...\nDYE CY3 \"Genisphere Kit\"\nDYE CY5 \"Genisphere Kit\"\n...\nPRINTING_ROBOT NCSU_FBC \"Brown-type robot at NCSU\"\n...\nPRINTING_CONFIGURATION Stanford4x16x24 4 16 24 QUADRANTS\nPRINTING_CONFIGURATION Stanford4x22x24 4 22 24 QUADRANTS\n...\nTISSUE D4M\nD4\nNeedles Unstressed (Control)\nTISSUE D4I\nD4\nNeedles Intermediate Stressed\n...\n\nFigure 3: Example description of a microarray experiment in Expresso which includes\nthe name of the experiment, the dyes used, description of the printing robot, printing\nformats, and the tissue configurations. The lines are in a self-describing format where the\nstart field identifies the nature of information modeled.\n\nFigure 4: A dendrogram obtained using Expresso, ClustalW, and njplot showing the\nrelationship of known HSP proteins of cDNAs among the EST stress set. ESTs are\nidentified on the dendrogram by their origin and the number assigned to them in the\nmicroarray project. The corresponding Arabidopsis HSP sequence is included in each\ncase.\n\n\f28 Lenwood S. Heath\n\nlevel\nCloneID Comparison Expression\n…\n…\n…\nCvsM\n4\npositive\nCvsM\n5\npositive\nCvsM\n20\nnegative\n…\n…\n…\nCvsS\n7\npositive\nCvsS\n8\nnegative\n\ncategory\nCloneID Category Name\n…\n…\n5\nRPPP\n5\nCarbon Metabolism\n7\nThiol-Utilizing Enzymes\n8\nHeat\n20\nDrought Stress Responsive\n…\n…\n\ncategory(X,Environment) :- category(X,Heat).\ncategory(X,Carbon Metabolism) :category(X,RPPP).\n\nFigure 5: Input database design for inductive logic programming (ILP). The level table\ncontains information about the expression levels of individual clones, for all\ncomparisons. It constitutes the positive examples. The category table records available\nfunctional classifications of all clones. Background knowledge consists of category\ncontainment relations, e.g., ``any clone that is classified under the `heat' category also\nbelongs to the `environment' category.'' The negative examples are not shown. RPPP is\nthe reductive pentose phosphate pathway.\n\n\f29 Lenwood S. Heath\n\nFigure 6: Categories and Groupings in Development and Metabolism.\n\n\f30 Lenwood S. Heath\n\nFigure 7: Categories and Groupings within Protective Processes.\n\n\f31 Lenwood S. Heath\n\nFigure 8: Design of Microarrays in Expresso. Slides containing the selected 384 cDNAs\nwere printed as indicated above.\nTable 1: Heat Shock Protein Responses Among the EST Stress Set in Genotypes C and\nD. Control versus Mild Drought Stress Comparison.\nHSP Type\nHSP 23\n\nHSP80\n\nHSP70\n\nHSP100\n\nEST Origin\n\nGenotype C Genotype D\n\nXylem #8\n\n+\n\n0\n\nShoot Tip #228\n\n+\n\n+\n\nShoot Tip #213\n\n0\n\n0\n\nShoot Tip #229\n\n0\n\n0\n\nShoot Tip #206\n\n+\n\n-\n\nXylem #67\n\n0\n\n+\n\nShoot Tip #227\n\n0\n\n0\n\nXylem #296\n\n+\n\n+\n\nShoot Tip #226\n\n0\n\n+\n\nXylem #64\n\n+\n\n+\n\n\f"
        ],
        [
         "30",
         "30",
         "cs.CE",
         "Computational Engineering",
         "0701167v1.pdf",
         "Large-Scale Query and XMatch, Entering the Parallel Zone\nMaría A. Nieto-Santisteban,\nAniruddha R. Thakar\nAlexander S. Szalay\nJohns Hopkins University\nJim Gray\nMicrosoft Research\n\nDecember 2005\nTechnical Report\nMSR-TR-2005- 169\n\nMicrosoft Research\nMicrosoft Corporation\nOne Microsoft Way\nRedmond, WA 98052\n\nA version of this paper was presented at the Astronomical Data Analysis Software and Systems XV in San Lorenzo de\nEl Escorial, Madrid, Spain, October 2005, to appear in the ASP Conference Series.\n\n1\n\n\fLarge-Scale Query and XMatch, Entering the Parallel Zone\nMaría A. Nieto-Santisteban, Aniruddha R. Thakar, Alexander S. Szalay\nJohns Hopkins University\nJim Gray\nMicrosoft Research\nAbstract. Current and future astronomical surveys are producing catalogs with millions and billions of objects. On-line\naccess to such big datasets for data mining and cross-correlation is usually as highly desired as unfeasible. Providing\nthese capabilities is becoming critical for the Virtual Observatory framework. In this paper we present various\nperformance tests that show how using Relational Database Management Systems (RDBMS) and a Zoning algorithm\nto partition and parallelize the computation, we can facilitate large-scale query and cross-match.\n\n1. Introduction\nThe primary goal of the Virtual Observatory is to create the infrastructure necessary to make distributed digital archives\naccessible and interoperable in such a way that astronomers can maximize their potential for scientific discovery by\nquerying and cross-matching multi-wavelength data between multiple archives on-the-fly. While small-area (few\narcminutes or few thousand objects) searches are possible at present using tools like Open SkyQuery1, large-scale\nrequests involving all or a large fraction of the sky cannot be performed on demand due to system and network\nlimitations.\n\n2. Partitioning and Parallelization\nThe logical approach for speeding up access to data in large (multi-TB) datasets is to apply\npartitioning and parallelism within individual data services (SkyNodes). In this way, queries\nlooking at different parts of the sky can be distributed among servers. Queries covering wide sky\nareas or full scans can be executed by different servers in parallel. Using the Zoning algorithm that\nwe have developed, we can parallelize query and cross-match computations by distributing the data\nand workload among a cluster of database servers.\n2.1.\n\nHigh-Speed Access Using Zones\n\nThe concept behind the Zoning algorithm is to map the celestial sphere into stripes of certain height called Zones. Each\nobject at position (ra, dec) is assigned into a zone by using the fairly simple formula ZoneID = floor ((dec + 90) / h),\nwhere h is the zone height.\nZoning the data has two main advantages when working with databases. First, the data and computation workload\npartition very easily by assigning different sets of zones to different servers and then executing the queries in parallel.\nWhen using Zoning to distribute the data, the height parameter, h, can be initially any value. After some tests, we chose\nh = 4 arcminutes for our experiments which gives very good data distribution and computational performance. Second,\nwhen the database is relational, using zones helps to speed up neighborhood searches as explained in detail in (Gray et\nal. 2004). Once the objects have been assigned a ZoneID, we take advantage of the RDBMS high efficient indexing\ncapabilities and build an index on ZoneID. This allows us to use pure relational algebra expressed in SQL to speed up\nsimple queries like Cone Searches, or more sophisticated as those involved in finding clusters.\n\n1\n\nhttp://www.openskyquery.net/\n\n1\n\n\f3. Test Case: SDSS, 2MASS, and USNO\nWe have used a vertical partition (subset of attributes) of the Sloan Digital Sky Survey Data Release 3 (SDSS DR3), the\nTwo Micron All Sky Survey (2MASS), and the United States Naval Observatory B (USNOB) catalogs to run large-scale\naccess and cross-match queries. SDSS DR3 contains about 142 million objects covering a non-contiguous area of 5,282\nsquared degrees. In order to simplify our cross-match experiments, we used most of the 2MASS (28,445,694 objects)\nand USNO (117,698,363 objects) area that overlaps with SDSS DR3 (Figure 1-left). The next task was to distribute the\nworkload in a homogeneous way. This step implied to zone the data and assign zone subsets to different servers in the\ncluster (Figure 1-right). At this point, the databases were replicated across the cluster.\nThe most efficient approach to query a single catalog is to distribute the workload homogenously so all nodes process\nthe same amount of data. But cross-match queries require choosing a catalog leading the partitioning. What is the best\npartitioning choice is not always as simple as it might seem. For example, a reasonable choice would be to make the\ncatalog with the smallest number of objects lead the partitioning. The assumption is that minimizing the size of the\ndataset works best because we reduce the number of operations on each server. This would be 2MASS in our test case.\nLooking at Figure 1, we can see that making 2MASS the leading partitioning catalog to do a cross-match with SDSS\nwould be a terrible choice. While certain servers would be overloaded, others would basically remain idle. How to\nautomatically decide what is the best distribution approach is not easy, if not impossible, without having precise\ninformation about the catalog densities and coverage footprints.\n\nFigure 1. Left: SDSS DR3, 2MASS and USNOB test-case footprints. Right: Zone distribution of SDSS DR3 data.\n\n4. Performance\n4.1.\n\nLarge-Scale Query\n\nThe query below requires a full table scan. It screens 142 million objects and retrieves only those meeting the\nmagnitude filter. There is no index on the magnitude attribute.\nSELECT p.objid, p.ModelMagR\nINTO Results\nFROM SDSSDR3:PhotoPrimary p\nWHERE p.ModelMagR BETWEEN 9.0 AND 10.0\nAccording to Table 1, this query would take 11 minutes to be executed by a single server but 2 to 3 minutes when\nexecuted in parallel. While the latter is still far from being ideal, many users may be willing to wait for 3 minutes but\nvery few will ever wait for 11 minutes in front of their browsers. It is important to note the big difference between\nelapsed and CPU time. This value shows that even though we can speed up the response time by incrementing the\nnumber of servers, the main reason for the “slow” response remains in the I/O bottleneck.\n\n2\n\n\fTable 1. Full Scan Performance\n\n4.2.\n\nServers\n\nElapsed (s)\n\nCPU (s)\n\nI/O (MB)\n\n1\n8\n\n682\n147\n113\n\n36\n15\n7\n\n881\n184\n168\n\nMAX\nAVG\n\nLarge-Scale Cross-Match\n\nWe run the same cross-match procedures between SDSS DR3 and 2MASS using 1, 2, 4, and 8 servers to measure\nscalability. Figure 2 shows that scalability is certainly possible in terms of CPU time. This is directly related to a good\nworkload distribution. On the center, the initial high slope for the one and two-server cases is due to I/O as right graph\nshows. However, the basically flat line plotting total elapsed time indicates that by using four or more servers we can as\nwell speed up the cross-match computation linearly. It is worth noticing the small difference between Total CPU and\nTotal Elapse that indicates good use of the CPU.\n\nFigure 2 SDSS vs 2MASS Cross-Match Performance.\n\n5. Conclusions and Future Work\nThe tests and results presented here demonstrate that by zoning, partitioning and parallelizing the workload and using\nRDBMS technologies, we can reduce linearly the response time. However, the large-scale query and cross-match\nproblem in the VO framework is still far from being completely resolved. In order to implement an interoperable\nsystem capable of managing the large-scale, the IVOA still needs to affirm protocols like the Asynchronous Activities\nwhich will provide a standard way to manage long jobs. The VOSpace/VOStore protocols and services are necessary so\nwe can have locations where to put large query results, and means to share them with others. Authentication and\nAuthorization mechanisms are essential to track who can access what and know what belong to whom. These protocols\nare under development and they will be implemented into our systems as they become recommendations.\n\nReferences\n\nGray, J., Szalay, A. S., Thakar, A. R., Fekete, G., Nieto-Santisteban, M. A., O’Mullane, W. J., Heber, G., & Rots, A. H.\n2004, Microsoft Technical Report MSR-TR-2004-32. ftp://ftp.research.microsoft.com/pub/tr/TR-2004-32.pdf\n\n3\n\n\f"
        ],
        [
         "31",
         "31",
         "cs.CE",
         "Computational Engineering",
         "1307.7820v1.pdf",
         "Faster Algorithms for RNA-folding using the\nFour-Russians method\nBalaji Venkatachalam, Dan Gusfield, and Yelena Frid\n\narXiv:1307.7820v1 [q-bio.QM] 30 Jul 2013\n\nDepartment of Computer Science, UC Davis\n{balaji, gusfield, yafrid}@cs.ucdavis.edu\n\nAbstract. The secondary structure that maximizes the number of noncrossing matchings between complimentary bases of an RNA sequence of\nlength n can be computed in O(n3 ) time using Nussinov’s dynamic programming algorithm. The Four-Russians method is a technique that will\nreduce the running time for certain dynamic programming algorithms\nby a multiplicative factor after a preprocessing step where solutions to\nall smaller subproblems of a fixed size are exhaustively enumerated and\nn3\nsolved. Frid and Gusfield designed an O( log\n) algorithm for RNA folding\nn\nusing the Four-Russians technique. In their algorithm the preprocessing\nis interleaved with the algorithm computation.\nWe simplify the algorithm and the analysis by doing the preprocessing\nonce prior to the algorithm computation. We call this the two-vector\nmethod. We also show variants where instead of exhaustive preprocessing, we only solve the subproblems encountered in the main algorithm\nonce and memoize the results. We give a simple proof of correctness and\nexplore the practical advantages over the earlier method.\nThe Nussinov algorithm admits an O(n2 ) time parallel algorithm. We\nshow a parallel algorithm using the two-vector idea that improves the\nn2\ntime bound to O( log\n).\nn\nWe have implemented the parallel algorithm on graphics processing units\nusing the CUDA platform. We discuss the organization of the data structures to exploit coalesced memory access for fast running times. The ideas\nto organize the data structures also help in improving the running time\nof the serial algorithms. For sequences of length up to 6000 bases the\nparallel algorithm takes only about 2.5 seconds and the two-vector serial\nmethod takes about 57 seconds on a desktop and 15 seconds on a server.\nAmong the serial algorithms, the two-vector and memoized versions are\nfaster than the Frid-Gusfield algorithm by a factor of 3, and are faster\nthan Nussinov by up to a factor of 20.\n\n1\n\nIntroduction\n\nComputational approaches to find the secondary structure of RNA molecules are\nused extensively in bioinformatics applications. The classic dynamic programming (DP) algorithm proposed in the 1970s has been central to most structure\nprediction algorithms. While the objective of the original algorithm was to maximize the number of non-crossing pairings between complementary bases, the\n\n\fdynamic programming approach has been used for other models and approaches,\nincluding minimizing the free energy of a structure. The DP algorithm runs in\ncubic time and there have been many attempts at improving its running time.\nHere, we use the Four-Russians method for speeding up the computation.\nThe Four-Russians method, named after Aralazarov et al. [4], is a method to\nspeed up certain dynamic programming algorithms. In a typical Four-Russians\nalgorithm there is a preprocessing step that exhaustively enumerates and solves\na set of subproblems and the results are tabled. In the main DP algorithm,\ninstead of filling out or inspecting individual cells, the algorithm takes longer\nstrides in the table. The computation for multiple cells is solved in constant\ntime by utilizing the preprocessed solutions to the subproblems. The longer\nstrides to fill the table reduce the runtime by a multiplicative factor. The size\nof the subproblems is chosen in a way that does not make the preprocessing too\nexpensive.\nFrid and Gusfield [11] showed the application of the Four-Russians approach\nfor RNA folding. In their algorithm, the preprocessing is interleaved with the\nalgorithm computation. They fill out a part of the DP table and use these entries\nto complete a part of the preprocessing. The preprocessed entries are used later\nin the computation.\nWe show a simpler algorithm where all the preprocessing is completed before\nthe start of the main algorithm. This simplifies the correctness proof and the\nruntime analysis. This approach helps in obtaining a log n factor improvement\nfor the parallel algorithm. In comparing various methods for RNA folding, Zakov\nand Frid (personal communication) had independently observed that the algorithm in [11] could be modified to do the preprocessing at once. It is essentially\nthe idea as described here.\nIn this paper we explore the implications of the one-pass preprocessing idea.\nThis description of the algorithm leads naturally to two other variants. We empirically evaluate these variants and also the implementation of the parallel algorithm.\nThe parallel architecture of general-purpose graphical processing units (GPUs)\nhave been exploited for many real-world application in addition to applications\nin gaming and visualization problems. GPUs have also been used to speed up\nRNA folding algorithms [6,23,24]. Here we show how the Four-Russians method\nallows an organization of the data structures for fast memory accesses. We also\ndescribe the organization of the parallel hierarchy to exploit the inherent parallelism of the solution.\nIn the rest of the section, we describe the problem in relation to the other\nproblems in RNA folding. To keep the paper self-contained, we will first describe the two-vector algorithm, our application of the Four-Russians method to\nthe RNA folding problem. We will use that description to describe the original\nFour-Russians method for RNA folding by Frid and Gusfield [11]. This discussion leads to two other variants where the preprocessing is done on demand,\ninstead of the exhaustive preprocessing in the two-vector method and the FridGusfield algorithm. In section 4 we discuss the O(n2 / log n) parallel algorithm.\n\n\fWe will then describe the implementation of a parallel algorithm using CUDA.\nThe final sections have discussion on empirical observations and conclusions. Due\nto space limitations, this manuscript focuses mostly on the theoretical aspects\nand describes the experimental results briefly. Detailed discussion can be found\nin [26].\nRelated work The O(n3 ) dynamic programming algorithm due to Nussinov et\nal. [21,20] maximizes the number of non-crossing matching complimentary bases.\nThere have been many methods since Zuker and Stiegler [31] that infer the\nfolding using thermodynamic parameters [25,19] which are more realistic than\nmaximizing the number of base pairs. These methods have been implemented in\nmany packages including UNAFold [18], Mfold [30], Vienna RNA Package [15],\nRNAstructure [22].\nProbabilistic methods include stochastic context-free grammars [10,9], the\nmaximum expected accuracy (MEA) method, where secondary structures are\ncomposed of pairs that have a maximal sum of pairing probabilities, eg., MaxExpect [17], Pfold [16], CONTRAfold [8] which maximize the posterior probabilities of base pairs; and Sfold [7], CentroidFold [14] that maximize the centroid\nestimator. There are also other methods that use a combination of thermodynamic and statistical parameters [2] and methods that use training sets of known\nfolds to determine their parameters, eg., CONTRAfold [8], and Simfold[3] and\nContextFold[28].\nIn addition to the Four-Russians method, other methods to improve the\nrunning time include Valiant’s max-plus matrix multiplication by Akutsu [1]\nand Zakov et al. [29]; and sparsification, where the branch points are pruned to\nget an improved time bound [27,5].\nCUDA, the programming platform for GPGPUs, has been used to solve\nmany bioinformatics problems. Chang, Kimmer and Ouyang [6] and Stojanovski,\nGjorgjevikj and Madjarov [24] show an implementation of the Nussinov algorithm on CUDA. Rizk et al. [23] describe the implementation for Zuker and\nStiegler method involving energy parameters. These methods are discussed in\nsection 5.2.\n\n2\n\nThe Nussinov Algorithm\n\nIn this paper, we consider the basic RNA folding problem of maximizing the number of non-crossing complimentary base pair matchings. Complimentary bases\ncan be paired, i.e., A with U and C with G. A set of disjoint pairs is a matching. The pairs in a matching must not cross, i.e., if bases in positions i and\nj are paired and if bases k and l are paired, then either they are nested, i.e.,\ni < k < l < j or they are non-intersecting, i.e., i < j < k < l. The objective is\nto maximize the number of pairings under these constraints.\nThe following algorithm, due to Nussinov [21] maximizes the number of noncrossing matchings. For an input sequence S of length n over the alphabet A,\nC, G, U, the recurrence is defined as follows. Let D(i, j) denote the optimal cost\n\n\fof folding for the subsequence from i to j. For all i, D(i, i − 1) = D(i, i) = 0 and\nfor all i < j:\n\u001a\nb(S(i), S(j)) + D(i + 1, j − 1)\nD(i, j) = max\n(1)\nmaxi+1≤k≤j D(i, k − 1) + D(k, j)\nwhere b(., .) = 1 for complimentary bases and 0 otherwise. The DP table is the\nupper triangular part of the n × n matrix. The optimal solution is given by\nD(1, n). The table can be filled column-wise from the first column till the nth .\nThere are other ways of filling the table too, eg., along the diagonals — the (i, i)diagonal first, (i, i + 1)-diagonal next and so on, until the last diagonal with one\nentry, D(1, n). To allow for traceback we need to store the bases that are paired\nto get the maximum value. Let D∗ (i, j) denote the corresponding indices. These\nare obtained by substituting arg max in place of max in the above recurrence\nand can be computed along with the max value.\nThe first part of the recurrence can be solved in constant time. The second\npart is more expensive, incurring Θ(n) look ups and maximum computations.\nThere are O(n2 ) entries in the DP table and each cell can be computed in O(n)\ntime, giving an O(n3 ) time algorithm.\n\n3\n\nThe Four-Russians Algorithms\n\nIn this section we discuss three variants of the Four-Russians algorithm. We will\nfirst describe the two-vector approach. Since it is simpler than the other methods\nwe will use the description to discuss two other variants.\n3.1\n\nTwo-vector algorithm\n\nTo apply the Four-Russians technique we start with the following observation:\nLemma 1. The values along a column from bottom to top and along a row from\nleft to right are monotonically non-decreasing. Consecutive cells differ at most\nby 1.\nProof. Consider neighboring cells (i, j) and (i + 1, j). D(i, j) represents the solution of a longer sequence than D(i + 1, j). Therefore the former value should be\nat least as large as the latter. Suppose D(i, j) differed from D(i + 1, j) by more\nthan one. Then we can remove any matching for i. This has at most one fewer\nbase pair matching and is a valid solution for the subsequence (i + 1, j) with a\nlarger value than its current value, contradicting the optimality of D(i + 1, j).\nAn analogous argument holds along the columns.\nOnce the cells D(i, l), D(i, l+1), . . . , D(i, l+q−1) are computed, for some l ∈\n{i, . . . , j − q}, they can be represented by D(i, l) + V0 , D(i, l) + V1 , . . . , D(i, l) +\nVq−1 , where Vp = D(i, l + p) − D(i, l), for p ∈ {0, . . . , q − 1}. Let us define, v0 = 0\nand vp = Vp − Vp−1 , for p ∈ {1, . . . , q − 1}. From lemma 1, vp ∈ {0, 1}, for all\n\n\fp ∈ [0, q − 1]. Let v denote the binary vector v0 , v1 , . . . , vq−1 of differences and\nlet V denote the vector of running totals V0 , V1 , . . . , Vq−1 .\nSince the vp ’s are defined from Vp ’s, the inverse function is well defined:\nPi\nVp = k=0 vk . Thus D(i, l) together with the vector v represents q consecutive\ncells of the table.\nSimilarly, since the values are non-increasing down a column, D(i + l +\n1, j), . . . , D(i + l + q, j) be represented by the pair D(i + l + 1, j), v̄, where\nv̄ ∈ {0, −1}q . We call v the horizontal difference vector or the horizontal vector\nand we call v̄ the vertical difference vector or the column vector. The corresponding vector of sums is denoted V̄ .\nConsider q consecutive cells from l + 1 to l + q used in computing D(i, j):\nD(i, j) ←\n←\n\nmax\n\nl+1≤k≤l+q\n\nmax\n\n0≤k≤q−1\n\nD(i, k − 1) + D(k, j)\n\n(2)\n\nD(i, l) + Vk + D(i + l + 1, j) + V̄k\n\n← D(i, l) + D(i + l + 1, j) +\n\nmax\n\n0≤k≤q−1\n\nVk + V̄k\n\n(3)\n\nAs before, we use arg max in place of max to obtain D∗ (i, j), which facilitates\nthe traceback.\nAs noted above the second line of the recurrence (1), looping over elements,\nis more expensive and we will use (3) instead of (2) to compute the D and D∗\nvalues in the Four-Russians method. That is, we will use (3) for groups of q cells\neach instead of one loop of (1). Since the V vectors are in bijection with the v\nvectors, we will do the preprocessing using v. Let v and v̄ be the corresponding\nvectors in (3). The following algorithm evaluates the max computation.\nInput: horizontal difference vector v and vertical difference vector v̄\n1: max-val ← 0 and max-index ← 0\n2: sum1 ← 0 and sum2 ← 0\n3: for k = 0 to q − 1 do\n4:\nsum1 ← sum1 + vi\n5:\nsum2 ← sum2 + v̄i\n6:\nif sum1 + sum2 > max-val then\n7:\nmax-val ← sum1 + sum2\n8:\nmax-index ← k\n9:\nend if\n10: end for\n11: return (max-val, max-index)\nUsing this instead of (2) is not advantageous in itself. However, if this algorithm is given as a black box, D(i, j) can be computed in constant time by\ninvoking the black box once. In the preprocessing stage, we will run the above\nalgorithm for all possible vector pairs of length q and store the results in table\nR. Table R is indexed by a pair of numbers in the range [2q ] to represent the two\nvectors (v, v̄). Since there are two entries in the table, the lookup is a constant\ntime operation. We will show later that this exhaustive enumeration is not too\nexpensive.\n\n\fIn the Nussinov algorithm described in the previous section, the recurrence\nis evaluated using (2) and it takes O(q) time. In the Four-Russians method,\nusing the preprocessing step, the max computation is available through a table\nlookup and the recurrence for q terms can be completed in constant time. This\nreduction in the computation time is the reason for the speedup by a factor of\nq.\na\n\ni\n\nb\n\nc\n\nj\n\n...\na\n\n..\n.\n\n..\n.\n\n...\n\n...\nb\n\n..\n.\nc\n\nFig. 1: A diagrammatic representation of the two-vector method. The row and\ncolumn blocks are matched as labelled. The gray boxes and the gray dashes show\nthe initial value and difference vectors. The group of cells in b correspond to the\nFour-Russians loop in lines 15–19 of Algorithm 1; the cells in a are used in the\nloop in lines 9–11 and the cells in c form the loop in lines 12–14.\n\nThe two-vector method modifies the Nussinov algorithm as follows. All the\nrows and columns of the table are grouped into groups of q cells each. The\nrecurrence over these q cells is computed in constant time using the preprocessing\ntable. The recurrence involves D(i, k − 1) + D(k, j), i.e., the value in the (k − 1)st\ncolumn is used with the k th row. Therefore the row and column groupings differ\nby one. That is, the columns are grouped (0, 1, . . . , q − 1), (q, q + 1, . . . , 2q − 1)\netc. The rows are grouped (1, 2, . . . , q), (q + 1, q + 2, . . . , 2q) etc. This ensures\nthat the row and column groups are well characterized. That is, to fill the cell\n(i, j), the k th group along row i needs to be combined with the k th group below\n(i, j) in column j.\nThe cells of the table are filled in the same order as before. When the last\ncell of a row- or a column- group is evaluated the corresponding row and column\nvectors are computed and stored. To fill cell (i, j), we retrieve the first element\nand the horizontal vector of the group from row i and the first element and\nthe column vector from the corresponding group in column j. The recurrence is\nsolved using (3) by a table lookup. The final value for D(i, j) is the maximum\n\n\fAlgorithm 1 Procedure for the two-vector Four-Russians speedup. The DP\ntable is filled column-wise.\n1: R ← preprocess all pairs of vectors of length q\n2: for j = 1 to n do\n3:\nD(j, j) ← 0\n4:\nfor i = j − 1 down to 1 do\n5:\nD(i, j) ← b(S[i], S[j]) + D(i + 1, j − 1)\n6:\nLet (i, i) be in the I th group in row i.\n7:\nLet (i, j) be in the J th group horizontally in the ith row and J 0 th group\nvertically in the j th column.\n8:\nLet iq be the right-most entry of group I and jq be the left-most entry\nin group J\n9:\nfor k = i + 1 to iq do\n// For all cells in the first group\n10:\nD(i, j) ← max(D(i, j), D(i, k − 1) + D(k, j))\n11:\nend for\n12:\nfor k = jq to j do\n// For all cells in the last group\n13:\nD(i, j) ← max(D(i, j), D(i, k − 1) + D(k, j))\n14:\nend for\n15:\nfor k = 1 to J − I do\n// For all groups in between\n16:\nLet p be the left-most cell in the kth group to the right of I and q\nbe the top-most cell in the kth group below J 0 .\n17:\nLet vp and vq be the corresponding horizontal and vertical difference\nvectors.\n18:\nD(i, j) ← max(D(i, j), D(i, p) + D(q, j) + R(vp , vq ))\n19:\nend for\n20:\nif i mod q = 0 then\n// compute the vertical difference vector\n21:\ncompute and store the v vector i/q th group for column j\n22:\nend if\n23:\nif j mod q = q − 1 then\n// compute the horizontal difference\nvector\n24:\ncompute and store the v vector (j − 1)/q th group for row i\n25:\nend if\n26:\nend for\n27: end for\n\nvalue over all the groups. There might be residual elements in the row that do\nnot fall in these groups. There are at most 2q such elements. These are solved\nseparately using Nussinov’s method. Algorithm 1 has the algorithm listing and\nFigure 1 describes the algorithm pictorially.\nRuntime Analysis. In the precomputation phase, there are 2q q-length vectors\nand 22q pairs of vectors. The precomputation takes O(q) time per vector pair.\nThus the total time for precomputation is O(q22q ).\nThe main algorithm: There are O(n2 ) cells and to fill each cell it takes\nO(n/q + q) time. That is, it takes O(n/q) time to look up the initial value\nand the difference vector and the R table lookups for the the O(n/q) groups. It\ntakes O(q) time for the residual elements. Thus it takes O(n2 × (n/q + q)) time\n\n\fto fill the table. Every cell is involved in at most two vector computations, where\nthe difference to its neighbor is computed once for the row and for the column\nvector. This takes an amortized O(n2 ) time which is dominated by the rest of\nthe algorithm.\nWhen q = log n, the total time for the entire algorithm is O(log n 22 log n +\n2\nn + n2 × ( logn n + log n)) = O(n2 log n + n3 / log n) = O(n3 /logn).\n3.2\n\nOther Variants\n\nFG Algorithm. Frid and Gusfield [11] first showed how the Four-Russians approach could be applied to the RNA-folding problem. We will call their algorithm the FG algorithm. FG and two-vector algorithms are variants of the same\nidea. We will highlight the differences in preprocessing and the maximum value\ncomputation by the Four-Russians technique. In particular, we will show the\nmaximum computation in step 18 of Algorithm 1.\nAfter computing the q-contiguous cells of a group in a row, the value in the\ninitial cell D(i, p) and the horizontal difference vector vp are known. They run\nthe preprocessing algorithm in page 5 for this fixed vp vector together with all\npossible vertical difference vectors. They add the value of D(i, p) to the maximum\nand table the result. This preprocessing step is computed for every block of every\nrow. The preprocessing table R is indexed by row number, group number and a\nvector (which is a potential column vector). The horizontal vectors need not be\nstored.\nTo fill cell (i, j), they iterate over all groups and find the q-length column\nvectors. The preprocessed value for this vector in the corresponding block is\nretrieved from the table and the result is added to D(q, j).\nThe preprocessing is for horizontal vectors seen in the table. Since the horizontal vectors are not known beforehand, the precomputation cannot be done\nprior to the main algorithm. Instead, it is interleaved with the computation of\nthe table. They fill part of the DP table and use the vectors to complete some\npreprocessing, which in turn is used fill another part of the table and so on.\nSince the preprocessing is done for every group of every row, the same horizontal vector can be seen multiple times in the table. This leads to duplicated\nwork and slower running time than the two-vector algorithm.\nMemoization The two-vector method computes the preprocessing over all possible vector pairs and the FG method for only the horizontal vectors that are\nseen in the table. Stated this way, a hybrid approach suggests itself.\nIn our next variants, we memoize the results for a pair of vectors. Like the twovector approach, the preprocessing is done only once for a vector pair and like the\nFG algorithm, it is only for the vectors seen in the table and the preprocessing\nis interleaved with the main algorithm. Since the preprocessing table is indexed\nby two vectors, unlike the FG algorithm, the results are computed only once for\nevery vector seen.\nIn the partially memoized version, upon completion of elements of a group, if\na new horizontal vector is seen, we pair it with all possible 2q column vectors and\n\n\fthe results are tabled. In the completely memoized version, the result for a pair\nof vectors is computed the first time the pair is observed and the result is stored\nin the table. The result for future occurrences of the same pair are obtained by\na table lookup. the rest of the algorithm is identical to the two-vector method.\nAll these variants take O(n3 / log n) time but the memoized versions potentially store fewer vectors than the two vector method and will have a similar\nworst-case runtime in practice as the two-vector method. But, as argued before,\nthe FG method does duplicated work and will be slower in practice.\n\n4\n\nParallel Algorithm\n\nThe Nussinov DP algorithm can be parallelized with n processes to get an O(n2 )\nparallel algorithm. We assign one parallel process to a column. In the ith iteration, each process computes the value for the ith diagonal entry. That is, the\nsuccessive diagonals are solved in iterations and in each iteration the entries of\nthe diagonal are solved in parallel. To compute the value for cell (i, j), the entries\nin the row to its left and in the column below (i, j) are needed. Since these values\nare computed in earlier iterations, each diagonal cell can be filled independent\nof the other processes.\nA process has to compute the value for O(n) cells and for each cell it needs\nto access O(n) other cells. Thus the total computation takes O(n2 ) time with n\nprocesses.\nThe parallel algorithm for process j for j = 1, 2, . . . , n:\n1: D(j + 1, j) ← 0, D(j, j) ← 0\n2: for i = j down to 1 do\n3:\nD(i, j) ← D(i + 1, j − 1) + b(S[i], S[j])\n4:\nfor k = i + 1 to j do\n5:\nD(i, j) ← max{D(i, j), D(i, k − 1) + D(k, j)}\n6:\nend for\n7:\nSynchronize with other processes\n8: end for\nWe will describe the use the two-vector Four-Russians method to obtain\nan O(n2 / log n) algorithm below. The preprocessing step that enumerates the\nsolution for 2q × 2q difference vectors is embarrassingly parallel and we do not\ndiscuss the parallel algorithm for it.\nAs before, we have n processes one for each column. Each process solves the\nentries of the column from bottom to top. Instead of computing the maximum\nover each cell in the inner loop (lines 3 – 5 in the parallel algorithm above), we\nuse the Four-Russians technique to solve q cells in one step by looking up the\ntable computed in the preprocessing step.\nLet dH (i, j) be the horizontal difference vector for cells D(i, j), . . . , D(i + q −\n1, j) and let dV (i, j) be the vertical difference for cells D(i, j), . . . , D(i + q − 1, j).\nWe modify the inner loop of the parallel algorithm as follows:\n1: for k 0 = 0 to bj/qc − 1 do\n2:\nk = i + k0 ∗ q\n\n\fD(i, j) = max{D(i, j), D(i, k) + D(k + 1, j) + R[dH (i, k)][dV (k + 1, j)]}\nend for\nfor k = bj/qc × q to j do\nD(i, j) ← max{D(i, j), D(i, k) + D(k + 1, j)}\nend for\nCompute the horizontal and vertical differences and store them in dH (i −\nq + 1, j) and dV (i, j) respectively.\nFor each entry, the first loop takes O(n/q) time and the second loop takes\nO(q) time. Since all the processes are solving the k th diagonal in the k th iteration,\nall of them execute the same number of steps before synchronization. Note that\nwe compute the horizontal and vertical differences for every node, unlike in\nsection 3.1 where they are computed every q th cell, to ensure that every process\nperforms the same number of steps and simplify the analysis. The difference\nvectors can be computed in O(q) time. These can also be computed in constant\ntime by shifting the previous difference vector and appending the new difference.\nBut we will not assume this simplification for the time bound computation.\nThus each entry can be computed in O(n/q + q) time. There are O(n) entries\nfor each process, thus the total time taken for all processes to terminate is\nO(n2 /q + nq). With q = log n as before, this gives an O(n2 / log n) algorithm.\n3:\n4:\n5:\n6:\n7:\n8:\n\n5\n5.1\n\nParallel Implementation\nGPU Architecture\n\nGraphics processing units (GPUs) are specialized processors designed for computationally intensive real-time graphics rendering. Compute Unified Device Architecture (CUDA) is the computing engine designed by NVIDIA for their GPUs.\nThe programmer can group threads in a block, which in turn can be organized\nin a grid hierarchy. Memory hierarchy includes thread-specific local memory,\nblock-level shared memory for all threads in the block and global memory for\nthe entire grid. The access times increases along the hierarchy from local to\nglobal memory.\nSince the access to global memory is slower (more clock cycles than local\nmemory access), it is efficient for the threads within a block to access contiguous\nmemory locations. Then the hardware coalesces memory accesses for all threads\nin a block into one request. More specifically, in our application, if a matrix is\nstored in row-major order and if the threads in a block access contiguous elements\nof a row, then the accesses can be coalesced. However, accessing elements along a\ncolumn is inefficient as distant memory elements have to be fetched from different\ncache lines.\nPrograms that observe the hardware specifications can exploit the optimizations in the system and are fast in practice. We designed the program that\nexploits the parallel structure of the DP algorithm and the hardware features of\nthe GPU.\n\n\f5.2\n\nRelated Work\n\nAs mentioned earlier, the cells of a diagonal are independent of one another\nand can be computed in parallel. In Stojanovski et al. [24], elements of the\ndiagonal are assigned to a block of threads. This design does not handle memory\ncoalescence for either row or column accesses. Chang et al. [6] allocate an n × n\ntable and reflect the upper-triangular part of the matrix on the main diagonal.\nSuccessive elements of a column are fetched from the row in the reflected part\nof the matrix. When threads of a block are assigned to elements of a diagonal,\nthe successive column accesses for a thread are to consecutive memory cells.\nHowever, this does not allow coalesced access for threads within a block. Rizk\nand Lavenier [23] show an implementation for RNA folding under energy models.\nThey show a tiling scheme where a group of cells are assigned to a block of threads\nto reuse the data values that are fetched from a column. In this paper, we show\nthat storing the row and column vectors in different orders for two-vector method\ncan further improve the efficiency.\n5.3\n\nDesign of the Four-Russians CUDA Program\n\nWe briefly describe the design of the CUDA program; a longer discussion can be\nfound in [26].\nWe group cells together into tiles, where each tile is a composite of q × q cells.\nThe tiles along a diagonal can be computed independent of each other. Each tile\nis assigned to a block of threads and computed in parallel. After all the entries\nof the tile are computed, only the horizontal and vertical differences are stored.\nTo fill a tile, the horizontal differences of all the tiles to the left and vertical\ndifferences from the tiles underneath are accessed. These difference vectors are\nstored in different orders. The horizontal difference vectors of the rows of a tile\nare stored in contiguous memory locations and the tiles are stored in row-major\norder. The vertical difference vectors of the columns of a tile are stored together\nand the tiles are grouped in column-major order.\nTo fill a tile, the horizontal difference vectors from a tile to the left are fetched.\nWhen each thread retrieves one vector, the block of threads accesses contiguous\nmemory locations and the memory accesses are coalesced. Successive iterations\nfetch the tiles along a row which are in contiguous memory locations. Similarly\nthe vertical differences of a tile below are accessed in one coalesced memory\naccess by the threads of the block.\n\n6\n\nEmpirical Results\n\nPrior to empirical evaluation, the FG algorithm was expected to be the slowest\ndue to redundant work. The memoized versions were expected to be faster than\nthe two-vector algorithm, as they preprocess only a subset of the 22q vectors\nseen in the table.\n\n\fWe ran the programs on complete mouse non-coding RNA sequences. We\nalso tested the performance on random substrings on real RNA sequences and\nrandom strings over A,C,G,U.\nThe FG algorithm, while faster than Nussinov, was the slowest among the\nFour-Russians methods, as expected. The completely memoized version was\nslower than the other two variants. This is because every lookup of the preprocessing table includes a check to see if the pair of vectors has already been\n3\nprocessed. There are 22q unique vector pairs but there are O( nq ) queries to the\npreprocessing table and each query involves checking if the vector pair has been\n2\nprocessed plus the processing time for new pairs. There are O( nq2 ) vector pairs\nin the table. For larger n (eg., n > 1000 and q = 8), all the 22q vectors are\nexpected to be present in the DP table. Generally, memoized subproblems are\nrelatively expensive compared to the lookup. Since the preprocessing here has\nonly q steps, the advantage of memoization is not seen.\nThe partially memoized version was slightly slower than the two vector algorithm. Again, the advantage of potentially less preprocessing than the two-vector\nmethod is erased by the need to check if a vector has been processed. The twovector method was the fastest on all sequence lengths tested.\nFor short sequences the two vector method took negligible time (less than 0.2\nseconds up to 1000 bases) and are not reported. For longer sequences, we noticed\nthat using longer vector lengths reduced the running time and the improvement\nsaturated beyond q = 8 or 9. Beyond this, the extra work in preprocessing\novershadowed the benefit. A similar trend was seen for the memoized versions\ntoo. However, for the FG method q = 3 gave the best speedup and longer vector\nlengths had a slower running time due to the extra preprocessing at every group.\nAll the programs were written in C++ compiled with the highest compiler\noptimizations. We only discuss the experimental results on a desktop and two\nGPU cards in this paper. Detailed notes on running times can be found in [26].\nWe measured the running times of the different versions of our serial algorithms on a desktop machine with a Pentium II 3GhZ processor and 1MB cache.\nThe running times of Nussinov and the speedups of various programs compared\nto Nussinov are shown in the table below. For sequences of length 6000, the\ntwo-vector method takes close to a minute on the desktop.\nSpeedup factors of the serial programs on the desktop\nTime\nSpeedup\nLength Nussinov Two-vector Partially Completely FG\n(in secs)\nMemoized Memoized\n2000\n16.5\n7.7\n7.3\n5.6\n3.0\n3000\n62.5\n8.8\n8.3\n6.4\n3.4\n4000\n196.6\n11.9\n11.4\n8.8\n4.7\n5000\n630.3\n21.1\n18.9\n14.7\n7.8\n6150\n1027.8\n18.1\n17.0\n13.3\n7.03\nFig. 2 shows the execution times on two GPU cards – GeForce GTX 550\nTi card with 1GB on-card memory and Tesla C2070 with 5GB memory. The\n\n\fFig. 2: Running time of the CUDA program on two GPUs. The programs run\ntwice as fast on the Tesla card than the GeForce card.\n\nprograms take about a second for sequences up to 4000 bases long, and takes\nabout 5 seconds and 2.5 seconds for sequences of length 6000. The running times\nfor various sequence lengths are shown in the table below.\nRunning times for the parallel program (in secs)\nLength On GeForce\nOn Tesla\n2000\n0.20\n0.14\n3000\n0.62\n0.38\n4000\n1.36\n0.74\n5000\n2.70\n1.39\n6000\n4.97\n2.50\n\n7\n\nConclusions and Future Work\n\nWe described the two-vector method for using the Four-Russians technique for\nRNA folding. This method is simpler than the Frid-Gusfield method. It also\nn2\nimproves the bound of the parallel algorithm by a log n factor to O( log\nn ). We\nshowed two other variants that memoize the preprocessing results. These methods are faster than Nussinov by up to a factor of 20 and the Frid-Gusfield method\nby a factor of 3.\nIn the future, it will be interesting to see the application of the Four-Russians\ntechnique for other methods that use energy models with thermodynamic parameters. The Frid-Gusfield method has been applied to RNA co-folding [12]\nand folding with pseudoknots [13] problems; the application of the two-vector\n\n\fmethod to those problems and its implications are also of interest. It will be interesting to compare our run time with the other improvements over Nussinov,\nlike the boolean matrix multiplication method [1].\n\nAcknowledgements\nThe first-listed author thanks Prof. Norm Matloff for the opportunity to lecture\nin his class; this project spawned out of that lecture. Thanks also to Prof. John\nOwens for access to the server with a Tesla card. Thanks to Jim Moersfelder\nand Vann Teves from Systems Support Staff for help in setting up the CUDA\nsystems.\n\nReferences\n1. T. Akutsu. Approximation and exact algorithms for RNA secondary structure\nprediction and recognition of stochastic context-free languages. J. Comb. Optim.,\n3(2-3):321–336, 1999.\n2. M. Andronescu, A. Condon, H. H. Hoos, D. H. Mathews, and K. P. Murphy.\nEfficient parameter estimation for RNA secondary structure prediction. Bioinformatics, 23(13):i19–i28, 2007.\n3. M. Andronescu, A. Condon, H. H. Hoos, D. H. Mathews, and K. P. Murphy. Computational approaches for RNA energy parameter estimation. RNA, 16(12):2304–\n2318, 2010.\n4. V. Arlazarov, E. Dinic, M. Kronrod, and I. Faradzev. On economical construction\nof the transitive closure of a directed graph (in Russian). Dokl. Akad. Nauk.,\n194(11), 1970.\n5. R. Backofen, D. Tsur, S. Zakov, and M. Ziv-Ukelson. Sparse RNA folding: Time\nand space efficient algorithms. Journal of Discrete Algorithms, 9(1):12 – 31, 2011.\n6. D.-J. Chang, C. Kimmer, and M. Ouyang. Accelerating the nussinov RNA folding\nalgorithm with CUDA/GPU. In ISSPIT, pages 120–125. IEEE, 2010.\n7. Y. Ding and C. E. Lawrence. A statistical sampling algorithm for RNA secondary\nstructure prediction. Nucleic Acids Research, 31(24):7280–7301, 2003.\n8. C. B. Do, D. A. Woods, and S. Batzoglou. CONTRAfold: RNA secondary structure\nprediction without physics-based models. Bioinformatics, 22(14):e90–e98, 2006.\n9. R. Dowell and S. Eddy. Evaluation of several lightweight stochastic context-free\ngrammars for RNA secondary structure prediction. BMC Bioinformatics, 5(1):71,\n2004.\n10. R. Durbin, S. R. Eddy, A. Krogh, and G. Mitchison. Biological Sequence Analysis:\nProbabilistic Models of Proteins and Nucleic Acids. Cambridge University Press,\n1998.\n11. Y. Frid and D. Gusfield. A simple, practical and complete O(n3 )-time algorithm for\nRNA folding using the Four-Russians speedup. Algorithms for Molecular Biology,\n5:13, 2010.\n12. Y. Frid and D. Gusfield. A worst-case and practical speedup for the RNA co-folding\nproblem using the four-russians idea. In WABI, pages 1–12, 2010.\n13. Y. Frid and D. Gusfield. Speedup of RNA pseudoknotted secondary structure\nrecurrence computation with the Four-Russians method. In COCOA, pages 176–\n187, 2012.\n\n\f14. M. Hamada, H. Kiryu, K. Sato, T. Mituyama, and K. Asai. Prediction of RNA secondary structure using generalized centroid estimators. Bioinformatics, 25(4):465–\n473, 2009.\n15. I. L. Hofacker. Vienna RNA secondary structure server. Nucleic Acids Research,\n31(13):3429–3431, 2003.\n16. B. Knudsen and J. Hein. Pfold: RNA secondary structure prediction using stochastic context-free grammars. Nucleic Acids Research, 31(13):3423–3428, 2003.\n17. Z. J. Lu, J. W. Gloor, and D. H. Mathews. Improved RNA secondary structure\nprediction by maximizing expected pair accuracy. RNA, 15(10):1805–1813, 2009.\n18. N. R. Markham and M. Zuker. UNAFold. Bioinformatics, 453:3–31, 2008.\n19. D. H. Mathews, M. D. Disney, J. L. Childs, S. J. Schroeder, M. Zuker, and D. H.\nTurner. Incorporating chemical modification constraints into a dynamic programming algorithm for prediction of RNA secondary structure. PNAS, 101(19):7287–\n7292, 2004.\n20. R. Nussinov and A. B. Jacobson. Fast algorithm for predicting the secondary\nstructure of single-stranded RNA. PNAS, 77(11):6309–6313, 1980.\n21. R. Nussinov, G. Pieczenik, J. R. Griggs, and D. J. Kleitman. Algorithms for loop\nmatchings. SIAM Journal on Applied Mathematics, 35(1):68–82, 1978.\n22. J. Reuter and D. Mathews. RNAstructure: software for RNA secondary structure\nprediction and analysis. BMC Bioinformatics, 11(1):129, 2010.\n23. G. Rizk and D. Lavenier. GPU accelerated RNA folding algorithm. In International\nConference on Computational Science, 2009.\n24. M. Stojanovski, D. Gjorgjevikj, and G. Madjarov. Parallelization of dynamic programming in nussinov RNA folding algorithm on the CUDA GPU. In ICT Innovations, 2011.\n25. I. Tinoco et al. Improved Estimation Of Secondary Structure In Ribonucleic-Acids.\nNature-New Biology, 246(150):40–41, 1973.\n26. B. Venkatachalam, Y. Frid, and D. Gusfield. Faster algorithms for RNA-folding\nusing the Four-Russians method. UC Davis Technical report, 2013.\n27. Y. Wexler, C. B.-Z. Zilberstein, and M. Ziv-Ukelson. A study of accessible motifs\nand RNA folding complexity. Journal of Computational Biology, 14(6):856–872,\n2007.\n28. S. Zakov, Y. Goldberg, M. Elhadad, and M. Ziv-Ukelson. Rich parameterization\nimproves RNA structure prediction. In V. Bafna and S. C. Sahinalp, editors,\nResearch in Computational Molecular Biology (RECOMB), volume 6577, pages\n546–562. Lecture Notes in Computer Science, Springer, 2011.\n29. S. Zakov, D. Tsur, and M. Ziv-Ukelson. Reducing the worst case running times of\na family of RNA and CFG problems, using Valiant’s approach. In WABI, pages\n65–77, 2010.\n30. M. Zuker. Mfold web server for nucleic acid folding and hybridization prediction.\nNucleic Acids Research, 31(13):3406–3415, 2003.\n31. M. Zuker and P. Stiegler. Optimal computer folding of large RNA sequences using\nthermodynamics and auxiliary information. Nucleic Acids Research, 9(1):133–148,\n1981.\n\n\f"
        ],
        [
         "32",
         "32",
         "cs.CE",
         "Computational Engineering",
         "1608.04998v1.pdf",
         "A Unified Finite Element Method\nfor Fluid-Structure Interaction\nYongxing Wang∗, Peter Jimack, Mark Walkley\n\narXiv:1608.04998v1 [cs.CE] 15 Aug 2016\n\nSchool of Computing, University of Leeds, Leeds, UK, LS2 9JT\n\nAbstract\nIn this article, we present a new unified finite element method (UFEM) for simulation of general Fluid-Structure interaction (FSI) which has the same generality\nand robustness as monolithic methods but is significantly more computationally efficient and easier to implement. Our proposed approach has similarities\nwith classical immersed finite element methods (IFEMs), by approximating a\nsingle velocity and pressure field in the entire domain (i.e. occupied by fluid\nand solid) on a single mesh, but differs by treating the corrections due to the\nsolid deformation on the left-hand side of the modified fluid flow equations (i.e.\nimplicitly). The method is described in detail, followed by the presentation of\nmultiple computational examples in order to validate it across a wide range of\nfluid and solid parameters and interactions.\nKeywords: Fluid-Structure interaction, Finite element method, Immersed\nfinite element method, Monolithic method, Unified finite element method\n\n1. Introduction\n\n5\n\n10\n\nNumerical simulation of fluid-structure interaction is a computational challenge because of its strong nonlinearity, especially when large deformation is\nconsidered. Based on how to couple the interaction between fluid and solid,\nexisting numerical methods can be broadly categorized into two approaches:\npartitioned/segregated methods and monolithic/fully-coupled methods. Similarly, based on how to handle the mesh, they can also be broadly categorized\ninto two further approaches: fitted mesh/conforming methods and unfitted/nonconforming mesh methods [1].\nA fitted mesh means that the fluid and solid meshes match each other at\nthe interface, and the nodes on the interface are shared by both the fluid and\nthe solid, which leads to the fact that each interface node has both a fluid velocity and a solid velocity (or displacement) defined on it. It is apparent that\n∗ Corresponding\n\nauthor\nEmail address: scywa@leeds.ac.uk (Yongxing Wang)\n\nPreprint submitted to Journal of COMPUT METHOD APPL M\n\nAugust 18, 2016\n\n\f15\n\n20\n\n25\n\n30\n\n35\n\n40\n\n45\n\n50\n\n55\n\nthe two velocities on each interface node should be consistent. There are typically two methods to handle this: partitioned/segregated methods [2, 3] and\nmonolithic/fully-coupled methods [4, 5, 6]. The former solve the fluid and solid\nequations sequentially and iterate until the velocities become consistent at the\ninterface. These are more straightforward to implement but can lack robustness\nand may fail to converge when there is a significant energy exchange between\nthe fluid and solid [3]. The latter solve the fluid and solid equations simultaneously and often use a Lagrange Multiplier to weakly enforce the continuity of\nvelocity on the interface [6]. This has the advantage of achieving accurate and\nstable solutions, however the key computational challenge is to efficiently solve\nthe large systems of nonlinear algebraic equations arising from the fully-coupled\nimplicit discretization of the fluid and solid equations. Fitted mesh methods can\naccurately model wide classes of FSI problems, however maintaining the quality\nof the mesh for large solid deformations usually requires a combination of arbitrary Lagrangian-Eulerian (ALE) mesh movement and partial or full remeshing\n[7]. These add to the computational expense and, when remeshing occurs, can\nlead to loss of conservation properties of the underlying discretization [8].\nUnfitted mesh methods use two meshes to represent the fluid and solid separately and these do not generally conform to each other on the interface. In\nthis case, the definition of the fluid problem may be extended to an augmented\ndomain which includes the solid domain. Similarly to the fitted case, there are\nalso two broad approaches to treat the solid domain: partitioned methods and\nmonolithic methods. On an unfitted mesh, there is no clear boundary for the\nsolid problem, so it is not easy to enforce the boundary condition and solve the\nsolid equation. A wide variety of schemes have been proposed to address this\nissue, including the Immersed Finite Element Method (IFEM) [9, 10, 11, 12, 13],\nthe Fictitious Domain (FD) method [14, 15, 16, 17], and the mortar approach\n[16, 18]. The IFEM developed from the Immersed Boundary method first introduced by Peskin [19], and has had great success with applications in bioscience\nand biomedical fields. The classical IFEM does not solve solid equations at all.\nInstead, the solid equations are arranged on the right-hand side of the fluid\nequations as an FSI force, and these modified fluid equations are solved on the\naugmented domain (occupied by fluid and solid). There is also the Modified\nIFEM [13], which solves the solid equations explicitly and iterates until convergence. Reference [14] presents a fractional scheme for a rigid body interacting\nwith the fluid, whilst [15] introduces a fractional step scheme using Distributed\nLagrange Multiplier (DLM)/FD for fluid/flexible-body interactions. In the case\nof monolithic methods, [16] uses a FD/mortar approach to couple the fluid and\nstructure, but the coupling is limited to a line (2D) representing the structure.\nReference [18] uses a mortar approach to solve fluid interactions with deformable\nand rigid bodies, and [17] also solves a fully-coupled FSI system with hierarchical B-Spline grids. There are also other monolithic methods based on unfitted\nmeshes [20, 21].\nIt can be seen that the major methods based on unfitted meshes either avoid\nsolving the solid equations (IFEM) or solve them with additional variables (two\nvelocity fields and Lagrange multiplier) in the solid domain. However, physically,\n2\n\n\f60\n\n65\n\n70\n\n75\n\n80\n\n85\n\n90\n\n95\n\n100\n\nthere is only one velocity field in the solid domain. In this article, we develop\na semi-explicit Unified FEM (UFEM) approach which only solves one velocity\nvariable in the whole/augmented domain. We shall use unfitted meshes to\nintroduce our UFEM, although the methodology can also be applied to fitted\nmeshes.\nThe word “unified” here has two meanings: (1) the equations for fluid and\nsolid are unified in one equation in which only one velocity variable is solved; (2)\na range of solid materials, from the very soft to the very hard, may be considered\nin this one scheme.\nThe term “semi-explicit” also has two components: (1) we linearize the solid\nconstitutive model (an incompressible neo-Hookean model) explicitly using the\nvalue from the last time step; (2) we couple the FSI interaction implicitly by\narranging the solid information on the left-hand side of control equations.\nThe main idea of UFEM is as follows. We first discretize the control equations in time, re-write the solid equation in the form of a fluid equation (using\nthe velocity as a variable rather than the displacement) and re-write the solid\nconstitutive equation in the updated coordinate system. We then combine the\nfluid and solid equations and discretize them in an augmented domain. Finally\nthe multi-physics problem is solved as a single field.\nThe UFEM differs from the classical IFEM approach which puts all the solid\nmodel information from the last time step explicitly on the right-hand side of\nthe fluid equations. This typically requires the use of a very small time step to\nsimulate the whole FSI system. This IFEM approach works satisfactorily when\nthe solid behaves like a fluid, such as a very soft solid, but can lead to significant\nerrors when the solid behaves quite differently from the fluid, such as a hard\nsolid. The UFEM scheme includes the solid information on the left-hand side\nand, as we will demonstrate, can simulate a wide range from very soft to very\nhard solids both accurately and efficiently.\nAs noted above, monolithic methods strongly couple the fluid and solid models, and discretize them into one implicit nonlinear equation system at each time\nstep. The unknowns include velocity, displacement and a Lagrangian multiplier\nto enforce consistency of velocity on an interface (fitted mesh) [4, 5, 6] or in\na solid domain (unfitted mesh) [16, 17, 18]. One may gain both a stable and\nan accurate solution from such fully-coupled schemes. However, it is clear that\nthis strategy is very costly, especially for the unfitted mesh case, in which the\nso called mortar integrals are involved [18]. The UFEM only solves for velocity\nas unknowns, which is cheaper, but does not lose stability or accuracy as shown\nby the numerical experiments reported in this paper.\nThe following sections are organized as follows. In section 2, the control\nequations and boundary conditions for fluid-structure interactions are introduced; In section 3, the weak form of the FSI system is presented based on the\naugmented fluid domain. In section 4, details of the linearization of the FSI\nequations are discussed and the numerical scheme is presented. In section 5,\nnumerical examples are described to validate the proposed UFEM.\n\n3\n\n\f2. Governing equations for FSI\nIn the following context, let\nZ\n(u, v)Ω =\n105\n\n110\n\nuvdΩ,\n\n(1)\n\nΩ\n\nwhere u and v are functions defined in domain Ω.\nAll subscripts, such as i, j, and k, represent spatial dimension. If they are\nrepeated in one term (including the bracket defined in (1)), it implies summation\nover the spatial dimension; if they are not repeated, they take the value 1 and\n2 for 2D, and 1 to 3 for 3D. All superscripts are used to distinguish fluid and\nsolid (f and s respectively), distinguish different boundaries (ΓD and ΓN ) or\nrepresent time step (n). For example, ufi and usi denote the velocity components\nf\ns\nof fluid and solid respectively, σij\nand σij\ndenote the stress tensor components\ns n\nof fluid and solid respectively, and (ui ) is a solid velocity component at time\ntn .\n\nFigure 1:\n\nSchematic diagram of FSI, Ω = Ωf ∪ Ωs , Γ = ΓD ∪ ΓN .\n\nIn our model we assume an incompressible fluid governed by the following\nequations in Ωf as shown in Figure 1:\nρf\n\nf\nσij\n= µf\n\nf\n∂σij\nDufi\n−\n= ρf gi ,\nDt\n∂xj\n\n∂ufj\n= 0,\n∂xj\n!\n∂ufj\n∂ufi\nf\n+\n− pf δij = τij\n− pf δij .\n∂xj\n∂xi\n\n(2)\n\n(3)\n\n(4)\n\nWe also assume an incompressible solid that is governed by the following\nequations in Ωs as shown in Figure 1:\nρs\n\ns\n∂σij\nDusi\n−\n= ρs gi ,\nDt\n∂xj\n\n4\n\n(5)\n\n\fdet (F) = 1,\ns\nσij\n= µs\n115\n\n120\n\n125\n\n\u0012\n\n∂xsi ∂xsj\n− δij\n∂Xk ∂Xk\n\n\u0013\n\n(6)\n\ns\n− ps δij = τij\n− ps δij .\n\n(7)\n\nf\ns\nare the deviatoric stress of the fluid and solid reand τij\nIn the above τij\nf\ns\nspectively, ρ and ρ are the density of the fluid and solid respectively, µf is\nthe fluid viscosity, and gi is the acceleration due to gravity. Note that (5)-(7)\ndescribe an incompressible neo-Hookean model that is based on [16] and is suitable for large displacements. In this model, µs is the shear modulus and ps is\nthe pressure of the solid (pf being the fluid pressure in (4)). We denote by xi\nthe current coordinates ofhthe solid\nor fluid, and by Xi the reference coordinates\ni\n∂xi\nD\nof the solid, whilst F = ∂X\nis\nthe\ndeformation tensor of the solid and Dt\nj\nrepresents the total derivative of time.\nOn the interface boundary Γs :\n\nufi = usi ,\n\n(8)\n\nf s\ns s\nσij\nnj = σij\nnj ,\n\n(9)\n\nwhere nsj denotes the component of outward pointing unit normal, see Figure 1.\nDirichlet and Neumann boundary conditions may be imposed for the fluid:\nufi = ūi\n\non ΓD ,\n\nf\nσij\nnj = h̄i\n\non\n\nΓN .\n\n(10)\n(11)\n\nFinally, initial conditions are typically set as:\nufi\n\nt=0\n\n= usi |t=0 = 0,\n\n(12)\n\nthough they may differ from (12).\nRemark 1 Using Jacobi’s formula [22]:\n\nwe have\n\n\u0012\n\u0013\nd\n−1 dF\ndet (F) = det (F) tr F\n,\ndt\ndt\n\n(13)\n\n∂usj\nd\ndet (F) = det (F)\n,\ndt\n∂xj\n\n(14)\n\n∂usj\n= 0.\n∂xj\n\n(60 )\n\nwhich, using (6), gives\n\nWe choose that the reference configuration is the same as the initial configuration, so (60 ) also implies (6). In our UFEM model, the incompressibility\nconstraint (60 ) will be used instead of (6).\n5\n\n\f130\n\n3. Weak form of FSI equations\nIn order to obtain a weak formulation we define a combined trial space for\nvelocity as:\no\n\u0011\nn\u0010\n\u0001\nW =\nufi , usi : ufi ∈ H 1 Ωf , usi ∈ H 1 (Ωs ) , usi |Γs = ufi s , ufi D = ūi ,\nΓ\n\nwith a corresponding combined test space for the velocity as:\n\u0011\nn\u0010\n\u0001\nW0 =\nvif , vis : vif ∈ H 1 Ωf , vis ∈ H 1 (Ωs ) , vis |Γs = vif s , vif\nΓ\n\nΓ\n\nΓD\n\no\n=0 .\n\n\u0001\nf\n\nBoth the trial and test spaces for pressure in Ωf are L2 Ω , and both the trial\nand test spaces for pressure in Ωs are L2 (Ωs ). We then perform the following\nsymbolic operations:\n\u0010\n\u0011\n\u0001\nEq.(2), vif f − Eq.(3), q f Ωf + (Eq.(5), vis )Ωs − (Eq.(60 ), q s )Ωs .\nΩ\n\nIntegrating the stress terms by parts, using constitutive equation (4) and (7)\nand boundary condition (11), the last operations give the following weak form\nof the FSI system.\n\u0010\n\u0011\n\u0001\nFind ufi , usi ∈ W , pf ∈ L2 Ωf and ps ∈ L2 (Ωs ) such that\n!\n!\n!\n!\nf\n∂ufj f\n∂vjf\nDufi f\nf ∂vi\nf\n,v\n− p ,\n−\n,q\n+ τij ,\nρ\nDt i\n∂xj\n∂xj\n∂xj\nf\nΩf\nΩf\nΩf\n\u0012\n\u0012\n\u0012\n\u0012 s\n\u0013\n\u0013\n\u0013 Ω\ns\u0013\ns\ns\n∂u\n∂v\nDu\n∂v\nj\nj\ni\ns\nρs\n, vs\n+ τij\n, i\n− ps ,\n−\n, qs\nDt i Ωs\n∂xj Ωs\n∂xj Ωs\n∂xj\nΩs\n\u0010\n\u0011\n\u0010\n\u0011\nf\nf\nf\ns\ns\n= h̄i , vi N + ρ gi , vi f + ρ (gi , vi )Ωs ,\nf\n\nΓ\n\n\u0010\n\n135\n\n\u0011\n\n∀ vif , vis ∈ W0 , ∀q f ∈ L2 Ω\n\n(15)\n\nΩ\n\n\u0001\nf\n\nand ∀q s ∈ L2 (Ωs ).\n\nNote that the integrals on the interface (boundary forces) are also cancelled out\nusing boundary condition (9). This is not surprising because they are internal\nforces for the whole FSI system considered here.\nWe next extend\npressure into solid\nin\u001a f the fluid velocity \u001aand\n\u001a f domain by\np\nin Ωf\nui in Ωf\nvif in Ωf\ntroducing ui =\n, vi =\n, p =\nand\nps in Ωs\nusi in Ωs\nvis in Ωs\n\u001a f\nf\nq\nin Ω\nq=\n, then extend the fluid computational domain from Ωf to an\nq s in Ωs\naugmented domain Ω, and define a trial space for velocity in Ω as:\n\b\nW = ui : ui ∈ H 1 (Ω) , R (ui ) = usi ∈ H 1 (Ωs ) , ui |ΓD = ūi ,\nwith a corresponding test space for the velocity as:\n\b\nW 0 = vi : vi ∈ H 1 (Ω) , R (vi ) = vis ∈ H 1 (Ωs ) , vi |ΓD = 0 ,\n6\n\n\fwhere R (ui ) = ui |Ωs is the restriction map.\n140\n\nNotice that pf and ps are not uniquely determined in (15). In fact, taking\np + c and ps + c instead of pf and ps respectively, the left-hand side of (15) does\nnot change. This situation can be avoided by fixing the pressure at a selected\npoint (P0 ) or by imposing the following constraint [23]:\nZ\nZ\nZ\nf\ns\np dΩ +\np dΩ =\npdΩ = 0.\n(16)\nf\n\nΩf\n\nΩs\n\nΩ\n\nWe shall use the former approach therefore define the trial space for pressure in\nΩ as:\n\b\nL20 (Ω) = p : p ∈ L2 (Ω), p |P0 = 0 .\n145\n\n150\n\n155\n\n160\n\nThe weak form of the FSI system in the augmented domain Ω can now be\nreformulated by rearranging equation (15) to yield the following formulation.\nFind ui ∈ W and p ∈ L20 (Ω) such that\n\u0012\n\u0013\n\u0012\n\u0013\n\u0012\n\u0013\n\u0012\n\u0013\nDui\n∂vj\n∂uj\nf ∂vi\nf\nρ\n, vi\n+ τij ,\n− p,\n−\n,q\nDt\n∂xj Ω\n∂xj Ω\n∂xj\nΩ\nΩ\n\u0012\n\u0013\n\u0013\n\u0012\n\u0001\n∂v\nDu\n(17)\ni\ni\nf\ns\n, vi\n+ τij\n− τij\n,\n+ ρs − ρf\nDt\n∂x\ns\ns\nj\nΩ\nΩ\n\u0001\n\u0001\n= h̄i , vi ΓN + ρf (gi , vi )Ω + ρs − ρf (gi , vi )Ωs ,\n∀vi ∈ W 0 and ∀q ∈ L2 (Ω).\nf\nRemark 2 The fluid deviatoric stress τij\nis generally far smaller than the\nf\ns\nsolid deviatoric stress τij , so we choose to neglect the fluid deviatoric stress τij\ns\nin Ω in what follows. Note that the classical IFEM neglects the whole fluid\nf\nstress σij\nwhen computing the FSI force [9]. An equivalent way of interpreting\nf\nneglecting τij\nin Ωs is to view the solid as being slightly visco-elastic, having\nthe same viscosity as the fluid.\nRemark 3 We treat the solid as a freely moving object in a fluid, so\nusi , vis ∈ H 1 (Ωs ) without any boundary constraints in the definition of W and\nW 0 respectively. Physically, however, if part of solid boundary is fixed, this fixed\nboundary can also be regarded as a fixed fluid boundary and implemented as a\nzero velocity condition in the fluid domain, hence the solid still can be treated\nas if it were freely moving. Furthermore, the interface boundary condition (8)\nis automatically built into the solution because we use an augmented solution\nspace W which requires ui |Ωs = usi .\n4. Computational scheme\nThe integrals in equation (17) are carried out in two different domains as\nillustrated in Figure 1. We use an Eulerian mesh to represent Ω and an updated\n\n7\n\n\fLagrangian mesh to represent Ωs , therefore the total time derivatives in these\ntwo different domains have different expressions, i.e:\nDui\n∂ui\n∂ui\n=\n+ uj\nDt\n∂t\n∂xj\n\nin Ω,\n\n(18)\n\nand\n\n∂usi\nDusi\n=\nin Ωs .\n(19)\nDt\n∂t\nStandard FEM isoparametric interpolation may be used to transfer data\nbetween the two meshes. Firstly, based on the above two equations (18) and\n(19), we discretize (17) in time using a backward finite difference. Then omiting\nthe superscript n + 1, showing the solution is at the end of the time step, for\nconvenience, we obtain:\n\u0013\n\u0012\n\u0013\n\u0012\n\u0013\n\u0012\n\u0013\n\u0012\nui − uni\n∂ui\n∂uj\n∂vj\nf ∂vi\n+ uj\n, vi\n,q\nρf\n+ τij\n− p,\n−\n,\n∆t\n∂xj\n∂xj Ω\n∂xj Ω\n∂xj\nΩ\n\u0012\n\u0013Ω\n\u0012\n\u0013\nn\n\u0001\nui − ui\n(20)\ns\nf\ns ∂vi\n+ ρ −ρ\n, vi\n+ τij ,\n∆t\n∂x\ns\nj Ωs\nΩ\n\u0001\n\u0001\n= h̄i , vi ΓN + ρf (gi , vi )Ω + ρs − ρf (gi , vi )Ωs .\nUsing the splitting method of [24, Chapter 3], equation (20) can be expressed\nin the following two steps.\n(1) Convection step:\n\u0013\n\u0012 ∗\n∗\nui − uni\n∗ ∂ui\nf\n+ uj\n, vi\n= 0;\n(21)\nρ\n∆t\n∂xj\nΩ\n(2) Diffusion step:\n\u0012\n\u0013\n\u0013\n\u0012\n\u0013\n\u0012\n\u0013\n\u0012\n∂vj\n∂uj\nui − u∗i\nf ∂vi\nf\n, vi\n+ τij ,\n− p,\n−\n,q\nρ\n∆t\n∂xj Ω\n∂xj Ω\n∂xj\nΩ\nΩ\n\u0012\n\u0012\n\u0013\n\u0013\n\u0001 ui − uni\ns\nf\ns ∂vi\n+ ρ −ρ\n, vi\n+ τij ,\n∆t\n∂xj Ωs\nΩs\n\u0001\n\u0001\nf\ns\nf\n= h̄i , vi ΓN + ρ (gi , vi )Ω + ρ − ρ (gi , vi )Ωs .\n165\n\n(22)\n\nThe treatment of the above two steps is described separately in the following\nsubsections.\n4.1. Linearization of the convection step\n\n170\n\nIn this section, two methods are introduced to treat the convection equation:\nthe implicit Least-squares method and the explict Taylor-Galerkin method, both\nof which can be used in the framework of our UFEM scheme. Some numerical\nresults for comparison between these two methods are discussed subsequently\nin section 5.\n\n8\n\n\f4.1.1. Implicit Least-squares method\nIt is possible to linearize (21) using the value of ui from the last time step:\nu∗j\n\n∂u∗i\n∂un\n∂u∗\n∂un\n≈ u∗j i + unj i − unj i .\n∂xj\n∂xj\n∂xj\n∂xj\n\nSubstituting (23) into equation (21) gives,\n\u0012\n\u0013 \u0013\n\u0012\n\u0013\n\u0012\nn\n∗\nn\n∗\n∗ ∂ui\nn ∂ui\nn\nn ∂ui\nui + ∆t uj\n+ uj\n, vi\n= ui + ∆tuj\n, vi\n.\n∂xj\n∂xj\n∂xj\nΩ\nΩ\n\n(23)\n\n(24)\n\nFor the Least-squares method [25], we may choose the test function in the\nfollowing form:\n\u0013\n\u0012\n∂uni\nn ∂wi\n+ uj\n,\n(25)\nvi = L (wi ) = wi + ∆t wj\n∂xj\n∂xj\nwhere wi ∈ W 0 . In such a case, the weak form of (21) is:\n\u0012\n\u0013\nn\n∗\nn\nn ∂ui\n(L (ui ) , L (wi ))Ω = ui + ∆tuj\n.\n, L (wi )\n∂xj\nΩ\n175\n\n(26)\n\nIn our UFEM a standard biquadratic finite element space is used to discretize\nequation (26) directly, although other spaces could be used.\n4.1.2. Explicit Taylor-Galerkin method\nIt is also possible to linearize equation (21) as:\n\u0012 ∗\n\u0013\n1 n ∂\nui − uni\n∗\nn\n+ uj\n(u + ui ) , vi\n= 0,\n∆t\n2 ∂xj i\nΩ\nor\n\n\u0012\n\n∂un\nu∗i − uni\n+ unj i , vi\n∆t\n∂xj\n\n(27)\n\n\u0013\n= 0.\n\n(28)\n\nΩ\n\nRe-write (28) as:\nu∗i = uni − ∆tunj\n\n∂uni\n,\n∂xj\n\nand substitute (29) into equation (27), we have\n\u0012 ∗\n\u0012\n\u0013 \u0013\nn\nn\nui − uni\n∆t n ∂\nn ∂ui\nn ∂ui\n+ uj\n−\nu\nuk\n, vi\n= 0.\n∆t\n∂xj\n2 j ∂xj\n∂xk\nΩ\n\n(29)\n\n(30)\n\nNotice that a second order derivative exists in the last equation. In practice,\none does not need to calculate the second order derivative, instead, Integration\nby parts may be used to reduce the order:\n\u0012\n\u0012\n\u0013 \u0013\n\u0012\n\u0013\n\u0012\n\u0013\n∂\n∂ui\n∂ui\n∂ui ∂vi\nuk\n, vi\n= uk\n, vi\n− uk\n,\n.\n(31)\n∂xj\n∂xk\n∂xk\n∂xk ∂xj Ω\nΓN\nΩ\n9\n\n\fThe boundary integral in the last equation can be neglected if ui is the solution\nof the previous diffusion step, which means no convection exists on the boundary\nafter the diffusion step. Using (31), equation (30) may be approximated as:\n\u0012 ∗\n\u0013\n\u0012\n\u0013\nui − uni\n∆t\n∂un\n∂un\n∂vi\n+ unj i , vi\n=−\nunk i , unj\n.\n(32)\n∆t\n∂xj\n2\n∂xk\n∂xj Ω\nΩ\nAt last the weak form of the Taylor-Galerkin method [24, Chapter 2] can be\nexpressed, by rearranging the last equation, as:\n\u0013\n\u0012\n\u0013\n\u0012\n∆t2\n∂un\n∂vi\n∂un\n−\nunk i , unj\n.\n(33)\n(u∗i , vi )Ω = uni − ∆tunj i , vi\n∂xj\n2\n∂xk\n∂xj Ω\nΩ\nThis Taylor-Galerkin method is explicit, however a small time step is usually\nneeded to keep the scheme stable.\n4.2. Linearization of the diffusion step\n∂\nIn both the above and the following context, the derivative ∂x\non the upi\ndated solid mesh is computed at the current known coordinates xni , that is to\n∂\ns\nsay ∂x\n= ∂x∂n . Furthermore, τij\nin equations (22), has a nonlinear relationship\ni\ni\nwith xi , i.e.:\n!\n\u0001\n∂xn+1\n∂xn+1\nj\ns\ns n+1\ns\ni\nτij = τij\n=µ\n− δij .\n(34)\n∂Xk ∂Xk\n\nUsing a chain rule, the last equation can also be expressed as:\nn+1\n\ns\nτij\n\n\u0001n+1\n\n= µs\n\n∂xnk ∂xnl ∂xj\n∂xn+1\ni\n− µs δij\n∂xnk ∂Xm ∂Xm ∂xnl\nn+1\n\nn+1\n\n∂xn+1 ∂xj\n∂xn+1 ∂xj\n+ µs i n\n− µs i n δkl\nn\n∂xk ∂xk\n∂xk\n∂xnl\n\n(35)\n\nor\n!\n∂xn+1\n∂xn+1\nj\ni\n=µ\n− δij\n∂xnk ∂xnk\n\u0013 n+1 ,\nn+1 \u0012\n∂xj\n∂xnk ∂xnl\ns ∂xi\n+µ\n− δkl\nn\n∂xk\n∂Xm ∂Xm\n∂xnl\n\u0001\ns n+1\nτij\n\ns\nand then τij\n\n\u0001n+1\n\ns\n\ncan be expressed by the current coordinate xni as follows:\nn+1\n\n\u0001\ns n+1\nτij\n\n(36)\n\ns\n\n=µ\n\n∂xj\n∂xn+1\ni\n− δij\nn\n∂xk ∂xnk\n\n10\n\n!\n+\n\n∂xn+1\n∂xn+1\nj\ns n\ni\n(τ\n)\n.\nkl\n∂xnk\n∂xnl\n\n(37)\n\n\fUsing xn+1\n− xni = un+1\n∆t which is the displacement at the current step, the\ni\ni\nlast equation can also be expressed as:\n!\n\u0001\n\u0001\n∂un+1\n∂un+1\n∂un+1\n∂un+1\nj\nj\ns\ns n+1\ns n\ni\ni\n+\n+ ∆t\n+ τij\n= µ ∆t\nτij\nn\nn\nn\nn\n∂xj\n∂xi\n∂xk ∂xk\n(38)\nn+1\nn+1\nn+1\nn+1\n\u0001\n∂u\n∂u\n∂u\n∂u\nn\nj\nj\nn\nn\ns\ns\n+ ∆t2 i n (τkl\n+ ∆t (τils )\n)\n+ ∆t i n τkj\n.\n∂xk\n∂xnl\n∂xk\n∂xnl\nThere are two nonlinear terms in the last equation. Using a Newton method,\nthey can be linearized as follows.\nn+1\n\nn+1\n\n∂uj\n∂unj\n∂un+1\n∂uni ∂uj\n∂uni ∂unj\n∂un+1\ni\ni\n=\n+\n−\n∂xnk ∂xnk\n∂xnk ∂xnk\n∂xnk ∂xnk\n∂xnk ∂xnk\n\n(39)\n\nand\n∂un+1\n∂unj\n∂un+1\n∂un+1\nj\ns n\ns n\ni\ni\n(τ\n)\n=\n(τkl\n)\nkl\nn\nn\nn\n∂xk\n∂xl\n∂xk\n∂xnl\nn+1\n∂uni s n ∂uj\n∂uni s n ∂unj\n+\n(τ\n)\n−\n(τ )\n.\n∂xnk kl\n∂xnl\n∂xnk kl ∂xnl\n\n(40)\n\nSubstituting (38)-(40) into (22) and dropping off the superscripts n + 1 of\nun+1\nfor notation convenience, this may be expressed as:\ni\n\u0012\n\u0013\n\u0012\n\u0013\n\u0001 usi − (usi )n\nui − u∗i\nf\ns\nf\nρ\n, vi\n, vi\n+ ρ −ρ\n∆t\n∆t\ns\nΩ\n\u0012\n\u0013\n\u0012\n\u0013\n\u0012 Ω \u0013\n∂u\n∂v\n∂v\n∂u\n∂u\ni\nj\ni\nj\nj\n+ µf\n+\n,\n,q\n− p,\n−\n∂xj\n∂xi ∂xj Ω\n∂xj Ω\n∂xj\nΩ\n\u0012\n\u0013\nn\nn\n∂u\n∂u\n∂u\n∂u\n∂u\n∂u\n∂v\ni\nj\ni\nj\ni\nj\ni\ns\n+ µ ∆t\n+\n+ ∆t\n+ ∆t\n,\n∂xj\n∂xi\n∂xk ∂xk\n∂xk ∂xk ∂xj Ωs\n\u0013\n\u0012\n∂uni s n ∂uj ∂vi\n∂ui s n ∂unj\n(41)\n(τkl )\n+\n(τkl )\n,\n+ ∆t2\n∂xk\n∂xl\n∂xk\n∂xl ∂xj Ωs\n\u0012\n\u0013\n∂ui s \u0001n\nn ∂uj ∂vi\nτkj + (τils )\n,\n+ ∆t\n∂xk\n∂xl ∂xj Ωs\n\u0001\n\u0001\nf\n= h̄i , vi ΓN + ρ (gi , vi )Ω + ρs − ρf (gi , vi )Ωs\n\u0012\n\u0013\nn ∂un\nn\n\u0001\n∂unj\nj\ns\n2 ∂ui\n2 ∂ui\ns n\ns n ∂vi\n+ µ ∆t\n+ ∆t\n(τ )\n− τij ,\n.\n∂xk ∂xk\n∂xk kl ∂xl\n∂xj Ωs\n180\n\nThe spatial discretization of the above linearized weak form will be discussed\nin the following section.\n4.3. Discretization in space\nIn the 2D case, which is considered in the remainder of this paper, a standard\nTaylor-Hood element Q2Q1 (9-node biquadratic quadrilateral for velocity and\n11\n\n\f185\n\n4-node bilinear quadrilateral for pressure) is used to discretize in space. We first\ndiscretize the domain Ω to get Ωh , then define finite dimensional subspaces of\nW and W 0 as follows.\nThe solution space for each component of velocity:\n\b\n\u0001\n\u0001\nh\nh\nW = uhi : uhi ∈ H 1h Ωh , Rh uhi = ush\ni , ui\n\nΓD\n\n= ūhi ,\n\nΓD\n\n=0 .\n\nwhilst test space for velocity is\n\b\n\u0001\n\u0001\nh\nW 0 = vih : vih ∈ H 1h Ωh , Rh vih = vish , vih\n\n190\n\nWe also discretize the domain Ωs to get Ωsh , and both\nthe discretized trial\n\u0001\nspace and test space on the solid domain are H 1h Ωsh based on the discussion\nof Remark 3.\n\u0001\n\u0001\nThe solution and test spaces for pressure are L2h\nΩh and L2h Ωh respec0\ntively, which represent the finite dimensional subspaces of L20 (Ω) and\u0001L2 (Ω), re- \u0001\u0001\nspectively, based on continuous piecewise bilinear functions. H\u00011h Ωh H 1h Ωsh\nrepresents the finite dimensional subspace of H 1 (Ω) H 1 (Ωs ) based upon continuous piecewise biquadratic functions. Then equation (41) can be discretized\nas:\n!\n\u0001\n\u0013\n\u0012 h\nsh n\n\u0001 ush\nui − u∗h\ni − ui\ni\nh\ns\nf\nsh\nf\nρ\n, vi\n+ ρ −ρ\n, vi\n∆t\n∆t\nΩh\nΩsh\n!\n!\n!\nh\nh\nh\nh\nh\n∂uj ∂vi\n∂vj\n∂uj h\n∂ui\nf\nh\n+µ\n+\n,\n− p ,\n−\n,q\n∂xj\n∂xi ∂xj\n∂xj\n∂xj\nΩh\nΩh\nΩh\n!\nsh\nsh\nsh ∂un\nn ∂ush\nsh\n∂u\n∂ui\n∂ui\n∂ui\n∂vi\nj\nj\nj\ns\n+ µ ∆t\n+\n+ ∆t\n+ ∆t\n,\n∂xj\n∂xi\n∂xk ∂xk\n∂xk ∂xk ∂xj\nΩsh\n!\nn\nsh\n(42)\nsh\nn\nsh\n∂uj\n∂ui\n∂ui s n ∂uj ∂vi\ns n\n+ ∆t2\n(τkl\n)\n+\n(τkl )\n,\n∂xk\n∂xl\n∂xk\n∂xl ∂xj\nΩsh\n!\nsh\nsh\nsh\n\u0001n\n∂ui\n∂v\nn ∂uj\n+ ∆t\nτs\n+ (τils )\n, i\n∂xk kj\n∂xl ∂xj\nsh\n\u0001\n\u0001\n\u0001Ω\n\u0001\nh\nf\nh\ns\nf\n= h̄i , vi ΓN h + ρ gi , vi Ωh + ρ − ρ\ngi , vish Ωsh\n\u0012\n\u0013\nn ∂un\nn\nsh\n\u0001\n∂unj\nj\n2 ∂ui\ns n\ns n ∂vi\ns\n2 ∂ui\n+ µ ∆t\n+ ∆t\n(τ )\n− τij ,\n.\n∂xk ∂xk\n∂xk kl ∂xl\n∂xj Ωsh\nNotice that in the continuous space W , we have the restriction map R (ui ) =\nh\nui |Ωs = usi , while in the discretized space W , we use the standard FEM isoparametric transformation Rh to represent the map, i.e.\n\u0001\nh\nush\nuhi ,\n(43)\ni =R\nwhere subscript i denotes the velocity components in each space dimension.\n12\n\n\fT\n\nT\n\ne si = (e\ne i = (e\nusi1 , u\nesi2 · · · u\nesiN s ) denote the ith comLet u\nui1 , u\nei2 · · · u\neiN f ) and u\nponents of the nodal velocity vectors on the fluid and solid meshes respectively,\nT\nT\nand ϕ = (ϕ1 , ϕ2 · · · ϕN f ) and ϕs = (ϕs1 , ϕs2 · · · ϕsN s ) denote the vector of velocity basis functions on the fluid and solid meshes respectively, where N f and\nN s are the number of nodes of fluid and solid mesh respectively. Then equation\n(43) can be expressed as:\nu\nesik ϕsk = Rh (e\nuik ϕk ) .\n\n(44)\n\ne i to u\ne si as follows:\nThe FEM isoparametric transformation defines Rh from u\nu\nesik = Rh (e\nuil ) = u\neil Rlk ,\n\n(45)\n\nwhere Rlk = ϕl (xk ), xk (k = 1, 2 · · · N s ) is the current coordinate of the k th\nnode on the solid mesh. Therefore,\nush\nesik ϕsk = u\neil Rlk ϕsk .\ni =u\n\n(46)\n\nFor velocity test functions, we similarly have\ns s\nvish = veik\nϕk = veil Rlk ϕsk ,\n\n(47)\n\nT\n\nei = (e\nwhere v\nvi1 , vei2 · · · veiN f ) is an arbitrary nodal velocity (virtual velocity)\nvector on the fluid mesh, which satisfies the homogeneous Dirichlet boundary\ncondition.\nOn the fluid mesh, velocity and pressure can also be expressed as follows:\n\n195\n\nuhi = u\neik ϕk ,\n\n(48)\n\nvih = veik ϕk ,\n\n(49)\n\nph = pek ψk ,\n\n(50)\n\nq h = qek ψk ,\n\n(51)\n\nT\ne =\nwhere ψ = (ψ1 , ψ2 · · · ψN p ) is the vector of pressure basis functions, p\nT\nT\ne = (e\n(e\np1 , pe2 , · · · peN p ) is the nodal pressure vector, and q\nq1 , qe2 , · · · qeN p ) is an\narbitrary nodal pressure vector. N p denotes the number of nodes on the fluid\nmesh at which only pressure is defined.\n\n13\n\n\fSubstituting (46)-(51) into (42), we have\n\u0012\n\u0013\nu\neik − u\ne∗ik\nf\nρ\nϕk , veim ϕm\n∆t\nΩh\n\u0012\n\u0013\nn\n\u0001\nu\ne\n−\nu\ne\nil\ns\nf\ns\ns\nil\n+ ρ −ρ\nRlk ϕk , veir Rrm ϕm\n∆t\nΩsh\n\u0012\n\u0013\n∂ϕ\n∂ϕ\n∂ϕ\nk\nk\nm\n+ µf u\neik\n+u\nejk\n, veim\n∂xj\n∂xi\n∂xj Ωh\n\u0012\n\u0012\n\u0013\n\u0013\n∂ϕk\n∂ϕm\n− u\nejk\n− pek ψk , vejm\n, qem ψm\n∂xj Ωh\n∂xj\nΩh\n\u0012\n\u0013\ns\ns\n∂ϕ\n∂ϕ\n∂ϕs\n+ µs ∆t u\neil Rlk k + u\nejl Rlk k , veir Rrm m\n∂xj\n∂xi\n∂xj Ωsh\n\u0013\n\u0012\nn\ns ∂un\n∂ui ∂ϕsb\n∂ϕsm\n∂ϕb j\ns\n2\n+u\neja Rab\n, veir Rrm\n+ µ ∆t u\neia Rab\n∂xk ∂xk\n∂xk ∂xk\n∂xj Ωsh\n\u0013\n\u0012\nn\ns\ns\n∂uj\n∂ϕ\n∂ϕ\ns n\n)\n, veir Rrm m\n+ ∆t2 u\neia Rab b (τkl\n∂xk\n∂xl\n∂xj Ωsh\n\u0012\n\u0013\nn\ns\n∂ui s n ∂ϕb\n∂ϕsm\n2\n+ ∆t u\neja Rab\n(τ )\n, veir Rrm\n∂xk kl ∂xl\n∂xj Ωsh\n\u0012\n\u0013\n∂ϕsb s \u0001n\n∂ϕs\n∂ϕs\nn\n+ ∆t u\neia Rab\nτkj + (τils ) u\neja Rab b , veir Rrm m\n∂xk\n∂xl\n∂xj Ωsh\n\u0001\n\u0001\ns\nf\nf\n= h̄i , veim ϕm ΓN h + ρ (gi , veim ϕm )Ωh + ρ − ρ (gi , veir Rrm ϕsm )Ωsh\n\u0012\n\u0013\nn ∂un\nn\n\u0001\n∂unj\n∂ϕsm\nj\ns\n2 ∂ui\n2 ∂ui\ns n\ns n\n+ µ ∆t\n+ ∆t\n(τ )\n− τij , veir Rrm\n.\n∂xk ∂xk\n∂xk kl ∂xl\n∂xj Ωsh\ne= u\neT\neT\nLet u\n1 ,u\n2\nmatrix form:\n\n\u0001T\n\ne= v\ne1T , v\ne2T\nand v\n\n\u0001T\n\n(52)\n\n, we then express (52) in the following\n\ne−u\nen\ne−u\ne∗\nu\nu\ne T DT Ms D\n+v\n∆t\n∆t\ne T Ke\ne T Be\neT BT u\ne+v\ne T DT Ks De\n+v\nu+v\np+q\nu\n\neT M\nv\n\n(53)\n\neT f + v\ne T DT f s ,\n=v\nor\neT , q\neT\nv\n\n\u0014\n\u0001 A\nBT\n\n\u0015\u0012 \u0013\n\u0012 \u0013\n\u0001 b\ne\ne\nB u\neT , q\neT\n= v\n,\ne\np\n0\n0\n\n(54)\n\nwhere\nA = M/∆t + K + DT (Ms /∆t + Ks ) D\nand\nb = f + DT f s + Me\nu∗ /∆t + DT Ms De\nun /∆t.\nThe matrix\nM=ρ\n\nf\n\n\u0014\nM11\n\n\u0015\nM22\n\n14\n\n(55)\n\n\fis the velocity mass matrix of the fluid, where\n\u0001\n(M11 )km = (M22 )km = (ϕk , ϕm )Ωh , k, m = 1, 2, · · · N f .\nThe matrix\ns\n\ns\n\nM = ρ −ρ\n\nf\n\n\u0014\n\u0001 Ms11\n\n\u0015\n(56)\n\nMs22\n\nis the velocity mass matrix of the solid, where\n(Ms11 )km = (Ms22 )km = (ϕsk , ϕsm )Ωsh , (k, m = 1, 2, · · · N s ) .\nK is the stiffness matrix of the fluid:\n\u0014\nK11\nK = µf\nK21\nwhere\n\n\u0012\n(K11 )km = 2\n\n∂ϕk ∂ϕm\n,\n∂x1 ∂x1\n\n\u0015\nK12\n,\nK22\n\u0012\n\n\u0013\n+\nΩh\n\n∂ϕk ∂ϕm\n,\n∂x2 ∂x2\n\n(57)\n\n\u0013\n,\nΩh\n\n\u0012\n\n\u0012\n\u0013\n\u0013\n∂ϕk ∂ϕm\n∂ϕk ∂ϕm\n(K22 )km = 2\n+\n,\n,\n,\n∂x2 ∂x2 Ωh\n∂x1 ∂x1 Ωh\n\u0012\n\u0013\n∂ϕk ∂ϕm\n(K12 )km =\n,\n,\n∂x1 ∂x2 Ωh\n\u0012\n\u0013\n∂ϕk ∂ϕm\n(K21 )km = (K12 )mk =\n,\n,\n∂x2 ∂x1 Ωh\nand k, m = 1, 2, · · · N f .\nKs is the stiffness matrix of the solid:\n\u0014 s\nK11\nKs =\nKs21\n\n\u0015\nKs12\n,\nKs22\n\n(58)\n\nwhere\n\u0012 s\n\u0013\n\u0012 s\n\u0013\n∂ϕb ∂ϕsm\n∂ϕb ∂ϕsm\n,\n+ µs ∆t\n,\n(Ks11 )bm = µs ∆t2\n∂x1 ∂x1 Ωsh\n∂x2 ∂x2 Ωsh\n\u0012 s n\n\u0013\n\u0012 s n\n\u0013\ns\n∂ϕb ∂u1 ∂ϕm\n∂ϕb ∂u2 ∂ϕsm\n+ 2µs ∆t2\n,\n+ µs ∆t2\n,\n∂xk ∂xk ∂x1 Ωsh\n∂xk ∂xk ∂x2 Ωsh\n\u0013\n\u0012 s\n\u0013\n\u0012 s\nn\ns\n∂ϕb s n ∂u1 ∂ϕm\n∂ϕb s n ∂un2 ∂ϕsm\n+ 2∆t2\n(τkl )\n,\n+ ∆t2\n(τkl )\n,\n∂xk\n∂xl ∂x1 Ωsh\n∂xk\n∂xl ∂x2 Ωsh\n\u0012 s\n\u0013\n\u0012 s\n\u0013\ns\ns\n∂ϕb s n ∂ϕm\n∂ϕb s n ∂ϕm\n+ 2∆t\n(τ ) ,\n+ ∆t\n(τ ) ,\n.\n∂xk k1\n∂x1 Ωsh\n∂xk k2\n∂x2 Ωsh\nIt can be seen from the pattern of the above matrices that one can get Ks22\nby changing the subscript 1 to 2, and changing 2 to 1 in the formula of Ks11 .\n\n15\n\n\fSimilarly, the elements of Ks12 can be expressed as:\n\u0012 s\n\u0013\n\u0013\n\u0012 n\n∂ϕb ∂ϕsm\n∂u1 ∂ϕsb ∂ϕsm\ns\ns\ns\n2\n(K12 )bm = µ ∆t\n,\n+ µ ∆t\n,\n∂x1 ∂x2 Ωsh\n∂xk ∂xk ∂x2 Ωsh\n\u0012\n\u0012 n\n\u0013\n\u0013\ns\ns\n∂u1 s n ∂ϕsb ∂ϕsm\n2\ns n ∂ϕb ∂ϕm\n+ ∆t\n+ ∆t (τ1k )\n,\n(τ )\n,\n,\n∂xk kl ∂xl ∂x2 Ωsh\n∂xk ∂x2 Ωsh\nand (Ks21 )bm = (Ks12 )mb , (b, m = 1, 2, · · · N s ).\nThe matrix B has the following expression.\n\u0014 \u0015\nB1\nB=\n,\nB2\nwhere\n\n\u0012\n(B1 )mk =\n\n(k = 1, 2, · · · N\n\np\n\n∂ϕm\nψk ,\n∂x1\n\n(59)\n\n\u0012\n\n\u0013\n, (B2 )mk =\nΩh\n\nand m = 1, 2, · · · N\n\nf\n\n∂ϕm\nψk ,\n∂x2\n\n\u0013\nΩh\n\n\u0001\n\n. The vector\n\u0012 \u0013\nf\nf= 1\nf2\n\n(60)\n\nis the fluid force vector, where\n(f1 )m = ρf (g1 , ϕm )Ωh + h̄1 , ϕm\n\n\u0001\nΓN h\n\n,\n\nand\n(f2 )m = ρf (g2 , ϕm )Ωh + h̄2 , ϕm\nm = 1, 2, · · · N\n\nf\n\n\u0001\n\n\u0001\nΓN h\n\n. The vector\nfs =\n\n\u0012 s\u0013\nf1\nf2s\n\n(61)\n\nis the solid force vector, where\n\u0001\n(f1s )m = ρs − ρf (g1 , ϕsm )Ωsh\n\u0012\n\u0013\ns\n\u0001\n∂un ∂unj\n∂un s n ∂unj\ns n ∂ϕm\n+ µs ∆t2 1\n+ ∆t2 1 (τkl\n)\n− τ1j\n,\n∂xk ∂xk\n∂xk\n∂xl\n∂xj Ωsh\nand\n\u0001\n(f2s )m = ρs − ρf (g2 , ϕsm )Ωsh\n\u0012\n\u0013\ns\n\u0001\n∂un ∂unj\n∂un s n ∂unj\ns n ∂ϕm\n+ µs ∆t2 2\n+ ∆t2 2 (τkl\n)\n− τ2j\n,\n∂xk ∂xk\n∂xk\n∂xl\n∂xj Ωsh\n(m = 1, 2, · · · N s ). Finally, matrix D is the FEM interpolation matrix which\ncan be expressed as:\n\u0014 T\n\u0015\nR\nD=\n.\n(62)\nRT\n16\n\n\fe and q\ne, one can obtain the\nUsing the arbitrariness of our test vectors v\nfollowing linear algebraic equation for the whole FSI system from equation (54):\n\u0014\n\u0015\u0012 \u0013 \u0012 \u0013\ne\ne\nA B u\nb\n=\n.\n(63)\nT\ne\np\nB\n0\n0\n4.4. The UFEM algorithm\n200\n\n205\n\n210\n\n215\n\nHaving derived a discrete system of equations we now describe the solution\nalgorithm at each time step.\n\u001a f \u0001n\ne\nu\nin Ωf\nn\nen =\n(1) Given the solid configuration (xs ) and velocity field u\ns n\n(e\nu )\nin Ωs\nat time step n.\n(2) Discretize the convection equation (26) or (33) and solve it to get an intermediate velocity u∗ .\n(3) Compute the interpolation matrix and solve equation (63) using u∗ and\nn\ne n+1 .\n(e\nus ) as initial values to get velocity field u\nn+1\n(4) Compute solid velocity (e\nus )\n= De\nun+1 and update the solid mesh by\ns n+1\ns n\ns n+1\n(x )\n= (x ) + ∆t (e\nu )\n, then go to step (1) for the next time step.\nRemark 4 When implementing the UFEM algorithm, it is unnecessary to\nperform the matrix multiplication DT Ks D in (53) globally, because the FEM\ninterpolation is locally based. All the matrix operations can be computed based\non the local element matrices only. Alternatively, if an iterative solver is used,\nT s\nit is actually unnecessary\n\u0001 to compute D K D. What an iterative step needs\nT s\nis to compute D K D u for a given vector u, therefore one can compute Du\nfirst, then Ks (Du), and last DT (Ks Du).\n5. Numerical experiments\n\n220\n\n225\n\nIn this section, we present some numerical examples that have been selected\nto allow us to assess our proposed UFEM. We shall demonstrate the convergence of UFEM in time and space, and compare results obtained by the UFEM\nwith those obtained using monolithic approaches and IFEM, as well as compare\nagainst results from laboratory experiment.\nIn order to improve the computational efficiency, an adaptive spatial mesh\nwith hanging nodes is used in all the following numerical experiments. Readers\ncan reference Appendix A for details of the treatment of hanging nodes.\n5.1. Oscillation of a flexible leaflet oriented across the flow direction\n\n230\n\nThis numerical example is used by [15, 16, 17] to validate their methods. We\nfirst use the same parameters as used in the above three publications in order\nto compare results and test convergence in time and space, then use a range\nof parameters to show the robustness of our UFEM. The implicit Least-squares\nmethod is used to treat the convection step in all these tests unless otherwise\n\n17\n\n\fFigure 2:\n\n235\n\nComputational domain and boundary conditions, taken from [16].\n\nstated. The computational domain and boundary conditions are illustrated in\nFigure 2.\nThe inlet flow is in the x-direction and given by ux = 15.0y (2 − y) sin (2πt).\nGravity is not considered in the first test (i.e. g = 0), and other fluid and solid\nproperties are presented in Table 1.\nFluid\nL = 4.0 m\nH = 1.0 m\nρf = 100 kg/ m3\nµf = 10 N · s/ m2\n\nLeaflet\nw = 0.0212 m\nh = 0.8 m\nρs = 100 kg/ m3\nµs = 107 N / m2\n\nTable 1: Properties and domain size for test problem\nwith a leaflet oriented across the flow direction.\n\n240\n\n245\n\n250\n\nThe leaflet is approximated with 1200 linear triangles with 794 nodes (medium\nmesh size), and the corresponding fluid mesh is adaptive in the vicinity of the\nleaflet so that it has a similar size. A stable time step ∆t = 5.0 × 10−4 s is\nused in these initial simulations. The configuration of the leaflet is illustrated\nat different times in Figure 3.\nPreviously published numerical results are qualitatively similar to those in\nFigure 3 but show some quantitative variations. For example, [16] solved a\nfully-coupled system but the coupling is limited to a line, and the solid in their\nresults (Figure 7 (l)) behaves as if it is slightly harder. Alternatively, [15] used\na fractional step scheme to solve the FSI equations combined with a penalty\nmethod to enforce the incompressibility condition. In their results (Fig. 3 (h))\nthe leaflet behaves as if it is slightly softer than [16] and harder than [17]. In [17]\na beam formulation is used to describe the solid. The fluid mesh is locally refined\nusing hierarchical B-Splines, and the FSI equation is solved monolithically. The\nleaflet in their results (Fig. 34) behaves as softer than the other two considered\nhere. Our results in Figure 3 are most similar to those of [17]. This may be\nseen more precisely by inspection of the graphs of the oscillatory motion of the\nleaflet tip in Figure 4 corresponding to Fig. 32 in [17]. We point out here that\n18\n\n\f(a) t = 0.1s\n\n(b) t = 0.2s\n\n(c) t = 0.6s\n\n(d) t = 0.8s\n\nFigure 3:\n\n255\n\n260\n\nConfiguration of leaflet and magnitude of velocity on the adaptive fluid mesh.\n\nthe explicit Taylor-Galerkin method is also used to solve the convection step\nfor this test, and we gain almost the same accuracy using the same time step\n∆t = 5.0 × 10−4 s.\nHaving validated our results for this example against the work of others, we\nshall use this test case to further explore more details of our method.\nWe commence by testing the influence of the ratio of fluid and solid mesh\nsizes rm =(local fluid element area)/(solid element area). Fixing the fluid mesh\nsize, three different solid mesh sizes are chosen: coarse (640 linear triangles with\n\n19\n\n\fFigure 4:\n\n265\n\n270\n\nEvolution of horizontal and vertical displacement at top right corner of the leaflet.\n\n403 nodes rm ≈ 1.5), medium (1200 linear triangles with 794 nodes rm ≈ 3.0)\nand fine (2560 linear triangles with 1445 nodes rm ≈ 5.0), and a stable time\nstep ∆t = 5.0 × 10−4 s is used. From these tests we observe that there is a slight\ndifference in the solid configuration for different meshes, as illustrated at t = 0.6s\nin Figure 5, however the difference in displacement decreases as the solid mesh\nbecomes finer. Further, we found that 1.5 ≤ rm ≤ 5.0 ensures the stability of\nthe proposed UFEM approach. Note that we use a 9-node quadrilateral for the\nfluid velocity and 3-node triangle for solid velocity, so rm ≈ 3.0 means the fluid\nand solid mesh locally have a similar number of nodes for velocity.\n\n(a)coarse\n\n(b)medium\n\n(c)fine\n\nFigure 5:\n\nConfiguration of leaflet for different mesh ratio rm ,\nand contour plots of displacement magnitude at t = 0.6s.\n\n275\n\nWe next consider convergence tests undertaken for refinement of both the\nfluid and solid meshes with the fixed ratio of mesh sizes rm ≈ 3.0. Four different\nlevels of meshes are used, the solid meshes are: coarse (584 linear triangles\nwith 386 nodes), medium (1200 linear triangles with 794 nodes), fine (2560\nlinear triangles with 1445 nodes), and very fine (3780 linear triangles with 2085\nnodes). The fluid meshes have the corresponding sizes with the solid at their\nmaximum refinement level. As can be seen in Figure 6 and Table 2, the velocity\n20\n\n\fis converging as the mesh becomes finer.\n\n(a) Coarse\n\n(b) Medium\n\n(c) Fine\n\n(d) Very fine\n\nFigure 6:\n\nContour plots of horizontal velocity at t = 0.5s.\n\nBetween different mesh sizes\ncoarse and medium\nmedium and fine\nfine and very fine\n\nDifference of maximum\nhorizontal velocity at t = 0.5s\n0.01497\n0.00214\n0.00190\n\nTable 2: Comparison of maximum velocity for different meshes.\n280\n\nIn addition, we consider tests of convergence in time using a fixed ratio of\nfluid and solid mesh sizes rm ≈ 3.0. Using the medium solid mesh size and the\nsame fluid mesh size as above, results are shown in Figure 7 and Table 3. From\nthese it can be seen that the velocities are converging as the time step decreases.\nSteps sizes compared\n∆t = 2.0 × 10−3 and ∆t = 1.0 × 10−3\n∆t = 1.0 × 10−3 and ∆t = 5.0 × 10−4\n∆t = 5.0 × 10−4 and ∆t = 2.5 × 10−4\n\nDifference of maximum\nhorizontal velocity at t = 0.5s\n0.00854\n0.00517\n0.00263\n\nTable 3: Comparison of maximum velocity for different time step size.\n\n285\n\nFinally, in order to assess the robustness of our approach, we vary each of\nthe physical parameters using three different cases as shown in Figure 8. A\nmedium mesh size with fixed rm ≈ 3.0 is used to undertake all of these tests.\n21\n\n\f(a) ∆t = 2.0 × 10−3 s\n(breaks down at t = 0.61s).\n\n(b) ∆t = 1.0 × 10−3 s.\n\n(c) ∆t = 5.0 × 10−4 s.\n\n(d) ∆t = 2.5 × 10−4 s.\n\nFigure 7:\n\nContour plots of horizontal velocity at t = 0.5s.\n\n(a) ρr = 1, Re = 100 and F r = 0.\n\n(b) ρr = 1, µ̄s = 103 and F r = 0.\n\n(c) Re = 100, µ̄s = 103 and F r = 0.\n\n(d) Re = 100 and µ̄s = 103 .\n\nFigure 8:\n\nParameters sets and results, ∆t = 5.0 × 10−4 s for Group (b)∼(d).\n\nThe dimensionless parameters shown in Figure 8 are defined as: ρr =\nµs\n, Re\nρf U 2\n\n290\n\nρf U H\nµf\n\ngH\nU2\n\nρs\n, µ̄s\nρf\n\n=\n\n=\nand F r =\nwhere the average velocity U = 10 in this\nexample. T = 1 is the period of inlet flow.\nIt can be seen from the results of group (a) that the larger the value of\n\n22\n\n\f295\n\n300\n\nshear modulus µ̄s the harder the solid behaves, however a smaller time step is\nrequired. For the case of µ̄s = 109 , the solid behaves almost like a rigid body,\nas we would expect. From results of group (b), it is clear that the Reynolds\nNumber (Re) has a large influence on the behavior of the solid. The density\nand gravity have relatively less influence on the behavior of solid in this problem\nwhich can be seen from the results of group (c) and group (d).\n5.2. Oscillation of a flexible leaflet oriented along the flow direction\nThe following test problem that we consider is taken from [26], which describes an implementation on a ALE fitted mesh. It has since been used as a\nbenchmark to validate different numerical schemes [17, 18]. The geometry and\nboundary conditions are shown in Figure 9.\n\nFigure 9:\n\nComputational domain and boundary condition for oscillation of flexible leaflet.\n\n(a) Leaflet displacement and fluid pressure.\n\nFigure 10:\n\n305\n\n(b) Mesh refinement near the structure.\n\nContour plots of leaflet displacement and fluid pressure.\n\nFor the fluid, the viscosity and density are µf = 1.82 × 10−4 and ρf =\n1.18×10−3 respectively. For the solid, we use shear modulus µs = 9.2593×10−5\nand density ρs = 0.1. The leaflet is divided by 1063 3-node linear triangles with\n666 nodes, and the corresponding fluid mesh locally has a similar node density\nto the leaflet (rm ≈ 3.0). First the Least-squares method is tested and a stable\ntime step ∆t = 1.0 × 10−3 s is used. A snapshot of the leaflet deformation\n23\n\n\fFigure 11:\n\n310\n\n315\n\n320\n\n325\n\nDistribution of pressure across the leaflet on the three lines in Figure 10 (b).\n\nand fluid pressure at t = 5.44s are illustrated in Figure 10. In Figure 11, the\ndistributions of pressure across the leaflet corresponding to the three lines (AB,\nCD and EF) in Figure 10 (b) are plotted, from which we can observe that the\nsharp jumps of pressure across the leaflet are captured.\nThe evolution of the vertical displacement of the leaflet tip with respect to\ntime is plotted in Figure 12(a). Both the magnitude (1.34) and the frequency\n(2.94) have a good agreement with the result of [26], using a fitted ALE mesh\nand of [17], using a monolithic unfitted mesh approach. The Taylor-Galerkin\nmethod is also tested which uses ∆t = 2.0 × 10−4 s as a stable time step, and a\ncorresponding result is shown in 12(b) which has a similar magnitude (1.24) and\nfrequency (2.86). These results are all within the range of values in [17, Table\n4]. Note that since the initial condition before oscillation for these simulations\nis an unstable equilibrium, the first perturbation from this regime is due to\nnumerical disturbances. Consequently, the initial transient regimes observed for\nthe two methods (implicit Least-squares and explicit Taylor-Galerkin methods)\nare quite different. It is possible that an explicit method causes these numerical\nperturbations more easily, therefore makes the leaflet start to oscillate at an\nearlier stage than when using the implicit Least-squares approach.\n5.3. Solid disc in a cavity flow\n\n330\n\n335\n\nThis numerical example is used to compare our UFEM with the IFEM, which\nis cited in [11, 27]. In order to compare some details, we also implement the\nIFEM, but we implemented it on an adaptive mesh with hanging nodes, and we\nuse the isoparametric FEM interpolation function rather than the discretized\ndelta function or RKPM function of [9, 10].\nThe fluid’s density and viscosity are 1 and 0.01 respectively, and the following\nsolid properties are chosen to undertake the tests: ρs =1 and µs =0.1 or 1. The\nhorizontal velocity on the top boundary of the cavity is prescribed as 1 and the\nvertical velocity is fixed to be 0 as shown in Figure 13. The velocities on the\n\n24\n\n\f(a) Implicit Least-squares method.\n\nFigure 12:\n\n(b) Explicit Taylor-Galerkin method.\n\nDisplacement of leaflet tip as a function of time.\n\nother three boundaries are all fixed to be 0, and pressure at the bottom-left\npoint is fixed to be 0 as a reference point.\n\nFigure 13:\n\nFigure 14:\n\nComputational domain for\ncavity flow, taken from [27].\n\n340\n\n345\n\n350\n\nAdaptive mesh for\ncavity flow.\n\nIn order to compare the UFEM and IFEM, we use the same meshes for fluid\nand solid: the solid mesh has 2381 nodes and the fluid mesh locally has a similar\nnumber of nodes (adaptive, see Figure 14). First the implicit Least-squares\nmethod is used to solve the convection step, and the time step is ∆t = 1.0×10−3 .\nFigure 15 and Figure 16 show the configuration of the disc at different stages,\nfrom which we do not observe significant differences of the velocity norm even\nfor a long run as shown in Figure 16 (b). Then the explicit Taylor-Galerkin\nmethod is tested, and we achieve almost the same accuracy by using the same\ntime step. The magnitudes of velocity at the same stages of Figure 16 are\npresented in Figure 17.\nWe should mention that for the case µs = 0.1, as the disc arrives at the\ntop of the cavity (time > 3.0) the quality of the solid mesh does begin to\ndeteriorate using our UFEM. We do not currently seek to improve the mesh\nquality (using an arbitrary Lagrangian-Eulerian (ALE) update [7], for example)\nhowever this would be necessary in order to reduce the shear modulus further\n25\n\n\f(a) t = 2.0s.\n\n(b) t = 3.0s.\n\nFigure 15:\n\n355\n\nVelocity norm for a soft solid (µs = 0.1) in a driven cavity flow\nusing UFEM (left) and IFEM (right).\n\nwithout compromising the quality of the solid mesh.\nConversely, a large µs makes the solid behave like a rigid body. For the\nproposed UFEM, we can use µs = 100 or larger without changing the time step,\nwhereas for the IFEM the simulation always breaks down for µs = 100, however\nsmall the time step, due to the huge FSI force on the right-hand side of the FSI\nsystem.\n5.4. Solids in a channel with gravity\n\n360\n\n365\n\nWe first simulate a falling disc due to gravity in order to further validate the\naccuracy of the UFEM. We then show a simulation of the evolution of different\nshapes of solids falling and rising in a channel in order to show the flexibility\nand robustness of the proposed UFEM.\nThe test of a falling disc in a channel is cited by [10, 18] in order to validate\nthe IFEM and a monolithic method respectively. The computational domain\nand parameters are illustrated in Figure 18 and Table 4 respectively. The fluid\nvelocity is fixed to be 0 on all boundaries except the top one.\n\n26\n\n\f(a) t = 5.0s.\n\n(b) t = 25.0s.\n\nFigure 16: Velocity norm for a soft solid (µs = 1.0) in a driven cavity flow\nusing UFEM (left) and IFEM (right), Least-squares method for convection step.\n\n(a) t = 5.0s\n\nFigure 17:\n\n(b) t = 25.0s\n\nVelocity norm for µs = 1.0, Taylor-Galerkin method for convection step.\n\nThere is also an empirical solution of a rigid ball falling in a viscous fluid\n[18], for which the terminal velocity, ut , under gravity is given by\n\u0001\n\u0012 \u0012 \u0013\n\u0010 r \u00112\n\u0010 r \u00114 \u0013\nρs − ρf gr2\nL\nln\n− 0.9157 + 1.7244\n− 1.7302\n, (64)\nut =\n4µf\nr\nL\nL\n\n370\n\nwhere ρs and ρf are the density of solid and fluid respectively, µf is viscosity\nof the fluid, g = 980 cm/ s2 is acceleration due to gravity, L = W / 2 and r is\nthe radius of the falling ball. We choose µs = 108 dyne/ cm2 to simulate a\n27\n\n\fFigure 18:\n\nFigure 19:\n\nComputational domain for different\nshapes of solids with different properties.\n\nComputational domain for\na falling disc.\n\nFluid\nW = 2.0 cm\nH = 4.0 cm\nρf = 1.0 g/ cm3\nµf = 1.0 dyne · s/ cm2\ng = 980 cm/ s2\n\nDisc\nd = 0.0125 cm\nh = 0.5 cm\nρs = 1.2 g/ cm3\nµs = 108 dyne/ cm2\ng = 980 cm/ s2\n\nTable 4: Fluid and material properties of a falling disc.\n\n375\n\n380\n\n385\n\nrigid body here, and µs = 1012 dyne/ cm2 is also applied, which gives virtually\nidentical result.\nThree different meshes are used: the disc boundary is represented with 28\nnodes (coarse), 48 nodes (medium), or 80 nodes (fine). The fluid mesh near\nthe solid boundary has the same mesh size, and a stable time step t = 0.005s\nis used for all the three cases. The Least-squares method is used to treat the\nconvection step in all these tests. A local snapshot of the vertical velocity with\nthe adaptive mesh is shown in Figure 20. From the fluid velocity pattern around\nthe disc, we can observe that the disc behaves like a rigid body as expected. In\naddition, the evolution of the velocity of the mid-point of the disc is shown in\nFigure 21, from which it can be seen that the numerical solution converges from\nbelow to the empirical solution.\nReference [18] uses a monolithic method to simulate multiple rigid and deformable discs in a gravity channel. We have implemented this example and\nobtain very similar results. Rather than replicate these here however, we instead show a more complex example, as illustrated in Figure 19. The computational domain, boundary conditions and the fluid properties are the same as\nthe above one-disc test. All the solids are numbered at their initial positions as\n\n28\n\n\fFigure 20:\n\nContour of vertical velocity at t = 1s (fine mesh).\n\nFigure 21: Evolution of velocity at the center of a falling disc.\n(The blue solid line represents the empirical solution from formula (64).)\n\n390\n\n395\n\nshown in Figure 19 with A(0, −1), B(0.2, −1.2), C(−0.5, −1.1), D(−0.5, −1.5),\nE(−0.2, −1.3), F (−0.7, −2.9) and G(0, −3). The center and radius (r1 ) of the\n3rd solid (circle) are (0, −2) and 0.2 respectively, and the center and radius (r2 )\nthe 4th solid (octagon) are (0.3, −2.7) and 0.2 respectively. The solid properties\nare illustrated in Table 5.\nA high resolution of each solid boundary is used in this simulation as shown\nin Figure 22 (a), which can guarantee the mesh quality during the whole process\nof evolution, and a stable time step t = 0.002s is used. Snapshots of the solids\nat different times are shown in Figure 22 and 23.\n6. Conclusion and future works\n\n400\n\nIn this article we introduce a new unified finite element method (UFEM) for\nfluid-structure interaction, which can be applied to a wide range of problems,\n29\n\n\fNo. of solid\n1\n2\n3\n4\n5\n\nDensity g/ cm3\n1.3\n1.2\n1.0\n0.8\n0.7\n\n\u0001\n\nShear modulus dyne/ cm2\n104\n103\n10\n106\n102\n\n\u0001\n\nTable 5: Properties for multi-solids falling in a channel as shown in Figure 19.\n\n405\n\n410\n\n415\n\n420\n\nfrom small deformation to very large deformation and from very soft solids\nthrough to very rigid solids. Several numerical examples, which are widely used\nin the literature of Immersed FEM and monolithic methods, are implemented\nto validate the proposed UFEM.\nThe UFEM combines features from the IFEM and from monolithic methods. Nevertheless, it differs from each of them in the following aspects. Firstly,\nUFEM is a semi-explicit (explicitly linearizing the constitutive equation of solid\nand implicitly coupling FSI interaction) scheme, similar to IFEM, however\nUFEM solves the solid equations and fluid equations together while the classical\nIFEM does not solve the solid equations; secondly, both UFEM and monolithic\nmethods solve solid equations, however UFEM solves one velocity field in the\nsolid domain using FEM interpolation, while monolithic methods solve one velocity field and one displacement field in the solid domain using Lagrangian\nmultipliers. In summary therefore we believe that UFEM has the potential to\noffer the robustness and range of operation of monolithic methods, but at a\ncomputational cost that is much closer to that of the immersed finite element\nmethods.\nThe following generalizations of our proposed UFEM approach will be considered in the future: (1) Implementation in 3D using adaptive mesh with hanging nodes; (2) implementation for non-Newtonian flow; (3) an efficient preconditioned iterative solver for the UFEM algebraic system; (4) a second order\nsplitting scheme in time.\nAppendix A. A method to treat hanging nodes\n\n425\n\n430\n\nAn adaptive mesh with hanging nodes reduces the number of degrees of freedom compared to uniform refinement, hence, decreases the cost of computation.\nHowever, the nature of hanging nodes has the potential to cause discontinuity\nand breaks the framework of the finite element shape functions, which, therefore,\nneeds special treatment in finite element codes.\nIn order to treat the hanging nodes, one can construct a conforming shape\nfunction [28, 29] or constrain and cancel the degree of freedom at the hanging\nnodes [29, 30]. The former is very appealing and leads to optimal convergence,\nbut it is difficult to extend to high-order shape functions [31]. In this article we\nwill adopt the latter method and only use 2-level hanging nodes, which means\nat most 2 hanging nodes are allowed in one element (this can be guaranteed by\n30\n\n\f(a) t = 0.0\n\n(c) t = 0.5\n\n(d) t = 0.6\n\n(e) t = 0.7\n\n(f) t = 0.8\n\n(g) t = 0.9\n\n(h) t = 1.0\n\nFigure 22:\n\n435\n\n440\n\n(b) t = 0.3\n\nContours of vertical velocity at different times.\n\nimposing safety layers to ensure that neighbouring element nodes differ by more\nthan one level of refinement). The implementation of arbitrary-level hanging\nnodes can be found in [31, 32, 33].\nFor a quadrilateral element, when the velocity is interpolated by biquadratic\nshape functions and the pressure is interpolated by bilinear shape functions, the\nimplementation of hanging nodes must be different for each, as shown in Figure\n\n31\n\n\f(i) t = 1.1\n\n(j) t = 1.2\n\n(m) t = 1.6\n\nFigure 23:\n\n445\n\n(k) t = 1.3\n\n(n) t = 1.8\n\n(o) t = 2.0\n\nContours of vertical velocity at different times\n\n(l) t = 1.4\n\n(p) t = 2.4\n\n(continued).\n\nA.1.\nFor example, when velocity is interpolated, point D is a hanging node for\nelement II, and point E is a hanging node for element III. When pressure is\ninterpolated, point C is a hanging node for both the element II and III. Take\nelement II for example, if we use the constraint method to cancel the hanging\nnodes degree of freedom, then\nuD\ni =\n\n3 A 1 B 3 C\nu − ui + ui\n8 i\n8\n4\n\n32\n\ni = (1, 2)\n\n(A.1)\n\n\fFigure A.2: Element II in Figure A.1\nin the reference coordinate system.\n\nFigure A.1: Elements with hanging\nnodes.\n\n1 A 1 B\np + p\n(A.2)\n2\n2\nwhere ui and p are velocity components and pressure respectively defined at the\ncorresponding nodes. The interpolation coefficients can be calculated by putting\nedge AB in a one dimensional finite element reference coordinate system.\nNotice that when computing the element matrix II, point B is outside of the\nelement, but the element matrix II still contributes to node B because of the\nhanging node D. So we can treat the two points, B and D, as a master-slave\ncouple, which means letting them share the same equation number in the final\nglobal linear equation system. However one should modify the element matrix\nII according to (A.1) and (A.2) in the following way before assembling it to the\nglobal matrix.\nSuppose the element II is enumerated in the reference coordinate system\nas shown in Figure A.2. Then, formulae (A.1) and (A.2) imply the following\nequations:\npC =\n\n450\n\n455\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nu1i\nu2i\nu3i\nu4i\nu5i\nu6i\nu7i\nu8i\nu9i\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n = Dv \n\n\n\n\n\n\n\n\n\n\n\n\n\nu1i\nu2i\nu3i\nu4i\nu5i\nuB\ni\nu7i\nu8i\nu9i\n\n\n\n\n\n\n\n\n,\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\nv\nD =\n\n\n\n\n\n\n\n33\n\n\n1\n1\n1\n1\n3\n4\n\n3\n8\n\n− 18\n1\n1\n\n\n\n\n\n\n\n . (A.3)\n\n\n\n\n\n\n1\n\n\f\n\np1\n p2 \n\n 3  = Dp \n p \n\n4\np\n\n\n\np1\npB \n,\np3 \np4\n\n\n1\n\np\n\nD =\n\n\n1\n2\n\n1\n2\n\n\n.\n\n1\n\n1\n\n(A.4)\n\nOne should use matrices Dv and Dp to modify the element matrix II. Suppose Ke is the stiffness matrix of element II without consideration of hanging\nnodes, and the unknowns are arranged in the following column vector.\nu11 , u21 , · · · u91 , v11 , v12 · · · v19 , p1 , p2 · · · p4\n460\n\nfor j=1 to n\nkji1 = kji1 + kji0 · 3/8\nkji2 = kji2 + kji0 · 3/4\nend\n\nfor j=1 to n\nki0 j = −ki0 j /8\nkji0 = −kji0 /8\nend\n\nfor j=1 to n\nkji1 = kji1 + kji2 /2\nend\n\nfor j=1 to n\nki2 j = −ki2 j /2; kji2 = −kji2 /2\nend\n\nwhere i1 = 21 and i2 = 20 based on (A.5). Executing all the above pieces of\ncodes is equivalent to performing the following matrix multiplication.\n v\nD\n\n\nT\nDv\np\n\n v\nD\n Ke \n\nD\n\n475\n\n(A.5)\n\nLet i0 = 6, i1 = 3, and i2 = 2 (based on (A.5)), sequentially executing the\nabove three pieces of codes would modify the matrix Ke corresponding to the\nfirst component of velocity, and let i0 = 15, i1 = 12, and i2 = 11 (based on\n(A.5)), executing the above codes would modify the matrix Ke corresponding\nto the second component of velocity. Similarly, in order to modify the matrix\ncorresponding to pressure, one can execute the following codes which are based\non formula (A.2):\nfor j=1 to n\nki1 j = ki1 j + ki2 j /2\nend\n\n470\n\n.\n\nIt is clear that Ke = [kij ] is a n×n (n=22) matrix, and it could be modified by\nthe following pseudocode, which distribute the contribution of hanging nodes\nto the corresponding nodes according to formula (A.1).\nfor j=1 to n\nki1 j = ki1 j + ki0 j · 3/8\nki2 j = ki2 j + ki0 j · 3/4\nend\n\n465\n\n\u0001T\n\n\nDv\nDp\n\n.\n\n(A.6)\n\nThe modification of the mass matrix is similar but easier if a lumped mass\nis adopted, though it is unnecessary to present details here. Once the element\nmatrix is modified, it can then be assembled directly to the global matrix and\ntherefore implement the constraint of the hanging nodes, because the hanging\nnode shares the same equation number with its related node in the neigbouring\nelement.\n\n34\n\n\fReferences\nReferences\n480\n\n485\n\n[1] G. Hou, J. Wang, A. Layton, Numerical methods for fluid-structure interactiona review, Commun. Comput. Phys 12 (2) (2012) 337–377. doi:\n10.4208/cicp.291210.290411s.\n[2] U. Küttler, W. A. Wall, Fixed-point fluid–structure interaction solvers with\ndynamic relaxation, Computational Mechanics 43 (1) (2008) 61–72. doi:\n10.1007/s00466-008-0255-5.\n[3] J. Degroote, K.-J. Bathe, J. Vierendeels, Performance of a new partitioned procedure versus a monolithic procedure in fluid–structure interaction, Computers & Structures 87 (11-12) (2009) 793–801. doi:10.1016/\nj.compstruc.2008.11.013.\n\n490\n\n495\n\n[4] M. Heil, An efficient solver for the fully coupled solution of largedisplacement fluid–structure interaction problems, Computer Methods in\nApplied Mechanics and Engineering 193 (1-2) (2004) 1–23. doi:10.1016/\nj.cma.2003.09.006.\n[5] M. Heil, A. L. Hazel, J. Boyle, Solvers for large-displacement fluid–structure\ninteraction problems: segregated versus monolithic approaches, Computational Mechanics 43 (1) (2008) 91–101. doi:10.1007/s00466-008-0270-6.\n\n500\n\n[6] R. L. Muddle, M. Mihajlović, M. Heil, An efficient preconditioner for\nmonolithically-coupled large-displacement fluid–structure interaction problems with pseudo-solid mesh updates, Journal of Computational Physics\n231 (21) (2012) 7315–7334. doi:10.1016/j.jcp.2012.07.001.\n\n505\n\n[7] R. C. Peterson, P. K. Jimack, M. A. Kelmanson, The solution of\ntwo-dimensional free-surface problems using automatic mesh generation, International journal for numerical methods in fluids 31 (6)\n(1999) 937–960. doi:10.1002/(SICI)1097-0363(19991130)31:6<937::\nAID-FLD906>3.0.CO;2-p.\n[8] M. A. Walkley, P. H. Gaskell, P. K. Jimack, M. A. Kelmanson, J. L.\nSummers, Finite element simulation of three-dimensional free-surface\nflow problems, J Sci Comput 24 (2) (2005) 147–162. doi:10.1007/\ns10915-004-4611-0.\n\n510\n\n515\n\n[9] L. Zhang, A. Gerstenberger, X. Wang, W. K. Liu, Immersed finite element method, Computer Methods in Applied Mechanics and Engineering\n193 (21) (2004) 2051–2067. doi:doi:10.1016/j.cma.2003.12.044.\n[10] L. Zhang, M. Gay, Immersed finite element method for fluid-structure\ninteractions, Journal of Fluids and Structures 23 (6) (2007) 839–857.\ndoi:10.1016/j.jfluidstructs.2007.01.001.\n\n35\n\n\f[11] X. Wang, L. T. Zhang, Interpolation functions in the immersed boundary\nand finite element methods, Computational Mechanics 45 (4) (2009) 321–\n334. doi:10.1007/s00466-009-0449-5.\n\n520\n\n525\n\n[12] X. Wang, C. Wang, L. T. Zhang, Semi-implicit formulation of the immersed\nfinite element method, Computational Mechanics 49 (4) (2011) 421–430.\ndoi:10.1007/s00466-011-0652-z.\n[13] X. Wang, L. T. Zhang, Modified immersed finite element method for fullycoupled fluid–structure interactions, Computer Methods in Applied Mechanics and Engineering 267 (2013) 150–169. doi:10.1016/j.cma.2013.\n07.019.\n[14] R. Glowinski, T. Pan, T. Hesla, D. Joseph, J. Périaux, A fictitious domain\napproach to the direct numerical simulation of incompressible viscous flow\npast moving rigid bodies: Application to particulate flow, Journal of Computational Physics 169 (2) (2001) 363–426. doi:10.1006/jcph.2000.6542.\n\n530\n\n535\n\n540\n\n[15] Z. Yu, A DLM/FD method for fluid/flexible-body interactions, Journal of\nComputational Physics 207 (1) (2005) 1–27. doi:10.1016/j.jcp.2004.\n12.026.\n[16] F. P. Baaijens, A fictitious domain/mortar element method for fluidstructure interaction, International Journal for Numerical Methods in\nFluids 35 (7) (2001) 743–761. doi:10.1002/1097-0363(20010415)35:\n7<743::AID-FLD109>3.0.CO;2-A.\n[17] C. Kadapa, W. Dettmer, D. Perić, A fictitious domain/distributed lagrange\nmultiplier based fluid–structure interaction scheme with hierarchical bspline grids, Computer Methods in Applied Mechanics and Engineering\n301 (2016) 1–27. doi:10.1016/j.cma.2015.12.023.\n[18] C. Hesch, A. Gil, A. A. Carreño, J. Bonet, P. Betsch, A mortar approach for\nfluid–structure interaction problems: Immersed strategies for deformable\nand rigid bodies, Computer Methods in Applied Mechanics and Engineering\n278 (2014) 853–882. doi:10.1016/j.cma.2014.06.004.\n\n545\n\n550\n\n[19] C. S. Peskin, The immersed boundary method, Acta numerica 11 (2002)\n479–517. doi:10.1016/j.cma.2015.12.023.\n[20] A. Robinson-Mosher, C. Schroeder, R. Fedkiw, A symmetric positive definite formulation for monolithic fluid structure interaction, Journal of Computational Physics 230 (4) (2011) 1547–1566. doi:10.1016/j.jcp.2010.\n11.021.\n[21] E. Hachem, S. Feghali, R. Codina, T. Coupez, Anisotropic adaptive meshing and monolithic variational multiscale method for fluid–structure interaction, Computers & Structures 122 (2013) 88–100. doi:10.1016/j.\ncompstruc.2012.12.004.\n36\n\n\f555\n\n[22] E. Bodewig, M. Abramowitz, Matrix calculus, Elsevier BV, 2014.\n[23] D. Boffi, L. Gastaldi, A fictitious domain approach with lagrange\nmultiplier for fluid-structure interactions, Numer. Math.doi:10.1007/\ns00211-016-0814-1.\n\n560\n\n[24] O. Zienkiewic, The finite element method for fluid dynamics, 6th Edition,\nElsevier BV, 2005.\n[25] P. B. Bochev, M. D. Gunzburger, Least-squares finite element methods,\nVol. 166, Springer Science & Business Media, 2009.\n[26] W. A. Wall, Fluid-struktur-interaktion mit stabilisierten finiten elementen,\nPh.D. thesis, Universitt Stuttgart (1999). doi:10.18419/OPUS-127.\n\n565\n\n570\n\n[27] H. Zhao, J. B. Freund, R. D. Moser, A fixed-mesh method for incompressible flow–structure systems with finite solid deformations, Journal of\nComputational Physics 227 (6) (2008) 3114–3140. doi:10.1016/j.jcp.\n2007.11.019.\n[28] A. K. Gupta, A finite element for transition from a fine to a coarse grid,\nInternational Journal for Numerical Methods in Engineering 12 (1) (1978)\n35–45. doi:10.1002/nme.1620120104.\n[29] T.-P. Fries, A. Byfut, A. Alizada, K. W. Cheng, A. Schrder, Hanging nodes\nand XFEM, International Journal for Numerical Methods in Engineering\n86 (4-5) (2010) 404–430. doi:10.1002/nme.3024.\n\n575\n\n580\n\n[30] W. Bangerth, O. Kayser-Herold, Data structures and requirements for hp finite element software, ACM Transactions on Mathematical Software 36 (1)\n(2009) 1–31. doi:10.1145/1486525.1486529.\n[31] N. Zander, T. Bog, S. Kollmannsberger, D. Schillinger, E. Rank, Multilevel hp-adaptivity: high-order mesh adaptivity without the difficulties of\nconstraining hanging nodes, Computational Mechanics 55 (3) (2015) 499–\n517. doi:10.1007/s00466-014-1118-x.\n[32] P. Šolı́n, J. Červený, I. Doležel, Arbitrary-level hanging nodes and automatic adaptivity in the hp-FEM, Mathematics and Computers in Simulation 77 (1) (2008) 117–132. doi:10.1016/j.matcom.2007.02.011.\n\n585\n\n[33] E. Ooi, H. Man, S. Natarajan, C. Song, Adaptation of quadtree meshes\nin the scaled boundary finite element method for crack propagation modelling, Engineering Fracture Mechanics 144 (2015) 101–117. doi:10.1016/\nj.engfracmech.2015.06.083.\n\n37\n\n\f"
        ],
        [
         "33",
         "33",
         "cs.CE",
         "Computational Engineering",
         "1705.11063v2.pdf",
         "BOUNDEDNESS-PRESERVING IMPLICIT CORRECTION OF\nMESH-INDUCED ERRORS FOR VOF BASED HEAT AND MASS\nTRANSFER\n\narXiv:1705.11063v2 [cs.CE] 21 Jun 2017\n\nS. HILL1,3 , D. DEISING2 , T. ACHER1 , H. KLEIN3 , D. BOTHE2 , AND H. MARSCHALL2,*\nAbstract. Spatial discretisation of geometrically complex computational domains often entails unstructured meshes of general topology for Computational\nFluid Dynamics (CFD). Mesh skewness is then typically encountered causing\nsevere deterioration of the formal order of accuracy of the discretisation, or\nboundedness of the solution, or both. Particularly methods inherently relying on the accurate and bounded transport of sharp fields suffer from all\ntypes of mesh-induced skewness errors, namely both non-orthogonality and\nnon-conjunctionality errors.\nThis work is devoted to a boundedness-preserving strategy to correct for\nskewness errors arising from discretisation of advection and diffusion terms\nwithin the context of interfacial heat and mass transfer based on the Volumeof-Fluid methodology. The implementation has been accomplished using a\nsecond-order finite volume method with support for unstructured meshes of\ngeneral topology. We examine and advance suitable corrections for the finite\nvolume discretisation of a consistent single-field model, where both accurate\nand bounded transport due to diffusion and advection is crucial. In order to\nensure consistency of both the volume fraction and the species concentration\ntransport, i.e. to avoid artificial heat or species transfer, corrections are studied\nfor both cases. The cross interfacial jump and adjacent sharp gradients of\nspecies concentration render the correction for skewness-induced diffusion and\nadvection errors additionally demanding and has not so far been addressed in\nthe literature.\n\n1. Introduction\nAlgebraic Volume-of-Fluid (VoF) methods [1–4] are widely used for numerical\nsimulations of flows consisting of two incompressible fluid phases with a sharp deformable interface separating them. Recently a model for interfacial species transfer within the algebraic VoF framework, namely the Continuous Species Transfer\n(CST) model, was presented in [5, 6]. Within this modeling framework, heat transfer is to be seen as a special case without jump and can thus be described by\na simplified CST model equation. Both methods, VoF and CST, are derived by\napplying the Conditional Volume-Averaging (CVA) technique to the local instantaneous sharp interface model equations and the corresponding interfacial jump\nconditions so as to transform them into volume-averaged single-field equations. In\nthis form the governing equations are readily suitable for consistent discretisation\nusing a Finite Volume Method (FVM).\nWhen considering computational domains entailing complex geometries, unstructured meshes of general topology are required. Then, depending on the mesh\n(1) Linde Engineering AG, Pullach, Germany\n(2) Mathematical Modeling and Analysis, Fachbereich Mathematik, Technische Universität Darmstadt, Darmstadt, Germany\n(3) Institute of Plant and Process Technology, Faculty of Mechanical Engineering,\nTechnical University of Munich, Munich, Germany\nE-mail addresses: Corresponding Author: * marschall@mma.tu-darmstadt.de.\nKey words and phrases. Skewness Correction; VoF method; Heat and Mass Transfer.\n1\n\n\f2\n\nIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\nquality, discretisation errors related to mesh skewness arise and can severely detoriate the numerical fidelity of the method resulting in loss of boundedness, accuracy\nand order of convergence. Typically, two types of skewness errors are to be distinguished: errors due to non-conjunctionality and errors due to non-orthogonality at\nthe three-point stencil around a cell face (see Figure 1). In order to avoid detoriation of numerical fidelity on such meshes, skewness correction approaches have been\ndevised for FV discretisation and are nowadays well established practice. Classical approaches are explicit and correct for non-orthogonality during discretisation\nof diffusion terms and non-conjunctionaly during FV discretisation of advection\nterms, respectively. For an overview of state-of-the-art classical approaches to skewness correction the interested reader is referred to [7–9]. A non-classical explicit\napproach is the ghost-point method set out by Perić in [8], correcting for both\nnon-conjunctionality and non-orthogonality simultaneously.\n\n(a) Non-conjunctionality.\n\n(b) Non-orthogonality.\n\nFigure 1. Types of mesh skewness.\nApplying classical (explicit) skewness correction approaches to the VoF advection equation on distorted meshes results directly in severe unboundedness of the\nvolume fraction and as a consequence in substantial problems with stability and/or\naccuracy. The reason for this resides in the fact that due to the typically high\ndensity ratios even small errors in the volume fraction cause large errors in mass\nfraction and consequently in the momentum. It has been found that in such cases\nimplicit correction approaches are needed, see [10–12] and more recently [13, 14].\nTaking interfacial heat and mass transfer into consideration renders the situation\neven more challenging, since there are now both the discretised advection and diffusion terms to be skewness corrected. While it is obvious that classical explicit\napproaches remain unsuitable (likewise causing unboundedness errors as for the\nVoF transport), up to now it is unclear whether implicit correction approaches can\nbe used to correct for mesh skewness when applied to the discretised form of the\nCST model for interfacial heat and mass transfer.\nThis contribution is concerned with enhancements of implicit correction approaches based on a detailed investigation into their suitability to maintain numerical fidelity such as boundedness, accuracy and order of convergence in the\ncontext of VoF/CST-based simulations of interfacial heat and mass transfer on\ndistorted meshes of general topology. In contrast to previous work, we introduce\nimplicit skewness corrections for finite volume discretisations of both advection and\ndiffusion terms. Moreover, we use finite volume meshes which have been distorted\nin a targeted manner rather than randomly (as is common practice in the existing\nliterature) in order to allow a systematic study on mesh-induced errors and avoid\nerror cancellation. The objectives of this work are two-fold, viz.\n• to devise a suitable method for mesh skewness correction for the discretised\nadvection and diffusion terms of the single-field CST model for VoF-based\n\n\fIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\n3\n\ninterfacial heat and mass transfer. Notably, one of the CST model terms\nis discretised in a non-standard way [6], that is, despite originating from\na diffusive transport term, it has been found that it is best to treat one\nterm implicitly by means of a FV divergence rather than a FV laplacian\noperator.\n• to identify and clarify the influence of a jump in the scalar transport variable\non the order of convergence when refining a systematically distorted finite\nvolume mesh and applying implicit skewness corrections.\nThis study has been accomplished using and extending OpenFOAM (version 4.x).\n2. Standard finite volume discretisation and Mesh-Induced Errors\nIn order to better understand the proposed approach to implicit skewness correction for VoF/CST-based interfacial heat and mass transfer on distorted meshes\nof general topology, the errors on skewed meshes in finite volume discretisation of a\nprototypical scalar advection-diffusion equation are briefly recapped and a review\nof prominent publications addressing these errors is given.\nThe starting point of our investigations is the advection-diffusion equation for\nan arbitrary scalar quantity φ, i.e.\n(1)\n\n∂t φ + ∇ · (φu) = −∇ · (−Γφ ∇φ) + Sφ .\n\nIn the finite volume context, the standard discretisation procedure is based on the\napplication of Gauss’ rule, transferring the divergence terms into face integrals, and\non the mid-point rule. This yields a formal second-order accurate approximation.\nIn collocated finite volume methods, as employed in this work, the values of quantity φ are stored in the cell centre locations, which necessitates the interpolation of\ncell-centred data onto the face centres. Below cell-face interpolation will be denoted\nby (·)f . In order to keep the discretisation second order accurate, the interpolation schemes have to be at least of the same order. However, in general, even if\nhigher order interpolation schemes are selected, the convergence order as well as\nthe overall accuracy can still deteriorate, particularly in cases of complex computational domain geometries and meshes of general topology (polyhedral meshes).\nThe errors stem directly from local mesh skewness, namely non-orthogonality and\nnon-conjunctionality. They can be corrected during discretisation of the respective\ntransport terms in (1).\n2.1. Advection term. The standard finite volume discretisation of advective terms\nreads\nI\nZ\nX\nn · (φu) dS ≈\n∇ · (φu) dV =\nSf · (φu)f\n(2)\n\nS\n\nV\n\n≈\n\nX\nf\n\nf\n\nSf · uf φf =:\n\nX\n\nFf φf ,\n\nf\n\nwith the volumetric face flux Ff := Sf · uf . The face-centred values of φ are\nobtained by using interpolation schemes. In general, second order accuracy of\nthe discretisation formally relies on the assumption that the face-centre position\ncorresponds to the intersection point of the face and the connecting line between\nthe neighbouring cell centres. If this assumption does not hold, as the constellation\ndepicted in Figure 1a shows by way of example, so called non-conjunctionality\nerrors occur.\nThe standard approach to account for this error is to correct the face-interpolated\nvalue φf ′ = f (φN , φP ) by an explicit term reading\n(3)\n\nφf = φf ′ + ∇φf ′ · m ,\n\n\f4\n\nIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\nwhere m denotes the vector from the face intersection point to the face centre.\nThe above time-explicit correction of the mesh-induced non-conjunctionality error is very simple and computationally inexpensive but may lead to unboundedness\nof the transported quantity [7], e.g. when high-resolution schemes (based on TVD,\nNVD face limiters etc.) are utilised for the discretisation as is common practice in\nFVM. Magalhaes et al. [15] introduce a similar approach for adaptive hexahedral\nmeshes to account for non-conjunctionality errors, where the face value is reconstructed from the face-neighbouring cell-centred values and the respective gradients,\nand then averaged:\n1\n(4)\nφf = ([φP + ∇φP · (xf − xP )] + [φN + ∇φN · (xf − xN )]) .\n2\nAn entirely different correction approach, the so-called ghost point-based interpolation scheme, which corrects the face interpolation by introducing a virtual non-skew\nstencil, was proposed by Ferziger and Peric [8] and successfully employed e.g. in [16].\nHowever, if boundedness of the transported quantity is of utmost importance, as\ne.g. in the VoF-based transport of the volumetric phase fraction or species concentration fields, the above explicit methods are not usable as they do not ensure this\nproperty.\nTherefore, this work makes use of a modified implicit version of the standard\ncorrection approach described by Equation (3), which makes use of an improved\nmethod for preserving the boundedness of the transported quantity and is described\nin detail in Section 3.\n2.2. Diffusive term. Analogously to Equation (2), the discretised form of the\ndiffusive term is given by\nI\nX\nX\n(5)\nn · (Γφ ∇φ) dS ≈\nSf · (Γφ ∇φ)f ≈\n(Γφ )f Sf · (∇φ)f .\nS\n\nf\n\nf\n\nThe term (Γφ )f Sf · (∇φ)f is preferably to be discretised in a compact way, that is\nonly information of the two cells neighbouring the respective face is considered:\nφN − φP\n.\n(6)\n(Γφ )f Sf · (∇φ)f = (Γφ )f |Sf |∇f⊥ φ = (Γφ )f |Sf |\n|d|\nThis approximation of the surface normal gradient ∇f⊥ φ, however, only holds true\nfor orthogonal meshes. As a consequence, a numerical error is introduced in case\nof local non-orthogonality, i.e. if Sf ∦ d (see Figure 1b).\nOne way to account for this error was introduced in [17] and [18], where cell\nvertex-interpolated values, obtained by either inverse distance weighting or leastsquares interpolation, are utilized to correct for mesh non-orthogonality, which in\neffect significantly increases the numerical stencil. This, however, counteracts the\naccurate discretization of discontinuities, which are inherently smoothed by a large\ndiscretization stencil.\nA different approach, proposed by [19–21], is to split up the vector Sf in such a\nway that\n(7)\n\nSf = ∆ + k ,\n\nwith the restriction of ∆ being parallel to d. Employing the above splitting, the\ndiscretisation can be performed as\nφN − φP\n(8)\nSf · (∇φ)f = ∆ · (∇φ)f + k · (∇φ)f = |∆|\n+ k · (∇φ)f .\n|d|\nFor the computation of the magnitude of ∆ and k different options are available\n(Jasak [7], Demirdzic [22]). For further reading regarding non-orthogonal corrections the interested reader is also referred to the works of Demirdzic [19, 20, 22],\n\n\fIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\n5\n\nas well as Mathur and Murthy [21]. These pionieering works represent the main\ncontributions to non-orthogonality corrections and their development history. An\noverview of alternative but similar correction approaches can also be found in [23].\nAll of the above mentioned approaches eventually result in a formulation similar\nto or exactly the same as that given in (8). If in addition the mesh exhibits lo-\n\nFigure 2. Generally skewed unstructured mesh due to nonconjunctionality and non-orthogonality.\ncal non-conjunctionality, i.e. is generally skewed (see Figure 2), further corrections\nmay have to be applied. One available explicit method for correcting both skewness\nerrors simultaneously is the ghost-point interpolation proposed by [8]. Generally,\nnon-conjunctionality corrections in the discretisation of diffusion terms are simply\nomitted without comment. In this work, however, their influence on accuracy will\nbe investigated.\n3. Proposed Bounded Implicit Correction of Mesh-Induced Errors\nAs the explicit correction of mesh-induced errors can lead to unboundedness,\nsuitable implicit corrections must be devised. For this purpose, we first briefly recap\nboundedness-preserving criteria from the literature and then adapt the formulation\nof the skewness correction in a way such that they can be applied.\n3.1. Boundedness Criteria of Advection Term on Unstructured Meshes.\nTo enforce boundedness during discretisation of the advection term, different boundedness criteria based on positivity [24], are readily available in literature. Most\nprominent (inter alia) are the Total Variation Diminishing (TVD) [25] and Local Extrema Diminishing (LED) [26] criteria as well as the transient Convection\nBoundedness Criterion (CBC) [27].\n\nFigure 3. Considered stencil for discretisation.\nDefining an interpolation scheme based on a flux limiter formulation as [25, 28]\n(9)\n\nφf = φC + δf Ψ(r) (φD − φC ) ,\n\n\f6\n\nIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\n(a) Based on face-neighbour cells only\n\n(b) Based on extended stencil\n\nFigure 4. Choice of upwind node on unstructured meshes [10, 29]\n\n(a) Construction of virtual upwind node po- (b) Reconstruction of virtual upwind node\nsition\nvalue\n\nFigure 5. Virtual upwind node on unstructured meshes based\non [29, 30]\nwith the mesh weight δf =\ncan be transferred into\n(10)\n\ndCf ·Sf\ndCD ·Sf\n\nand the limiter function Ψ(r), the TVD conditions\n\n0 ≤ Ψ(r) ≤ min\n\n\u0012\n\n1 1 − Cof 1\nr\n,\nδf\nCof\nδf\n\n\u0013\n\n,\n\nwith Cof denoting the face Courant number and r the gradient ratio, being defined\nas\n\n\nX\nφC − φU\n∆t\n,\nmax (Ff , 0)\n, r=\n(11)\nCof = \n|V |\nφD − φC\nf\n\nf ,UD\n\nwhere Ff denotes the volumetric face flux and (·)f ,UD denotes upwind interpolation.\nContrary to the condition Sweby [28] derived for a flux limiter function Ψ, the condition in Equation (10) does not require the mesh to be uniform. Additionally, the\nabove limiter condition makes use of the whole boundedness region, while the original limiter condition introduced in [28] only offers a very conservative estimation\nof the boundedness region.\nOn unstructured meshes of general topology the computation of r leads to the\nproblem that φU is required, the definition of the upwind node, however, is not clear.\nIn literature, a variety of different possible remedies have been introduced: [10]\nchooses the upwind cell from all face-neighbouring cells of the centre node as the\none whose centre position is closest to the line through centre and downwind nodes\n(see Figure 4a), whereas [29] presents alternatives based on an extended stencil\n(see Figure 4b) and compact stencil (see Figure 5a). The latter is the most widely\nutilized approach and introduces a virtual upwind node as the backward projection\nof the downwind to centre node. The value in the virtual upwind node is then\n\n\fIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\n7\n\ncomputed by a second-order Taylor series approximation (see Figure 5b)\nφU ∗ ≈ φD − 2∇φC · dCD .\n\n(12)\n\nOn uniform hexahedral meshes the exact value for the upwind cell value is recovered,\nif the gradient in Equation (12) is calculated using Gauss’ approximation. Different\nstrategies for calculating the r-factor and/or virtual upwind value reported in literature can be found in [29–33]. To maintain a bounded solution, different limiting\nstrategies for the virtual upwind have been proposed in literature [31,32]. The simplest strategy is to bound the virtual upwind value by the physical bounds of the\ntransported quantity (e.g. between 0 and 1 for the volumetric phase fraction). [31]\nintroduced a local limiting strategy based on the values in the neighbouring (nb)\ncells\n\u0001 nb \u0001\n(13)\nφU ∗ = min max φnb\nmin , φU ∗ , φmax ,\nwhich on unstructured meshes requires an efficient search algorithm to find the\ncomputational cell containing the virtual upwind node. This approach is adopted\nin the presented work, employing the local vicinity search algorithm of [34, 35].\n\n3.2. Implicit Mesh-induced Error Correction for Advection. In order to apply the previously described boundedness criteria to advection schemes accounting\nfor non-conjunctionality errors, the formulation of the correction (Equation 3) has\nto be rearranged. To this end, an approach has recently been introduced by Denner and vanWachem [13, 14], which is based upon including the explicit correction\ninto the interpolation weights. This is done by extending the general interpolation scheme already defined in Equation (9) by the correction term and solving the\nresulting equation for a newly defined interpolation weight aC :\nφf = φC + δf Ψ(r) (φD − φC ) + m · ∇φf ′\n!\n\n(14)\n\n= φC + aC (φD − φC )\n⇒ aC = δf Ψ +\n\nm · ∇φf ′\n.\nφD − φC\n\nTo obtain a bounded solution the TVD condition (Equation 10) now no longer\nhas to be enforced on the flux limiter Ψ of the base scheme, but on the resulting\ninterpolation weight aC :\n\u0012\n\u0013\n1 − Cof\n(15)\n0 ≤ aC ≤ min r\n,1\nCof\nThus, in this approach the correction is still explicit but is incorporated directly\ninto the interpolation weights which allows for a bounded and formally time-implicit\ndiscretisation.\nAnother problem which must be addressed when aiming at boundedness-preserving\nnon-conjunctionality corrections, is the coupling between face-interpolation and\ngradient computation, which has been noted by numerous researchers [10–12]. A\nface interpolation with non-conjunctionality correction (cf. Equation 14) depends\non the face gradient that is interpolated from gradients in the face-neighbouring\ncell centres, which in return may require the face interpolated values to be known\n(e.g. using Gauss’ theorem)\n1 X\n(16)\n∇φC , ∇φD =\nφf Sf and ∇φf ′ = f (∇φC , ∇φD ) .\n|VP |\nf\n\nTo overcome this problem, [11] and [10] introduce different iterative methods, requiring the computation of the face interpolated value and face gradient multiple\n\n\f8\n\nIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\ntimes in a correction loop. [12] introduces an improved approach, based on an implicit calculation of the cell-centred gradient using a least-squares method.\nIt is shown in this work (Section 5.1), that the method utilized to compute\nthe gradient in the correction term has a major influence on the accuracy of the\nnon-conjunctionality correction for advection terms. In this work, the least-squares\ngradient computation introduced in [12] and the gradient computation using standard Gauss’ theorem are utilized and compared in respect of the accuracy of nonconjunctionality corrections. Both gradient computations are explicit without any\ncorrection loops.\n3.3. Implicit Mesh-induced Error Correction for Diffusion. A straight forward way to account for both mesh-induced errors simultaneously is to consecutively apply the non-conjunctionality and non-orthogonality corrections already\ndescribed. In a first step the non-conjunctional corrected gradient of the transported variable φ analogous to Equation (3) is obtained:\nh\ni\nSf · (∇φ)f = Sf · (∇φ)f ′ + ∇(∇φ)f ′ · m .\nUsing Equation (8) to discretise Sf · (∇φ)f ′ finally yields:\nSf · (∇φ)f\n(17)\n\nh\ni\n= ∆ · (∇φ)f ′ + k · (∇φ)f ′ + Sf · ∇ (∇φ)f ′ · m\nh\ni\nφN − φP\n+ k · (∇φ)f ′ + Sf · ∇ (∇φ)f ′ · m\n= |∆|\n|d|\n\nAnalogously to the approach previously introduced for advection terms, the\nabove discretisation of the diffusive term can also be rearranged in a way that allows for implicit bounding. This is done by rearranging Equation (17) to formally\ncorrespond to the uncorrected discretisation of the diffusive term (cf. Equation 6):\n(18)\n\n!\n\n(Γφ )f Sf · (∇φ)f = (Γφ )f ,m |Sf |\n\nφN − φP\n.\n|d|\n\nComparing Equation (18) with Equation (6) reveals that the diffusive coefficient\n(Γφ )f ,m has to be defined as\nh\ni\n\nSf · ∇ (∇φ)f ′ · m\n(Γφ )f |d| |∆| k · (∇φ)f ′\n\n.\n+\n+\n(19)\n(Γφ )f ,m =\n|Sf |\n|d|\nφN − φP\nφN − φP\nTheoretically this formulation now enables the implicit application of limiting\ncriteria to the diffusive coefficient (Γφ )f ,m , which in turn contains the explicit correction. However, until now it remains uncertain as to whether limiting is needed\nat all and if so, the criterion required is missing. Both issues will be addressed in\nthe results section.\n4. Utilized Finite Volume Discretisation\nThe developed correction strategies are applied to the discretised phase fraction\nadvection equation, i.e. the VoF transport eqn.\n(20)\n\n∂t α + ∇ · (αu) = 0\n\nand the discretised species transport equation underlying the CST model [6]. Employing a harmonic mean diffusion coefficient, the simplest version of the latter can\n\n\fIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\n9\n\nbe written as (cf. [6])\n(21)\n\n∂t c + ∇ · (cu) = ∇ · (hDih ∇c) + ∇ · (hDih Kc∇α) ,\nH −1\nand H the Henry coefficient.\nwith K =\n1 + α (H −1)\n\nThe above equations can be discretised in many different ways to account for meshinduced errors. Suitable candidates – based on the selection in the previous Sections – are identified in the remainder of this Section and tested in Section 5.\nThe discretised form of the phase fraction transport equation, using time-centered\nCrank-Nicolson time discretisation, reads\n\u0001\n∆tn X 1 o\n(22)\nαCn = αCo −\nαf + αfn Ffo , with Ffo = ufo · Sf .\n|V |\n2\nf\n\nThe most crucial decision in discretising the advection term is how to compute\nthe face-interpolated value αf . This work considers four different schemes, namely\nthe Upwind UDS, Central Differences (CDS), CICSAM [1] and a specific local\nblended scheme (LB). The employed LB scheme is similar to the CICSAM scheme\nin that it uses the CICSAM scheme’s blending function to blend between the central\ndifferencing and downwind scheme, resulting in a formally unbounded interpolation\nscheme. In the LB case, boundedness will be enforced by implicit correction once,\nwhile in the CICSAM case, boundedness is enforced twice: by the non-linear limiter\nof the scheme and by limiting the skewness corrected face interpolation.\nEach of the schemes is tested without correction (UC), with standard explicit\ncorrection (EC) and with the newly proposed correction, from now on referred to as\nSemi-Implicit Skewness Correction (SISC) scheme. Both corrections make use of the\ngradient operator which is discretised by both, Gaussian Gradient (GG) and Least\nSquares Fit (LSF). An overview of all interpolation schemes, correction approaches\nand gradient calculation methods varied against each other for discretisation of the\nadvection term is given in Table 1.\nTable 1. Overview of all methods used to discretise the faceinterpolated value αf .\nInterpolation Scheme\n\nCorrection Approach Gradient Calculation\n\nUDS\nCDS\nCICSAM\nLB\n\nUC\nEC\nSISC\n\nGG\nLSF\n\nIn this work, we test different gradient schemes for calculation of the correction\npart and utilize the least-squares gradient to compute the virtual upwind value\non non-uniform meshes. It is emphasised that on uniform Cartesian meshes the\nGaussian gradient should be utilized for the virtual upwind calculation to maintain\naccuracy.\nIt is crucial to discretise the left hand side of the species transport equation (21)\nidentical to the phase fraction transport equation in order to avoid artificial species\ntransfer, i.e. identical discretisation schemes for the respective terms and corrections need to be employed. A suitable discretisation of the right hand side as\nutilized in the presented work reads\nh\ni X\nh\ni\nX\nn\nn\nn\nn\nn\nSf · (∇c)f −\n(23)\nhDih,f\n(Kc)f Sf · (∇α)f .\nhDih,f\nf\n\nf\n\n\f10\n\nIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\nSince the phase fraction transport equation is solved first, the phase fraction field\nαn at the new time level is already explicitly available, allowing for the fully time\nimplicit discretisation shown above. The gradient in face normal direction in both\nterms need to be discretised identically, whereby in the scope of this work three\ndifferent discretisation methods are tested. Firstly, without any correction, which\ntherefore corresponds to Equation (6). Secondly, by applying the explicit nonorthogonal correction as proposed by Jasak [7], cf. Equation (8), and thirdly, by\nadditionally accounting for non-conjunctionality errors as formulated in (19). These\nthree variants are from now on referred to as UC, NO and NO/NC, respectively.\nn\nThe diffusion coefficient hDih,f\nand the term (Kc)f are linearly interpolated, once\nTable 2. Overview of all used methods varied against each other\nto discretise the diffusive terms of the CST model.\nn\nhDih,f\n, (Kc)f\n\nSf · (∇φ)f\nUC\nNO\nNO/NC\n(a) Laplacian\n\nInterpolation Scheme\n\nCorrection Appr.\n\nGradient Calc.\n\nCDS\n\nUC\nEC\n\nGG\n\n(b) Diffusive flux\n\nwith and once without non-conjunctionality correction. Table 2 summarises all\nused discretisations for the different terms of the CST model.\n5. Results and Discussion\nTo evaluate the performance of all discretisation methods presented in Section 4,\nincluding the newly developed correction strategies SISC and NO/NC, they are\nverified using a series of test cases involving steep gradients and discontinuities.\nTwo aspects are considered to judge the discretisation performance: the obtained\nsolution should accurately approximate the analytical solution and at the same time\nboundedness must be preserved (as required for both phase fraction and species\nconcentration).\nThe evaluation comprises of two steps. First, the discretisation of the phase fraction transport equation is varied according to Table 1 and verified. Then secondly,\nthe diffusive terms in the CST-model are discretised with different variants as given\nin Table 2. In both steps,the respective discretization performance is verified via\ndifferent test cases. In all test cases 2D meshes are used. To provoke mesh induced\n\nFigure 6. Structure of distorted mesh comprising of nonorthogonal and non-conjunctionality errors.\nerrors the mesh is systematically distorted in such a way that non-orthogonality as\nwell as non-conjunctionality errors are present. Figure 6 shows the mesh structure\nwhich is identical for all cases.\n\n\fIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\n11\n\n5.1. Correction of VoF-based phase fraction advection term.\n5.1.1. Test Cases. First we consider a simple 2D plug flow, whereby the transported\nfield is advected from left to right by a constant velocity field (see Figure 7a). The\nsecond case examines the shape-preserving properties of different advection schemes\nwhen a circular shape is translated through a distorted mesh over a distance equal\nto 12 times its diameter (see Figure 7b). Here, the utilised average mesh resolution\nis approximately 15 cells per diameter of the circle. The numerical time step in both\nsimulations is set in such a way that the Courant number limit Co = |u|∆t/∆x ≤\n0.1 is maintained.\n\n(a) Plug flow.\n\n(b) Translation of circular shape.\n\nFigure 7. Set-up of test cases to evaluate different discretisations\nof the advection term. The dashed red lines indicate the paths over\nwhich α is plotted for evaluation reasons.\n5.1.2. Results. Results are visualized by plotting the phase fraction profile over the\npath ‘L1’ and ‘L2’ respectively (see Figure 7). Without any correction all interpolation schemes show poor results in both test cases. As might be expected the\nusage of CDS or LB generates strongly unbounded, inaccurate solutions in all cases.\nWhile boundedness is ensured by the use of UDS or CICSAM interpolation, the\naccuracy is insufficient. More specifically, high diffusion is observed when applying\nUDS and severe deformation is ovserved with CICSAM. The respective plots are\nshown in Figure 8, where it should be noted that some lines cannot be detected as\nthe unboundedness of the solution is too severe.\nPlug Flow\n\nTranslation of circular shape\n\n1\n0.8\n\nInitial Profile\nUDS\nCDS\nCICSAM\nLB\n\nα\n\n0.6\n0.4\n0.2\n0\nL1\n\nL2\n\nFigure 8. Phase fraction profiles for different advection schemes\nwithout any correction.\nAdding explicit correction worsens the results even further. Except for the UDS\nscheme all generated solutions are unbounded and all of them suffer heavily from a\nlack of accuracy. This is the case irrespective of whether the gradient is calculated\nusing GG or LSF, where applying the latter still produces the better results.\nNow, evaluating the SISC scheme, we can state that for all tested combinations\nboundedness is preserved up to 10−6 (see Figure 9). Another interesting finding\n\n\f12\n\nIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\nα\n\nis that the LSF of the gradient outperforms the Gaussian gradient in all cases.\nExcept for the UDS scheme, where the strong diffusion overrides all other effects,\nthe difference is quite significant. Using the SISC-bounded CDS, diffusion has a\nhuge effect and smears out the initial step profiles. Applying CICSAM yields a\nsignificant improvement because it is better able to preserve steep gradients. The\nbest result, however, is achieved by the LB scheme, where almost the exact shape\nis preserved when using LSF for gradient computation.\nCDS\n\nCICSAM\n\nLB\n\nL1\n\nL1\n\nL1\n\n1\n0.8\n0.6\n0.4\n0.2\n0\n\nα\n\n1\n0.8\n0.6\n0.4\n0.2\n0\nL2\n\nL2\n\nInitial Profile\n\nGG\n\nL2\nLSF\n\nFigure 9. Phase fraction profiles using the SISC scheme in combination with different base schemes and gradient computations.\nTo finally assess all relevant scheme combinations the phase fraction field is also\nvisualized (see Figures 10 and 11). Set-ups not considered as relevant are those\nwhich lead to unbounded results and those using the Gaussian gradient approximation because LSF performs equally or better in all cases. Other than the results\ngained with the SISC scheme this leaves only the non-corrected UDS and CICSAM\nschemes, and for the explicitly corrected set-ups solely the UDS scheme.\n\nInitial state\n\nUDS / UC\n\nCICSAM / UC\n\nUDS / EC\n\nFigure 10. Visualisation of phase fraction field for all boundedness preserving scheme combinations excluding SISC.\nFigure 10 depicts the fields excluding SISC. The UDS scheme without and also\nwith explicit correction is dominated by diffusive effects which leads to a smeared\n\n\fIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\n13\n\nout α-distribution. CICSAM results in a slightly sharper solution, which, however,\nstill can not be considered as satisfactory. It can therefore be concluded that none of\nthe commonly used advection schemes meet the requirements concerning accuracy\nand boundedness.\n\nInitial state\n\nUDS / SISC\n\nCDS / SISC\n\nCICSAM / SISC\n\nLB / SISC\n\nFigure 11. Visualisation of phase fraction field for the case of the\napplied SISC scheme in combination with different base schemes.\nLooking at the distribution of the phase fraction obtained with the SISC scheme\n(depicted in Figure 11) allows a more extensive evaluation. The UDS scheme clearly\nshows a diffusive effect which is so pronounced that neither the shape of the plug\nflow nor the circular shape are preserved. To a lesser degree, this effect is also visible when CDS is used. CICSAM and the LB scheme show good results in both test\ncases. However, although CICSAM preserves sharp gradients quite well, the LB\nscheme is able to keep the initial steepness of the profile. Addressing the ability to\npreserve the circular shape it can be seen that both schemes tend to transform the\ncircle into a quadratic shape, for the mesh topology under consideration. Hence, for\nalgebraic VoF methods the mesh topology has a major influence on accuracy (see\nFigure 12). Flux/cell-face alignment is important for the accuracy of algebraic VoF\nmethods and thus polyhedral mesh topologies are to be favoured over to quadrilateral (or hexahedral) meshes. Numerical simulations using standard discretisation\non polyhedral meshes, however, suffer from mesh-induced skewness errors and hence\nthe present work becomes generally important for algebraic VoF methods.\nTo conclude it can be stated that the proposed correction scheme, in contrast\nwith all other tested schemes, is able to keep the boundedness of the solution and\nat the same time, if combined with a suitable interpolation scheme, is capable of\nproducing accurate results.\n5.2. Correction of CST-based species transfer term. Different variants for\ndiscretising the terms of the CST-model are tested, with special attention devoted\nto mesh induced error correction (cf. Table 2). The three main questions are:\n• How severe is the impact of skewed meshes on the accuracy of the CSTmodel and which of the proposed correction strategies is needed to restore\nthe expected order of accuracy on non-distorted meshes?\n• Does explicit correction lead to unboundedness of the solution? In other\nwords, is implicit limiting necessary?\n• Is it sufficient to account for non-orthogonality (NO) only, or must also\nnon-conjunctional error correction (NO/NC) be taken into account when\ndiscretising a gradient in face normal direction?\n\n\f14\n\nIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\nInitial state\n\nCICSAM / UC\n\nLB / SISC\n\nFigure 12. Resulting α-distribution in case of applied SISC and\nCICSAM scheme when using a uniform cartesian (upper figures)\nand polyhedral (lower figures) mesh.\nEven though the last question is commonly answered in the negative it shall nevertheless be discussed here, since species concentration fields commonly exhibit\nlarge discontinuities at the interface in both value and gradient. Consequently, errors introduced by skewed meshes are more pronounced and may necessitate a full\nskewness correction to obtain accurate results.\n5.2.1. Test Case. In order to find the scheme which best meets the requirements,\nthe simulation results of a planar diffusion case are evaluated. To avoid effects\nof the convective term the velocity field is set to zero. To evaluate the different\ncorrection strategies, species transfer from a surrounding stagnant gas into a stagnant liquid film is considered (see Figure 13). Five different Henry coefficients\n(H = 0.033, 0.2, 1, 5, 30) are tested, where the diffusion coefficient in the gas phase\nis Dg = 1 · 10−1 m2 /s and in the liquid phase Dl = 1 · 10−5 m2 /s. The initial gas\nconcentration is c = 1 and the liquid phase is initialized with c = 0.\n\nFigure 13. Set-up of test case to evaluate different corrected discretisations of the diffusive term. Here the species diffuses from\nthe gas phase (upper half) into the liquid phase (lower half). The\ndashed red line indicates the path over which the concentration c\nis plotted for evaluation reasons. The extension of the domain in\nthe y-direction is 0.04 m.\nIn order to obtain the time-dependent numerical exact reference solution, the\ndiffusion equation is discretised in 1D using the Finite Difference Method (FDM).\nHere the same numerical set-up (domain size, boundary and initial conditions) is\nemployed and the domain is sufficiently resolved. The solution which is obtained\n\n\fIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\n15\n\nby implicitly solving the system of equations with Octave version 3.2.4 is referred\nto later as Exact.\n5.2.2. Results. Applying a specific correction when discretising the face-interpolated\nvalues (Table 2b), the results in some cases slightly improve, in others, however, deteriorate. A possible explanation is that to effectively correct the calculation of the\nface-interpolated values within a steep profile, a small gradient stencil and, at the\nsame time, an accurate gradient computation are required. To maintain a small\nstencil, we utilize the Gaussian gradient for correction of the face-interpolation\n(Kc)f . However, as severely distorted meshes are used, the accuracy of the approximation suffers which is probably the cause for the observed poor performance.\nConsequently in the following only the variation of the gradient discretisation (Table 2a) is discussed while the face-interpolated values are discretised by simply using\nthe uncorrected CDS.\n1\n\nH: 0.033\n\n0.8\nc\n\n0.6\n0.4\n0.2\nH: 0.2\n\n0\nL3\n\nH: 1\nL3\n\nL3\n\n1\n0.8\n\nExact\nUC\nNO\nNO/NC\n\nc\n\n0.6\n0.4\n0.2\n0\n\nH: 5\n\nH: 30\nL3\n\nL3\n\nFigure 14. Profile of concentration c after time t = 0.5 s for different Henry coefficients H and correction strategies. The computational grid used is the systematically distorted hexahedral mesh\ndepicted in Figure 6 with an average cell width of ≈ 1.3 · 10−4 m.\nResults are visualized by plotting the profile of the concentration c over the path\n‘L3’ (see Figure 13). As shown in Figure 14, this is done for all considered Henry\ncoefficients H and correction strategies. Despite the heavily distorted mesh, all\napproaches lead to reasonable results. For all Henry coefficients, the uncorrected\napproach tends to overestimate the diffusion and therefore does not correspond to\nthe exact result. However, NO correction and NO/NC correction, result in a good\napproximation to the exact solution indicating that both approaches effectively\ncorrect the mesh-skewness induced errors (see Figure 15).\nEnlarged sections of two representative concentration profiles are shwon in Figure 15. It is shown that even for the lowest and highest Henry coefficients, no\nunboundedness occured, which indicates that no limiting of the correction terms is\nrequired. However, one might intuitively try limiting the diffusive coefficient in the\nNO/NC formulation (cf. Equation 19), which is also tested in the present work.\nDue to the lack of a mathematically sound limiter criterion, the most intuitive one\nis applied, which is to restrict the modified diffusion coefficient to positive values.\nWhen applied, the results deteriorated and the solution showed flickering in time.\n\n\f16\n\nIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\n1\n\n5\nExact\nUC\nNO\nNO/NC\n\n4\n\nExact\nUC\nNO\nNO/NC\n\n0.8\n0.6\n\nc\n\nc\n\n3\n2\n\n0.4\n\n1\n\n0.2\n\n0\n\n0\nL3\n\nL3\n\n(a) H=0.033.\n\n(b) H=30.\n\nFigure 15. Detailed view of concentration profiles for different\nHenry coefficients after time t = 0.5 s. The average cell width is\n≈ 1.3 · 10−4 m.\nBased on this finding and the fact that the limiting does not seem to be crucial no\nfurther investigations were carried out and all the results presented below do not\ninclude any limiting.\n\nL1 Error\n\n10−1\n\nH: 0.033\n\nH: 0.1\n\nH: 0.2\n\nH: 1\n\nH: 5\n\nH: 10\n\nH: 15\n\nH: 30\n\n10−2\n\n10−3\n\nL1 Error\n\n10−2\n10−3\n10−4\n\nL1 Error\n\n10−2\n\nUC\nNO\nNO/NC\nfirst order\nsecond order\n\n10−3\n10−4\n10−4\n\n10−3\n∆x\n\n10−3\n∆x\n\nFigure 16. Mesh convergence study at t = 0.5 s\nTo conclusively assess the convergence performance of the different schemes a\nmesh convergence study was conducted. The average molar concentration in the\ngas phase after t = 0.1 s and t = 0.5 s was chosen as error measure . The latter\n\n\fIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\n17\n\nL1 Error\n\nis almost constant as the diffusive coefficient in the gas phase exceeds that in the\nliquid phase by orders of magnitude. By this error definition the focus of the\nevaluation is not on the representation of the exact concentration profile (which we\nhave already investigated, see Figure 14 and 15) but on the correct prediction of\nthe time integrated species flux over the interface. Figures 16 and 17 show the L1\nerror plotted over the average cell size for each Henry coefficient at different times t.\nIt can be seen that the skewness-correction approaches employed generally lead to\na substantial improvement of accuracy and convergence order. In all cases at least\nfirst order, for H = 1 (heat transfer) even second order convergence is achieved.\nIn contrast the uncorrected discretization converges with a much lower order or\nexhibits no convergence at all.\nH: 0.1\n\nH: 0.2\n\nH: 1\n\nH: 5\n\nH: 10\n\nH: 15\n\nH: 30\n\n10−1\n\n10−2\n\n10−2\nL1 Error\n\nH: 0.033\n\n10−3\n10−4\n\nL1 Error\n\n10−2\n\nUC\nNO\nNO/NC\nfirst order\nsecond order\n\n10−3\n10−4\n10−4\n\n10−3\n∆x\n\n10−3\n∆x\n\nFigure 17. Mesh convergence study at t = 0.1 s\nThe observed convergence characterisic in the corrected case corresponds exactly\nto the formal order of the CST formulation, which is second order for H = 1 and\ntends to first order for H ≪ 1 and H ≫ 1 even on uniform cartesian meshes. It\ntherefore can be stated that applying the corrections restores the formal order of\nconvergence on skewed meshes.\nIn the uncorrected case if H ≤ 1 and in the corrected case if H = 30, unusual\nconvergence behaviour is detected, that is the plot shows a local minimum of error. This can be explained by the fact that in these cases the concentration profile\ncrosses the exact one when beeing evaluated using different mesh resolutions. In\nconsequence a minimum error is found at some random intermediate mesh resolution and seems to converge for lower and to diverge for higher resolutions. In\naddition, these cases obviously converge to a wrong solution. The magnitude of\nthe absolute error thus corresponds to the L1 error value it converges to for further\nrefined meshes.\n\n\f18\n\nIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\n(a) Distorted quadrilateral (b) Randomly distorted\nmesh.\ntetrahedral mesh.\n\nFigure 18. Structure of additionally generated meshes in order\nto further test correction of diffusion equation.\nBy considering both the convergence study and also the concentration profiles\ndepicted in Figure 14, the question about the impact of non-conjunctionality errors\nwhen discretising diffusive terms can finally be answered: it is indeed negligible,\nsince neither the NO nor the NO/NC profiles differ significantly, even for cases\nwhere sharp gradients are obviously present.\n1\n\nH: 0.033\n\n0.8\nc\n\n0.6\n0.4\n0.2\nH: 0.2\n\n0\nL3\n\nH: 1\nL3\n\nL3\n\n1\n0.8\n\nExact\nUC\nNO\nNO/NC\n\nc\n\n0.6\n0.4\n0.2\n0\n\nH: 5\n\nH: 30\nL3\n\nL3\n\nFigure 19. Profile of concentration c after time t = 0.5 s for\ndifferent Henry coefficients H and correction strategies. The computational grid used is the randomly distorted hexahedral mesh\ndepicted in Figure 18a with an average cell width of ≈ 4 · 10−4 m.\nConsidering the heavily distorted mesh, the achieved solution accuracy might be\nsurprising. Hence, we also raise the question as to whether errors may possibly be\ncancelled out because of the use of a systematically distorted mesh (see Figure 6).\nTo eliminate this possibility all test cases are additionally solved on a randomly\ndistorted hexahedral and tetrahedral mesh, both depicted in Figure 18, as these\nare often used in literature.\nFigures 19 and 20 show the results analogous to the already discussed set of\ngraphs in Figure 14. In both cases, independent of the selected discretisation\nmethod or Henry coefficient, the results almost perfectly match the solution, which\nleads to the conclusion that randomly distorted meshes (unlike the systematically\n\n\fIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\n1\n\n19\n\nH: 0.033\n\n0.8\nc\n\n0.6\n0.4\n0.2\nH: 0.2\n\n0\nL3\n\nH: 1\nL3\n\nL3\n\n1\n0.8\n\nExact\nUC\nNO\nNO/NC\n\nc\n\n0.6\n0.4\n0.2\n0\n\nH: 5\n\nH: 30\nL3\n\nL3\n\nFigure 20. Profile of concentration c after time t = 0.5 s for\ndifferent Henry coefficients H and correction strategies. The computational grid used is the tetrahedral mesh depicted in Figure\n18b. with an average cell width of ≈ 4 · 10−4 m.\n\ndistorted meshes studied here) result in error cancelation and are thus inappropriate\nfor the systematic investigation of skewness correction approaches.\n6. Summary and Conclusions\nThis work is aimed at the targeted investigation and enhancement of meshskewness correction approaches with special focus on implicit and bounded corrections in the context of VoF-based interfacial heat and mass transfer. The presence\nof a jump and/or steep gradients substantiate a sound reconsideration of the skewness correction approaches, which are commonly recognised as accepted knowledge\nin the literature. Discretisation procedures for the advection and diffusion terms\nare verified separately using different test cases on systematically distorted meshes.\nRandomly distorted meshes are shown to lead to error cancelation and thus are\nproven to be inappropriate for a systematic study of mesh-induced errors.\nFor the phase-fraction advection term we investigate the boundedness and accuracy achieved with different discretisation strategies, i.e. the shape-preservation\nand sharpness properties of different approaches to skewness correction. The main\nfinding is that implicit correction using the novel semi-implicit skewness correction\n(SISC) approach is imperative. Further, it is shown that the gradient scheme used\nfor computation of the correction term has a strong influence on accuracy. In our\nstudy, the performance of the least square gradient computation is found superior\nover Gauss gradient approximation.\nFor the diffusive heat/species transfer terms in the CST method, again we investigate the boundedness and accuracy achieved with different discretisation strategies.\nAdditionally, focus is on convergence on strongly distorted meshes. It is shown that\nthe approach for semi-implicit skewness correction (SISC) used for the advection\nterm can be adopted for diffusion terms as well. However, for the test case studied\nhere applying a relatively small time step, boundedness of the diffusion term is also\nattained using explicit correction approaches, although not a-priori guaranteed as\nin the implicit approach. Non-orthogonality correction (NO) has been found to\n\n\f20\n\nIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\nperform reasonably well even for the transport of fields with discontinuities. Simultaneuos implicit correction of non-orthogonality and non-conjunctionality errors\n(NO/NC) has shown no further benefit even for quantities exhibiting strong solution curvature, i.e. strong spatial changes in the gradient. Finally, the authors have\ndemonstrated that the convergence order on non-distorted meshes can be retained\neven on strongly distorted meshes only when suitable correction approaches such\nas NO or NO/NC are used.\nTo conclude, it is shown that the CST-method is able to predict accurate solutions on moderatly distorted meshes even without corrections. For severe distortions the accuracy can still be maintained by applying suitable, i.e. bounded\nimplicit, mesh-skewness corrections.\nReferences\n[1] Onno Ubbink. Numerical Prediction of Two Fluid Systems with Sharp Interfaces. PhD thesis,\nDepartment of Mechanical Engineering, Imperial College of Science, Technology & Medicine,\nJanuary 1997.\n[2] S. Muzaferija and M. Peric. Computation of Free-Surface Flows using Interface-Tracking\nand Interface-Capturing Methods. In O. Mahrenholtz and M. Markiewicz, editors, Advances in Fluid Mechanics, volume 24, pages 59–100. Computational Mechanics Publications,\nSouthampton, 1998.\n[3] O. Ubbink and R. I. Issa. A method for capturing sharp fluid interfaces on arbitrary meshes.\nJ. Comput. Phys., 153(1):26–50, 1999.\n[4] S. Muzaferija, M. Peric, P. Sames, and T. Schellin. A two-fluid Navier-Stokes solver to simulate water entry. In Twenty-Second Symposium on Naval Hydrodynamics, pages 638–649.\n1999.\n[5] H. Marschall, K. Hinterberger, C. Schüler, F. Habla, and O. Hinrichsen. Numerical simulation\nof species transfer across fluid interfaces in free-surface flows using OpenFOAM. Chem. Eng.\nSci., 78:111–127, 2012.\n[6] D. Deising, H. Marschall, and D. Bothe. A unified single-field model framework for VolumeOf-Fluid simulations of interfacial species transfer applied to bubbly flows. Chem. Eng. Sci.,\n139:173 – 195, 2016.\n[7] Hrvoje Jasak. Error analysis and estimation for the finite volume method with applications to\nfluid flows. PhD thesis, Department of Mechanical Engineering, Imperial College of Science,\nTechnology & Medicine, January 1996.\n[8] J.H. Ferziger and M. Peric. Computational Methods for Fluid Dynamics. Springer Verlag\nBerlin Heidelberg, 2002.\n[9] F. Moukalled, L. Mangani, and M. Darwish. The Finite Volume Method in Computational\nFluid Dynamics – An advanced introduction with OpenFOAM and Matlab. Springer International Publishing Switzerland, 2015.\n[10] Thomas Nicholas Croft. Unstructured Mesh – Finite Volume Algorithm for Swirling, Turbulent, Reacting Flows. PhD thesis, School of Computing and Mathematical Scienes – University\nof Greenwich, 1998.\n[11] S.J. Zhang and X. Zhao. Higher Order Data Reconstruction Schemes for Unstructured Grid.\nIn 36th AIAA Fluid Dynamics Conference and Exhibit, San Francisco, California, June 5-8\n2006.\n[12] Sijun Zhang and Kunal Jain. Least square data reconstruction approach for unstructured\ngrid. In Proceedings of 5th Joint ASME/JSME Fluids Engineering Conference, 2007.\n[13] Fabian Denner and Berend van Wachem. Compressive VOF method with skewness correction\nto capture sharp interfaces on arbitrary meshes. J. Comput. Phys., 279:127–144, 2014.\n[14] Fabian Denner and Berend van Wachem. TVD differencing on three-dimensional unstructured meshes wwith monotonicity-preserving correction of mesh skewness. J. Comput. Phys.,\n298:466–479, 2015.\n[15] Joao Magalhaes, Duarte Albuquerque, Jos M.C. Pereira, and Jos C.F. Pereira. Adaptive\nmesh finite-volume calculation of 2D lid-cavity corner vortices. J. Comput. Phys., 243:365–\n381, 2013.\n[16] Duarte Albuquerque. Numerical Computation of Incompressible Flows on Adaptive and Unstructured Grids. PhD thesis, Instituto Superior Technico, Lisboa, Portugal, 2013.\n[17] Yu Jiang and J. Przekwas. Implicit, pressure-based incompressible Navier-Stokes equations\nsolver for uunstructured meshes. In 32nd Aerospace Sciences Meeting and Exhibit, San Francisco, California, June 5-8 1994.\n\n\fIMPLICIT MESH-SKEWNESS CORRECTION FOR VOF BASED MASS TRANSFER\n\n21\n\n[18] Lars Davidson. A pressure correction method for unstructured meshes with arbitrary control\nvolumes. International Journal for Numerical Methods in Fluids, 22:265–281, 1996.\n[19] I. Demirdzic and M. Peric. Finite Volume Method for prediciton of flud flow in arbitrarily\nshaped domains with moving boundaries. Numerical Methods in Fluids, 10:771–790, 1990.\n[20] I. Demirdzic and S. Muzaferija. Numerical method for coupled fluid flow, heat transfer and\nstress analysis using unstructured moving meshes with cells of arbitrary topology. Comput.\nMethod. Appl. M., 125:235–255, 1995.\n[21] S. R. Mathur and J.Y. Murthy. A pressure-based method for unstructured meshes. Numerical\nHeat Transfer, Part B: Fundamentals, 31:195–215, 1997.\n[22] I. Demirdzic. On the discretization of the diffusion term in Finite-Volume continuum mechanics. Numerical Heat Transfer, Part B: Fundamentals, 68:1–10, 2015.\n[23] C.D. Perez-Segarra, C. Farre, M. Soria, and J. Cadafalch. Analysis of differrent numerical\nschemes for the resolution of convection-diffusion equations using Finite Volume mmethod on\nunstructured grids. In 42nd AIAA Aerospace Sciences Meeting and Exhibit, Reno, Nevada,\nJanuary 5-8 2004.\n[24] C. Hirsch. Numerical computation of internal and external flows – The fundamentals of\nComputational Fluid Dynamics, volume 1. John Wiley & Sons, Ltd., second edition edition,\n2007.\n[25] Ami Harten. High resolution schemes for hyperbolic conservation laws. J. Comput. Phys.,\n49(3):357 – 393, 1983.\n[26] Anthony Jameson. Positive schemes and shock modeling for compressible flows. International\nJournal for Numerical Methods in Fluids, 20:743–776, 1995.\n[27] B.P. Leonard. Universal Limiter for Transient Interpolation Modeling of the Advective Transport Equations: The ULTIMATE Conservative Differencing Scheme. Technical report, Nasa\nTechnical Memorandum 100916, 1988.\n[28] P. Sweby. High resolution schemes using flux limiters for hyperbolic conservation laws. SIAM\nJ. Numer. Anal., 21(5):995–1011, 1984.\n[29] H. Jasak, H.G. Weller, and A.D. Gosman. High Resolution NVD differencing scheme for\narbitrarily unstructured meshes. Int. J. Numer. Meth. Fluids, 31:431–449, 1999.\n[30] M.S. Darwish and F. Moukalled. TVD schemes for unstructured grids. International Journal\nof Heat and Mass Transfer, 46:599–611, 2003.\n[31] V. Przulj and B. Basara. Bounded convection schemes for unstructured grids. In 15th AIAA\nComputational Fluid Dynamics Conference, Anaheim, CA, June 11-14 2001.\n[32] Lian xia Li, Hua sheng Liao, and Li jian Qi. An improved r-factor algorithm for TVD schemes.\nInternational Journal of Heat and Mass Transfer, 51:610–617, 2008.\n[33] J. Hou, F. Simons, and R. Hinkelmann. Improved total variation diminishing schemes for advection simulation on arbitrary grids. International Journal of Numerical Methods in Fluids,\n70:359–382, 2012.\n[34] R. Löhner. Robust, vectorized search algorithms for interpolation on unstructured grids. J.\nComput. Phys., 118:380–387, 1995.\n[35] Tomislav Maric, Holger Marschall, and Dieter Bothe. lentFoam – A hybrid Level Set/Front\nTracking method on unstructured meshes. Computers & Fluids, 113:20–31, 2015.\n\n\f"
        ],
        [
         "34",
         "34",
         "cs.CE",
         "Computational Engineering",
         "1507.01894v2.pdf",
         "On pore–scale modeling and simulation of reactive transport in 3D\ngeometries\nOleg Ilieva,b,∗, Zahra Lakdawalaa,c, Katherine H. L. Leonarda, Yavor Vutovd\na Fraunhofer Institute for Industrial Mathematics ITWM, Germany.\nPorous Media SRI Center, King Abdullah University of Science and Technology, Kingdom of Saudi Arabia.\nc DHI-WASY GmbH, Berlin, Germany.\nd Institute of Information and Communication Technologies, Department of Scientific Computations, Bulgarian Academy\nof Sciences, Sofia, Bulgaria.\nb Numerical\n\narXiv:1507.01894v2 [cs.CE] 8 Jul 2015\n\nAbstract\nPore–scale modeling and simulation of reactive flow in porous media has a range of diverse applications,\nand poses a number of research challenges. It is known that the morphology of a porous medium has\nsignificant influence on the local flow rate, which can have a substantial impact on the rate of chemical\nreactions. While there are a large number of papers and software tools dedicated to simulating either\nfluid flow in 3D computerized tomography (CT) images or reactive flow using pore–network models, little\nattention to date has been focused on the pore–scale simulation of sorptive transport in 3D CT images,\nwhich is the specific focus of this paper. Here we first present an algorithm for the simulation of such\nreactive flows directly on images, which is implemented in a sophisticated software package. We then\nuse this software to present numerical results in two resolved geometries, illustrating the importance of\npore–scale simulation and the flexibility of our software package.\nKeywords: Reactive transport modeling, Pore–scale model, Finite volume method, CFD, Surface\nreactions, Filtration\n\n1. Introduction\nUnderstanding and controlling reactive flow in porous media is important for a number of environmental and industrial applications, including oil recovery, fluid filtration and purification, combustion and\nhydrology. Traditionally, the majority of theoretical and experimental research into transport within\nporous media has been carried out at macroscopic (Darcy) scale. Despite the progress in developing\ndevices to perform experimental measurements at the pore–scale, experimental characterization of the\npore–scale velocity, pressure and solute fields is still a challenging task. Due the influence of the pore–\nscale geometry on the processes of interest, direct numerical simulation (DNS) promises to be a very\nuseful computational tool in a wide range of fields, and in combination with experimental studies, can\nbe used to determine quantities of interest that are not experimentally quantifiable (Blunt et al., 2013).\nSignificant progress over the past 10 – 15 years in the pore–scale simulation of single phase flow has\nresulted in the computation of permeability tensors for natural and technical porous media becoming\na standard procedure. A number of academic as well as commercial software tools, capable of processing 3D CT images in addition to virtually generating porous media, are available, for example Aviso,\nGeoDict and Ingrain. Most of those software tools have the additional ability of simulating two–phase\nimmiscible flow at the pore–scale directly on a computational domain obtained through the segmentation\nof 3D CT images, often using the lattice Boltzmann (LB), the level set or volume of fluid methods. In\ncontrast, substantially less work on the DNS of reactive flow has been performed, and only a few software\ntools with this capability exist. A limited number of computational studies examining reactive transport\nwhere the reactions only occur within the fluid phase (and not at a surface) exist (Molins et al., 2012;\nShen et al., 2011). In contrast, the literature and computational tools examining full 3D reactive flow\n∗ Corresponding\n\nauthor\nEmail addresses: iliev@itwm.fraunhofer.de (Oleg Iliev), zla@dhigroup.com (Zahra Lakdawala),\nleonard@itwm.fraunhofer.de (Katherine H. L. Leonard), yavor@parallel.bas.bg (Yavor Vutov)\nPreprint submitted to Elsevier\n\nJuly 9, 2015\n\n\fwhere the reactions occur at the pore wall (surface reactions), is sparse. The majority of existing studies and available numerical simulation packages describing pore–scale sorptive transport are based on\npore–network mathematical models (see, for example, Raoof et al. (2010); Varloteaux et al. (2013) and\nliterature therein), where the geometry needs to be converted into an idealized series of connected pores\nand throats to represent the porous medium. However, during this process, information on the morphology of the underlying media can be lost (see, for example, Raoof et al. (2010) and Lichtner and Kang\n(2007)). In this paper we present an algorithm and a sophisticated software package, called Pore–Chem,\nwhich uses cell centered finite volume (FV) methods to numerically solve 3D solute transport with surface reactions at the pore–scale. In particular the software package has the ability to solve the systems of\nequations modeling colloidal reactive transport on a geometrical domain obtained directly through imaging techniques, such as computerized tomography, which allows for a very accurate spatial description\nof the computational domain.\nIn this paper we describe the transport of a generic solute through the Navier–Stokes (NS) system\nof equations coupled to a convection–diffusion (CD) equation. The CD equation is complemented by\nboundary conditions which describe various types of surface reactions comprising a Robin boundary\ncondition for the solute coupled to an ordinary differential equation (ODE) describing the dynamics\nof the adsorption at the pore wall. To represent the computational domain, a voxelized geometry is\nconsidered, where each individual voxel is either solid or fluid. The solute transport model is chosen\ndue to its applicability to a broad range of problems. In addition, our goal is to describe the transport\nand reaction of sub–micron particles, for which inertial effects of the individual particles are negligible.\nDiscrete models, where each particle is modeled as an individual entity, are necessary when considering\nlarger particles, for example with a radius greater than one micron, for which inertial effects become\nimportant. Several commercial software packages, for example GeoDict (GeoDict, 2012–2014), solve a\nrange of discrete mathematical models describing colloid transport and adsorption. However, numerical\nsimulation of these models is often significantly more computationally expensive than a continuous\nmathematical model and accounting for different reaction kinetics is usually not possible in such packages.\nReactive flow in porous media is intrinsically a multiscale problem. The goal of our developments is\nto support problems where scale separation is possible and in cases where it is not possible. The first\ncase, where the separation of scales is viable, is usually the focus of asymptotic homogenization theory.\nIn the second case, when scale separation is not possible, numerical upscaling methods like multiscale\nfinite element methods are often applied. During the homogenization procedure, when applicable, certain\nassumptions are imposed, allowing for the derivation of macroscopic (Darcy scale) equations from the\nmicroscale formulation, with effective parameters, such as the permeability and the effective reaction\nrate, obtained through solution of a cell problem (Hornung, 1997). A number of studies have employed\nhomogenization theory to derive a macroscopic description of sorptive reactive transport for particular\nparameter regimes. The homogenization of solute transport in porous media in the presence of surface\nreactions has been performed for both high Péclet numbers (convection dominated regime) (Allaire et al.,\n2010b,a; Allaire and Hutridurga, 2012), and when the Péclet number is of order one (Hornung, 1994;\nKumar et al., 2013; van Duijn, 1991). In addition to being able to solve cell problems in a number of\nsettings, our software has the ability of solving a much broader class of problems at the pore–scale,\nwithout being restricted by the assumptions required during homogenization. Furthermore, it provides\nthe possibility to study various different types of surface reactions described by different kinetics.\nThe remainder of the paper is organized as follows. In Section 2 the mathematical model is presented\nand cast into dimensionless variables. The method of achieving a numerical solution to the system of\nequations is outlined in Section 3, and illustrative results using this method are presented in Section 4.\nFinally, conclusions are drawn in Section 5.\n2. Mathematical model\nWe now detail the mathematical model, which describes the transport and reaction of a generic solute\nat a 2D interface within a 3D pore–scale resolved geometry, where we assume that the number of solute\nparticles is sufficiently large that representation within a continuum framework is valid.\nLet us denote the spatial domain of interest by Ω, an open subset of R3 . We assume that we\ncan decompose Ω into a solid domain, denoted by Ωs , and a fluid domain, denoted by Ωf , such that\nΩ = Ωs ∪ Ωf . Denoting the external boundary of our domain, being the closure of Ω, by ∂Ω, we partition\nthis into an inlet, ∂Ωin , an outlet, ∂Ωout , and external walls, ∂Ωwall , so that\n∂Ω = ∂Ωin ∪ ∂Ωout ∪ ∂Ωwall .\n2\n\n(1)\n\n\fWe note that, although we here consider only one inlet and one outlet, extension to consider multiple\ninlets and outlets is straightforward. Finally, we denote the interfacial boundary between the fluid and\nsolid portions of the domain by Γ = Ωf ∩ Ωs . To allow for different types of reactive boundary conditions\nto be described, we decompose Γ into N ≥ 1 different boundary types as follows\n−1\nΓ = ∪N\ni=0 Γi ,\n\n(2)\n\nwhere Γi has distinct properties. Making such a decomposition enables us to attribute different reaction\nrates or kinetic descriptions to different portions of the domain, allowing for different types of solid\nmaterial.\nIn order to describe the flow of the water through the membrane, by appealing to the conservation\nof momentum, the incompressible NS equations are used:\n\u0012\n\u0013\n∂v\n(3a)\nρ\n+ v · ∇v = −∇p + µ∇ · (∇v) ,\n∂t\nx ∈ Ωf , t > 0,\n(3b)\n∇ · v = 0,\nwhere v(x, t) and p(x, t) are the velocity and pressure of the fluid respectively, while µ ≥ 0 and ρ ≥ 0\nare the viscosity and the density of the fluid which we assume are constants (Bear, 1988; Acheson, 1995).\nSuitable boundary conditions on ∂Ω are given by\nv = Vin ,\np = Pout\n\nx ∈ ∂Ωin ,\nx ∈ ∂Ωout ,\n\nv = 0,\n\nx ∈ ∂Ωwall ,\n\nt > 0.\n\n(4a)\n(4b)\n(4c)\n\nwhere Vin is the inlet velocity with kVin k > 0, Pout is the pressure at the outlet, and n is the outward\npointing normal to the boundary ∂Ω. Although here we have used no–slip and no–flux flow conditions\nfor the external walls, symmetry or periodic boundary conditions can also be imposed which may be\nmore appropriate depending on the problem to be solved. Further boundary conditions are required to\nbe specified on the reminder of the boundary to Ωf , being the fluid–solid interface. To allow for the slip\nof flow along the fluid–solid interface, and the inclusion of additional effects such as charged fluids or\nmatrices, we use the Navier–Maxwell slip conditions, given by\n\u0011\n\u0010\nT\nx ∈ Γi , t > 0,\n(5)\nv · n = 0,\nv · t = βi n · ∇v + (∇v) · t,\n\nwhere βi is the slip length on x ∈ Γi for i = 0, 1, . . . N − 1 measured per unit length, t is any unit tangent\nto the surface such that t · n = 0, and the superscript T denotes the transpose (Lauga et al., 2007). In\nthe case that βi = 0 for some i, then the standard no–slip and no–flux boundary conditions for the flow\nare enforced on Γi . We specify initial conditions through\nv(x, 0) = v0 (x),\n\np(x, 0) = p0 (x),\n\nx ∈ Ωf ,\n\n(6)\n\nwhere v0 and p0 are known functions. We discuss the choice for these in Section 3.\nWe denote the concentration of the solute within the fluid by c(x, t), measured in particle number\nper unit volume. Appealing to the conservation of mass and assuming no fluid–phase reactions occur,\nthe spatio–temporal evolution of the solute concentration is given by\n∂c\n+ ∇ · (vc) = D∇ · (∇c) ,\n∂t\n\nx ∈ Ωf , t > 0,\n\n(7)\n\nwhere D ≥ 0 is the solute diffusion coefficient which we assume to be scalar and constant. We assume a\nknown concentration of the solute at the inlet, and prescribe zero flux of the solute at the outlet and on\nthe external walls as follows:\nc = cin ,\n\nx ∈ ∂Ωin ,\n\n∇c · n = 0,\n\nx ∈ ∂Ωout ∪ ∂Ωwall ,\n\nwhere cin > 0 is assumed to be constant.\n3\n\nt > 0,\n\n(8a)\n(8b)\n\n\f2.1. Models for surface reactions\nWe are required to specify boundary conditions for c(x, t) on x ∈ Γi , to describe the surface reactions\noccurring here. In general, there are two stages of adsorption of a particle from the bulk solution to the\nsolid surface. The first stage involves the diffusion of particles from the bulk solution to the subsurface\nand the second stage then involves the transfer of particles from the subsurface to the surface. After the\nadsorption of a molecule at the interface, there is a reorientation of the colloid molecules at the surface,\nwhich results in a change in the surface tension (Kralchevsky et al., 2008). Assuming that both the rate\nof diffusion of the particle from the bulk to the subsurface, and the rate of the transfer of the particles\nfrom the subsurface to the surface are important in determining the overall rate of reaction, we use a\nmixed kinetic–diffusion adsorption description, given by\n− D∇c · n =\n\n∂m\n= fi (c, m),\n∂t\n\nx ∈ Γi . t > 0,\n\n(9)\n\nHere m(x, t) is the surface concentration of the particle under consideration (Kralchevsky et al., 2008),\nmeasured in units of number per unit surface area, which contrasts with c(x, t) being measured in units\nof number per unit volume. The function fi (c, m) describes the kinetics of the rate of change of the\nsurface concentration on the ith reactive boundary for i = 0, . . . N − 1 (Danov et al., 2002). Equation (9)\nstates that the change in the surface concentration is equal to the flux across the surface, where the\nmovement from the bulk to the surface is termed adsorption, and movement from the surface to the bulk\nis termed desorption. If Γi is nonreactive, for some i, then fi = 0, so the adsorbed concentration on\nthis boundary type remains constant, and a no–flux boundary condition for the solute concentration is\nprescribed. For reactive boundaries, the choice of fi and its dependence on c and m is highly influential\nin correctly describing the reaction dynamics at the solid–fluid interface. A number of different isotherms\nexist for describing these dynamics, dependent on the solute attributes, the order of the reaction, and\nthe interface type.\nThe simplest of these is the Henry isotherm, which assumes a linear relationship between the surface\npressure and the number of adsorbed particles, and takes the form\nfi = κa,i c − κd,i m,\n\nx ∈ Γi , t > 0.\n\n(10)\n\nHere κa,i ≥ 0 is the rate of adsorption, measured in unit length per unit time, and κd,i ≥ 0 is the rate of\ndesorption, measured per unit time, at reactive boundary type i for i = 0, . . . N − 1. Equation (10) states\nthat the rate of adsorption is proportional to the concentration of particles in solution at the reactive\nsurface. As such, the rate of adsorption predicted does not saturate at higher surface concentrations.\nHowever, physically we expect the rate of adsorption to decrease as the quantity of adsorbed particles\nincreases and the available surface area for adsorption decreases. Even though the Henry isotherm\npredicts no limit to surface concentration and does not model any interaction between the particles, it\nhas been used in a large number of analytical studies due to its linearity.\nThe Langmuir adsorption isotherm was the first to be derived mathematically, and is suitable to\ndescribe the adsorption of a monolayer of localized non–ionic non–interacting molecules at a 2D solid\ninterface, and a derivation from statistical physics may be found in (Baret, 1969). It is also frequently\nused to describe the adsorption of molecules at a solid–liquid interface, and is described by\n\u0013\n\u0012\nm\n− κd,i m,\nx ∈ Γi , t > 0.\n(11)\nfi = κa,i c 1 −\nm∞,i\nHere m∞,i > 0 is the maximal possible adsorbed surface concentration, measured in number per unit\narea, at reactive boundary type i for i = 0, . . . N − 1. In comparison to the Henry isotherm, the\nLangmuir isotherm predicts a decrease in the rate of adsorption as the adsorbed concentration increases\ndue to the reduction in available adsorption surface. The Henry isotherm, given in Equation (10), is a\nlinearization of Equation (11), explaining why it produces an accurate representation only at low surface\nconcentrations.\nMore complex descriptions of the reaction kinetics exist to describe non–localized adsorption and\nparticles which interact. For example, the Frumkin isotherm describes localized adsorption where particle\ninteraction is accounted through an additional parameter:\n\u0013\n\u0013\n\u0012\n\u0012\n2βm\nm\n,\nx ∈ Γi , t > 0.\n(12)\n− κd,i m exp −\nfi = κa,i c 1 −\nm∞,i\nkT\n4\n\n\fwhere k is the Boltzmann constant and T is the temperature in Kelvin. The parameter β ≥ 0 describes\nhow cooperative the reaction is, and is related to the interaction energy between the particles. In the\ncase that β = 0, the Langmuir isotherm is recovered. The Frumkin isotherm is infrequently used in the\nmathematical modeling of colloidal transport, which may be due its nonlinearity and the difficulties in\ndetermining the interaction energy between the particles.\nWe make the assumption that the adsorption or desorption of our solute does not alter the position\nof the reactive boundary, which in the case of small volumes of particles being adsorbed is sufficiently\naccurate. By the conservation of mass, such an assumption implies any adsorption or desorption on the\nsurface is represented by a corresponding increase or decrease in the density of the solid material through\ntime. In some cases, for example when the molecules are big or the number being adsorbed is large,\ninterface evolution needs to be considered and may be done in a similar manner to Tartakovsky et al.\n(2008); Roubinet and Tartakovsky (2013) and Boso and Battiato (2013). This is particularly important\nin the application of rock dissolution and precipitation, where large geometrical changes are observed.\nTo close the system of equations, we impose the initial conditions\nc(x, 0) = c0 (x),\n\nx ∈ Ωf ,\n\nm(x, 0) = m0,i (x),\n\nx ∈ Γi ,\n\n(13)\n\nwhere c0 and m0,i are known for i = 0, 1, . . . N − 1.\nOur problem is, therefore, described by two systems of equations with one–way coupling; the incompressible NS equations, described by (3) – (6) and the CD equation described by (7) – (8), with reactive\nboundaries conditions (9), initial conditions (13), and a description of the reaction kinetics, for example\nEquation (10), (11) or (12). We now cast the equations into dimensionless variables, before detailing the\nmethods used to obtain a numerical solution.\n2.2. Nondimensionalization\nUsing a caret notation to distinguish the dimensionless variable from its dimensional equivalent, we\nlet\nx = Lx̂,\n\nv = Vin v̂,\n\nt=\n\nLt̂\n,\nVin\n\np = Pout + ρVin2 p̂,\n\nc = cin ĉ,\n\nm = cin Lm̂,\n\nM = cin L3 M̂ ,\n\nwhere L > 0 is a typical length scale of the computational domain and Vin = kVin k. As our computational\ndomain consists of voxels, the relationship between each voxel and its material property is conserved\nupon nondimensionalization, while the length, area and volume of each voxel scales with L, L2 and\nL3 respectively. Given this, we let Ω̂, Ω̂s and Ω̂f , with boundaries ∂ Ω̂in , ∂ Ω̂out , ∂ Ω̂wall and Γ̂i for i =\n0, 1, . . . N −1 represent the dimensionless versions of the equivalent dimensional domains and boundaries,\nwhere the voxel size is scaled accordingly. In dimensionless variables, we, therefore, have\n\u0013\n\u0012\n∂ v̂\nˆ 2 v̂,\nˆ\nˆ + 1 ∇\n+ v̂ · ∇v̂ = −∇p̂\n(14a)\nRe\n∂ t̂\nˆ · v̂ = 0,\nx̂ ∈ Ω̂f , t̂ > 0,\n∇\n(14b)\n\u0010\n\u0011\n1\n∂ĉ\nˆ · (v̂ĉ) =\nˆ · ∇ĉ\nˆ ,\n+∇\n∇\n(14c)\nPe\n∂ t̂\nwhere\n\nRe =\n\nLρVin\n,\nµ\n\nPe =\n\nVin L\n,\nD\n\n(15)\n\nare the global Reynolds and Péclet numbers respectively, being the ratio between the inertial and viscous\nforces and the ratio between advective and diffusive transport rates respectively. Boundary conditions\n\n5\n\n\fare given by\nv̂ = n,\n\nx̂ ∈ ∂ Ω̂in ,\n\n(16a)\n\nˆ ·n=0\np̂ = 0 and ∇v̂\n\nx̂ ∈ ∂ Ω̂out ,\n\n(16b)\n\nv̂ = 0,\n\nx̂ ∈ ∂ Ω̂wall ,\n\n(16c)\n\n\u0012\n\u0010 \u0011T \u0013\nˆ + ∇v̂\nˆ\nv̂ · n = 0, and v̂ · t = β̂i n · ∇v̂\n· t,\n\nx̂ ∈ Γ̂i ,\n\n1 ˆ\n∂ m̂\n∇ĉ · n =\n= fˆi (ĉ, m̂),\nPe\n∂ t̂\nĉ = 1,\nˆ · n = 0,\n∇ĉ\nwith β̂i =\n\nt̂ > 0.\n\n(16d)\n\nx̂ ∈ Γ̂i ,\n\n(16e)\n\nx̂ ∈ ∂ Ω̂in ,\n\n(16f)\n\nx̂ ∈ ∂ Ω̂out ∪ ∂ Ω̂wall ,\n\n(16g)\n\nβi\n. In the case of the Henry isotherm, we have\nL\nfˆi = DaIa,i ĉ − DaId,i m̂,\n\nx̂ ∈ Γ̂i , t̂ > 0,\n\n(17)\n\nwhereas, nondimensionalization of the Langmuir and Frumkin isotherms yields\n\u0013\n\u0012\nm̂\nI\nˆ\n− DaId,i m̂,\nx̂ ∈ Γ̂i , t̂ > 0,\nfi = Daa,i ĉ 1 −\nm̂∞,i\nand\n\n\u0012\nfˆi = DaIa,i ĉ 1 −\n\nm̂\nm̂∞,i\n\n\u0013\n\n\u0010\n\u0011\n− DaId,i m̂ exp −β̂ m̂ ,\n\nrespectively, for i = 0, 1, . . . N − 1, where β̂ =\nnumbers are given by\nDaa,i =\n\nκa,i\n,\nVin\n\n(18)\n\nx̂ ∈ Γ̂i , t̂ > 0,\n\n(19)\n\n2βcin L\nm∞,i\nand m̂∞,i =\n. In (17) – (19) the Damköhler\nkT\ncin L\nand\n\nDad,i =\n\nκd,i L\n,\nVin\n\n(20)\n\nand describe, for each i, the ratio of the rate of reaction (either adsorptive or desorptive) to the rate of\nadvective transport. The initial conditions are given by\nv̂(x̂, 0) = v̂0 (x̂),\nm̂(x̂, 0) = m̂0,i (x̂),\n\np̂(x̂, 0) = p̂0 (x̂),\n\nĉ(x̂, 0) = ĉ0 (x̂),\n\nx̂ ∈ Ω̂f ,\n\n(21a)\n\nx̂ ∈ Γ̂i ,\n\n(21b)\n\nv0 (x)\np0 (x) − Pout\nc0 (x)\nm0,i (x)\n, p̂0 (x̂) =\n, ĉ0 (x̂) =\nand m̂0,i (x̂) =\nfor i = 0, 1, . . . N − 1.\nVin\nρVin2\ncin\ncin L\nWe now proceed to discuss the numerical methods used to obtain an approximate solution to our\ndimensionless system of equations given by (14), with boundary conditions given by (16) – (18) and\ninitial conditions specified through (21).\nwhere v̂0 (x̂) =\n\n3. Numerical methods\nThe full system of equations cannot be solved using analytical techniques, and so numerical methods\nneed to be employed to calculate an approximate solution. We employ FV methods to numerically\nsolve our system of equations, motivated by their local mass conserving properties. Other methods, for\nexample LB or finite difference methods may also be used for solving the flow problem. We note that\nour CD solver is completely compatible with LB methods (the compatibility of our solver with finite\ndifference methods depends on the grid selection). Although the authors are not aware of a detailed\ncomparison of the performance of FV and LB methods in solving the NS equations at the pore–scale,\nsome incomplete internal studies indicate that LB methods can be advantageous for geometries with a\nvery low porosity and a high tortuosity, while FV methods are favorable in other cases.\nDue to the one–way coupling between our two systems of equation, the velocity and pressure solutions\nare at steady state. For the sake of generality, we consider the unsteady equations, and begin by solving\n6\n\n\fthe system of equations describing the fluid flow, namely (14a), (14b) along with (16a) – (16d), to obtain\n∂ v̂\na steady state numerical solution where\n= 0 is satisfied. This is achieved using a Chorin fractional\n∂ t̂\ntimestepping method and we refer the reader to Čiegis et al. (2007) and Lakdawala (2010) for full details\nand for further references, where the methods used are described.\nOnce the solution of v̂ is obtained, we proceed to solve the system of equations describing the solute\ntransport and reaction, (14c) along with the boundary conditions (16e) – (18) and initial conditions\n(21), using a FV method with a cell-centered grid. For the sake of brevity, full details of the numerical\nmethod employed are not given, but we refer the reader to, for example Causon et al. (2011), for more\ndetailed information on FV methods. Firstly, dimensionless\ntime is uniformly partitioned into Q time\n\u0001\npoints, denoted by t̂0 , t̂1 , . . . t̂Q−1 with t̂k = k ∆t̂ , where ∆t̂ is the dimensionless timestep size. Then\nthe spatial domain, Ω̂, is split into P non–overlapping cubic finite volumes, Bl for l = 0, 1, . . . P1 , which\nP −1\nspan the 3D computational domain, such that Ω̂ = ∪l=0\nBl . Considering a single representative finite\nvolume, Bl , we denote its six faces by Fl,j with center x̂j where the subscript j = e, w, n, s, t, b denotes\nthe east, west, north, south, top and bottom faces respectively. Integration of (14c) over the control\nvolume, Bl , and time interval [t̂k , t̂k+1 ], upon application of the divergence theorem, yields\nZ t̂k+1 Z\n1\nˆ · n dV dτ,\n∇ĉ\nĉ(x̂, t̂ ) dV +\nv̂ĉ · n dS dτ =\nĉ(x̂, t̂ ) dV −\nPe t̂k\n∂Bl\nt̂k\n∂Bl\nBl\nBl\nP\nwhere ∂Bl is the boundary of Bl , so that ∂Bl = j=e,w,n,s,t,b Fl,j . Denoting the center of the finite\nZ\nZ\nφ(x̂, t̂)dS = |Bl |φ(x̂c , t̂),\nφ(x̂, t̂)dS = Âj φ(x̂j , t̂) and\nvolume by xc and using the approximations\nZ\n\nZ\n\nk+1\n\nZ\n\nk\n\nt̂k+1\n\nZ\n\nBl\n\nFl,j\n\nfor some scalar function φ for j = n, s, e, w, t, b, where Âj is the area of the face Fl,j , we have\n\u0001\n|Bl | ĉ(x̂c , t̂k+1 ) − ĉ(x̂c , t̂k )\nZ t̂k+1 \u0010\n\u0011\nÂe [v̂ĉ]x̂e − Âw [v̂ĉ]x̂x + Ân [v̂ĉ]x̂n − Âs [v̂ĉ]x̂s + Ât [v̂ĉ]x̂t − Âb [v̂ĉ]x̂b dτ\n+\nt̂k\n\n1\n=\nPe\n\nZ\n\nt̂k+1\n\nÂe\n\nt̂k\n\n\u0014\n\n∂ĉ\n∂ x̂\n\n\u0015\n\n− Âw\nx̂e\n\n\u0014\n\n∂ĉ\n∂ x̂\n\n\u0015\n\n+ Ân\nx̂w\n\n\u0014\n\n∂ĉ\n∂ ŷ\n\n\u0015\n\n− Âs\nx̂n\n\n\u0014\n\n∂ĉ\n∂ ŷ\n\n\u0015\n\n+ Ât\nx̂s\n\n\u0014\n\n∂ĉ\n∂ ẑ\n\n\u0015\n\n− Âb\nx̂t\n\n\u0014\n\n∂ĉ\n∂ ẑ\n\n\u0015 !\n\ndτ.\n\nx̂b\n\nDenoting ĉj (τ ) = ĉ(x̂j , τ ) and v̂j (τ ) = v̂(x̂j , τ ) for j = n, s, e, w, t, b, c, by first order finite difference\nmethods we have\n|Bl | ĉc (t̂\n=\n\nZ\n\nk+1\n\nt̂k+1\nt̂k\n\nk\n\n\u0001\n) − ĉc (t̂ ) +\n\n2\nPe\n\nZ\n\nt̂k+1\n\nÂe v̂e ĉe − Âw v̂w ĉw + Ân v̂n ĉn − Âs v̂s ĉs + Ât v̂t ĉt − Âb v̂b ĉb dτ\nt̂k\n\n\u0012\n\u0013\nĉe − ĉc\nĉc − ĉw\nĉn − ĉc\nĉc − ĉs\nĉt − ĉc\nĉc − ĉb\ndτ, (22)\nÂe\n− Âw\n+ Ân\n− Âs\n+ Ât\n− Âb\nδx̂\nδx̂\nδ ŷ\nδ ŷ\nδẑ\nδẑ\n\nwhere δx̂, δ ŷ and δẑ are the width, length and height of the control volume Bl . By virtue of using\n2\n3\nvoxelized geometry, we know that δx̂ = δ ŷ = δẑ, Âj = (δx̂) for all j = n, s, e, w, t, b, and |Bl | = (δx̂) .\nBy the implicit Euler method, we, therefore, have\n− ĉkc\nδx̂ ĉk+1\nc\n∆t̂\n\n\u0001\n\n+ v̂ek+1 ĉk+1\n− v̂w ĉk+1\n+ v̂nk+1 ĉk+1\n− v̂sk+1 ĉk+1\n+ v̂tk+1 ĉk+1\n− v̂bk+1 ĉk+1\ne\nw\nn\ns\nt\nb\n=\n\n\u0001\n2\n, (23)\nĉk+1 + ĉk+1\n+ ĉk+1\n+ ĉk+1\n+ ĉk+1\n+ ĉk+1\n− 6ĉk+1\nw\nn\ns\nt\nc\nb\nPe δx̂ e\n\nwhere ĉkj = ĉ(x̂j , t̂k ) for j = n, s, e, w, t, b, c. In the case that one of the faces of the control volume lies on\na boundary, the appropriate boundary conditions must be discretized; for the inlet, outlet and solid (or\nsymmetry) boundaries this is straightforward due to the Dirichlet and zero Neumann boundary condition\nimposed there via (16f) and (16g). Therefore, we omit the details for the discretization of the boundary\nconditions on the external boundary ∂ Ω̂. The appropriate discretization of the reactive boundary conditions, prescribed on the fluid–solid interface through (16e) and the corresponding description of the\nreaction kinetics, here either (17), (18) or (19), is slightly more involved and deserves a more detailed\ndiscussion.\n7\n\n\fIn a fully implicit and coupled discretization the resulting discrete equations are nonlinear and the\nNewton method needs to be used (Kelley, 1995). In a broad class of practically interesting problems\nwe have considered to date, we have not faced very strong coupling between the dissolved and adsorbed\nconcentrations. Therefore, a fully implicit and coupled discretization was not required and we have found\nthat an operator splitting approach, or just a Picard linearization, has worked well. In this approach,\nthe dissolved particle concentration is computed at t̂ = t̂k+1/2 , and then the value is used to compute the\ndeposited mass at the time t̂ = t̂k+1 . Runge–Kutta methods, or other methods for numerically solving\nstiff ODEs, are also straightforward to implement, and may be the subject of future studies if required\n(Kelley, 1995). In order to illustrate the method used in Pore–Chem, we describe the discretization for\nthe Langmuir isotherm, which is achieved as follows. Firstly (18) is substituted into (16e), which is then\nsplit into a Robin boundary condition and an ordinary differential equation:\n\u0013\n\u0012\n1 ˆ\nm̂\nI\n− ∇ĉ · n = Daa,i ĉ 1 −\n− DaId,i m̂,\n(24a)\nPe\nm̂∞,i\n\u0012\n\u0013\nx̂ ∈ Γ̂i , t̂ > 0.\nm̂\n∂ m̂\n= DaIa,i ĉ 1 −\n− DaId,i m̂.\n(24b)\nm̂∞,i\n∂ t̂\nĉ\n+ DaId,i = 0, then either ĉ = DaId,i = 0 or DaIa,i = DaId,i = 0. In both of these cases,\nm̂∞,i\nby (18), fˆi = 0 and so no reactions occur at the spatiotemporal point under consideration, in which\nˆ · n = 0 and a zero Neumann boundary condition, which is straightforward to\ncase, by (24a), we have ∇ĉ\nĉ\n+ DaId,i > 0, assuming that ĉ(x̂, t̂) is constant over the time period\nimplement. Otherwise, if DaIa,i\nm̂∞,i\nin question, namely t̂ ∈ [t̂k , t̂k+1 ], and equal to ĉ(x̂) at each spatial point, (24b) may be integrated to\ngive\n\u0001 k+1 \u0001\nI\nDaIa,i ĉ(x̂) − B exp − DaIa,i ĉ(x̂)m̂−1\n∞,i + Dad,i t̂\nk+1\nm̂(x̂, t̂\n)=\n,\nx̂ ∈ Γ̂i , t̂ > 0.\n(25)\nI\nDaIa,i ĉ(x̂)m̂−1\n∞,i + Dad,i\n\nIf DaIa,i\n\nHere B is a constant of integration which may be evaluated at t̂ = t̂k to give\n\u0001 k\u0001\n\u0001\n\u0001\nI\n−1\n.\nB = DaIa,i ĉ(x̂) 1 − m̂(x̂, t̂k )m̂∞,i\n− DaId,i m̂(x̂, t̂k ) exp DaIa,i ĉ(x̂)m̂−1\n∞,i + Dad,i t̂\n\n(26)\n\nUpon substitution into (25), we have\nm̂(x̂, t̂k+1 )\n=\n\n\u0001\n\u0001\n\u0001\n\u0001\n−1\nI\nk\nI\nk\nI\nDaIa,i ĉ(x̂, t̂k ) − DaIa,i ĉ(x̂, t̂k ) 1 − m̂(x̂, t̂k )m̂−1\n∞,i − Dad,i m̂(x̂, t̂ ) exp − Daa,i c(x̂, t̂ )m̂∞,i + Dad,i (∆t̂)\nI\nDaIa,i ĉ(x̂, t̂k )m̂−1\n∞,i + Dad,i\n\n,\n\n(27)\n\nfor x̂ ∈ Γ̂i , t̂ > 0, where we have approximated ĉ(x̂) by ĉ(x̂, t̂k ). This is done to prevent nonlinear terms\nin unknown variables from appearing in the discretized version of (16e). Discretization of the other two\nisotherms is implemented in a similar manner.\nConsequently, we may approximate the Robin boundary condition, (16e), on the reactive face Fl,j ∈ Γ̂i\nfor j = e, w, n, s, t, b and i = 0, 1, . . . N − 1 using finite difference methods fully implicitly as follows:\n−\n=−\n\n\u0012\n\n\u0013\nck+1\n2n\n+ 2n c\n+ DaIa,i ck+1\nj\nPe δ x̂\nPe δ x̂\n!\n\u0001\n\u0001\n\u0001\n\u0001!\n−1\nI\nk\nI\nk\nI\np\nI\nI\nDaa,i m̂j − DaIa,i ckj 1 − m̂kj m̂−1\nDaa,i\n∞,i − Dad,i m̂j exp − Daa,i cm m̂∞,i + Dad,i (∆t)\nI\n+ Dad,i\n,\nI\nm̂∞,i\nDaIa,i ckj m̂−1\n∞,i + Dad,i\n(28)\n\nwhere n = ±1 is the direction of the outward pointing normal. Using (28), the appropriate terms are\nassembled, along with (23) minus the relevant diffusive flux term, for the finite volume on which the\nreactive surface lies into a matrix Ak+1 and vector gk+1 , where Ak+1 ĉk+1 = gk+1 and ĉk+1 is a vector\nof the dimensionless solutions ĉ(x̂, t̂k+1 ) at the discretized points of the computational domain. Once\nall the terms for all the finite volumes within the domain have been assembled into Ak+1 and gk+1 , the\n8\n\n\fequation Ak+1 ĉk+1 = gk+1 is solved using a biconjugate gradient stabilized method to give the updated\nfluid concentration, ĉk+1 , at each discrete spatial point in Ω̂f , while the updated adsorbed concentration,\nm̂k+1 , is given by (26). Time is then updated, the next timestep considered, and we proceed in the usual\nmanner until the final time point is reached.\nNumerical simulation of the equations is performed in Pore–Chem and a schematic, outlining the\nnumerical algorithm used, is given in Figure 1. In the following section, the dimensionless equations are\nsolved, but we present results in dimensional quantities.\n4. Results\nWe present illustrative results, using the numerical method outlined in Section 3, on two separate\ncomputational domains, a real geometry and a virtually generated geometry, for two applications where\nsurface reactions are important. The first set of simulations are performed on a portion of palatine sandstone, obtained using micro computerized tomography (µ–CT) by Frieder Enzmann at the Johannes\nGutenberg University of Mainz (Becker et al., 2011). Surface reactions are highly important in many\nfields of the Earth sciences, including calcite growth, oxidation–reduction reactions, formation of biofilms,\nto name only a few (Steefel et al., 2005). The second set of results is performed on a computational domain virtually constructed to be representative of a commercially available microfiltration functionalized\nmembrane. The use of functionalized membranes is a promising method for removing contaminants from\nwater, and involves treating the pore walls of the membrane so that they adsorb certain microorganisms\nor drugs (Ulbricht, 2006). Such membranes have pore sizes on the sub–micron scale and the resolution\nprovided by µ–CT imaging techniques is not high enough to give representative images, motivating the\nuse of a virtually generated geometry. The two computational domains under consideration are shown\nin Figure 2.\nFor the numerical simulations presented, a dead–end setup is used, with a schematic illustrating\nthe domain and boundary conditions shown in Figure 3. Layers of pure water voxels are added at the\ninlet and at the outlet, as shown in Figure 2 and illustrated in Figure 3, to allow free flow to develop.\nThis results in a total computational domain size of 200 × 200 × 300 voxels for the sandstone geometry,\nand 100 × 100 × 120 voxels for the membrane geometry. For the results presented one type of reactive\nsolid–fluid interface is considered, so that N = 1, and consequently we now drop the i subscript notation\nrelating to the type of reactive boundary.\n4.1. Fluid flow\nAs described in Section 3, we are first required to solve for the fluid flow. We assume that the fluid\ngenerates no slip as it passes over the membrane, and we set the slip length, β, to be zero so that, by (5),\nthere is zero normal and tangential velocity at the fluid–solid interface, Γ. We set inflow velocities to be\ntypical for the application under consideration, and use Vin = 1 mm/s for the membrane geometry and\nVin = 1.5×10−4 mm/s for the rock geometry. The parameters chosen for the inlet velocities, and the fluid\ndensity and viscosity, yield small Reynolds numbers; Re = 4.2 × 10−7 in the case of the rock geometry\nand Re = 7.83 × 10−3 for the membrane geometry. Therefore, the fluid flow in both computational\ndomains is in a Stokes regime. Solving (14a) and (14b), along with the boundary conditions (16a) –\n(16d), numerically until steady–state is achieved, yields the solutions as reproduced in Figure 4. Due to\nour assumption that the maximal possible number of adsorbed particles is sufficiently small to ignore the\neffects of geometry modification, these remain constant through time. Examination of Figure 4 reveals\nthe dependence of the local velocity field on the morphology of the computational domains.\n4.2. Contaminant transport\nWe now turn our attention to solving the reactive flow. Illustrative parameters are used, and we\nemploy the Henry isotherm, given by Equation (10), for the rock geometry, and the Langmuir isotherm,\ngiven by Equation (11), for the membrane geometry. We choose cin arbitrarily to be 2×104 number/mm3\nand we set the initial concentration of fluid contaminant to be equal to the inflow boundary condition,\nso that ĉ0 (x̂) = 1 for x̂ ∈ Ω̂f , while the quantity of adsorbed contaminant is initially assumed to\nbe zero, m̂0 (x) = 0. Furthermore, for the membrane geometry, we set the dimensionless maximal\nsurface concentration of adsorbed contaminant, m̂∞ , to be 10−4 , which equates to a dimensional value\nof m∞ = 0.014 number/mm2 . Due to the form of the equations, the choice of cin does not influence\nthe dimensionless system of equations describing the transport and reaction of the contaminant given by\n(14c) along with the boundary conditions (16e) – (19), except for through the parameter m̂∞ . Therefore,\n9\n\n\fInput file\n(description\nof problem,\nboundaries,\nparameters)\n\nStart\nsimulation\n\nGeometry File\nProcess geometry\n\nCast problem (geometry\n& parameters) into\ndimensionless equivalents\n\nyes\n\nExport flow\nsolution visualization file\n\nSolving\ndimensional\nproblem?\n\nno\nSolve fluid flow,\nv̂, p̂ ∈ Ω̂f (Chorin\nmethod with fractional timestepping\n∂ˆv̂\nuntil\n< tol\n∂ˆt̂\nis achieved\n\nSet attributes\nfor boundaries\n\nAssemble matrix Ak+1\nand right–hand–side vector\ngk+1 for the fluid concentration, including Robin\nBoundary conditions\non reactive boundaries\n\nno\n\nStart time loop\n(k = 0, t̂k = 0)\n\nHas end time\nbeen reached?\n(Is t̂k ≥ T̂end ?)\n\nyes\nSolve Ak+1 ck+1 =\ngk+1 using an iterative\nlinear solver to give\nĉk+1 , the vector of\nsolutions for the fluid\nconcentration , ĉ ∈ Ω̂f\n\nUpdate time\n(k = k + 1, t̂k = t̂k−1 + ∆t)\n\nSolve the reactive ODE to\nobtain the retained concentration, m̂ for x̂ ∈ Γ̂i\n\nIs current time\n= SaveStep\ntime?\n\nStop\n\nno\n\nFigure 1:\n\nyes\n\nExport\nefficiency\nsolution visualization file\n\nFlow chart to illustrate how numerical computation of the transport with surface reactions is implemented\nwithin Pore–Chem, with main features indicated only. The light red ovals indicate input files, while the yellow ovals\nindicate output files. The green and the blue rectangular boxes indicate steps involved for the flow and efficiency solvers\nrespectively, while the dark red boxes indicate steps involved in both. The white diamonds indicate decision making steps.\nIn the case that the dimensional system of equations is being solved, the appropriate dimensionless quantities of interest\nare replaced by the dimensional equivalent.\n\n10\n\n\f(a) Rock geometry\n\n(b) Membrane geometry\n\nFigure 2: The two computational domains under consideration, plotted in 3D on the left–hand side of the\nfigure, and in 2D through a representative cross section on the right–hand side of the figure. Figure (a)\nshows the palatine sandstone geometry obtained through µ–CT, with a voxel size of 1.4×10−6 m, as used\nin Becker et al. (2011). Figure (b) shows a virtually generated geometry which aims to reproduce the\nmorphology of a commercially available microfiltration membrane, with a voxel length of 7.03 × 10−8 m\n(3 s.f.) This geometry was created using the software GeoDict (GeoDict, 2012–2014), and a comparison\nto experimentally evaluated quantities is made in Di Nicolò et al. (2015).\n\n11\n\n\f∂Ωin\n\nΓ\n∂Ωwall\n\n∂Ωout\n\nFigure 3: Schematic to illustrate the computational domain. The solid microfiltration membrane is show\nin grey while the water is shown in white. Voxels are added to the top and bottom of the domain, at the\ninlet and outlet, to enable free–flow to develop. Modified, with permission, from Di Nicolò et al. (2015).\n\n(a) Velocity magnitude, rock\n\n(b) Pressure, rock\n\n(c) Velocity magnitude, membrane\n\n(d) Pressure, membrane\n\nFigure 4: Velocity magnitude, (a) and (c), and pressure, (b) and (d), fields, measured in mm/s and kPa\nrespectively for both the computational domains considered. Due to our assumption that the adsorption\nof the solute does not alter the geometry, these remain constant throughout the experimental time frame.\nThe black boxes shows the outline for the entire computational domain, where we exclude the additional\nvoxels embedded at the inlet and the outlet for 3D plotting purposes.\n12\n\n\ft=0s\n\nt = 1.2 × 103 s\n\nt = 2.4 × 103 s\n\nt = 3.8 × 103 s\n\nFigure 5: The retained concentration of contaminant, m, for the rock geometry at 1200 second time\nintervals where Daa = 0.1, Dad = 0.001 and Pe = 2.0 × 10−5. The fluid concentration remains effectively\nconstant throughout the domain and so this is not plotted.\n\nF\nl\nu\ni\nd\n\nA\nd\ns\no\nr\nb\ne\nd\n\nt = 5 × 10−5 s\n\nt = 1 × 10−4 s\n\nt = 1.5 × 10−4 s\n\nt = 2 × 10−4 s\n\nFigure 6: Numerical results at 5 × 10−5 s intervals for the fluid concentration, c, and the adsorbed\nconcentration, m, of contaminant in the membrane geometry.\nincreasing or decreasing cin for fixed m̂∞ purely scales the dimensional concentration (both fluid and\nadsorbed) by a constant factor.\nFigure 5 illustrates the concentration of the contaminant on the solid surface over time for the rock\ngeometry, where we use Daa = 0.1, Dad = 0.001 and Pe = 2.0 × 10−5 . Due to the slow rate of reaction\nand small Péclet number, the quantity of solute adsorbed over this time period is not large enough to\nsignificantly alter the fluid concentration over the time period under consideration. Furthermore, as\nthe rate of reaction is much smaller than the mass transport rate, the adsorbed concentration remains\nspatially homogeneous.\nIn contrast, with the parameters Pe = 10 and Daa = Dad = 10 for the membrane geometry, we\nsee significant spatial heterogeneity in both the dissolved and adsorbed concentrations, as illustrated in\nFigure 6. As time progresses, the fast rates of reaction result in a depletion of the dissolved concentration\nat the pore wall and a dependence of both the dissolved and adsorbed concentrations on the local\nmembrane morphology.\n5. Conclusions\nWe have presented an algorithm for solving solute transport at the pore–scale within a resolved\nporous medium, with reversible surface adsorption at the pore wall. A pore–scale description of reactive\n13\n\n\ftransport, as opposed to a description at the Darcy scale, allows for a very accurate representation of\nthe processes of interest. The system of equations comprise the NS equations and a CD equation, with\nRobin boundary conditions coupled to an ODE accounting for the surface reactions. Assuming that each\nparticle is sufficiently small in size not to alter the flow of the fluid within the computational domain,\nand that its reaction at the wall does not significantly alter the pore–scale geometry, there is a one–\nway coupling between the NS and CD systems of equations. Although, for simplicity, we consider one\nspecies of solute and examine only surface reactions, extension to several different species of solute with\nboth volumetric and surface reactions is straight forward and implemented within our software package\nPore–Chem. The algorithm presented employs a FV method, and in this paper we have particularly\nfocused on the discretization method used to solve the reactive boundary conditions for the adsorption\nand desorption at the interface. Illustrative numerical results, using our software package Pore–Chem,\nare presented on two separate geometries. The first of these is a 3D µ–CT image of a piece of Palatine\nSandstone rock, while the second geometry is virtually generated within GeoDict (GeoDict, 2012–2014)\nto be representative of a commercially available functionalized membrane. The results demonstrate the\npotential of such a numerical package, with the ability to solve reactive transport directly on images\nand on virtually generated geometries, in further progressing the understanding of the interplay between\nthe transport and reaction rates at the pore–scale. In a future publication, currently in preparation, we\nwill investigate the influence of the computational domain morphology on the reaction dynamics, and\ninvestigate the effect of different parameter regimes on the numerical results and quantities of interest.\nThe advantages of using a pore–scale description are multiple. Firstly, it allows us to simulate reactive\ntransport over a range of different parameter regimes, and in particular, outside the applicability region\nof the equivalent upscaled model. In highly disordered media the Péclet number can significantly vary\nlocally, which poses problems for asymptotic upscaling methods, but not for a pore–scale description.\nSecondly, different kinetic models for the reactions can be used without the need to re–perform the\nupscaling procedure. In the future we plan to extend the algorithm in order to solve coupled multiscale\nproblems using the heterogeneous multiscale method, in a similar manner to Battiato et al. (2011) and\nIliev et al. (2014). Such a development will enable problems at larger spatial scales to be considered,\nwhich could aid further research into the influence of pore–scale processes in a number of highly interesting\nand important research applications.\nReferences\nD.J. Acheson. Elementary Fluid Dynamics. Oxford applied mathematics and computing science series. Clarendon Press,\n1995.\nG. Allaire and H. Hutridurga. Homogenization of reactive flows in porous media and competition between bulk and surface\ndiffusion. IMA Journal of Applied Mathematics, 77(6):788–815, 2012. doi: 10.1093/imamat/hxs049.\nG. Allaire, R. Brizzi, A. Mikelić, and A. Piatnitski. Two-scale expansion with drift approach to the Taylor dispersion for\nreactive transport through porous media. Chemical Engineering Science, 65(7):2292–2300, April 2010a. doi: 10.1016/j.\nces.2009.09.010.\nG. Allaire, A Mikelić, and A Piatnitski. Homogenization approach to the dispersion theory for reactive transport through\nporous media. Society of Industrial and Applied Mathematics Journal on Mathematical Analysis, 42(1):125–144, 2010b.\ndoi: 10.1137/090754935.\nJ. F. Baret. Theoretical model for an interface allowing a kinetic study of adsorption. Journal of Colloid and Interface\nScience, 30(1):1–12, 1969.\nI. Battiato, D. M. Tartakovsky, A. M. Tartakovsky, and T. D. Scheibe. Hybrid models of reactive transport in porous and\nfractured media. Advances in Water Resources, 34(9):1140–1150, September 2011. doi: 10.1016/j.advwatres.2011.01.012.\nJ. Bear. Dynamics of Fluids in Porous Media. Dover Civil and Mechanical Engineering Series. Dover, 1988. ISBN\n9780486656755.\nJ. Becker, E. Glatt, and A. Wiegmann. Combining pore morphology and flow simulations to determine two-phase properties\nof 3d tomograms. Extended Abstract for Flows and mechanics in natural porous media from pore to field scale,\nPore2Filed, IFP Energies nouvelles, 2011.\nM. J. Blunt, B. Bijeljic, H. Dong, O. Gharbi, S. Iglauer, P. Mostaghimi, A. Paluszny, and C. Pentland. Pore-scale imaging\nand modelling. Advances in Water Resources, 51:197–216, January 2013. doi: 10.1016/j.advwatres.2012.03.003.\nF. Boso and I. Battiato. Homogenizability conditions for multicomponent reactive transport. Advances in Water Resources,\n62:254–265, 2013. doi: 10.1016/j.advwatres.2013.07.014.\nP. D. M. Causon, P. C. G. Mingham, and L. Qian. Introductory Finite Volume Methods for PDEs. Bookboon, 2011. ISBN\n9788776818821.\nK. D. Danov, D. S. Valkovska, and P. A. Kralchevsky. Adsorption relaxation for nonionic surfactants under mixed barrierdiffusion and micellization-diffusion control. Journal of Colloid and Interface Science, 251(1):18–25, 2002.\nE. Di Nicolò, O. Iliev, and K. Leonard. Virtual generation of microfiltration membrane geometry for the numerical\nsimulation of contaminent transport. Technical Report 245, Fraunhofer-Institut für Techno- und Wirtschaftsmathematik\nITWM, 2015.\nGeoDict, 2012–2014. The virtual material laboratory GeoDict. http://www.geodict.com, 2012 – 2014.\n\n14\n\n\fJger W. Mikelić A. Hornung, U. Reactive transport through an array of cells with semi-permeable membranes. ESAIM:\nMathematical Modelling and Numerical Analysis - Modlisation Mathmatique et Analyse Numrique, 28(1):59–94, 1994.\nU. Hornung. Homogenization and Porous Media. Interdisciplinary Applied Mathematics. Springer New York, 1997. ISBN\n9780387947860.\nO. Iliev, Z. Lakdawala, and G. Printsypar. On a multiscale approach for filter efficiency simulations. Computers &\nMathematics with Applications, 67(12):2171–2184, 2014. doi: 10.1016/j.camwa.2014.02.022.\nC. T. Kelley. Iterative Methods for Linear and Nonlinear Equations, volume 16 of Frontiers in applied mathematics.\nSociety of Industrial and Applied Mathematics, 1995.\nP. A. Kralchevsky, K. D. Danov, and N. D. Denkov. Handbook of Surface and Colloid Chemistry, Third Edition, chapter\nChemical Physics of Colloid Systems and Interfaces. Taylor & Francis, 2008. ISBN 9781420007206.\nK. Kumar, I. S. Pop, and F. Radu. Numerical analysis for an upscaled model for dissolution and precipitation in porous\nmedia. Numerical Mathematics and Advanced Applications, pages 703–711, 2013.\nZ. Lakdawala. On Efficient Algorithms for Filtration Related Multiscale Problems. PhD thesis, University of Kaiserslautern,\n2010.\nE. Lauga, M. Brenner, and H. Stone. Microfluidics: The No-Slip Boundary Condition. In C. Tropea, A. L. Yarin, and J. F.\nFoss, editors, Springer Handbook of Experimental Fluid Mechanics, pages 1219–1240. Springer Berlin Heidelberg, 2007.\nP. C. Lichtner and Q. Kang. Upscaling pore-scale reactive transport equations using a multiscale continuum formulation.\nWater Resources Research, 43(12):W12S15+, 2007. doi: 10.1029/2006wr005664.\nS. Molins, D. Trebotich, C. I. Steefel, and C. Shen. An investigation of the effect of pore scale flow on average geochemical reaction rates using direct numerical simulation. Water Resources Research, 48(3):W03527, 2012. doi:\n10.1029/2011wr011404.\nA. Raoof, S. M. Hassanizadeh, and A. Leijnse. Upscaling transport of adsorbing solutes in porous media: Pore–network\nmodeling. Vadose Zone Journal, 9:624–636, 2010. doi: 10.2136/vzj2010.0026.\nD. Roubinet and D. M. Tartakovsky. Hybrid modeling of heterogeneous geochemical reactions in fractured porous media.\nWater Resources Research, 49(12):7945–7956, 2013. doi: 10.1002/2013wr013999.\nC. Shen, D. Trebotich, S. Molins, D. T. Graves, B. Van Straalen, T. Ligocki, and C. I. Steefel. High performance\ncomputations of subsurface reactive transport processes at the pore scale. In Proceedings of Scientific Discovery through\nAdvanced Computing, 2011. URL https://seesar.lbl.gov/anag/publications/treb/SciDAC2011_sim.pdf.\nC. Steefel, D. Depaolo, and P. Lichtner. Reactive transport modeling: An essential tool and a new research approach for\nthe Earth sciences. Earth and Planetary Science Letters, 240(3-4):539–558, 2005. doi: 10.1016/j.epsl.2005.09.017.\nA. M. Tartakovsky, D. M. Tartakovsky, T. D. Scheibe, and P. Meakin. Hybrid Simulations of Reaction-Diffusion Systems in\nPorous Media. Society for Industrial Applied Mathematics Journal on Scientific Computing, 30(6):2799–2816, January\n2008.\nM. Ulbricht. Advanced functional polymer membranes. Polymer, 47(7):2217–2262, 2006. doi: 10.1016/j.polymer.2006.01.\n084.\nKnabner P. van Duijn, C. J. Solute transport in porous media with equilibrium and non-equilibrium multiple-site adsorption:\nTravelling waves. J. reine angewandte Math, 415:1–49, 1991.\nC Varloteaux, M. T. Vu, S. Békri, and P. M. Adler. Reactive transport in porous media: pore-network model approach\ncompared to pore-scale model. Physical Review E: Statistical, Nonlinear, and Soft Matter Physics, 87(2), 2013. doi:\nhttp://dx.doi.org/10.1103/PhysRevE.87.023010.\nR. Čiegis, O. Iliev, and Z. Lakdawala. On parallel numerical algorithms for simulating industrial filtration problems.\nComputational Methods in Applied Mathematics, 7:118–134, 2007. doi: 10.2478/cmam-2007-0007.\n\n15\n\n\f"
        ],
        [
         "35",
         "35",
         "cs.CE",
         "Computational Engineering",
         "1408.4965v1.pdf",
         "A Domain Specific Approach to\nHeterogeneous Computing:\nFrom Availability to Accessibility\nGordon Inggs, David Thomas\n\nWayne Luk\n\nDepartment of Electrical and Electronic Engineering\nImperial College London\nLondon, United Kingdom\ng.inggs11;d.thomas1@imperial.ac.uk\n\nDepartment of Computing\nImperial College London\nLondon, United Kingdom\nw.luk@imperial.ac.uk\n\nOur position is that the solution to the heterogeneous\nprogramming challenge is the use of domain specific abstraction. Software developers are already familiar with domain\nspecific approaches, through the popularity of programming\nenvironments such as Matlab and software libraries such as\nOpenCV. We argue that through the use of domain specific\nlanguages and programming frameworks, three benefits may\nbe realised:\n1) Portable, Efficient Execution: A well-established property of domain specific approaches is that the relationships between computations may be captured in greater\ndetail [2], [3], the exploitation of which allows for safe,\nparallel scaling across different architectures.\n2) Domain Specific Metrics: the application domain provides unique measures of performance, which we call\nmetrics. These metrics allow for the performance of\ntasks upon platforms to be characterised within the\ncontext of the domain.\n3) Automatic Partitioning: Through the performance predictions provided by domain specific metric models as\nwell as the portable execution capability enabled by the\nextraction, domain tasks could then be shared automatically across the computational platforms available to the\nuser in an optimal manner.\nWe describe our domain specific approach to heterogeneous\ncomputing, illustrated by an example from the domain of\ncomputational finance. We then provide further details on our\ncase study in computational finance to evaluate the viability\nof this approach. Finally, we conclude by outlining the possibilities offered by domain specific heterogeneous computing\nto software developers.\n\nAbstract—We advocate a domain specific software development\nmethodology for heterogeneous computing platforms such as\nMulticore CPUs, GPUs and FPGAs. We argue that three specific\nbenefits are realised from adopting such an approach: portable,\nefficient implementations across heterogeneous platforms; domain specific metrics of quality that characterise platforms in\na form software developers will understand; automatic, optimal\npartitioning across the available computing resources. These\nthree benefits allow a development methodology for software\ndevelopers where they describe their computational problems\nin a single, easy to understand form, and after a modeling\nprocedure on the available resources, select how they would like\nto trade between various domain specific metrics. Our work\non the Forward Financial Framework (F 3 ) demonstrates this\nmethodology in practise. We are able to execute a range of\ncomputational finance option pricing tasks efficiently upon a\nwide range of CPU, GPU and FPGA computing platforms. We\ncan also create accurate financial domain metric models of walltime latency and statistical confidence. Furthermore, we believe\nthat we can support automatic, optimal partitioning using this\nexecution and modelling capability.\n\nI. O UR P OSITION\nThe increasing availability of heterogeneous computing\nplatforms such as Multicore CPUs, GPUs and especially\nFPGAs represents both an opportunity and challenge to high\nperformance computing software developers. As the number\nof software developers working in fields such as scientific\ncomputing, data analytics and computational finance increase,\nthe need for resolution to this dilemma is pressing.\nThese application-focused software developers would benefit from the performance and flexibility offered by heterogeneous platforms, however often lack the detailed architectural\nknowledge (or design ability in the case of FPGAs) to realise\nsuch benefits. We are encouraged by the growing portability\noffered by standards such as OpenCL [1], however the often\northogonal paradigms through which modern computing platforms need to be programmed hinders the cooperative use of\nplatforms. For example, code optimised to take advantage of\nthe extra control logic available in multicore CPUs woefully\nunder-performs upon the data parallel-oriented architecture of\nGPUs and vice-versa.\n\nCopyright is held by the author/owner(s).\n1st International Workshop on FPGAs for Software Programmers\n(FSP 2014), September 1, 2014, Munich, Germany.\n\nII. O UR A PPROACH\nWe believe that through a domain specific approach that\nharnesses the three benefits outlined above, a software development model for heterogeneous computing that incorporates\nFPGAs such as illustrated in Figure 1 becomes possible [4].\nWe have illustrated this approach using an example from the\nfinancial engineering domain.\n\n55\n\n\fFig. 1. Our proposed approach for Heterogeneous Computing Software Developers\n\nimport ForwardFinancialFramework as F3\nU_II = F3.Heston(0.05,100,0.09,1,-0.3,2,0.09)\n\nTask Description\n\nO_2 = F3.Barrier(U_II,True,100,5,4096,True,120)\nO_2.get_price(interactive=True)\n\nFPGA\nLatency\n\nGeneration of Design\nSpace\n\nLatency\n\nMulticore CPU\n\nConfidence\nInterval\n\nConfidence\nInterval\n\nHeterogeneous System\n\nLatency\n\nF^3 System Dialog\n\nTrade-off\nSelection\n\nConfidence\nInterval\n\nComputation\n\nprint(O_2.price)\n\nResult\n\n>>> $0.10 +- $0.01\n\n56\n\n\f1) The software developer specifies their task in a high\nlevel, domain specific form. In our example, we have\nused a computational financial application framework\nwritten in the Python programming language: An asset\nsuch as a stock or a unit of foreign currency is described\nusing a Heston model underlying object; a knock-out\nbarrier option which depends on this asset is described\nusing barrier option object; finally a method of the\noption object is called to initiate the option pricing task.\n2) The design space of the task is generated by modelling\nthe relationship between the domain specific metrics\nof the specified task upon the computational resources\navailable. In our example, the platforms are a multicore\nCPU and a FPGA while the two financial domain\nmetrics are wall-time latency of the computation and\nthe size of the 95% confidence interval.\n3) The software developer is able to make a selection of a\nparticular combination of metrics. This selection allows\nthe developer to balance their objectives while making\noptimal use of the computing resources at their disposal.\nFurthermore, this objective balancing does not require\ndetailed understanding of the computational resources\navailable. In our example, the user makes their selection\non a Pareto tradeoff curve of latency and the size of the\nstatistical confidence interval, a commonly used metric\nof quality from the computational finance domain.\n4) The task is then executed upon the resources available\nso as to achieve the specified combination of metrics. In\nour example, this would be to perform the pricing task\nto the required degree of statistical confidence.\n5) The result is then available to the user. In our example,\nthis is the option price, which is an attribute of the\nBarrier Option object in the application framework.\n\nTABLE I\nL ATENCY S PEEDUP OF F 3 I MPLEMENTATIONS AND R EFERENCES\nOVER S EQUENTIAL CPU I MPLEMENTATION\n\nTarget Platform\n\nF3\n\nReference\n\nXilinx 7Z045\nAltera Stratix V\nGXA7\nXilinx Virtex 6\nSX475T\nAMD\nOpteron\n6272\nAMD\nFirepro\nW5000\nIntel Xeon Phi\n3120P\nSee [5], [6]\n\nPlatform\nType\n\nKaiserslatuarn BlackScholes\nHeston\nAsian\nOption\nOption\nBenchmark\n\nFPGA\n\n9.53\n\n9.77\n\nFPGA\n\n274.87\n\n194.85\n\nFPGA\n\n223.93\n\n353.59\n\nCPU\n\n28.99\n\n25.36\n\nGPU\n\n58.40\n\n85.67\n\nCo-processor\n\n156.42\n\n421.63\n\nFPGA\nCPU\nGPU\n\n26.51\n11.53\n64.22\n\n248.64\n115.21\n103.06\n\nlibrary include both the underlyings that are used to model\nassets such as stocks or commodities, such as the BlackScholes or Heston Models, as well as the derivative product\ncontracts such as futures or options that derive value from\nthese underlyings.\nWhen the software developer calls the methods associated\nwith obtaining the price of the derivative objects, the framework is capable of generating, compiling and executing the\nnecessary implementations of pricing algorithms, such as the\nMonte Carlo algorithm, for a range of platforms, including\nMulticore CPUs, GPUs and FPGA. This heterogeneous implementation is capable of pricing the derivative in question,\nand returning the result to the software developer within the\nframework as an attribute of the derivative object.\n\nIII. O UR C ASE S TUDY\nTo evaluate this proposed approach to software development\nfor heterogeneous computing, we have undertaken a case\nstudy in the application domain of Computational Finance,\nfocusing on the sub-domain of forward looking derivatives\npricing. We have created the Forward Financial Framework\n(F 3 ) 1 , a computational finance application framework for\nheterogeneous computing that makes use of a variety of\nimplementation technologies such as OpenCL for GPUs and\nFPGAs, Maxeler Tools and Xilinx’s Vivado for FPGAs and\nmultithreaded C for multicore CPUs.\nWe have used F 3 to prove the first two benefits outlined in\nsection 1, portable heterogeneous execution as well as metric\nmodelling, and work on proving the third benefit, automatic\npartitioning is in progress.\n\nTable 1 provides the relative latency performance of F 3 ’s\nimplementations of the Kaiserslatuarn Option Pricing Benchmark2 , as well as a Black-Scholes model-based Asian Option.\nWe have compared F 3 ’s implementations to two hand-written,\nmanual implementations [5]–[7].\nThe results illustrate that from a single high level software\ntask description, software developers using F 3 could execute\ntasks across a wide range of heterogeneous platforms including\nseveral different FPGA toolflows, and achieve performance\nwithin the same order of magnitude as those implementations created by embedded computing experts. The efficiency\nthat a hypothetical software developer would harness results\nfrom the expert architectural knowledge that is built into the\nframework’s implementation generation capability. However,\nthis approach allows for the architectural knowledge of the\nframework developer to be shared with many software developers.\n\nA. Heterogeneous Execution\nIn F 3 , computational finance tasks are described in a high\nlevel, domain specific manner using a library of objects in\nthe Python programming language. The classes within the F 3\n\n2 http://www.uni-kl.de/en/benchmarking/option-pricing/\n\n1 https://github.com/Gordonei/ForwardFinancialFramework\n\n57\n\n\fFig. 2. Financial Domain Metric Modelling within F 3\n650\n\nMetric Model produced by F3\n\nActual Performance\n\n500\n400\n300\n\nLatency (S)\n\n200\n\n100\nServer CPU\nMaxeler FPGA\n\n50\n40\n\nDesktop CPU\n\nNvidia GPU\n\n30\n20\n\nAMD GPU\n0.03\n\n0.04\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\nmatrix as well as a matrix of domain specific metrics values,\nC.\nC is a matrix of domain specific metric values that has the\nsame shape as A, where each element represents the value\nrequired to achieve a vector of a targeted domain specific\nmetric values, ~t.\nTo generate a computational finance design space, we would\nselect a ~t of statistical confidence values and then use our\nmetric models to find the latency values to achieve this\nacross the platforms achieve this, i.e. our C. Our automatic\npartitioning tool would then seek A such that the value of\nF (A, C) is minimised.\nWe believe that the methods utilised in operations research,\nparticularly the bottleneck assignment problem could offer\nprovably optimal solutions to this problems.\nIV. O UR C ONCLUSION\n\n95% Confidence Interval ($)\n\nIn this paper we have argued that domain specific abstractions provide a means to make heterogeneous computing\naccessible to software developers. Our approach requires no\nknowledge of the computing platforms in questions, a crucial\nadvantage in the case of FPGAs. Our case study in computational finance, as illustrated by F 3 shows that portable\nheterogeneous execution and domain specific metric modelling\nis achievable. Furthermore, it suggests the possibility of automatic task partitioning across a range of platforms.\nOur approach does however require expert knowledge of\nthe application domain, the computational architectures being\ntargeted, as well as the mapping of the former onto the latter.\nHowever, given that popular application domains have often\nspawned specialised software frameworks, extending these\nto support multiple platforms is not inconceivable. Although\nthe scope of problems that can be addressed by a particular\ninstance of our approach is narrow, this approach enables the\nuse of heterogeneous computing platforms when none could\nbe used before.\n\nB. Performance Modelling\nF 3 also provides the capability to model the computational\nfinance metrics of wall-time latency and statistical confidence\ninterval size for the range of platforms that execution is\nsupported upon. This capability builds upon the heterogeneous\nexecution supported by the framework. The framework’s modelling capability requires a small amount of online benchmarking, executing a subset of the computational task on the\ntargeted platform. The model predictions are then generated\nusing knowledge from the computational finance domain embedded in the framework, exploiting a priori knowledge of the\nstructure of the pricing tasks.\nFigure 2 illustrates the performance of the predictive modelling capability of F 3 for a portfolio of option pricing tasks.\nThis portfolio was comprised of the Kasiserslaturn option\npricing benchmark as well as the Black Scholes Asian Option\ndescribed in the previous section. We used benchmarking runs\norders of magnitude shorter than the predication target.\nSoftware developers in computational finance are familiar\nwith metrics such as latency and confidence interval size, and\nso by presenting platforms in terms of trade-offs between these\nmetrics we make a diverse range of computing platforms including FPGAs tractable. Furthermore, as the next subsection\nwill describe, this feature enables automatic task partitioning.\n\nR EFERENCES\n[1] J. Stone, D. Gohara, and G. Shi, “OpenCL: A Parallel Programming\nStandard for Heterogeneous Computing Systems,” Computing in Science\nEngineering, vol. 12, no. 3, pp. 66 –73, May-June 2010.\n[2] H. Chafi, A. K. Sujeeth, K. J. Brown, H. Lee, A. R. Atreya, and K. Olukotun, “A Domain-Specific Approach to Heterogeneous Parallelism,” in\nPPOPP, 2011, pp. 35–46.\n[3] D. B. Thomas and W. Luk, “A Domain Specific Language for Reconfigurable Path-based Monte Carlo Simulations,” in Proc. Int. Conf. on\nField-Programmable Technology, 2007, pp. 97–104.\n[4] T. D. Braun, H. J. Siegel, and A. A. Maciejewski, “Heterogeneous\ncomputing: Goals, methods, and open problems,” in High Performance\nComputing—HiPC 2001. Springer, 2001, pp. 307–318.\n[5] C. de Schryver, I. Shcherbakov, F. Kienle, N. Wehn, H. Marxen,\nA. Kostiuk, and R. Korn, “An Energy Efficient FPGA Accelerator for\nMonte Carlo Option Pricing with the Heston Model,” in 2011 International Conference on Reconfigurable Computing and FPGAs (ReConFig),\n30 2011-dec. 2 2011, pp. 468 –474.\n[6] A. H. Tse, D. B. Thomas, K. H. Tsoi, and W. Luk, “Efficient\nreconfigurable design for pricing asian options,” SIGARCH Comput.\nArchit. News, vol. 38, no. 4, pp. 14–20, Jan. 2011. [Online]. Available:\nhttp://doi.acm.org/10.1145/1926367.1926371\n[7] G. Inggs, D. Thomas, and W. Luk, “A heterogeneous computing framework for computational finance,” in Proceedings of the 42nd International\nConference on Parallel Processing (ICPP), 2013, pp. 688–697.\n\nC. Automatic Partitioning\nWe are currently investigating how we may use the heterogeneous execution and metric modelling capabilities of F 3 to\npartition tasks across the platforms available automatically.\nTo express the task allocation problem more formally, we\nseek an automatic means to find a task allocation matrix, A,\nwhere each row represents a platform available and each column a computational task, thus each each element represents\nthe proportion of a specific task to a particular platform.\nWe seek values for A such that we minimise the value of\nF (A, C), which has a value based upon the task allocation\n\n58\n\n\f"
        ],
        [
         "36",
         "36",
         "cs.CE",
         "Computational Engineering",
         "1008.3551v1.pdf",
         "TECHNICAL REPORT\n\narXiv:1008.3551v1 [cs.CE] 20 Aug 2010\n\nYL-2010-004\n\nINVENTORY ALLOCATION FOR ONLINE GRAPHICAL DISPLAY\nADVERTISING\n\nJian Yang\nErik Vee\nSergei Vassilvitskii\nJohn Tomlin\nJayavel Shanmugasundaram\nTasos Anastasakos\nYahoo! Labs\n701 First Ave\nSunnyvale, CA 94089\n{ jianyang, erikvee, sergei, tomlin, jaishan,\ntasos}@yahoo-inc.com\nOliver Kennedy\nComputer Science Department\nCornell University\nIthaca, NY 14840\nokennedy@cs.cornell.edu\n\nBangalore • Barcelona • Haifa • Montreal • New York\nSantiago • Silicon Valley\n\n\fYahoo! Labs Technical Report No. YL-2010-004\n\n\fYahoo! Labs Technical Report No. YL-2010-004\nINVENTORY ALLOCATION FOR ONLINE GRAPHICAL DISPLAY\nADVERTISING\n\nJian Yang\nErik Vee\nSergei Vassilvitskii\nJohn Tomlin\nJayavel Shanmugasundaram\nTasos Anastasakos\nYahoo! Labs\n701 First Ave\nSunnyvale, CA 94089\n{ jianyang, erikvee, sergei, tomlin, jaishan, tasos}@yahoo-inc.com\nOliver Kennedy\nComputer Science Department\nCornell University\nIthaca, NY 14840\nokennedy@cs.cornell.edu\nABSTRACT: We discuss a multi-objective/goal programming model for the allocation\nof inventory of graphical advertisements. The model considers two types of campaigns:\nguaranteed delivery (GD), which are sold months in advance, and non-guaranteed delivery\n(NGD), which are sold using real-time auctions. We investigate various advertiser and\npublisher objectives such as (a) revenue from the sale of impressions, clicks and conversions,\n(b) future revenue from the sale of NGD inventory, and (c) “fairness” of allocation. While\nthe first two objectives are monetary, the third is not. This combination of demand types\nand objectives leads to potentially many variations of our model, which we delineate and\nevaluate. Our experimental results, which are based on optimization runs using real data\nsets, demonstrate the effectiveness and flexibility of the proposed model.\n\n1\n\n\fYahoo! Labs Technical Report No. YL-2010-004\n1.\n\nIntroduction\n\nOnline graphical display advertising is a form of online advertising where advertisers\ncan explicitly or implicitly target users visiting Web pages, and show graphical (e.g., image,\nvideo) ads to those users. For instance, a brokerage firm may wish to target Males from\nCalifornia who visit a Finance web site in the month of November 2010, and show an ad\npromoting its special offers to those users. Similarly, a different advertiser may wish to\nautomatically target users who visit a Finance web site, specifically those who are likely to\nclick on their ad highlighting a lower mortgage rate. Online graphical display advertising is\na multi-billion dollar industry that is related to, but distinct from, sponsored search advertising [2], where advertisers bid for keywords entered by users on a search page, and from\ncontent match advertising [8], where advertisers bid for clicks and text-matching techniques\n(as opposed to user targeting) are used to show contextually relevant text advertisements\non Web pages.\nAs with most forms of online advertising [7], one of the central questions that arises in\nthe context of online graphical display advertising is that of inventory allocation, i.e., determining how to allocate supply/inventory (user visits) to demand (advertiser campaigns) so\nas to optimize for various publisher and advertiser objectives. However, even formulating\nthe inventory allocation problem for online graphical display advertising is quite challenging,\nfor two reasons.\nFirst, the same inventory can be sold in two different forms: guaranteed delivery and nonguaranteed delivery. In guaranteed delivery, an advertiser can purchase a certain number of\ntargeted user visits from a publisher several months in advance, and the publisher guarantees\nthese visits and incurs penalties if the guarantees are not met. For instance, an advertiser\nmay wish to purchase 100 million user visits by Males in California on a Sports web site\nduring Superbowl 2011, and the Sports web site publisher will guarantee these user visits\neven though the serving date is months away from the booking date. On the other hand, in\nnon-guaranteed delivery, advertisers can bid in real-time in a spot market for user visits, and\nthe highest bidder obtains the right to show an ad to the user. For instance, if a user visits a\nFinance web page, then there may be multiple advertisers bidding for the ad slot on the page,\nand the highest bidder can show an ad to the user. An interesting aspect is that the same\nuser visits are eligible for both guaranteed delivery and non-guaranteed delivery. A typical\nuse case is that some inventory not fully allocated to guaranteed campaigns can be sold to\nnon-guaranteed campaigns. However, not all visits from this inventory will fetch the same\nprice in the spot-market. This leads to the first question addressed by this work: How does\na publisher allocate inventory to both guaranteed and non-guaranteed advertising campaigns,\nwhile still ensuring that the guaranteed advertiser objectives are met, and publisher revenue\nis maximized?\nSecond, unlike sponsored search and content match advertising, where the goals of advertisers are to obtain clicks/ conversions on ads, the goals of advertisers in online graphical\ndisplay can be quite varied. At one end of the spectrum are brand advertisers (e.g., major department stores), whose primary goal is to reach a large and diverse audience and\n2\n\n\fYahoo! Labs Technical Report No. YL-2010-004\npromote their brand, rather than immediate clicks or purchases. At the other end of the\nspectrum are performance advertisers (e.g., credit card companies), whose primary goal is\nto obtain immediate online clicks and conversions. In the middle, there are performancebrand advertisers (e.g., car companies), whose goal is both to promote the brand, as well as\nto obtain immediate leads of users who are in the market to buy a car. The varied goals of\nadvertisers also lead to multiple currencies by which graphical display advertisements are\nbought: brand advertisers typically buy impressions (expressed in CPM, or Cost Per Mille\n(1000 impressions)), while performance advertisers typically pay per click (CPC or Cost\nPer Click) or conversion (CPA, or Cost Per Action), while brand-performance advertisers\nmay use a combination of CPM and CPC/CPA. Thus, the second question we address is:\nHow does a publisher allocate inventory across diverse advertisers and payment types so\nthat advertiser and publisher objectives are met?\n1.1.\n\nContributions\n\nGiven the aforementioned unique requirements for online graphical display advertising,\none of the main technical contributions of this paper is an inventory allocation optimization\nmodel that can capture these requirements. At a high-level, the proposed allocation model\nrepresents forecasts of future inventory (user visits) and guaranteed advertiser campaigns\nas nodes in a bipartite graph. Each edge of the bipartite graph connects a user visit to an\neligible guaranteed advertiser campaign. In addition, each user visit is annotated with a\nforecast of the highest bid fetched on the non-guaranteed marketplace and a forecast of the\nexpected pay-out. Similarly, each edge of the graph is annotated with a forecast of the click\nor conversion probability for the advertiser campaign and the specific user visit.\nGiven the previous model, the objectives for online graphical display advertising are\ncaptured as follows. There are two parts to the objective function: one that captures guaranteed campaigns, and the other that captures non-guaranteed campaigns. The objective\nfor the latter is simply to maximize the revenue for the publisher, since advertisers only bid\nfor what the user visit is worth to them. The objective for the guaranteed campaigns, on\nthe other hand, is more complex because advertisers could be interested in brand awareness,\nor performance, or both. Furthermore, the publisher faces penalties for under-delivering –\nthat is displaying an advertisement to fewer users than agreed on.\nIn our model, delivery guarantees are treated as feasibility constraints. (If the instance is\ninfeasible, we trim the demand to find the feasible solution with the minimum under-delivery\npenalties.) The objective for guaranteed contracts has two parts. The brand awareness\nobjective is captured in terms of “representativeness” (see [15]), which tries to maximize\nthe reach of the guaranteed campaign by uniformly distributing the contracts among the\nuser visits to the extent possible 1 . The performance objectives for guaranteed campaigns\nare captured as the expected pay-out, i.e., the probability of clicks and conversions, times\nthe value of each click and conversion.\n1\n\nReach can also be specified in terms of users as opposed to user visits (as above) by using user cookie\ninformation, but we do not elaborate on this extension here\n\n3\n\n\fYahoo! Labs Technical Report No. YL-2010-004\nConsequently, the final allocation objective has three parts: non-guaranteed revenue,\nguaranteed representativeness, and guaranteed clicks/conversions. While multi-objective\nprogramming has been a standard technique for some time, previous optimization models\nfor online advertising (see e.g. [17, 18]) have used a single objective function. One of our\nmajor contributions is to use the multi-objective optimization framework [24] to model the\nsometimes conflicting objectives in a rigorous way.\nWhile the multi-objective optimization model described above captures the various objectives, it also introduces a new set of challenges both in terms of operability and in terms\nof computational feasibility. Specifically, with regard to operability, the question that arises\nis: how do we trade-off between the various objectives (such as representativeness and nonguaranteed revenue), which do not even have the same units? With regard to computational\nfeasibility, the question that arises is: how do we solve a multi-objective formulation efficiently over large volumes of data (tens of billions of user visits per day and hundreds of\nthousands of advertiser campaigns per year)? Another key technical contribution of this\npaper is a method that enables operators of the system to trade-off between multiple objectives based on the monetary unit of a single objective. For instance, an operator can\ntrade-off representativeness in terms of the impact it has on non-guaranteed revenue, which\nis expressed in monetary units. A significant advantage of this method is that it also allows\nfor an efficient solution, which can be solved on a small sample of the original bipartite\ngraph, without significantly compromising accuracy.\nWe note that there are two other closely related topics that impact inventory allocation:\npricing and ad serving. While the details of these methods are beyond the scope of this\npaper, the proposed allocation method works with quite general pricing and ad serving\ntechniques. Specifically, in terms of pricing, we assume that guaranteed campaigns are\npriced using some external method (which may itself use forecasts of non-guaranteed bids\nto ensure appropriate prices), and the inventory allocation model can work with the booked\nprices without regards to how exactly the prices were computed. Similarly, we assume that\nwe have some forecast of non-guaranteed bids, but do not make any assumptions on how\nthe bids by themselves are generated by individual campaigns. In terms of ad serving, the\nallocation model produces both a primal and dual solution, the latter of which can be stored\ncompactly and interpreted by the ad server to serve ads to user visits in an online, real-time\nmanner (e.g., see [10, 26]).\nWe have implemented the proposed inventory allocation model and the solution techniques, motivated by the context of an operational online graphical display advertising\nsystem. Our results using real user visits, guaranteed campaigns, non-guaranteed bids, and\nclick/conversion data, indicate that the proposed approach is both versatile in capturing\nand trading off between various advertiser and publisher objectives, as well as efficient to\nsolve with high accuracy.\nIn summary, the main contributions of this paper are:\n• A formalization of the inventory allocation model and objectives for online graphical\ndisplay advertising (Section 2).\n4\n\n\fYahoo! Labs Technical Report No. YL-2010-004\n• A multi-objective optimization formulation and various solution techniques for optimizing the multiple objectives (Section 3).\n• An experimental evaluation of the proposed techniques using real data sets (Section 4).\n\n2.\n\nModel and Objectives\n\nWe begin by first defining some notation, and then motivating and formalizing the\nvarious objectives in online graphical display advertising.\n2.1.\n\nSupply and Demand Model\n\nAs mentioned earlier, the main goal of inventory allocation is to match supply (user\nvisits) and demand (advertising campaigns). We thus begin by modeling user visits, advertising campaigns, and their interaction.\nUser visits can be represented as attributes-value pairs, where the attributes represent\nthe properties of a user, the properties of the page they visit, as well as the time stamp\nof the visit. An example user visit could be represented as: Gender = Male, AgeGroup =\n30-40, Interests = {Sports, Finance}, Location = California, ..., PageCategory = {Sports},\n..., Day = 15 Jan 2011, Time = 12:35pm GMT.\nA display advertising campaign targets a subset of user visits by specifying a targeting\npredicate. For instance, an advertising campaign that targets Males in California visiting\nSports pages in the month of Jan 2011 can be represented as: Gender ∈ {Male} ∧ Location\n∈ {California} ∧ PageCategory ∈ {Sports} ∧ Duration ∈ [1 Jan 2011 - 31 Jan 2011]. A\nuser visit is said to be eligible for an advertising campaign if the attribute-value pairs of\nthe user visit satisfy the targeting predicate of the advertising campaign. In the rest of this\npaper, we will focus on just the eligibility relationship between user visits and advertising\ncampaigns, and not on the specific attribute-value pairs or the targeting predicates.\nAdvertising campaigns can be of two types: guaranteed campaigns, whereby the publisher guarantees a fixed number of user visits to an advertiser in advance, and nonguaranteed campaigns, where advertisers bid in real-time for user visits. Both guaranteed\nand non-guaranteed campaigns can have one or more advertiser goals: to obtain user visits (this is the only goal that is guaranteed in a guaranteed campaign), to obtain clicks,\nand/or to obtain conversions. In non-guaranteed campaigns, however, these objectives are\nconverted into a bid by a bidding agent and thus, for the purpose of modeling, they can be\nrepresented as a bid for each user visit. Guaranteed campaigns, on the other hand, need\nto be modeled in more detail. Specifically, a guaranteed campaign has a user visit goal\n(the guarantee), a penalty function that specifies the penalty to be paid by the publisher if\nthe guarantee is not met, and a value for each click and/or conversion. A key aspect that\nenables yield optimization for clicks and/or conversions is the probability of a click and/or\nconversion given a user visit.\n\n5\n\n\fYahoo! Labs Technical Report No. YL-2010-004\nFinally, for reasons of scale, it is usually not practical to work with all future user visits\nfor a duration of many months (many large publishers have billions of impressions per day).\nConsequently, the inventory allocation problem often has to be solved on a sample of user\nvisits, and thus each sampled user visit is annotated with a sample weight.\nThe following notation summarizes the above discussion:\n• I: Set of user visits\n• si : Sample weight of user visit i ∈ I\n• ri : The payout for user visit i ∈ I by non-guaranteed campaigns\n• J : Set of guaranteed campaigns\n• dj : User visit goal for guaranteed campaign j ∈ J\n• Pj : N → R: Penalty function for guaranteed campaign j ∈ J , which maps underdelivery (how much a guarantee is missed) to a penalty.\n• Wjc : The value of a click for a guaranteed campaign j ∈ J\n• Wja : The value of a conversion (action) for a guaranteed campaign j ∈ J\n• pcij : The probability that a user corresponding to user visit i ∈ I clicks on an ad\ncorresponding to guaranteed campaign j ∈ J\n• paij : The probability that a user corresponding to user visit i ∈ I converts on an ad\ncorresponding to guaranteed campaign j ∈ J\n• Bj : Set of user visits ∈ I that are eligible for guaranteed campaign j ∈ J .\nIt is often convenient to view the user visits and guaranteed campaigns in a bipartite\ngraph, with user visits I on one side, guaranteed campaigns, J on the other, and an edge\n(i, j) between a user visit i and a guaranteed campaign j if i ∈ Bj , that is if i satisfies the\ntargeting predicates of j. An example of such graph is shown in Figure 1.\nIt should be noted that the construction of this graph, which may involve tens of thousands of nodes and millions of edges, is in itself a major computational task.\n2.2.\n\nObjectives\n\nIn this section, we motivate and formalize the various objectives that are relevant to the\ninventory allocation problem. In the next section, we introduce a mathematical model to\ntrade-off and optimize across these different objectives.\nAn overriding objective of publishers is to minimize the penalties incurred in case guarantees are not met. Minimizing penalties is important because the publisher not only incurs\nan immediate monetary loss, but also because the publisher could suffer longer-term losses\n\n6\n\n\fYahoo! Labs Technical Report No. YL-2010-004\n\nSupply:\nad opportunities\n\nDemand:\ncontracts\n\nFigure 1: Example of the allocation graph.\ndue to advertiser attrition. Define yij as the weight of user i ∈ I that is allocated to guaranteed\nP campaign j ∈ J . Then the amount of user visits delivered to a guaranteed campaign\nj is i yij , and the objective of minimizing penalties can be formalized as:\nX\nX\nmin\nPj (dj −\nyij )\n(1)\nj\n\ni\n\nHowever, in this paper we avoid this issue by ensuring feasibility of the constraints (see\nsection 3.1).\nAt a high-level, there are two parts to the inventory allocation problem, corresponding\nto non-guaranteed and guaranteed campaigns. The objective for non-guaranteed campaigns\nis simple: to maximize the revenue for the publisher, since advertisers only bid what each\nuser visit is worth to them. Modeling the objectives for guaranteed campaigns, on the other\nhand, is more complex because advertisers could be interested in brand awareness/reach or\nperformance (clicks, conversions) or both. We thus have three objectives: non-guaranteed\nrevenue, brand awareness/reach, and performance.\nBefore formalizing the above objectives, we introduce some notation:\nP\n• zi = si − j yij : The weight of user visit i ∈ I that is allocated to non-guaranteed\ncampaigns\nP\n• Sj = i∈Bj si : The total amount of user visits eligible for guaranteed campaign j ∈ J\nd\n\n• θij = si Sjj : The ideal fully representative target allocation fraction of user visit i ∈ I\nto guaranteed campaign j ∈ J (motivated further below)\n2.2.1. Non-Guaranteed Revenue The prices paid by non-guaranteed advertisers depend heavily on the particular user. Therefore, a natural goal of an allocation is to maximize\n7\n\n\fYahoo! Labs Technical Report No. YL-2010-004\nthe publisher’s revenue by allocating the highest valued user visits to the non-guaranteed\ncontracts. Since the amount of revenue that a publisher obtains from a user visit i is ri zi ,\nthis objective is written as:\nX\nmax\nri zi\n(2)\ni\n\n2.2.2. Brand Awareness/Reach There are two primary reasons why it is important\nto have a brand awareness/reach objective. The first reason is that brand advertisers typically want to reach a large swath of their target audience. For instance, a brand advertiser\nwho targets user visits from the US will likely be quite unhappy if all of their delivered user\nvisits are from fourteen year old males in Wyoming, and none from the rest of the population (even though all the delivered user visits technically satisfy the targeting predicate\nof the advertiser). In other words, brand advertisers often want a representative subset of\ntheir target audience2 .\nThe second reason for having a brand awareness and reach objective is more subtle, and\nit relates to the interaction with non-guaranteed campaigns. Specifically, if the primary goal\nof the publisher was to maximize short-term revenue, then he could allocate all of the highvalue impressions to the highest bidding non-guaranteed campaigns, and allocate only the\nremaining impressions to the guaranteed campaigns. However, this is clearly detrimental\nto the advertiser, and is also a dangerous road to take for the publisher: by selectively\nallocating the most expensive user visits to the non-guaranteed contracts, the publisher\nrisks alienating in the long term the guaranteed advertisers, many of whom pay a large\npremium for guarantees. In fact as Ghosh et al. [15] recently argued, in these situations\nprice serves as a signal of value, thus the user visits with the highest ri may also be the\nones most desired by the guaranteed contracts.\nTherefore, it is in the long-term interests of publishers to allocate each guaranteed\ncampaign a representative sub-set of targeted user visits. Ideally, every eligible user visit\ni ∈ Bj should be equally likely to see an ad from j. A similar argument holds on a temporal\nscale. A week long contract should have the same probability of being displayed on all days\nof the week. That is, it should be allocated uniformly during the course of the week—after\nall, if an advertiser wanted the contract to be shown only on Tuesday, she would have added\nthat to the targeting constraints.\nTo model these representativeness constraints, let θij be the ideal target allocation.\nNote that θij directly encodes the fact that no user visit i is preferred by j over others. To\nmaximize long term revenue, the publisher should strive to find an allocation close to θij .\nIn this paper we use the L2 -norm distance to measure closeness to the target allocation.\nWe denote Vj the importance of a representative allocation to advertiser j and write the\n2\nAdvertisers could potentially control their target allocation (other than uniformly at random) more\nclosely if they desire, at the cost of targeting more defined, and therefore likely more expensive, inventory\n\n8\n\n\fYahoo! Labs Technical Report No. YL-2010-004\nobjective as:\nmin\n\nX X Vj\n(yij − θij )2\n2θij\nj\n\n(3)\n\ni∈Bj\n\nFor consistency with other objectives, we will use the maximization form:\nmax −\n\nX X Vj\n(yij − θij )2\n2θij\nj\n\n(4)\n\ni∈Bj\n\nNote that alternative forms such as an entropy function or K-L divergence can also\nbe used, as they retain the essential features of separability (by advertiser) and convexity.\nSimilarly, other target allocations besides the perfectly uniform target allocation can also\nbe considered but these variants are beyond the scope of this paper.\n2.2.3. Clicks/Conversions for Guaranteed Campaigns An advertiser in a guaranteed campaign may have multiple objectives, such as clicks and conversions, besides\nobtaining the guaranteed user visits. The publisher’s objective is to maximize the yield\nfrom such goals, while also directing such clicks and conversions to the advertisers who\nvalue them the most. This can be modeled as trying to maximize the expected value of\nclicks and conversions across all user visits:\nXX\n\u0001\nWjc pcij yij + Wja paij yij\nj\n\ni∈Bj\n\nFor compactness we define:\nwij = Wjc pcij + Wja paij .\nThen we can rewrite the performance objective as:\nXX\nwij yij .\nmax\nj\n\n(5)\n\n(6)\n\ni∈Bj\n\nNote that when we refer to “clicks” below, it is to be understood to include the subsequent\nconversions.\n3.\n\nThe mathematical models\n\nIn the previous section we described the competing objectives faced by a publisher in\nallocating user visits to guaranteed contracts. In this section we formally state the mathematical problem that incorporates these objectives subject to the feasibility constraints.\nThere are three types of constraints that the allocation must satisfy to be feasible. First,\nthe desired number of user visits must be allocated for each guaranteed contract:\nX\nyij = dj (Demand Constraints)\n∀j\ni∈Bj\n\n9\n\n\fYahoo! Labs Technical Report No. YL-2010-004\nNext, each user visit can be allocated to exactly one guaranteed or non-guaranteed contract.\nX\nyij + zi = si (Supply Constraints)\n∀i\nj|i∈Bj\n\nFinally, we must ensure that the allocation is always non-negative.\nyij ≥ 0 (Non-Negativity Constraints)\nzi ≥ 0\n\n∀i, j\n\n∀i\n\nPutting together the objectives from the previous section with the set of constraints, we\nmay state our generic multi-objective optimization:\n\n P P\nV\n− j i∈Bj 2θjij (yij − θij )2\nP P\n\n\n(7)\nmax \n\nj P\ni∈Bj wij yij\ni ri zi\nsubject to\n\nP\n\nj|i∈Bj\n\nP\n\nyij + zi = si ∀i\n\n(8)\n\n∀j\n\n(9)\n\ni∈Bj\n\nyij = dj\n\nyij ≥ 0\n\n∀i, j\n\n(10)\n\nzi ≥ 0\n\n∀i\n\n(11)\n\nNote that the non-negative variables zi transform what would be inequality supply\nconstraints into equalities. Since zi is actually the leftover\nP supply inventory that will be\nsold to the non-guaranteed delivery spot market, the term i ri zi can be viewed as the total\nnon-guaranteed revenue that can be obtained for an allocation y, with remnant inventory z.\nThe second objective is the revenue obtained from clicks on the displayed advertisements.\n3.1.\n\nEnsuring feasibility\n\nIn this model formulation, guarantees are treated as constraints (Demand Constraints).\nConsequently, before we optimize for the other objectives, we need to ensure that the\nmodel is feasible, i.e., there is sufficient supply for guaranteed campaigns. Note that even if\na publisher is careful to accept only guaranteed campaigns that are feasible at the time of\nbooking, the model could become infeasible at a later point because forecasts of user visits\ncould change due to unforeseen events.\nIn order to make the model feasible, we add dummy user visits that have unlimited\nsupply but a very high cost of being used, and connect them to all the guaranteed campaigns. The cost associated with using each dummy user visit for a guaranteed campaign j\ncorresponds to the penalty Pj > 0 incurred by that guaranteed campaign in case of underdelivery (note that if Pj has multiple penalty values for under-delivery, then these can be\n10\n\n\fYahoo! Labs Technical Report No. YL-2010-004\nrepresented as multiple dummy user visits, each with a different cost of being used). The\ncost associated with using real user visits is 0.\nGiven the above set up, the goal is to find the minimum cost allocation to guaranteed\ncampaigns. Since this problem is a pure network with a linear objective function, it can be\nsolved very efficiently [4]. If the cost of the optimal allocation is 0, then it implies that all\nthe guaranteed campaigns can be satisfied, and hence that the original model is feasible. If\nthe cost of the optimal allocation is greater than 0, then it implies that some guaranteed\ncampaigns will under-deliver. Furthermore, the optimal allocation to the dummy user visits\nwill indicate how much each guaranteed campaign needs to under-deliver so that overall\npenalty cost is minimized. In this case, the user visit goal dj for each guaranteed campaign\nis reduced by the amount of allocation to the dummy user visits in order to make the model\nfeasible. We then follow one of the procedures described in the remainder of this section.\n3.2.\n\nMulti-objective programming\n\nWe may approach a multi-objective function model in a number of ways (see [24]).\nOne general approach is to obtain an “efficient frontier” of solutions where at each point\non the curve the value of one objective can only be improved at the expense of degrading\nanother (see the generic example in Figure 2, where we notionally trade off total revenue and\n“representativeness”). The user may then choose any point on this curve as the “solution”.\n\nFigure 2: Efficient Frontier\nMore algebraic approaches include using a weighted sum of the multiple objectives\nand/or using “goal programming”, whereby the objectives are handled sequentially, with\n11\n\n\fYahoo! Labs Technical Report No. YL-2010-004\nthe additional constraint(s) that previous objectives retain a certain fraction of their optimal\nvalue. Since we have three objectives here there are clearly several variations, which we\nexplore below.\nIn what follows, it will be convenient to define the 3 objective components as follows:\nX X Vj\n(yij − θij )2\n2θij\nj i∈Bj\nXX\nF2 (y) =\nwij yij\nF1 (y) = −\n\nF3 (z) =\n\nj\n\ni∈Bj\n\nX\n\nri zi\n\n(12)\n(13)\n(14)\n\ni\n\nDepending on the data available, we may formulate solution strategies which employ:\n1. A single parametrized objective function.\n2. A two-objective function model in various flavors\n3. A three-objective function model, also in several flavors.\n3.3.\n\nSingle objective\n\nWe consider a multi-component objective function:\nmax {γF1 (y) + ξF2 (y) + F3 (z)}\n\n(15)\n\nsubject to (8)-(11)\nThe parameter γ ≥ 0 is the weight for the “representativeness” component. The parameter ξ, in conjunction with the wij reflects the means by which we attribute value to\nclicks. We observe that a feature of this model is that the shadow value βi of the supply\nconstraint i is always no less than the reserve price, i.e. βi ≥ ri .\nHere, we assume that a value of γ is available, from either historical data or business\nconsiderations. We consider explicitly two cases for the value of ξ:\n• The values of the wij are computed via (5) with Wjc = Wja = 1. Then ξ is the value\nof a click relative to a unit of revenue from the objective F3 (z).\n• The wij are actual expected monetary value of allocating an impression of type i to\ncontract j, in which case ξ = 1.\nIn both cases, the model involves the addition of a linear term in y to the strictly convex\ndistance function F1 (y). The objective thus remains strictly convex quadratic in y and\nlinear in z. Optimization of this model is straightforward in principle, using a commercial\nsolver such as XpressMP [27]. If a sufficiently powerful large-scale nonlinear network code\nwere available, this could also be used since our constraints are of the pure network type.\n12\n\n\fYahoo! Labs Technical Report No. YL-2010-004\n3.4.\n\nTwo objectives\n\nIf all of the above data are not available to us, we must resort to multi-objective or goal\nprogramming. Let us first assume that γ is not known, but the click-related data (ξ, wij )\nare available. In this case, we may initially solve:\nmax {ξF2 (y) + F3 (z)}\n\n(16)\n\nsubject to (8)-(11). Note that this is a linear pure network minimum cost flow problem\nwhich can be solved very rapidly by special purpose software (e.g., see Bertsekas [4]). Let\nthe optimal value of this linear program (LP) be M ∗ .\nWe may now append a constraint specifying that at least a certain fraction ψ (0 < ψ < 1)\nof this monetary value be preserved and solve the model:\nmax F1 (y)\n\n(17)\n\nX\n\n(18)\n\nsubject to (8)-(11) and\nξ\n\nXX\nj\n\nwij yij +\n\nri zi ≥ ψM ∗\n\ni\n\ni∈Bj\n\nIt may be shown (see Appendix) that the unknown parameter γ is equal to the inverse of\nthe dual value for the constraint (18).\nSuppose now that a value for γ is available but ξ is not. We may initially solve\nmax F2 (y)\n\n(19)\n\nsubject to (8)-(11). Let the optimal value of this linear program (LP) be P ∗ . We may\nnow append a constraint specifying that at least a certain fraction ω of this “click value”\n(however it is quantified) be preserved, and solve the model:\nmax {γF1 (y) + F3 (z)}\n\n(20)\n\nXX\n\n(21)\n\nsubject to (8)-(11) and\nj\n\nwij yij ≥ ωP ∗\n\ni∈Bj\n\nThe third variant combines the two objectives F1 and F2 in the second stage model, after\nsolving a first stage model:\nmax F3 (y)\n(22)\nsubject to (8)-(11). Since this is also a linear pure network minimum cost flow problem is\ncan be solved very rapidly by special purpose software. Let the optimal value of this linear\nprogram be R∗\nIn this approach, the second model is of the form:\nmax {γF1 (y) + ξF2 (y)}\n13\n\n\fYahoo! Labs Technical Report No. YL-2010-004\n\nsubject to (8)-(11) and\n\nX\n\nri zi ≥ ηR∗\n\n(23)\n\ni\n\nwhere η (0 < η < 1) is the fraction of NGD revenue we wish to preserve.\nThis model would require us to have the relative weights on F1 and F2 available. This\nmodel is also guaranteed to be feasible if the original constraints (8)-(11) are feasible.\n3.5.\n\nThree objectives\n\nFrom an operability point of view, it is unlikely that both the parameters γ and ξ\nwould be known, or even understood very well. In that case, we cannot avoid turning to\ngoal programming and the use of more intuitive “knobs” for the business to use in the\ndecision making process. Our goal program reduces to a sequence of three models. In\nprinciple, these models could be solved in any order, but we take the point of view that the\nnonlinear objective F1 (y) should be optimized last, in order to avoid imposing a nonlinear\nconstraint. Such a non-linear constraint would be computationally crippling at the scale\nwe are contemplating. The last model therefore must optimize representativeness subject\nto constraints on non-guaranteed revenue and click value.\nWe again emphasize that, given appropriate wij , these last two quantities can reasonably\nbe thought of in monetary terms, whereas F1 may not.\nSince non-guaranteed revenue is undeniably monetary, the most intuitive procedure\nmight be to first solve for F3 , i.e. solve the problem (22) and then solve for maximum click\nvalue:\nmax F2 (y)\n(24)\nsubject to (8)-(11) and\nX\n\nri zi ≥ ηR∗\n\n(25)\n\ni\n\nwhere η, R∗ are as defined in constraint (23). Let the optimum objective function value be\nP ∗∗ .\nWe may now optimize the representativeness function F1 (y) subject to (8)-(11) and\nX\nri zi ≥ ηR∗\n(26)\ni\n\nXX\nj\n\nwij yij ≥ ωP ∗∗\n\n(27)\n\ni∈Bj\n\nClearly we could reverse the first two steps. This would likely produce different results,\nbut both would be feasible.\nThere is however a severe computational disadvantage to this 3 step process. While step\n1 is a pure network optimization model, the second step involves a side constraint (23) or\n(21) which destroys the orders of magnitude speed advantage of pure network optimizers\n14\n\n\fYahoo! Labs Technical Report No. YL-2010-004\nover general LP solvers. In an effort to avoid this, we might omit the side constraint from\nthe second model, leaving it a pure network model. However there is now no guarantee\nthat the third model, incorporating (26), will be feasible. We might then relax the side\nconstraints until feasibility is obtained, but rigor is lost.\n4.\n\nExperiments\n\nWe now present some experimental results using a snapshot of real online graphical\ndisplay advertising data sets. The main goal is to quantify the benefits of the multi-objective\noptimization approach proposed in this paper as compared to the more traditional singleobjective optimization approaches. We note that our specific focus is on quantifying the\nrelative benefits of the various optimization approaches for a given data/forecast snapshot,\nand not on online ad serving mechanisms (e.g., [10, 26]) that account for forecast errors,\nand could use periodic re-optimization.\n4.1.\n\nExperimental setup\n\nThe data snapshot consists of guaranteed campaigns, forecasts of user visits, nonguaranteed bids and clicks, all based on a subset of historical data from an operational\ndisplay advertising system. The generated optimizer graph has 32,390 supply nodes, 2,696\ndemand nodes and 1,407,753 edges. The supply weight (si ) ranges from 10.83 to 1.18 × 109 .\nThe user visit goal (dj ) ranges from 1 to 6.96 × 107 . The non-guaranteed price (ri ) ranges\nfrom 0.046 to 4.350. The click probability (pcij ) ranges from 1.290 × 10−6 to 0.947 (the\nexperiments only evaluate clicks, not conversions).\nFigure 3 shows the experimental flow based on the data snapshot. The first step is to invoke the User Visit Forecasting module to generate a forecast of user visits that correspond\nto the guaranteed campaigns, and construct the allocation graph. The next two (parallel)\nsteps are to invoke the Non-Guaranteed Forecasting and Click Forecasting modules to annotate the allocation graph with the non-guaranteed revenue information (ri ) and the click\nprobabilities (pcij , respectively. The final step is to run the Optimizer to solve the various\noptimization methods proposed in the paper.\nThe metrics that we measure for the various optimization runs are the non-guaranteed\nrevenue, the total value of clicks, and the representativeness of guaranteed campaigns. The\nexperiments are run for various values of the model parameter settings. For the experiments,\nall the allocation priorities Vj are set to 1 and all the click values Wjc are set to 10.\nThe experiments were run on a 64bit/64GB/2GHz Linux box using the optimization\npackage Xpressmp3 [27].\n3\n\nRecently acquired by Fair Isaac (FICO)\n\n15\n\n\fYahoo! Labs Technical Report No. YL-2010-004\n\nOptimized\nAllocation\n\nOptimization\nAnnotated\nAllocation\nGraph\n\nClick\nForecasting\n\nNon-Guaranteed\nForecasting\nAllocation\nGraph\n\nUser Visit\nForecasting\nGuaranteed\nCampaigns\n\nCampaign\nDatabase\n\nFigure 3: Experiment Flowchart\n4.2.\n\nForecast models\n\nAlthough the forecast models used here are not the focus of this paper, we include a\nbrief description of each for completeness.\n4.2.1. Supply Forecasting User visit forecasting uses time-series trend predictions to\npredict the trend/growth in user visits for various pockets of supply such as Sports or\nFinance. Our specific implementation uses SARIMA [23] for time-series predictions.\nIn addition, to generate the allocation graph, we need to produce a sample of user\nvisits. For this, we follow the sampling procedure outlined in Vee et al [26], which produces\na unbiased sample for the class of optimization models that we consider in this paper.\nSpecifically, for each guaranteed campaign, the supply forecast selects k eligible user visits\nuniformly at random. Once all of the user visits for all campaigns are chosen, the weights\ngiven to each user visit is normalized so that the total weight of any subset of user visits is\n— in expectation — equal to the predicted available supply. The allocation graph is then\ncreated by adding edges from the campaigns to their eligible user visits.\n4.2.2. Non-Guaranteed Forecasting The goal of the Non-Guaranteed Forecasting\nmodule is to predict the expected revenue obtained by selling a particular user visit in the\nnon-guaranteed marketplace. We observed that the prices paid for ad opportunities by the\nnon-guaranteed contracts followed a log normal distribution, with prices ranging from under\n$0.10 CPM to above $10 CPM. To predict the price of an individual user visit, we trained\na generalized least squares regression model on the logarithm of the prices. Each user visit\nwas annotated with a set of user features, for example, age, gender, etc, and a set of page\n\n16\n\n\fYahoo! Labs Technical Report No. YL-2010-004\nfeatures, for example Sports, Finance, etc. We used the value predicted by the model as\nthe non-guaranteed price ri .\n4.2.3. Click forecasting Click forecasting estimates the probability that a displayed\nad will be clicked in a particular user visit context. Estimation of click-through rates (CTR)\nis extensively applied in pay-for-performance systems that attempt to maximize expected\nrevenue [20, 22]. The estimates can be based on historical click-through performance statistics of features that are selected as significant predictors of CTR. In this work, we use a\nlogistic regression model with the following functional form:\np(click|{f1 , . . . , fK }) =\n\n1 + exp(\n\n1\nPK\n\nk=1 wk fk )\n\n(28)\n\nwhere fk are predicates of features of the user visit and the ad such as user age and gender,\npage content category, ad position in page, etc. Using historical data from web traffic that\nconsist of page visits with corresponding user visit and click statistics, we learned the logistic\nregression parameters.\nFinally, each edge in the allocation graph, which represents a contract and a corresponding eligible ad opportunity, is annotated with features fk . The click model is used to\nproduce an estimate of the probability of click pij for each graph edge.\n4.3.\n\nExperimental results\n\nWe now present our experimental results for various models and parameter settings.\nEach optimization for a particular parameter combination takes about 5 to 10 minutes,\nwhich is quite acceptable as an offline optimization step that can inform online serving\nalgorithms (e.g., [10, 26]).\n4.3.1. Baseline and Single-Objective Solutions As a baseline, we compute the\noptimization solution that only ensures feasibility by minimizing the total under-delivery\npenalty, but does not explicitly optimize for the other objectives. The result for this baseline\nsolution is shown in the first row of Table 1) and is used as the basis for normalization. We\nthen optimize for each single objective separately and report the normalized solutions in\nthe same table — here NGD refers to non-guaranteed revenue, and GD refers to guaranteed\nrepresentativeness. It can be seen that while optimizing a single objective may significantly\nimprove that objective, it has a potentially significant impact on the other objectives.\n4.3.2. Two-Objective Formulation We run the 2-step programming algorithm (Section 3.4) with 100 different values of the η parameter. Table 2 shows the objectives in\nnormalized scale for a subset of the generated efficient solutions. The whole efficient frontier by using all the 100 points is depicted in Figure 4, which shows the trade-off between\n\n17\n\n\fYahoo! Labs Technical Report No. YL-2010-004\nTable 1: Baseline and Single-Objective Solutions\nObjective\nBaseline\nNGD\nClick\nNGD+Click\nGD\n\nNGD\n1\n1.0165\n1.0062\n1.0131\n0.9968\n\nClick\n1\n1.0905\n2.9136\n2.9013\n0.9226\n\nNGD+Click\n1\n1.0229\n1.1722\n1.1774\n0.9903\n\nGD\n-1\n-0.9836\n-2.2214\n-1.7440\n-0.0027\n\nTable 2: 2-Step Goal Programming\nη\n0.8411\n0.8570\n0.8729\n0.8888\n0.9047\n0.9205\n0.9364\n0.9523\n0.9682\n0.9841\n0.9999\n\nNGD\n0.9968\n0.9978\n0.9993\n1.0016\n1.0047\n1.0079\n1.0094\n1.0110\n1.0127\n1.0131\n1.0131\n\nClick\n0.9226\n1.1269\n1.3260\n1.5171\n1.6990\n1.8810\n2.0801\n2.2782\n2.4763\n2.6861\n2.9013\n\nNGD+Click\n0.9903\n1.0090\n1.0277\n1.0464\n1.0651\n1.0838\n1.1026\n1.1213\n1.1340\n1.1587\n1.1774\n\nGD\n-0.0027\n-0.0028\n-0.0034\n-0.0046\n-0.0069\n-0.0109\n-0.0181\n-0.0328\n-0.0660\n-0.1474\n-1.7440\n\nFigure 4: Efficient Frontier\nthe monetary objective (non-guaranteed revenue plus click value) and the non-monetary\nobjective (representativeness).\nFigure 4 also shows the baseline and single-objective solutions as different symbols. The\nbaseline solution and most of the single-objective solutions are located in the interior of\n\n18\n\n\fYahoo! Labs Technical Report No. YL-2010-004\nthe feasible solution region and dominated by efficient solutions on the frontier. Further,\nthe efficient frontier shows that a very small loss in one objective can lead to a significant\ngain in the other objective, thereby enabling a better combined trade-off as compared to\nsingle-objective solutions.\nThe above results clearly demonstrate that explicitly capturing and solving for multiple\nobjectives dominates ignoring the objectives, or just optimizing for a single objective.\n4.3.3. Three-Objective Formulation We run the 3-step programming algorithm (Section 3.5) with 100 different combinations of the η and ω parameters. Table 3 shows the\nobjectives in normalized scale for a subset of the generated efficient solutions. Note that\nthe efficient frontier for the tri-objective optimization problem is a surface in a 3D space.\nOne way to visualize the efficient frontier is to project it to a 2D space by fixing the level\nin the third dimension. Figure 5 shows three such contours in the space of click value and\nGD representativeness for evenly-spaced NGD revenues. The trade-off among the three\nobjectives can be observed by comparing points on the same contour and between different\ncontours.\nTable 3: 3-Step Goal Programming\nη\n0.9961\n0.9961\n0.9961\n0.9981\n0.9981\n0.9981\n0.9999\n0.9999\n0.9999\n\n4.4.\n\nω\n0.3812\n0.6906\n1.0000\n0.4073\n0.7037\n1.0000\n0.6967\n0.8483\n0.9999\n\nNGD\n1.0125\n1.0125\n1.0125\n1.0145\n1.0145\n1.0145\n1.0165\n1.0165\n1.0165\n\nClick\n1.1075\n2.0067\n2.9058\n1.1740\n2.0281\n2.8823\n1.1209\n1.3649\n1.6090\n\nNGD+Click\n1.0208\n1.0990\n1.1772\n1.0284\n1.1027\n1.1770\n1.0255\n1.0468\n1.0680\n\nGD\n-0.0084\n-0.0180\n-1.7272\n-0.0129\n-0.0232\n-1.7225\n-0.3264\n-0.3280\n-0.5039\n\nDiscussion\n\nThe results above and especially the efficient frontier give us a global picture of the\ntrade-off among different objectives. It reveals what percentage of one objective can be\ngained at the cost of one percent of another objective and how the trade-off rate changes\nwith the location of the solution, thereby providing valuable insight in setting the right\nparameters in a real production system to truly reflect the business priority. Perhaps more\nimportantly, it demonstrates the significant benefit of a combined optimization model for\nmultiple objectives, as opposed to optimizing for just a single objective.\n5.\n\nFurther extensions\n\nWe have referred throughout to revenue gained through clicks. We might also include\nother non-monetary terms, using ξ or some other parameter as a conversion factor. One\n19\n\n\fYahoo! Labs Technical Report No. YL-2010-004\n\nFigure 5: Contours of Efficient Frontier\ncandidate for this treatment is relevance, when wij would encode the probability of relevance\nof ad j to user i and the payoff to advertiser j for such relevance.\nAnother candidate for future work is the exploration of ways to to incorporte uncertainty into the strictly deterministic model we have described here. Preliminary stochastic\nprogramming experiments [5] indicate that uncertainty in the inventory supply can have\na significant impact. Other sources of uncertainty arise in sampling error and click and\nconversion rate estimation.\n6.\n\nRelated work\n\nOperations Research techniques, and optimization in particular, have been used for\ndecades in planning advertising campaigns in other media, such as print, radio and television (see [6] for example). The potential of the WWW for more targeted advertising was\nrealized in the 1990’s and became a subject of research. Langheinrich et al [17] reported on\na study where they attempted to target display ads to users in order to optimize expected\nrevenue, without using intrusive data gathering techniques. While “unintrusiveness” seems\nto have become less of a concern to advertisers and publishers, their optimization approach\nwas influential. Tomlin [25], noting a similarity between this problem and the traffic distribution model (see [28]), suggested adding an entropy term to the linear cost function\nproposed in [17] to essentially smooth the allocations and prevent “bang-bang” solutions;\ncharacteristic of LP models. Chickering and Heckerman [9] employed an alternate smoothing technnique, involving binning, and gave computational experience. A good survey of\nthese and later developments is given in [18].\nSince that time, the development of Computational Advertising as a discipline has led to\nthe consideration of many variants of the graphical advertising allocation problem. A crucial\n\n20\n\n\fYahoo! Labs Technical Report No. YL-2010-004\ndevelopment in this process was the promulgation of the concept of “fairness” in allocation\nto offset extreme LP solutions. While we have used the approach of [15], an alternate\napproach is taken by [13]. Some of our early work was presented in [1] and [29]. Feldman\net al [14] have considered the online ad allocation problem in the more general setting of the\nmatching problem, while Roels and Fridgeirsdottir [21] have considered a dynamic version\nassuming the inventory follows a Markov process. Easterly [11] and Han [16] discuss online\nadvertising from a Revenue Management point of view.\nTwo other modules in the online advertising supply chain make use of the results of an\nallocation model such as we have described. These are Ad Serving and Admission Control.\nThe Ad Server interprets the output of the allocation model as a set of frequencies with\nwhich specific ads should be shown to users in the supply pools when they visit a web\npage. Considerable practical advantages ensue when the solution can be stored in compact\nform and rapidly reconstructed on the fly by the ad server when a page requests ads. This\nprocess has been studied by Devanur and Hayes [10] and Vee et al [26], using the dual values\nassociated with the demand constraints and, implicitly, the graph structure to determine\nthe contracts for which a new arriving impression is eligible. The optimization models we\nhave studied provide the necessary dual values, but online ad serving is not considered in\nthis paper.\nAdmission Control is the process of determining whether a proposed new guaranteed\ncampaign should be accepted, that is whether the existing obligations can still be satisfied\nin a modified solution if the new contract is accepted. Versions of this problem have been\nstudied by Feige et al [12] and Aleai et al [3] and considered as an (NP-hard) combinatorial\noptimization problem. Radovanovic and Zeevi [19] have proposed a relaxed, more tractable,\nvariant of the Admission Control process.\n7.\n\nConclusion\n\nIn this paper we have shown that multi-objective optimization provides an efficient\nand flexible framework for the allocation of user visits to guaranteed and non-guaranteed\ncampaigns. The proposed models also incorporate a very flexible means of taking into\naccount revenue derived from clicks, as well as non-monetary objectives — in particular\n“representativeness” or “fairness” — in the allocation of eligible user visits to campaigns.\nWe are able to do this using off-the-shelf software, such as a commercial scale optimization\nsystem (we used XpressMP) for large scale quadratic programming, or specialized network\noptimization codes for some steps.\nWe believe that the proposed models can be extended to other objectives of interest\nsuch as ad relevance. As part of future research, we are exploring extensions to the model\nto include stochastic elements, such as uncertainty in supply and demand, and nonlinear\npricing.\n\n21\n\n\fYahoo! Labs Technical Report No. YL-2010-004\n8.\n\nacknowledgments\n\nWe gratefully acknowledge the assistance of Sumanth Jagannath and Wenjing Ma in\nprocessing the data for our experiments, and Jianchang Mao for managing much of the\nclick model development.\nReferences\n[1] D. Agarwal, J. Tomlin, and J. Yang. A log-linear model for allocating overlapping\ninventory to on-line advertisers. Presented at INFORMS, October 2008.\n[2] G. Aggarwal, A. Goel, and R. Motwani. Truthful auctions for pricing search keywords.\nIn Proc. of the 7th ACM Conf. on Electronic Commerce, pages 1–7, 2006.\n[3] S. Aleai, E. Arcaute, S. Khuller, W. Ma, A. Malekian, and J. Tomlin. Online allocation\nof display advertisements subject to advanced sales contracts. In Proc. of the 3rd Int.\nWorkshop on Data Mining and Audience Intelligence for Advertising (ADKDD), pages\n69–77, 2009.\n[4] D. Bertsekas. Network Optimization: Continuous and Discrete Models. Athena Scientific Press, 1998.\n[5] V. Bharadwaj, M. Saunders, and J. Tomlin. Sensitivity of an inventory allocation\nmodel for online advertising with stochastic supply. Presented at EURO XXIV, July\n2010.\n[6] S. Bollapragada, H. Chen, M. Phillips, M. Garbiras, M. Scholes, T. Gibbs, and\nM. Humphreville. NBC’s optimization systems increase revenues and productivity.\nInterfaces, 32(1):47 – 60, 2002.\n[7] A. Z. Broder. Computational advertising and recommender systems. In RecSys ’08,\npages 1–2, Lausanne, Switzerland, 2008.\n[8] A. Z. Broder, M. Fontoura, V. Josifovski, and L. Riedel. A semantic approach to\ncontextual advertising. In Proc. ACM SIGIR, 2007.\n[9] D. M. Chickering and D. Heckerman. Targeted advertising with inventory management.\nIn EC ’00: Proceedings of the 2nd ACM conference on Electronic commerce, pages 145–\n149, New York, NY, USA, 2000. ACM.\n[10] N. Devanur and T. Hayes. The adwords problem: Online keyword matching with\nbudgeted bidders under random permutations. Presented at EC, 2009.\n[11] A. Easterly. Revenue management in online advertising: Challenges and opportunities.\nPresented at INFORMS, October 2009.\n\n22\n\n\fYahoo! Labs Technical Report No. YL-2010-004\n[12] U. Feige, N. Immorlica, V. Mirrokni, and H. Nazerzadeh. A combinatorial allocation\nmechanism with penalties for banner advertising. Presented at INFORMS, October\n2008.\n[13] J. Feldman, M. Henzinger, N. Korula, V. S. Mirrokni, and C. Stein. Online stochastic\npacking applied to display ad allocation. CoRR, abs/1001.5076, 2010.\n[14] J. Feldman, A. Mehta, V. Mirrokni, and S. Muthukrishnan. Online stochastic matching:\nBeating 1-1/e. Presented at FOCS, October 2009.\n[15] A. Ghosh, R. McAfee, K. Papineni, and S. Vassilvitskii. Randomized bidding for\nmaximally representative allocation. In Proc. WINE, December 2009.\n[16] P. Han. Yield optimization in display ad networks. Presented at INFORMS, October\n2009.\n[17] M. Langheinrich, A. Nakamura, N. Abe, T. Kamba, and Y. Koseki. Unintrusive\noptimization techniques for web advertising. In Proc. of World Wide Web Conf.\n(WWW1999), 1999.\n[18] A. Nakamura and N. Abe. Improvements to the linear programming based scheduling\nof web advertisements. J. Electronic Commerce Research, 5:75–98, 2005.\n[19] A. Radovanovic and A. Zeevi. Revenue maximization in reservation-based online advertising. Presented at INFORMS, October 2009.\n[20] M. Richardson, E. Dominowska, and R. Ragno. Predicting clicks: Estimating the\nclick-through rate for new ads. In Proc. of World Wide Web Conf. (WWW2007),\n2007.\n[21] G. Roels and K. Fridgeirsdottir. Dynamic revenue mangement for online display advertising. http://escholarship.org/uc/item/7kq702dz, 2008.\n[22] B. Shaparenko, O. Çetin, and R. Iyer. Data-driven text features for sponsored search\nclick prediction. In Proc. of the 3rd Int. Workshop on Data Mining and Audience\nIntelligence for Advertising (ADKDD), pages 46–54, 2009.\n[23] R. Shumway and D. Stoffer. Time Series Analysis and Its Applications. Springer, 2007.\n[24] R. Steuer. Multiple criteria optimization: theory, computation, and application. Wiley,\nNew York, 1986.\n[25] J. Tomlin. An entropy approach to unintrusive targeted advertising on the web. In\nProc. of World Wide Web Conf. (WWW2000), 2000.\n[26] E. Vee, S. Vassilvitskii, and J. Shanmugasundarum. Optimal online assignment with\nforecasts. Proc. of the ACM Conf. on Electronic Commerce, 2010.\n23\n\n\fYahoo! Labs Technical Report No. YL-2010-004\n[27] Dash Optimization. XpressMP—Optimizer Reference Manual, 2008.\n[28] A. Wilson. Entropy in Urban and Regional Modelling. Pion Press, London, 1970.\n[29] J. Yang and J. Tomlin. Advertising inventory allocation based on multi-objective\noptimization. Presented at INFORMS, 2008.\nAppendix\nThe goal programming model (17), (8)-(11), (18) in section 3.4 is equivalent to finding\na value of γ which preserves a specified fraction of revenue, given a value of ξ. To see this,\nlet us write the Lagrangian of the problem (15), (8)-(11) as:\nXX\nwij yij\nLγ (y, z, αγ , β γ , λγ , µγ ) = γf (y, θ) − ξ\nj\n\n−\n+\n\nX\n\nβiγ (\n\ni\n\nX\n\nX\n\nri zi −\n\nX\n\ni\n\nj\n\nyij + zi − si ) −\n\nX\n\nαjγ (\n\ni∈Bj\n\nX\n\nλγij yij −\n\nij\n\nj|i∈Bj\n\nyij − dj )\n\ni∈Bj\n\nX\n\nµγi zi\n\ni\n\nwhere the αjγ , βiγ , λγij and µγij are the appropriate Lagrange multipliers on the equality and\ninequality constraints.\nThe KKT optimality conditions for this problem are:\n∂Lγ\n∂f\n=γ\n− ξwij − αjγ + βiγ − λγij\n∂yij\n∂yij\n∂Lγ\n= −ri + βiγ − µγi\n∂zi\n∂f\nλγij = γ\n− ξwij − αjγ + βiγ\n∂yij\nµγi = βiγ − ri\nλγij yij\n\n= 0,\n\nµγi zi\n\n=0\n=0\n≥0\n≥0\n=0\n\nNow consider the Lagrangian of the problem (17), (8)-(11), (18), introducing Lagrange\nmultiplier ρ:\n\n−\n\nX\nj\n\nαjη (\n\nX\n\nLη (y, z, αη , β η , λη , µη , ρ) = f (y, θ)\nX η X\nyij − dj ) +\nβi (\nyij + zi − si )\ni\n\ni∈Bj\n\nj|i∈Bi\n\n−\n\nX\n\nληij yij −\n\nij\n\n−ρ(ξ\n\nXX\nj\n\nwij yij +\n\n24\n\nµηi zi\n\ni\n\nX\ni\n\ni∈Bj\n\nX\n\nri zi − ψM ∗ )\n\n\fYahoo! Labs Technical Report No. YL-2010-004\nThe KKT optimality conditions are:\n∂f\n∂Lη\n=\n− ρξwij − αjη + βiη − ληij\n∂yij\n∂yij\n∂Lη\n= −ρri + βiη − µηi\n∂zi\n∂f\nληij =\n− ρξwij − αjη + βiη\n∂yij\nµηi = βiη − ρri\nληij yij\nρ(ξ\n\nXX\nj\n\ni∈Bj\n\nwij yij +\n\nX\n\n= 0,\n\nµηi zi\n\n=0\n=0\n≥0\n≥0\n=0\n\nri zi − ψM ∗ ) = 0\n\ni\n\nComparing terms in the Lagrangians and the two sets of KKT conditions we see that\nαjη = ραjγ , βiη = ρβiγ , ληij = ρλγij , µηi = ρµγi . In particular, we see that γρ = 1. Thus we\nmay obtain the parameter γ as 1/ρ from the solution to the goal programming problem.\n\n25\n\n\f"
        ],
        [
         "37",
         "37",
         "cs.CE",
         "Computational Engineering",
         "0510034v1.pdf",
         "arXiv:cs/0510034v1 [cs.HC] 14 Oct 2005\n\nCOMODI: On the Graphical User Interface\nZsolt I. Lázár\nFaculty of Physics\nBabeş-Bolyai University\nStr. M. Kogãlniceanu Nr. 1, RO 400084\nCluj-Napoca, Romania\nzlazar@phys.ubbcluj.ro,\n\nAbstract\nWe propose a series of features for the graphical user\ninterface (GUI) of the COmputational MOdule Integrator\n(COMODI) [1][2]. In view of the special requirements\nthat a COMODI type of framework for scientific computing\nimposes and inspiring from existing solutions that provide\nadvanced graphical visual programming environments, we\nidentify those elements and associated behaviors that will\nhave to find their way into the first release of COMODI.\n\n1 Introduction\nThe COmputational MODule Integrator (COMODI)[2]\nis an interdisciplinary collaboration for addressing the problem of programming crisis in computational sciences [3].\nIn [1], [4] and [5] we have pointed out that there are at\nleast three major problems plaguing computational software\ndevelopment and usage. Computational scientists tend to\nspend much of their time on writing code already written\nby others or trying to adapt existing code to local needs.\nOn global scale this represents a costly lack of efficiency in\nexploiting human resources. Generally, low quality “home\nmade” code is the only alternative to the anguish of searching, adapting and learning third party software; a process\nthat further undermines the trust in external code. Finally,\nthe irreproducibility of computer experiments by peers contributes again to the inefficiency of computational research\nby wasting the important advantage of virtuality over real\nexperiments. We suggest that shifting towards a reuse oriented paradigm is necessary. In the same papers it is argued\nthat a new software tool, language or standard is not sufficient. One has to explicitely aim at a large scale movement. However, the movement can only evolve around an\nOpenSource software solution that has to offer both on the\n\nAndreea Fanea, Dragoş Petraşcu,\nVladiela Ciobotariu-Boer and Bazil Pârv\nFaculty of Mathematics and Computer Science\nBabeş-Bolyai University\nStr. M. Kogãlniceanu Nr. 1, RO 400084\nCluj-Napoca, Romania\n{afanea,petrascu,vladi,bparv}@cs.ubbcluj.ro\n\nshort and long run sufficient benefits so that the computational community with deeply rooted programming traditions would consider it as an alternative, and contribute actively to its development. COMODI has the sole reponsibility of igniting the process and then letting the events follow\ntheir natural course driven by the need and skill of the whole\ncommunity. Therefore, it should not be a radically futuristic\nsolution but rather a sturdy bridge between the old and new\nparadigm. During the ignition process, due to the lack of\ninvolvement of the community the task is especially challenging. The needs that are best to target are difficult to\nidentify and because of limited human resources the evolutionary trial and error approach that keeps OpenSource even\nthough inefficient yet healthy is not admissible. Hence, the\nrequirement analysis is the core issue of our investigation.\nIn [1] we point out that the complete solution consists in\ntwo distinct parts. On one side there are tools used for converting regular code developed by component authors into\na COMODI component, called the developer side, while on\nthe other, there is a GUI-based graphical environment with\nunderlying layers responsible for binding together components into arbitrarily complex computational projects displayed as in figure 4. We term the latter as the user side. The\ntwo sides are clearly distinguishable by the tools that are\nused, the type of human activity that they presume and the\nrequired programming skills. Present paper focuses on the\nuser side exploring its most prominent component, the GUI.\nAs shown in [1], [4] and [5] the idiosyncrasy of COMODI\nis the special support provided to component authors. On\nthe user side, GUIs for visual programming can look back\nto a relatively long evolution history. Several applications\nendowed with advanced features are in use today. Coming up with something that would assure a leading edge is\nmore than a challenge and is off the point. However, the\nconstituting premises of COMODI entail a series of special\nrequirements set for the user interface. These, while do not\nmake it a direct competitor in this segment, allow for us-\n\n\f3 The COMODI GUI\n\ning present solutions as a valuable source of inspiration in\nits design process. Below we summarize a few important\nsolutions worthwhile to consider.\n\nAs suggested in the introduction the user side GUI is\nnot the part that would bring about the envisioned breakthrough. Even if quickly acquiring the support of a large\nuser and developer community, the GUI will still have to\ngo through a long maturing process before it can close up\non similar commercial VP environments. However, it can\nsignificantly limit the impact of COMODI if the design of\nthe very first release doesn’t follow such clear principles as\nthe zero effort threshold commandment on the component\ndevelopment procedure [1]. Below we formulate a few requirements that are essential for COMODI to be appealing:\n\n2 Visual programming in scientific computing\nThough visual programming (VP) is not a widely spread\nphenomenon in computational science, the two have already\nmade contact in a few areas [6]. Visualization of scientific\ndata is clearly a leader in this respect. AVS (Advanced Visualization System) is one of the dominant commercial visualization packages available today. It provides modules and a\nuser interface which allows these modules to be connected\ntogether into a flow chart. The compatibility of the interfaces is checked automatically. Input data is processed sequentially by the modules and usually an image is produced.\nAVS thus implements a data-flow paradigm of scientific visualization. However, the developers of new AVS modules\nhave an involved procedure to follow. OpenDX (Open Data\neXplorer) is an OpenSource variant of AVS with similar features. It comes with hundreds of built-in specialized functions. One can contribute with new components by following an API. The new component will become available after recompiling the application. Outside data visualization\ncommercial solutions dominate the market. Simulink is a\nsoftware package for modeling, simulating, and analyzing\ndynamic systems. It provides a GUI for building models\nas block diagrams, using drag-and-drop mouse operations.\nOne can also customize and create ones own blocks. LabVIEW is a “program development application”. It provides\na graphical programming language, as opposed to a textbased language, used to create programs in a block diagram\nform for data acquisition and presentation. IRIS Explorer\nand the associated numerical and visualization libraries is\nanother attempt for a tool to increase productivity.\nIn the realm of open-source visual programming there\nhas been much less progress, but that is mainly due to the\nfact that the attempts are rather new. Based on a language\ninteroperability tool called Babel, a couple of projects have\nbeen started, each producing, for the moment, a rudimentary GUI. SciRun2 is a more mature computational framework that recently has turned to Babel to increase its connectivity [7].\nIn spite of the availability of the above and other high\nquality tools a solution that would deeply penetrate the community is still lacking. The reason is either being of limited\nscope, e.g. only visualization, or technically too challenging for component authors with natural sciences or engineering background, or because of not supporting the traditional ways of coding, or too expensive, or not supported by\nthe majority, or all the above. COMODI is trying to fill in\nthe above gaps.\n\n• the learning curve for exploiting the capabilities of the\nsystem should be smooth: The interdependence of the\naccessible features and the level of expertise required\nfor using them should be free of abrupt thresholds. A\ncareful design of default behaviors and wizards is essential;\n• it has to offer all the benefits of high level programming yet should keep low-level control at easy reach:\nThe user should be able to set the level of abstraction\nthat is wanted. Visualizing connectors, breakpoints\nand other “administrative” components should be possible but optional;\n• self-explanatory and easy to use: the inspection of\ncomponents’ interfaces should not require more than\na couple of clicks. Tooltips, status bar texts and alike\nfor the documentation of the component, its ports, and\nthe ports’ parameters should be ubiquitous;\n• highly-customizable: components should look similar\nenough so that using them would always be obvious\nbut allow for customization both by the author of the\ncomponent and the user. Similarly to controlling fonts\nin a browser, users should be able to override certain\nauthor settings. Many will prefer to see features similar to existing VP environments.\nWe consider that sequentiality, an idea that is strongly\nimprinted into scientific software developers’ vision, must\nalso be suggestively represented by the relative arrangement\nof components in a project graph and the location of ports\non a component. In the early stages of the maturing process\nof COMODI, the programming interfaces of components\nwill likely be designed at will, without respecting any standards. Hence, during the assembling process the user will\nhave to manually match provides and uses ports, and check\nthe compatibility of the connected ports at the level of each\nargument. Therefore, the inspection of all ports and parameters therein will have to be assisted by GUI elements made\nas handy as possible.\n2\n\n\fIn perspective, we expect that a reuse oriented community will enforce standards which not only that will speed\nup considerably the manual assembling process but will\nopen the way to automatic interface matching (search →\n(down)load → paste → configure) managed by AI modules.\n\npertaining to a component. The window can be accessed by\ndouble-clicking the component in the project graph.\n\n4 Representation of components and ports\nFor the first release of COMODI, components are regarded as the lowest granularity units of code, namely, functions and procedures. On the left-hand side of a component\nwe have its provide ports while on the right its uses or call\nports (see figure 1). We shall term the two as the provide and\nuses side, respectively. Data flows in/out via the black/white\nports. They represent entry and exit points of regular function calls. Provide ports are connection points to the parent\ncomponent located one level higher in the call tree. Conversely, uses ports connect the component to its children,\none level deeper in the call hierarchy. This representation\nis fairly intuitive as it maps almost directly to the source\ncode wherein the body of a function contains lines with the\ninvocation of other functions.\n\nFigure 2. Graphical user interface for inspecting and setting component properties\nThe Uses ports section displays the names of the ports as\nreferred to by the author, leaving the actual identity of the\nchild components to the user to decide. Double clicking one\nof the uses ports from the list would lead to another information window dedicated to that particular port, as shown\nin figure 3. Direct click on a uses port in the project graph\nopens up this very same window. Future versions will allow linking of complex data types in the parameter lists to\ncorresponding documentation windows.\n\nFigure 1. Component classification\nAs a general rule the diversification of component representation should be fully supported in order to assure\nguides for the eye, that is, a better overview of large project\ngraphs that extend over several screens manageable only\nvia scrolling and zooming. The difference in the components’ functionalities should be reflected in their appearance. Therefore we suggest that all uses ports should be\ndisplayed. Many components will still seem alike but this\nsimple rule will confer certain variety to the outlook of components. Statistically speaking, there will be a correlation\nbetween the weight of a component in terms of offered services and its geometric size. In perspective, the recommended solution is a skin repository referenced by components and downloaded by the framework upon request.\nIn order to manage connections the user should be provided with interfaces that make the inspection of all component, ports and parameter properties straightforward. Figure\n2 presents an interface containing most relevant information\n\n4.1 Mandatory and optional ports.\nIn the reuse oriented future, components should not simply expose some “dangling bonds” or, on the contrary, be\ncompletely autonomous. They should allow for as many\nuses ports as possible but in the same time provide from\nwithin the deployed package a default connection for all\nthese ports. For instance, a molecular dynamics component should allow the user to re-define any of the twoand three-body interaction functions between chemical elements. On the other hand, a component handling k-body\ninteractions of n chemical elements would be completely\nuseless if n!/(n−k)!k! different functions would need to be\n3\n\n\fFigure 3. Graphical user interface for inspecting port properties\n\nwired up before using it. Figure 2 demonstrates a possible\nalternative for handling this issue. For most, ideally all uses\nports a default connection will be defined, i.e., an exported\nor private function within the deployed component. Using\nthe checkboxes the user selects those ports that are to be\nconnected to user-defined external components. Those that\nare unchecked use the default internal references. Inactive\ncheckboxes represent the case for ports that do not have an\ninternal default connection therefore are always displayed\non the right side of the component and user provided connection is mandatory. Another possibility is to use external\ncomponents as default connections within the boundaries of\na composed component.\n\nFigure 4. Recommended representation of a\nproject in the framework COMODI. The components reach execution state starting from\nleft to right, bottom to top. In this case the\norder is: framework (C1), C2, C3, C3.1, C3.2,\nC4, C4.1\n\nHence the order of calls, the way it is suggested in figure\n4, is not strict. It is rather meant to suggest the position of\nthe corresponding call point in the source code of the component. In perspective, the extraction of non-tree elements\n(branches, loops) from inside the components out into the\nflow diagram is conceivable with the help of specialized\nconnector components.\nIn order to allow smoothless transition between GUI versions or even completely different environments – just as\ndifferent windows managers can be plugged into the same\nX Windows system – COMODI should come with a basic\nDTD that describes the representation independent state of\nthe system, e.g., topology of the project, execution state, etc.\nAll GUIs come with their own DTDs that are extensions of\nthe representation independent one. In case a GUI encounters a project or a component with an incomprehensible descriptor file, relying on the services of the framework, it will\nfind the “greatest common denominator” in the DTD inheritance hierarchy available online, transforms the descriptor\nfile accordingly and then proceed with the rendering.\n\n5 Representation of a project\nA project is a graph of interconnected components, as\nshown in figure 4. Connection means data flow via function interfaces from the output of one component to the input of another [8]. We can identify two different types of\ncommunication: a horizontal client-server type of a communication entailing a change in the call depth and a vertical pipe&filter type entailing data transfer between siblings.\nC3 – C3.2, exemplifying a client-server communication is\nthe direct representation of the pull model for module communication. It consists in a function calling another, which\nreturns data to the former. This is basically the only model\nfor communication in low-level languages such as C or Fortran. On the other hand, vertical calls (C3.1 – C3.2) imply the piping of an output into the input ports of another\ncomponent. This push model, is not directly supported by\nlow-level languages. However, as we shall show below it\ncan be transformed into equivalent client-server communication. Disregarding this aspect the structure is tree-like.\nAt this stage, branching and loops are concepts that are unknown to the project. All decisions pertaining to the order\nof child calls are made runtime and inside the components.\n\n6 Connectors\nConnectors are simple components mediating dataflow\nbetween “real” components. This indirection can be due to\nreasons such as the necessity for satisfying certain syntactical requirements of component wiring, splitting up outgoing\ndataflow and sharing it between child components, merging\n4\n\n\fincoming dataflow, broadcasting the same data to several\ncomponents, etc. For a review on connectors see [9] and ch.\n10 in [10].\n\n[2] COMODI homepage, http://phys.ubbcluj.ro/comodi/\n[3] D.E. Post, The Coming Crisis in Computational Science, Proceedings of the IEEE International Conference on High Performance Computer Architecture:\nWorkshop on Productivity and Performance in HighEnd Computing, Madrid, Spain, February 14, 2004;\nD.E. Post and L.G. Votta, Computational Science Demands a New Paradigm, Phys. Today, January 2005,\np.35\n[4] Zs.I. Lázár, J.R. Heringa, B. Pârv, and S.W. de\nLeeuw, COMODI: Component Based Programming\nin Scientific Computing: A Practical Approach,\narXiv:cs.CE/0509002\n\nFigure 5. Translating between pipe&filter and\nclient-server communication via a connector.\nHere we only want to single out one particular type of\nconnector. Above we noted that pipe&filter communication\nis an abstraction that can be realized by transforming it into\ntwo client-server calls via a connector. Figure 5a and 5b\nillustrates this transformation. As an additional feature the\nGUI can display the connector for those who are keen of\nseeing a perfect tree hierarchy or hide it if preferred. While\nthis connector can be managed automatically, most of the\nothers will need specialized user interfaces for manual configuration.\n\n[5] Zs.I. Lázár, B. Pârv, J.R. Heringa, S.W. de Leeuw,\nCOMODI: Guidelines for a Component Based Framework for Scientific Computing, Studia Babeş-Bolyai,\nSeries Informatica, Vol. XLIX, No. 2 (2004) 91\n[6] AVS homepage, http://www.avs.com/index wf.html;\nOpen Data Explorer homepage, http://www.opendx.org/; Simulink homepage, http://eurosim.tuwien.ac.at/simulink/simulink.html; LabVIEW homepage,\nhttp://home.ict.nl/∼labview/nl-index.html\n[7] Babel homepage, http://www.llnl.gov/CASC/components/babel.html;\nCCAFE homepage, http://www.cca-forum.org/ baallan/ccafe; XCAT homepage, http://www.extreme.indiana.edu/xcat; SCIRun homepage, http://www.sci.utah.edu\n\n7 Conclusions and outlook\nIn spite of a great deal of similarities with existing visual programming environments, the special scope of the\nCOMODI project endows the user framework’s GUI with\nfeatures that are not common in other solutions. The suggested graphical elements and behavior are intuitively in\nharmony with the low-level programming paradigm. Once\nreleased to the public domain, a fast development of the\nGUI is expected. Parallel to the development of the above\npresented GUI several others are expected to emerge targeting restricted groups of computational scientists. Therefore,\nduring the design process a special emphasis should be laid\non defining proper interlayer communication interfaces.\n\n[8] Zs.I. Lázár, B. Pârv, COMODI: Component Wiring in\na Framework for Scientific Computing, Studia BabeşBolyai, Series Informatica, Vol. XLIX, No. 2 (2004)\n103\n[9] A. Mayer, S. McGough, M. Gulamali, L. Young, J.\nStanton, S. Newhouse, J. Darlington, Meaning and\nBehaviour in Grid Oriented Components, Proceedings\nof the Third International Workshop on Grid Computing, Springer-Verlag (2002)\n[10] C. Szyperski, D. Gruntz and S. Murer, Component\nSoftware; Beyond Object Oriented Programming, 2nd\nedition, Addison-Wesley (2002)\n\nAcknowledgment\nThis work is supported by the National University Research Council of Romania with grant no.\n27687/14.03.2005.\n\nReferences\n[1] Zs.I. Lázár, L.I. Kovács, B. Pârv, COMODI: Architecture for a Component-Based Scientific Computing\nSystem, arXiv:cs/0509003\n5\n\n\f"
        ],
        [
         "38",
         "38",
         "cs.CE",
         "Computational Engineering",
         "1109.3488v2.pdf",
         "Using MOEAs to Outperform Stock Benchmarks in the Presence of Typical\nInvestment Constraints\nAndrew Clark1 and Jeff Kenyon2, Thomson Reuters\n\nAbstract\nPortfolio managers are typically constrained by turnover limits, minimum and maximum stock\npositions, cardinality, a target market capitalization and sometimes the need to hew to a style\n(such as growth or value). In addition, portfolio managers often use multifactor stock models to\nchoose stocks based upon their respective fundamental data.\n\nWe use multiobjective evolutionary algorithms (MOEAs) to satisfy the above real-world\nconstraints. The portfolios generated consistently outperform typical performance benchmarks\nand have statistically significant asset selection.\n\nIn finance, a portfolio is a collection of assets held by an institution or a private individual. The\nportfolio selection problem seeks the optimal way to distribute a given monetary budget on a set\nof available assets. The problem usually has two criteria: expected return to be maximized and\nrisk to be minimized. Classical mean-variance portfolio selection aims at simultaneously\nmaximizing the expected return of the portfolio and minimizing portfolio risk. In the case of\nlinear equality and inequality constraints, the problem can be solved efficiently by quadratic\nprogramming, i.e., variants of Markowitz‟s critical line algorithm. What complicates this simple\n1\n2\n\nAndrew Clark, Chief Index Strategist, Thomson Reuters Indices & Lipper, andrew.clark@thomsonreuters.com\nJeff Kenyon, Lead Software Engineer, Thomson Reuters Indices, jeff.kenyon@thomsonreuters.com\n\n1\n\n\fstatement of portfolio construction are the typical real-world constraints that are by definition\nnon-convex, e.g. cardinality constraints which limits the number of assets in a portfolio and\nminimum and maximum buy-in thresholds. In what follows, we use multi-objective\nevolutionary algorithms (MOEAs)3 as an active set algorithm optimized for portfolio selection.\nThe MOEAs generate the set of all feasible portfolios (those portfolios meeting the constraints),\ncalculates the efficient frontier for each and also their respective Sharpe ratio. The portfolio\nwith the best Sharpe ratio becomes the portfolio used for the next time period. We chose\nMOEAs to solve a non convex optimization problem because there are certain outstanding\nproblems in terms of their use: 1) In the literature MOEAs have not been used to solve multiperiod financial problems (or multi-period problems in general), 2) The number and types of\nconstraints in a real world financial portfolio problem exceeds what has been done with MOEAs\nso far and 3) It is not known if MOEA stock selection is statistically significant. We answer all\nof these questions with a yes thereby advancing the understanding and use of MOEAs. This\nespecially true when it comes to solving a moderately difficult, multi-period real world problem\nsuch as those encountered in finance.\n\nA multi-objective optimization problem (MOP) differs from a single objective optimization\nproblem because it contains several objectives that require optimization. When optimizing a\nsingle objective problem, the best single design solution is the goal. But for multiobjective\nproblem with several (possibly conflicting) objectives, there is usually no single optimal\nsolution. Because of this the decision maker is required to select a solution from a finite set of\npossible solutions by making compromises. A suitable solution should provide acceptable\n\n3\n\nSee Coello et al Evolutionary Algorithms for Solving Multi-Objective Problems, Kluwer, 2002, for a good\nintroduction to MOEAs.\n\n2\n\n\fperformance over all objectives. The main motivation for using evolutionary algorithms (EAs) to\nsolve multi-objective optimization problems is that EAs can deal simultaneously with a set of\npossible solutions which allows us to find several members of what is called the Pareto optimal\nset4 in a single run of the algorithm. This differs from deterministic mathematical programming\ntechniques where a series of separate runs is required. Additionally EAs are less susceptible to\nthe shape or continuity of the Pareto front, e.g., they can easily deal with discontinuous and\nconcave Pareto fronts. Discontinuity and concavity problems are known obstacles for\ndeterministic mathematical programming.\n\nAdapting any stochastic optimization algorithm (such as an EA) so it can perform a\nmultiobjective optimization requires a change to the method of archiving possible solutions. Any\nsolution on the Pareto front can be identified formally by the fact that it is not dominated by any\nother possible solution. A solution X is said to be dominated by solution Y if Y is at least as good\non all counts (constraints) and better on at least one constraint. Stated mathematically:\n\nfi (Y )\n\nfi ( X ) i 1, M and fi (Y )\n\nf i ( X ) for some i\n\nAs several possible solutions can be generated, an archive of the non-dominated (Pareto optimal)\nsolutions needs to be maintained. A possible archiving scheme is:\n\nAll feasible solutions (Pareto optimal vectors) generated are candidates for archiving\n\n4\n\nA solution or a set is considered Pareto optimal if there exists no feasible solution which would decrease some\nconstraint without causing a simultaneous increase in at least one other constraint.\n\n3\n\n\fIf a candidate solution dominates any existing members of the archive, the dominated\nsolutions are removed\nIf the new solution is dominated by any existing member of the archive, the new solution\nis not archived\nIf the new solution neither dominates nor is dominated by any members of the archive,\nthe new solution is added to the archive\nUsing such a scheme, as the search progresses, the archive will converge to the true trade-off\nsurface between constraints.\n\nAs to the EA part of MOEA, a generic EA assumes a discrete search space H and a function\nf :H\n\nwhere H is a subset of the Euclidean space\n(in a multiobjective problem H is a subset of the\n\nEuclidean space\n\nM\n\nwhere M is the number of constraints).\n\nThe general problem is to find\n\narg min f\nX H\n\nwhere X is a vector of the decision variables and f is the objective function.\n\nWith EAs it is customary to distinguish genotype – the encoded representation of the variables\nfrom phenotype – the set of variables themselves. The vector X is represented by a string (or\nchromosome) s of length l made up of symbols drawn from an alphabet A using the mapping\nc : Al\n\n4\n\nH\n\n\fIf the domain of c is total, i.e. the domain of c is all of Al, c is called a decoding function. The\nmapping c is not necessarily surjective. The range of c determines the subset of Al available for\nexploration by an evolutionary algorithm.\n\nThe range of c, Ξ\n\nAl\nis needed in order to account for the fact that some strings in the image Al under c may represent\ninvalid solutions to the original problem.\n\nThe search space Ξ can be determined by either Shannon or 2nd order Renyi entropy. If the\ndecision variables X are independent, Shannon entropy applies. If the decision variables are\ncorrelated then 2nd order Renyi entropy applies. A minimization of either entropy will help\ndefine the feasible search space Ξ.\n\nThe string length l depends on the dimensions of both H and A with the elements of the string\ncorresponding to genes and values to alleles. This statement of genes and alleles is often\nreferred to as genotype-phenotype mapping.\n\nGiven the statements above, the optimization becomes:\narg min g\nS L\n\ngiven the function\ng ( s)\n\nf (c(s))\n\n5\n\n\fFinally, in EAs it is helpful if c is a bijection. The important property of a bijection as it applies\nto EAs is that bijections have an inverse, i.e. there is unique vector x for every string and a\nunique string for each x.\n\nEAs involve techniques that implement mechanisms inspired by evolution such as reproduction,\nmutation, recombination, selection and survival of the fittest. Candidate solutions to the\noptimization problem play the role of individuals in a population and the objective function\ndetermines the environment within which the solutions \"live.” Evolution of the population then\ntakes place after the repeated application of the above operators5.\n\nIn this process, there are two main forces that form the basis of EAs: recombination and mutation\nwhich create the necessary diversity and thereby facilitate novelty. Selection acts as a force\nincreasing quality.\n\nMany aspects of EAs are stochastic. Changed pieces of information due to recombination and\nmutation are randomly chosen. On the other hand, selection operators can be either deterministic,\nor stochastic. In the latter case, individuals with a higher fitness have a higher chance to be\nselected than individuals with a lower fitness but typically even weak individuals have a chance\nto become a parent or to survive.\n\nChoosing the numerical values or techniques that will compute/simulate mutation, recombination\nand so forth tends to be heuristic. In the MOEA descriptions below, we use standard procedures\nand values to set the evolutionary parameters. For those interested in reading more about the\nvarious values and techniques that are used to select evolutionary parameters, the authors suggest\n\n5\n\nThe mathematical development of EA operators and functions is contained in the end note of this paper.\n\n6\n\n\fK. Deb, Multi-Objective Optimization using Evolutionary Algorithms, John Wiley and Sons,\n2000.\n\nThe two optimization problems we face are: generate a series of monthly portfolios that\noutperform the S&P 500 over the last 30 years and generate a set of monthly portfolios that\noutperform the Russell 1000 Growth index over the last 15 years.\n\nThe constraints we will operate under to attain our goals are: turnover is not to exceed 8% per\nmonth, the minimum stock position is set at 0.35% of the net asset value of the portfolio, the\nmaximum stock position is set at 4% of the net asset value of the portfolio6 and a target market\ncapitalization constraint where the average market capitalization of the portfolio must be greater\nthan the average market capitalization of all stocks available to purchase in the current month\n(this last constraint will mean both portfolio sets will be what is called “large-cap” The S&P 500\nand the Russell 1000 Growth are large-cap benchmarks). Another constraint common to both\nproblems is we must choose stocks that maximize the scores generated by a multi-factor stock\nmodel7. This constraint typifies the use of what is called fundamental financial data to select\nstocks that are potential candidates for the final monthly portfolios.\n\nAn additional constraint was added just for the Russell 1000 Growth problem: we cannot exceed\nthe average book-to-price value of all stocks available for purchase in the current month.\n\n6\n\nThe minimum and maximum position constraints also imply a cardinality constraint. Dividing 100% by 0.35%\nyields 286 which is the maximum number of stocks any portfolio can have. Dividing 100% by 4% gives 25 which\nthe minimum number of stocks any portfolio can have. The cardinality constraint is solved in the first MOEA while\nthe position constraints are solved in the second MOEA.\n7\nThe details and back testing of the multi-factor stock model can be found in Chapter 3 of Handbook of Portfolio\nConstruction, edited by John Guerard. We thank John for supplying the multi-factor scores and all the other data\nused in these models.\n\n7\n\n\fMeeting this constraint will mean we will generate the required growth portfolios for the Russell\n1000 Growth pool.\n\nWe solve the issues of the constraints by breaking them into two sets and use two MOEAs. The\nfirst MOEA generates potential portfolios that lie within the bounds of all the constraints except\nturnover and position. In the second MOEA, we trade off the turnover and position constraints\nas well as mean return and variance (the last being the typical factors used in mean-variance\noptimization). We set the rebalance period to quarterly versus monthly but stay within the stated\nturnover constraint (not to exceed 8% per month). The steps each MOEA takes are as follows.\n\nData Loading, Retrieving and Filtering\n1. Load the target candidate constituent sets from the multi-factor score files. “Sets” and\n“Files” because we need to build two sets of candidate portfolios each month, one for the\nlarge-cap portfolio and another for the large-cap growth portfolio (please note that the\nsize of these files goes from approximately 1000 stocks in 1980 to more than 3000 in\n2009).\n2. Remove candidates that may be excluded, on an a priori basis, of being unable to\ncontribute toward the portfolio goals.\na. Remove candidates with scores of less than 20 (this was a heuristic choice)\nb. Rank candidates by market cap and eliminate the bottom 12% of candidates. If the\nmarket cap at this level is greater than US$750M, use US$750M as the floor for\nthe cutoff (this cutoff is based upon common definitions of where small-cap\nstocks start to appear the U.S. stock market).\n\n8\n\n\f3. From this subset of candidate equities, retrieve daily price data going back 287\nobservations (for use in trading off mean and variance in second MOEA), and going\nforward 63 observations (for use in performance calculation of final [best] portfolio over\nthe next three months).\n4. Calculate the average market cap and average book to price score of the constituents in\nthe target benchmark index (average book-to-price is used to determine if a portfolio\nsatisfies the style constraint).\n5. The candidate constituents remaining serve as the primary input to the MOEA phase.\nMOEA Phase I\nIn this phase the goal is to identify a set of portfolios (ideally 50 or less) that will each be\nexamined by the second MOEA. Output of the MOEA for each portfolio is the identification of a\nsubset of the candidates (between 25 and 286) to be used as candidates for later optimization\n(again 25 because of the 4% maximum constraint and 286 because of 0.35% constraint).\n1. The MOEA is invoked, passing the candidate constituent data, portfolio constituents from\nprevious rebalances, the market cap average for the target benchmark index and\npopulation, generations, and mutation rate. For the large-cap growth portfolio, there is an\nadditional parameter for the average book-price score of the target benchmark portfolio.\na. MOEA algorithm is NSGA II, using single point crossover, bit flip mutation, and\nbinary tournament for selection. For the passed parameters, population represents\nthe number of proposed solutions that will be carried from generation to\ngeneration (note that the number of non-dominated solutions is often considerably\nsmaller); generations is the number of generations to be evaluated within the\n\n9\n\n\fevolutionary algorithm; and mutation rate indicates the rate at which a dominated\nportfolio will be modified as it moves from generation to generation.\nb. For the large-cap portfolio, population is 500, generations are 1200, and mutation\nrate is 0.03 (3%).\nc. For large-cap growth portfolio, population is 50 (for the three-objective problem,\na population of 500 tended to generate several hundred non-dominated solutions;\nthe population number is dropped to avoid generating more solutions than can be\nexplored in the MVO phase). All other parameters are the same.\n2. Generation-zero portfolios are started with 156 equities randomly selected. If there are\nportfolios from a previous rebalance available, those portfolios are seeded into the\npopulation.\n3. The large-cap portfolio objectives are the maximization of multi-factor score and average\nmarket cap. The large-cap growth portfolio adds a third objective to minimize the bookto-price score.\n4. Penalties are in place to enforce the cardinality and market cap constraints. For large-cap\ngrowth, there is also a penalty for exceeding the book-price average passed in.\n5. The MOEA phase produces two files as output. One file contains the objective values for\nthe solution set, and the other contains the portfolios (where each equity has a 0/1 value)\ndescribing the Pareto efficient frontier.\n6. Any portfolios passing constraints from the previous rebalance are added to the set of\nportfolios generated by the MOEA\n\n10\n\n\fMOEA Phase II\nIn preparation for running the second MOEA, daily returns are calculated for all equities that\nmay appear in a portfolio (to avoid having to recalculate a daily return series for an equity\nmultiple times), and the daily risk free rate closest to the rebalance date is retrieved (for use in\ncalculating the Sharpe ratio). Then for each portfolio from the previous MOEA, the following\nactions are taken:\n1. Identify the equities designated as candidates for the portfolio being processed, and form\nthe returns matrix. The minimum number of returns to be used is 126 (six months);\nbecause the number of observations must exceed the number of candidate equities (the\nso-called “curse of dimensionality”), the maximum number of returns in the series is\ndetermined by portfolio size, up to 287.\n2. In addition to forming the returns matrix (step 1 above), we also need to generate mean\nexpected returns, variances, and the covariance matrix.\n3. The MOEA is called, using pointers to the above files, a pointer to the file containing the\nprevious winning portfolio, and the MOEA parameters (population = 100, generations =\n600, mutation rate = 0.01).\n4. Following the lead of an earlier replication of MVO results using MOEA, the algorithm\nused is SPEA2. SBXCrossover, polynomial mutation, and selection by binary\ntournament. The SPEA2 archive size is the same as the prior population size.\n5. Random portfolios are used for generation zero. If available, the previous winning\nportfolio is seeded into the generation zero set.\n6. In each evaluation, the randomly assigned weights are normalized (so that they sum to 1).\n\n11\n\n\f7. In weighting strategy #1 (non-zero values are 0.0035 <= w <= 0.04), weights greater than\n0.00175 are rounded up to 0.0035, while weights under that amount are rounded to zero.\nWeights over 0.04 are set to 0.04. All weight adjustments are added to a ledger, and then\ndebited or credited at the end of the adjustment process (evenly divided among those that\ncan accept the debit/credit amount without moving outside the set weight boundary).\n8. In weighting strategy #2 (all equities are weighted, 0.0035 <= w <= 0.04), values less\nthan 0.0035 are rounded up to 0.0035, while values over 0.04 are set to 0.04. All weight\nadjustments are added to a ledger, and then debited or credited at the end of the\nadjustment process (evenly divided among those that can accept the debit/credit amount\nwithout moving outside the set weight boundary).\n9. There are four objectives for the MOEA: maximize return, minimize risk, minimize\nturnover and meet the maximum and minimum holdings constraint. The first two\nobjectives are the standard MVO calculations8. The turnover and position objectives\ncalculate the shift in weight between the previous winning portfolio and the proposed\nportfolio. It attempts to insure that no portfolios break the holdings or turnover bounds.\n10. Once all MVOs have been run, the winning portfolio is then selected. Ideally, this is the\nportfolio with the best Sharpe ratio that has also passed the market cap and turnover\nconstraints. If no portfolios pass the constraints, the portfolio with the best turnover is\nconsidered the winner.\n11. The performance for the winning portfolio (and the target benchmark index) for the\nperiod until the next rebalance is then calculated and logged.\n\n8\n\nSee Investments, Z. Bodie, A. Kane, A. J. Marcus Richard D. Irwin, Inc., Homewood, IL. 1989,\npg. 203, formulas 7.10 and 7.11.\n\n12\n\n\fMOEA IIa\nIf the turnover constraint is not met in Step 9, we use a third MOEA that trades off the best\nSharpe ratio portfolio with its existing stock positions against turnover.\n\nThe form this MOEA takes is somewhat similar to solving the minimum cut problem in graph\ntheory using MOEAs9. The difference between our MOEA and the MOEAs that have been used\nto solve the minimum cut problem is that we are interested in reducing or enlarging the weight of\none or more edges while keeping within the position limit constraints (though we do allow the\nstock holding to go to zero if needed). Once stocks are re-weighted and the turnover constraint\nreached, a new efficient frontier is calculated and the new Sharpe portfolio examined. If the new\nSharpe portfolio meets the turnover and position constraints, the MOEA in IIa stops and the best\nSharpe portfolio for this feasible portfolio from MOEA I is stored. The process is repeated for\nall the portfolios passed from MOEA I that do not meet the turnover constraints after their first\nSharpe portfolio is formed.\n\nOnce turnover constraints are met for the Sharpe portfolios that fail step 9 but pass what maybe\ncalled Step 9a, Steps 10 and 11 are now executed\n\nIn Exhibit 1 are the 1, 3, 5 and 10 year annualized (transaction cost adjusted) returns for the\nlarge-cap MOEA portfolios and the S&P 500. The period covered is from December 1979\nthrough December 2009 (121 months)\n\n9\n\nSee for example “Computing Minimum Cuts by Randomized Search Heuristics,” F. Neumann, et al,\nhttp://arnetminer.org/viewpub.do?pid=228033\n\n13\n\n\fExhibit 1\n1 Year 3 Year 5 Year 10 Year\nS&P 500\n\n9%\n\n30%\n\n54%\n\n137%\n\nMOEA\n\n13%\n\n44%\n\n84%\n\n239%\n\nExhibit 2 has the annualized risk and cumulative return on 10,000 USD for the S&P 500 and the\nMOEA portfolios for the same time period\n\nExhibit 2\nCumulative\nSharpe Ratio Information Ratio Return on 10,000 USD\nS&P 500\n\n1.1\n\nN/A\n\n37,070\n\nMOEA\n\n1.8\n\n0.14\n\n49,500\n\nIn Exhibit 3 are the 1, 3, 5 and 10 year annualized (transaction cost adjusted) returns for the\nlarge-cap growth MOEA portfolios and the Russell Growth 1000. The period covered is from\nDecember 1996 through December 2009 (53 months)\n\nExhibit 3\n1 Year 3 Year 5 Year 10 Year\nR1000 Growth\n\n10%\n\n33%\n\n61%\n\n159%\n\nMOEA\n\n14%\n\n48%\n\n93%\n\n271%\n\n14\n\n\fExhibit 4 has the annualized risk and cumulative return on 10,000 USD for the Russell Growth\n1000 and the MOEA portfolios for the same time period\n\nExhibit 4\nCumulative\nSharpe Ratio Information Ratio Return on 10,000 USD\nR1000 Growth\n\n0.14\n\nN/A\n\n25,688\n\nMOEA\n\n0.24\n\n0.32\n\n31,298\n\nAs seen in Exhibits 1 – 4 the percent return and USD return of the MOEA portfolios is\napproximately double the value of their benchmarks.\n\nOn a risk-adjusted basis, the results are somewhat mixed. The information ratio for the large-cap\ngrowth MOEA is very significant while its Sharpe ratio is only a little larger than the Russell\n1000 Growth Sharpe ratio and both are little different from 0 (zero). For the large-cap MOEA,\nits Sharpe ratio is significantly larger than its benchmark, but its information ratio is very small.\nSo it is not clear that the MOEA portfolios are the better risk-adjusted portfolios in all cases. By\nnot underperforming their benchmarks on a risk-adjusted basis, the implication is that at a\nminimum the MOEAs reside on a higher curve in risk-return space and could be the more\nattractive portfolios to investors.\n\nAs to the other constraints: 1) In 100% of all cases, the MOEA portfolios on a weighted market\ncapitalization basis met or exceeded the market capitalization constraint, 2) For the smallest and\n\n15\n\n\flargest positions constraints based on net asset value, none of the MOEA portfolios broke this\nconstraint on either the minimum or maximum side, 3) Turnover did occasionally exceed the 8%\nlimit per month. These occurrences tended to happen early in the 1980‟s portfolios just as the\nMOEA was getting on its feet. And the turnover limit was broken in a small number of later\nportfolios as well. The authors conjecture that in the latter cases the non-dominated feasible\nsolutions handed off to the second MOEA were composed of individual stocks different enough\nfrom the prior quarter‟s portfolio that all the resulting portfolios prevented the turnover\nconstraint from being met. This is an open question however and needs further investigation.\n\nAs to one of the reasons why we chose to use MOEAs - is the stock selection of the quarterly\nportfolios statistically significant - we find the answer to be yes. As measured by John Guerard10\nthe MOEA asset selection was very significant. This is a very pleasant surprise to the authors,\nespecially as the portfolios typically contained 150 – 200 stocks. Our results demonstrate that\nMOEAs can generate statistically significant asset selection while operating under real world\nconstraints and using fundamentally driven stock scores.\n\nIn this paper we demonstrate that MOEAs in the presence of real world constraints can generate\nportfolios that have higher returns than their benchmarks, comparable (if not better) risk adjusted\nreturns versus their benchmarks and statistically significant asset selection.11\n\n10\n\nPrivate communication\nWe have been asked the question: what are MOEA run times like versus mixed integer programming. We asked a\ncolleague of ours – Todd Morrison – to use the same data supplied to us by John Guerard and the same constraints\nto solve the same optimization problems but by using mixed integer programming. The total time needed to\ngenerate all the portfolios for each problem was less using MOEAs than mixed integer programming. There is an\nimportant caveat however. The major difference in performance occurs before the number of stocks approaches\n3000 or more. At that point MOEAs and mixed integer programming take approximately the same time to form the\nfinal quarterly portfolio. Further, these test results must be taken with a large grain of salt. The tests discussed here\n11\n\n16\n\n\fWe arrive at these portfolios by dividing the MOEA in two: the first MOEA generates all the\nnon-denominated feasible sets that meet all the constraints except turnover and minimum and\nmaximum position. The second MOEA trades off the last two constraints along with mean and\nvariance to come up with the final portfolio that has the best Sharpe ratio (or meets the maximum\nturnover if the other constraints are not met).\n\nAs MOEA I can generate feasible solutions to constrained stock selection problems, it could be\nof help to portfolio managers when deciding which stocks stay in, which move out and which\nare added to her portfolio. The run time of MOEA I, operating under the constraints above and\nin the presence of 3000 or more stocks, is 15 – 30 minutes in order to generate a set of 20 - 30\nfeasible solutions (the run time is less if a smaller solution set is needed). The advantage the\nfeasible solution set gives to the portfolio manager is that at a glance she can see the trade-offs\nbetween possible stock portfolios, all of which meet most (if not in some cases all) the\nconstraints to varying degrees.\n\nFinally, as best as we know, this is the first multi-period use of MOEAs in stock portfolio\nconstruction. We are encouraged by the results and hope others will extend and improve upon\nour work.\n\nare only two of many problems that can be solved using either technique. A test bed needs to be developed before a\nclear statement of equality or superiority can be made.\n\n17\n\n\fAcknowledgements\nThe authors would like to thank John Guerard of McKinley Capital for the challenge he set us.\nWe would not have tested MOEAs in the way described above without John’s challenge.\n\nENDNOTE\nThe „gory” details of EAs such as the hows and whys of mutation are not described above. What\nfollows are those details.\n\nFirst we will define the EA fitness function. As H is nonempty set and c : Al\nf :H\n\n , we can define the fitness scaling function Ts : \n\nH and\n\n and a related fitness function\n\n Ts  f  c .\n\nIn this definition it is understood that the objective function f is determined by the application,\nwhile the specification of the decoding function c12 and the fitness scaling function Ts are design\nissues13.\n\nExecution of an EA typically begins by randomly sampling with replacement from Al. The\nresulting collection is the initial population denoted P. More generally a population is a\ncollection P = {a1 ,…,aμ } of individuals ai\n\nAl . The number of individuals μ is referred to as\n\nthe population size.\n12\n\nRemember that if the domain of c is total, i.e. the domain of c is all of A I, c is called a decoding function. The\nmapping c is not necessarily surjective. The range of c determines the subset of Al available for exploration by the\nevolutionary algorithm.\n\n18\n\n\fFollowing initialization, execution proceeds iteratively. Each iteration consists of an application\nof one or more evolutionary operators. The combined effect of the evolutionary operators\napplied in a particular generation t\n\nN is to transform the current population P(t) into a new\n\npopulation P(t+1).\n\nIn the population transformation,\n\n (the parent and offspring population sizes\n\n,\n\nrespectively). A mapping T : H\n\nH\n\nis called a population transformation. If T ( P)\n\nthen P is a parent population and P/ is the offspring population. If\n\nP\n\nthen they are called\n\nsimply the population size.\n\nThe population transform (PT) resulting from an evolutionary operatory (EO) often depends on\nthe outcome of a random experiment. In Merkle and Lamont 14 this result is referred to as a\nrandom population transform (RPT) or random PT\n\n and\n\nTo define RPT, let\n\nR:\n\nT (H ,\n\n\n\nbe a set (the sample space). A random function\n\nH ) is called a random population transformation. The distribution of\n\n\n\nPTs resulting from the application of an EO depends on the operator parameters, in other words\nan EO maps its parameters to a RPT.\n\n14\n\nL.D. Merkle and G.B. Lamont, “A Random Function Based Framework for Evolutionary Algorithms”, in\nProceedings of the Seventh International Conference on Genetic Algorithms, Morgan Kauffman, San Mateo, CA.,\n1997\n\n19\n\n\fNow that we have defined both the fitness function and RPT, we can define in general an\nevolutionary operator: let\n\n , X be a set (the parameter space) and\n\n:X\n\nT\n\n,T H ,\n\na set. The mapping –\n\nH\n\n\nis an evolutionary operator. The set of evolutionary operators is denoted as EVOP H , , X ,\n\n.\n\nThere are three common evolutionary operators: recombination, mutation and selection. These\nthree operators are roughly analogous to their similarly named counterparts in genetics. The\napplication of them in EAs strictly follows Darwin - survival of the fittest.\n\nIn Merkle and Lamont‟s definition of the recombination operator r\nthere exists P\n\nH ,\n\nX and\n\nEVOP H , , X ,\n\nIf\n\nsuch that one individual in the offspring population r P\n\ndepends on more than individual of P then r is referred to as a recombination operator.\n\nA mutation is defined in the following manner. Let m EVOP H , , X ,\n, for every\n\nX and for every\n\n. If for every P H\n\nand if each individual in the offspring population m\n\ndepends on at most one individual of P then m is called a mutation operator.\n\n20\n\nP\n\n\fFinally for selection let s\n\nEVOP H , , X T H , ),\n\ncases and if s satisfies a s\n\n,\n\n( P)\n\n. If P H ,\n\na P then s is a selection operator.\n\n21\n\nX,\n\n:H\n\n in all\n\n\f"
        ],
        [
         "39",
         "39",
         "cs.CE",
         "Computational Engineering",
         "0611044v1.pdf",
         "УДК 004.9:66.013.512\nВ.В. Мигунов, Р.Р. Кафиятуллов\nЗАЩИТА ИНФОРМАЦИИ В КОМПЛЕКСНОЙ САПР\nРЕКОНСТРУКЦИИ ПРОМЫШЛЕННЫХ ПРЕДПРИЯТИЙ\nВведение. По мере перехода к электронному представлению информации,\nкоторое делает возможным ее хранение и транспортировку на все более\nкомпактных носителях и передачу со все большими скоростями по каналам\nсвязи, задача защиты информации (ЗИ) в электронном виде приобретает\nособую актуальность. Возникающие здесь проблемы и методы очень\nразнообразны: от законодательных, административных и организационных до\nтаких аспектов технической защиты, как предотвращение внедрения программвирусов или физическая защита помещений (п. 5.1.3. [1]).\nТеория и практика ЗИ в нашей стране развиваются как путем\nраспространения глубоко разработанных методов защиты сведений,\nсоставляющих государственную тайну, на конфиденциальные сведения [1], так\nи на основе относительно новых разработок в сфере защиты коммерческой\nтайны [2, 3]. При этом в первую очередь рассматривается ЗИ от умышленных\nдействий. Например, в п. 5.1.2. [1] говорится: \"Основными направлениями\nзащиты информации являются: обеспечение защиты информации от хищения,\nутраты, утечки, уничтожения, искажения и подделки за счет НСД и\nспециальных воздействий; обеспечение защиты информации от утечки по\nтехническим каналам при ее обработке, хранении и передаче по каналам связи\"\n(НСД - несанкционированный доступ). В [3]: \"В настоящей работе предпринята\nпопытка возможно более полного охвата угроз безопасности субъектов\nинформационных отношений. Однако следует иметь в виду, что научнотехнический прогресс может привести к появлению принципиально новых\nвидов угроз и что изощренный ум злоумышленника способен придумать новые\nспособы преодоления систем безопасности, НСД к данным и дезорганизации\nработы информационных систем\". Лишь во вторую очередь принято\nрассматривать угрозы от неумышленных действия. В частности, для САПР эти\nугрозы и методы борьбы с ними изучены мало.\nВ противоположность подходам к ЗИ, акцентированным на\nпротиводействии злому умыслу, настоящая работа посвящена изучению угроз\nинформации, возникающих вследствие непредумышленных действий\nпользователей САПР, и методов ЗИ,\nограниченных сферой влияния\nпрограммного обеспечения САПР. В основе рассмотрения лежит практический\nопыт развития комплексной САПР реконструкции предприятий TechnoCAD\nGlassX, эксплуатируемой в проектно-конструкторском подразделении крупного\nпредприятия химической промышленности [4] в течение 10 лет. Из\nособенностей такой САПР, охарактеризованных в [5], существенной для ЗИ\n\n\fявилась комплексность. Отметим, что именно непредумышленные действия\nявляются основными угрозами информации. По сведениям из [2]: \"... в 1998\nгоду.\nОсновные\nпричины\nповреждений\nэлектронной\nинформации\nраспределились следующим образом: неумышленная ошибка человека - 52%\nслучаев, умышленные действия человека - 10% случаев, отказ техники - 10%\nслучаев, повреждения в результате пожара - 15% случаев, повреждения водой 10% случаев\". В [3] приводятся\nданные, относящиеся к более позднему\nвремени: \"Согласно статистики, 65% потерь - следствие непреднамеренных\nошибок. Пожары и наводнения можно считать пустяками по сравнению с\nбезграмотностью и расхлябанностью\".\nУгрозы информации в сфере влияния САПР. В первую очередь\nбеспокоящие проектировщика угрозы информации в САПР – это угрозы порчи\nили утраты электронных документов (файлов с чертежами), приводящие к\nнеобходимости повторно выполнять уже сделанную работу. По опыту\nэксплуатации САПР выделены 4 угрозы, которые в первую очередь отмечаются\nпользователями:\n1. Случайное удаление, изменение элементов чертежа.\n2. Утеря результатов работы за период с последнего сохранения чертежа на\nдиск вследствие отказов: сети электроснабжения, узлов компьютера,\nоперационной системы и самой САПР.\n3. Случайная подмена чертежа другим с тем же именем файла. Для\nидентификации чертежей проектировщики часто употребляют номер\nпроектируемого корпуса. При этом монтажный чертеж трубопровода и схема\nавтоматизации, созданные на разных рабочих местах, могут иметь одно и то же\nимя файла. При передаче чертежей между рабочими местами возникает угроза\nнеумышленной замены чертежа.\n4. Утрата уверенности в целостности чертежа, возникающая при\nотсутствии системы ответственного электронного архивирования (хранения)\nфайлов чертежей. Чертеж может измениться в результате неосторожных\nдействий при его просмотре или печати, при использовании его как заготовки\nдля создания другого чертежа.\nРеализация ЗИ в САПР от этих угроз далее описана соответственно как\n\"Откат\", \"Автосохранение\", \"Автоматическое резервное копирование\" и\n\"Электронная подпись\".\nОткат. Откат, или возврат изменений (команды \"Отменить\" и \"Вернуть\") широко применяемый метод борьбы с 1й угрозой. Его теория глубоко развита\nдля баз данных [6] как работа с транзакциями. В условиях комплексной САПР\nвозникает специфика, связанная с тем, что при выходе на довольно\nпродолжительное время в режим работы в специализированном проблемноориентированном расширении [7] пользователь занимается параметрическим\nпроектированием, не имеет связи с чертежом и накапливает информацию не в\n\n\fнем, а в создаваемом параметрическом представлении (ПП). Примеры таких\nрасширений: проектирование профилей наружных сетей водоснабжения и\nканализации, молниезащиты зданий и сооружений. Функции отката рассмотрим\nсначала для чертежа, затем для ПП его частей.\nВсе операции разработки чертежа удается свести к двум первичным,\nатомарным - добавление и удаление элемента чертежа. Изменение типа линии,\nслоя, цвета и др. рассматриваются как удаление и последующее добавление в\nчертеж измененного элемента. Операции над множеством элементов (сдвиг,\nрастяжение и др.) моделируются как множество одинаковых операций, каждая\nиз которых выполняется над одним элементом. Вводится понятие \"шаг\nизменения\", которое включает все операции, проведенные пользователем над\nоднократно выбранными элементами. Тем самым достигается соответствие\nинтуитивного представления об \"изменении\" и количества возвращаемых за\nодин шаг атомарных изменений - аналог транзакций в базах данных. На рис.1\nкаждый шаг обозначен параллелограммом, \"стабильные\" состояния чертежа флажками, а стрелки показывают направление изменений. Последовательность\nшагов при возврате изменений существенна, иначе: может потребоваться\nудаление элементов, которых в чертеже нет; пользователь не будет знать, какое\nизменение возвратится в первую очередь. Учитываются изменения чертежа в\nпоследнем сеансе работы с ним, возврат возможен до завершения сеанса.\n\nРис.1. Модель пошаговых изменений - транзакций\nВ рабочий файл последовательно записываются удаляемые и добавляемые\nгеометрические элементы вместе с признаком - добавлен или удален элемент. В\nрезультате, как показано на рис.2, один шаг представляется в виде сцепленных\nв железнодорожный состав различных геометрических элементов, образующих\nкак бы сумму атомарных изменений. Внутри одного шага последовательность\nэлементов несущественна, если все элементы разные. Если же для удобства\nпрограммирования приходится в один шаг включать, например, удаление и\nпоследующее восстановление одного и того же элемента, то порядок\nследования атомарных изменений при возврате должен быть строго обратным.\n\nРис.2. Общая структура одного шага изменения чертежа\nДля возврата изменения делается проход по файлу, пока номер изменения\nсовпадает с нужным. Для каждого из найденных элементов с этим номером\n\n\fизменения производится следующее: если он был удален, он добавляется в\nчертеж; если он был добавлен, то в чертеже ищется совпадающий с ним и\nудаляется из чертежа. Применяя команды \"Отменить\" и \"Вернуть\", можно как\nугодно долго перебирать состояния чертежа в текущем сеансе работы.\nПП части чертежа есть комплекс ее свойств, или информационная модель.\nВидимая геометрическая часть генерируется по ПП. С точки зрения отката\nнаиболее существенное отличие работы с ПП от работы непосредственно с\nчертежом заключается в интеллектуальных связях элементов друг с другом.\nПри удалении одной трубы из ПП аксонометрической схемы трубопроводной\nсистемы автоматически удаляются все нанесенные на нее обозначения\nтрубопроводной арматуры, элементов трубопроводов, тексты и сноски,\nуказывающие на трубу и удаленные обозначения, выносные линии размеров.\nВ этих условиях система отката не имеет явно очерченных границ того, что\nже изменяется за один шаг. Верхней границей является ПП полностью. В\nрамках модульной технологии разработки расширений САПР все ПП\nпреобразуются к единой компактной форме, задаваемой адресом в памяти и\nдлиной [7], поэтому заменяемым объектом выбрано все ПП. Это решение\nуниверсально для различных расширений САПР.\nПри любом изменении новая версия ПП записывается в рабочий файл.\nОперации над множеством здесь возможны, но их не надо выражать в\nмножестве актов удаления и добавления, каждому шагу изменений\nсоответствует одно ПП. Отмена изменений и возврат реализуются путем\nперехода к предыдущей или к следующей версии ПП с полной его заменой.\nСитуации соответствует рис.1 без детализации каждого шага по рис.2.\nПоследовательность шагов изменения роли не играет, каждая записанная\nверсия ПП дает полный комплект всех сведений для генерации изображения.\nВозможен скачок через несколько шагов.\nАвтосохранение. Для борьбы с угрозой 2 применяется широко\nраспространенный метод автоматического сохранения информации через\nзаданный промежуток времени - автосохранение. Специфика комплексной\nСАПР приводит к необходимости автосохранения не только чертежа, но и\nтекущих параметрических представлений. С интервалом не менее заданного на\nдиске автоматически создаются копии чертежа и ПП текущей задачи,\nпредыдущая копия с диска удаляется. При нормальном завершении задачи или\nработы с чертежом автоматически созданные копии удаляются. В случае\nнекорректного\nзавершения\nработы\n(например,\nиз-за\nотключения\nэлектропитания) эти копии остаются, и при следующей загрузке пользователю\nбудет предложено восстановить чертеж и ПП задачи.\nАвтоматическое резервное копирование. Защита от случайной подмены\nфайла чертежа обеспечивается путем автоматического резервного копирования\nвсех файлов, подвергшихся изменению за период с предыдущего копирования,\n\n\fв отдельный каталог диска с именем, определяемым датой создания резервных\nкопий. Пользователь назначает дни недели, в которые следует проводить\nавтоматическое резервное копирование, и при первом запуске САПР в эти дни\nоно осуществляется. Забота об удалении ставших ненужными резервных копий\nлежит на пользователе.\nЭлектронная подпись. Для сохранения и контроля целостности чертежа\n(борьба с угрозой 4) применяется метод электронной подписи. Он состоит в\nтом, что при наличии хотя бы одной подписи запрещено любое изменение\nчертежа. Подписи защищены личными паролями. Наличие подписи на чертеже\nгарантирует совпадение его содержания с тем, что было в момент подписания.\nСнять подпись может любой пользователь, после чего чертеж можно изменять.\nНо при этом будет видно, что чертеж не подписан.\nЗаключение. Описанные четыре угрозы информации и методы ее защиты\nв комплексной САПР реконструкции предприятий являются в значительной\nстепени общими и для других программных средств редактирования\nпользовательских файлов, поскольку состав угроз определен практическими\nпотребностями. Индивидуальные особенности реализация ЗИ продиктованы\nпрежде всего комплексностью САПР, поддерживающей несколько\nинформационных моделей объектов проектирования.\n\nБИБЛИОГРАФИЧЕСКИЙ СПИСОК\n1. Специальные требования и рекомендации по технической защите\nконфиденциальной информации (СТР-К). Решение Коллегии Гостехкомиссии\nРоссии № 7.2/02.03.01г. [Электронный ресурс] – Режим доступа:\nhttp://www.confidentiality.strongdisk.ru, свободный. – Загл. с экрана. – Яз. рус.\n2. Беляев А.В. \"Методы и средства защиты информации\" (курс лекций). – ЧФ\nСПбГТУ, 2000. [Электронный ресурс] – Режим доступа:\nhttp://www.citforum.ru/internet/infsecure/its2000_03.shtml, свободный. – Загл. с\nэкрана. – Яз. рус.\n3. Домарев В.В. Безопасность информационных технологий. Системный\nподход. – Киев: ООО ТИД «Диасофт», 2004. – 992с.\n4. Мигунов В.В. TechnoCAD GlassX − отечественная САПР реконструкции\nпредприятия//САПР и графика, 2004, №№ 4, 5, 6.\n5. Мигунов В.В. Особенности комплексной САПР реконструкции\nпромышленных предприятий//Труды Международных научно-технических\nконференций \"Интеллектуальные системы (IEEE AIS'04)\" и\n\"Интеллектуальные САПР (CAD-2004)\". Научное издание в 3-х томах. – М:\nИз*д-во Физико-математической литературы, 2004, т.2, С.70-75.\n\n\f6. Дейт, К.Дж. Введение в системы баз данных.: Пер с англ. – 6-е изд. – Киев:\nДиалектика, 1998. – 784с.\n7. Мигунов В.В. Модульная технология разработки проблемноориентированных расширений САПР реконструкции предприятия//Известия\nТульского государственного университета. Серия \"Экономика. Управление.\nСтандартизация. Качество\". – Вып. 1. – Тула: Изд-во ТулГУ, 2004. – С.56-66.\n\n\f"
        ],
        [
         "40",
         "40",
         "cs.CE",
         "Computational Engineering",
         "1111.2514v1.pdf",
         "A MORE APPROPRIATE PROTEIN CLASSIFICATION USING\nDATA MINING\n2\n\n1\n\n3\n\nMUHAMMAD MAHBUBUR RAHMAN, ARIF UL ALAM, ABDULLAH-AL-MAMUN,\n4\nTAMNUN E MURSALIN\n1\n\nLecturer, Department of CSE, American International University-Bangladesh,\n2\n4\n\nLecturer, Department of ETE, University of Liberal Arts Bangladesh,\n\nAssoc Prof., Department of CSE, University of Liberal Arts Bangladesh\n\nABSTRACT\nResearch in bioinformatics is a complex phenomenon as it overlaps two knowledge domains, namely,\nbiological and computer sciences. This paper has tried to introduce an efficient data mining approach for\nclassifying proteins into some useful groups by representing them in hierarchy tree structure. There are\nseveral techniques used to classify proteins but most of them had few drawbacks on their grouping. Among\nthem the most efficient grouping technique is used by PSIMAP. Even though PSIMAP (Protein Structural\nInteractome Map) technique was successful to incorporate most of the protein but it fails to classify the\nscale free property proteins. Our technique overcomes this drawback and successfully maps all the protein\nin different groups, including the scale free property proteins failed to group by PSIMAP. Our approach\nselects the six major attributes of protein: a) Structure comparison b) Sequence Comparison c) Connectivity\nd) Cluster Index e) Interactivity f) Taxonomic to group the protein from the databank by generating a\nhierarchal tree structure. The proposed approach calculates the degree (probability) of similarity of each\nprotein newly entered in the system against of existing proteins in the system by using probability theorem\non each six properties of proteins. This function generates probabilistic value for deriving its respective\nweight against that particular property. All probabilistic values generated by six individual functions will be\nadded together to calculate the bond factor. Bond Factor defines how strongly one protein bonds with\nanother protein base on their similarity on six attributes. Finally, in order to group them in hierarchy tree,\nthe aggregated probabilistic value will be compared with the probabilistic value of the protein that resides\nat the root. If there is no root protein (i.e. at the initial state), the first protein will be considered as the root\nand depending on the probabilistic value it can change its relative position. Recursively, at each node, we\nhave applied this technique to calculate the highest probable position for a particular protein in the tree.\nKeywords: Bioinformatics, Protein, Protein Grouping Techniques, PSIMAP, Scale Free Protein.\nresearch group, so far has attempted to classify\nprotein focusing on only one or two above stated\nproperties. As for example, BMC bioinformatics\nresearch group has developed an in silico\nclassification\nsystem\nentitled\nHODOCO\n(Homology modeling, Docking and Classification\nOracle), in which protein Residue Potential\nInteraction Profiles (RPIPS) are used to summarize\nprotein -protein interaction characteristics. This\nsystem applied to a dataset of 64 proteins of the\ndeath domain super family this was used to classify\neach member into its proper subfamily. Two\nclassification methods were attempted, heuristic\nand support vector machine learning. Both methods\nwere tested with a 5-fold cross-validation. The\nheuristic approach yielded a 61% average accuracy,\n\n1. INTRODUCTION:\nClassification of protein based on their\nvarious properties is a crucial issue in different\nfields of biological science. Researches in\npharmacy, biochemistry, genetic engineering even\nin agriculture vastly rely on appropriate protein\ngrouping techniques. Emphasizing the importance\nof protein classification some research groups in\nbioinformatics have initiated their projects with a\nview to deriving appropriate algorithms for protein\nclassification. Protein can be classified based on\ntheir some properties, namely, a) Structure\ncomparison\nb)Sequence\nComparison\nc)\nConnectivity d) Cluster Index e) Interactivity f)\nTaxonomic and age diversity[1]. Individual\n\n1\n\n\fwhile the machine learning approach yielded an\n89% average accuracy. Though this is a good\ntechnique but it concentrates on only proteinprotein interaction property [2].\nWan K. Kim, Dan M. Bolser and Jong H.\nPark [1] had used PSIMAP for large-scale coevolution analysis of protein structural interlogues.\nThey investigated the degree of co-evolution for\nmore than 900 family pairs in a global protein\nstructure interactome map. They have constructed\nPSIMAP by systematic extraction of all protein\ndomain contacts in the web based Protein Data\nBank. Their PSIMAP contained 37387 interacting\ndomain pairs with five or more contacts within 5 A.\nThey have first confirmed that correlated evolution\nis observed extensively throughout the interacting\npairs of structural families in PDB, indicating that\nthe observation is a general property of protein\nevolution. The overall average correlation was 0.73\nfor a relatively reliable set of 454 family pairs, of\nwhich 78% showed significant correlation at 99%\nconfidence. In total, 918 family pairs have been\ninvestigated and the correlation was 0.61 on\naverage. But the statistical validity was weak for\nthe family pairs with small N (the number of\nmember domain pairs) of their research. This is the\nfirst step in protein classification technique two\ncombine two properties of proteins, namely,\nstructure comparison and interactivity.\n\ntheoretic analysis of a large-scale protein structural\ninteractome. They presented a global analysis of\nPSIMAP using several distinct network measures\nrelating to centrality, interactivity, fault-tolerance,\nand taxonomic diversity. But to get proper structure\nand layout they put several proteins according to\nmaximum similarity. As a result some proteins are\nplaced in wrong places. And lots of scale free\nproteins do not get proper places. Sungsam Gong,\nGiseok Yoon, Insoo Jang, Dan Bolser, Panos Dafas\nand some other famous scientist developed PSIBase\nfor Protein Structural Interactome map (PSIMAP).\nThey introduced PSIbase: the PSIMAP web server\nand database. It contains (1) domain–domain and\nprotein–protein interaction information from\nproteins whose 3D-structures are identified, (2) a\nprotein interaction map and its viewer at protein\nsuper family and family levels, (3) protein\ninteraction interface viewers and (4) structural\ndomain prediction tools for possible interactions by\ndetecting homologous matches in the Protein Data\nBank (PDB) from query sequences. They\ndeveloped an algorithm. According to that\nalgorithm the basic mechanism to check\ninteractions between any two domains or proteins is\nthe calculation of the Euclidean distance in order to\nsee if they are within a certain distance threshold.\nPSIMAP checks every possible pair of structural\ndomains in a protein to see if there are at least five\nresidue contacts within a 5Å distance [18].\n\nMr. Jong Park and Dan Bolser established\na bioinformatics research group in UK named\nMRC-DUNN. They stated their research on protein\nnetwork. They worked on structure of proteins.\nThey also used PSIMAP concept. But the limitation\nis that they only focused on protein intractability\nand taxonomic diversity. As a result their concept\ndid not help that much on protein structure analysis\nusing PSIMAP concept.\n\nDaeui Park, Semin Lee, Dan Bolser,\nMichael Schroeder some other scientists at\nbeginning of 2005 have developed Comparative\ninteractomics analysis of protein family interaction\nnetworks using PSIMAP (protein structural\ninteractome map) They have confirmed that all the\npredicted protein family interactomes (the full set\nof protein family interactions within a proteome) of\n146 species are scale-free networks, and they share\na small core network comprising 36 protein\nfamilies related to indispensable cellular functions.\nTo construct the protein family interaction network\nin a particular proteome, they first assigned the\nknown 3D structural families (on which PSIMAP is\nbased) to the protein sequences. 146 completely\nsequenced\nspecies\nfrom\nthe\nEuropean\nBioinformatics Institute (EBI) and their 578,625\nprotein sequences were used (Pruess, et al., 2003).\n\nAgain in February 2003, Mr. Jong Park\nand Dan Bolser tried to integrate Biological\nnetwork evolution hypothesis to protein structural\ninteractome. PSI-MAP was used to identify all the\nstructurally observed interactions at the structure\nfamily level. To assess the functional and\nevolutionary differences between the most\ninteractive and the least interactive folds, they used\nthe latest HIINFOLD and LOINFOLD comparison\nsets (Park and Bolser, 2001): high interaction\nstructure families and low interaction structure\nfamilies. The major problem of their system is that\nthey said that scale free topology is robust. But in\npractical it’s not true.\nBMC bioinformatics research group has\ndeveloped a concept of Visualization and graph-\n\nThe above study clearly shows that yet\nnow there is no technique has developed to classify\nproteins incorporating all six major properties.\nThough in protein grouping technique PSIMAP is\none of the remarkable achievements in this context\nbut it has some drawbacks [1, 19] especially in\n\n2\n\n\fgrouping the proteins in different classes based on\nsome essential features. To get the optimum output\nusing PSIMAP in this context researchers have to\nput some proteins in comparative places [1]. As a\nresult actual classification cannot be done using\nPSIMAP. This affects bad lay out for 3-d structure\ndesign of protein [1]. These proteins which cannot\nbe placed in proper groups may be termed as scale\nfree proteins [1, 3, 4, and 5]. We have tried to\ndevelop a smart algorithm to put right proteins in\nright places with an optimum output.\n\n2.\n\nAnalyzing the limitations of PSIMAP our\nproposed algorithm has incorporated all six major\nproperties of proteins and succeeded to eliminate\nany scale free protein.\n3.\n\n2. LIMITATIONS OF EXISTING\nALGORITHMS IN PROTEIN GROUPING\nWe have studied and analyzed PSIMAP\n(Protein Structural Interactome Map) [1],\nVisualization and graph-theoretic analysis of a\nlarge-scale protein structural interactome [1, 9-16]\nto predict some protein functions. The predicted\nproteins’ functions are domain-domain interaction,\nscale free property, age and taxonomic diversity,\nconnectivity, interaction matrix and cluster index\n[1, 17] .We gave our main attention on one of the\nrecent functions, scale free property of proteins.\nAccording to scale free property, some proteins can\nnot be placed any where in the whole proteins\nnetwork. We have developed our algorithm based\non above proteins’ functions, probability theorem\nand graph theory to remove scale free proteins from\nproteins network and finally we have grouped them.\n\n4.\n\n5.\n\nWith a view to designing a special\nalgorithm for classification of proteins, we have\nexamined the available searching algorithm and\ntheir effectiveness for our specific purpose. It may\nbe mentioned that as we have planned to design a\ntree structure for providing a good lay out for\nprotein groups, we have given special attention to\nsearching algorithm in analyzing the algorithms we\nhave considered time complexity, and their\napplicability in our specific context. The following\nsearching algorithms have revealed their\ninefficiency to fulfill our objectives:\n\nalgorithms for our specific purpose. Although\nthese three algorithms work efficiently on\nconsiderably small size of data [8, 20]. But our\nobjective is to design an algorithm which can\nefficiently work on a huge database like\nProtein Data Bank on the Web. In fact PDB\ncontains huge data on protein and perhaps it is\nthe largest web based protein database [1, 21].\nAgain we also have not considered A* search\nalgorithm for our searching technique. Because\nA* search algorithm is used to search a shortest\npath from root to a given goal node [8, 20]. But\nin this field of work we do not have any goal\nnode where the newly coming node will be\nplaced. Rather we have to find the exact\nposition of the newly coming protein out by\ndynamically.\nThe DFS and BSF algorithms are widely used\nfor finding out shortest path from source to\ndestination. However, as in grouping proteins\nas our attempt is to generate a tree rather than a\ngraph we have discarded these algorithms too.\nBesides, in discarding these algorithms we\nhave also considered their time complexities in\norder of 0(n+e) [8] which are very high for our\nobjective.\nBest-first search is the updated version of depth\nfirst search algorithm. So it also inherits\nproperties from DFS. So for the similar reasons\nwe have not considered this algorithm..\nFinally Binary search tree algorithm can be\nconsidered for its less time complexity,\neffectiveness and efficiency [8]. However as in\nbinary search tree, each node can have at most\ntwo children node which would not be adopted\nfor our protein classification algorithm because\neach group of proteins have many members\nand all of them may have more than two\nchildren coming out from a particular node.\n\nConsidering limitations of the above\nstated popular search algorithms we have\nconsidered to derive a special algorithm to fulfill\nour specific objective. For this, we have used\nweighted search concept for searching and selecting\nthe exact position of a newly coming proteins in the\nbig protein database. We have used partially BFS\nconcept and also DFS concept based on weighted\nsearch concept to get the desired position of the\nprotein.\n\n1. Hash Table, Selection Search and Linear Search\nalgorithms\nincorporating\nwith\nsorting\nalgorithm are used to search a particular key\nvalue. We have not considered these searching\n\n3. METHODOLOGY\nWe have designed the algorithm using\nincorporating six major properties of protein. We\n3\n\n\fsimilarity of p1 and p2 with respect to Cluster\nindex\n\nhave calculated probability of each protein newly\nentered in the system against of existing proteins in\nthe system. In our approach we have considered six\nfunctions for calculating probabilities based on six\nproperties of proteins. The individual function\ngenerates probabilistic value for deriving its\nrespective weight against that particular property.\nAll probabilistic values generated by six individual\nfunctions will be added together. The aggregated\nprobabilistic value will be compared with the\nprobabilistic value of the protein that resides at the\nroot. If there is not root protein (i.e. at the initial\nstate), the first protein will be considered as the root\nand depending on the probabilistic value it can\nchange its relative position.\nBased on guided search algorithm we chose the\nnode which has the highest probability of level 1.\nThen it will start calculation and comparison the\nprobabilistic values of level 2 of selected node from\nlevel 1. Then we chose the node having highest\nprobability and continued until getting the exact\nposition of newly entered protein.\nIn this way, a super kingdom tree for all proteins\nwill be generated.\n\nP (p1.p2 .Interactivity) = Similarity between p1 and\np2 with respect to Interactivity / expected similarity\nof p1 and p2 with respect to Interactivity\nP (p1.p2.Taxonomic and age diversity) = Similarity\nbetween p1 and p2 with respect to Taxonomic and\nage diversity / expected similarity of p1 and p2\nwith respect to Taxonomic and age diversity\nSo the total probability of p1 with respect to p2\nP (p1.p2) = P (p1.p2.Structure) + P\n(p1.p2.Sequence) + P (p1.p2.Connectivity) + P\n(p1.p2.Cluster index) + P (p1.p2.Interactivity) + P\n(p1.p2.Taxonomic and age diversity)\nA Proof of our algorithm\nTo prove the efficiency of our algorithm, we have\nused some dummy data containing probabilistic\nvalues for each function.\nLet p1, p2, p3, p4, p5, p6, p7, p8, p9,\np10, p11, p12, p13, p14, p15 are some proteins of\nwhich structure, sequence, interactivity, cluster\nindex [1, 17], connectivity and taxonomic and age\ndiversity values known. Based on these dummy\nvalues we have proved our proposed algorithm.\n\n3.1. DETERMINING THE BOND FACTOR\nWe have applied the general probability\nfunction to calculate similarity factor of proteins of\neach function individually\nLet, if an event is A, then the probability formula\nfor calculation probability of A is\nP (A) = Total Output / Expected\nOutput Now if there are n events, then\nThe total Bond Factor of all events is P (Total) = P\n(A1) + P (A2) + P (A3) + P (A4) + ……………. +\nP (An)\nUsing the above formulae, the similarity\nfactor of a protein p1 against another protein p2 is\nof above functions are given below:\n\nTable 1: Probabilistic values for Structure\nsimilarities of the above proteins\nP2\n\nP3\n\nP4\n\nP5\n\nP6\n\nP7\n\nP8\n\nP9\n\nP10\n\nP11\n\nP12\n\nP13\n\nP14\n\nP15\n\n40\n\n20\n\n80\n\n35\n\n28\n\n60\n\n35\n\n70\n\n10\n\n05\n\n30\n\n10\n\n10\n\n95\n\n40\n\n100\n\n40\n\n20\n\n90\n\n10\n\n05\n\n10\n\n50\n\n20\n\n70\n\n45\n\n35\n\n25\n\n45\n\nP3\n\n20\n\n40\n\n100\n\n30\n\n20\n\n60\n\n10\n\n32\n\n12\n\n30\n\n50\n\n10\n\n05\n\n12\n\n03\n\nP4\n\n80\n\n20\n\n30\n\n100\n\n10\n\n21\n\n35\n\n40\n\n50\n\n10\n\n60\n\n12\n\n60\n\n05\n\n50\n\nP5\n\n35\n\n90\n\n20\n\n10\n\n100\n\n00\n\n30\n\n20\n\n60\n\n12\n\n35\n\n73\n\n13\n\n40\n\n10\n\nP6\n\n28\n\n10\n\n60\n\n21\n\n00\n\n100\n\n10\n\n00\n\n01\n\n60\n\n34\n\n21\n\n90\n\n07\n\n95\n\nP7\n\n60\n\n05\n\n10\n\n35\n\n30\n\n10\n\n100\n\n21\n\n32\n\n41\n\n55\n\n00\n\n30\n\n05\n\n01\n\nP8\n\n35\n\n10\n\n32\n\n40\n\n20\n\n00\n\n21\n\n100\n\n00\n\n00\n\n01\n\n55\n\n11\n\n32\n\n50\n\nP9\n\n70\n\n50\n\n12\n\n50\n\n60\n\n01\n\n32\n\n00\n\n100\n\n90\n\n12\n\n35\n\n21\n\n24\n\n90\n\n4\n\nP2\n\nP (p1.p2.Cluster index) = Similarity between p1 and\np2 with respect to Cluster index / expected\n\n100\n\nP (p1.p2.Connectivity) = Similarity between p1 and\np2 with respect to Connectivity/ expected similarity\nof p1 and p2 with respect to Connectivity\n\nP1\n\nP (p1.p2.Sequence) = Similarity between p1 and p2\nwith respect to Sequence/ expected similarity of p1\nand p2 with respect to Sequence\n\nP1\n\nP (p1.p2.Structure) = Similarity between p1 and p2\nwith respect to structure / expected similarity of p1\nand p2 with respect to structure\n\n\fP4\n\nP5\n\nP6\n\nP7\n\nP8\n\nP9\n\nP10\n\nP11\n\nP12\n\nP13\n\nP14\n\nP15\n\n73\n\n48\n\n17\n\n50\n\n29\n\n75\n\n12\n\n12\n\n40\n\n17\n\n13\n\n85\n\n29\n\n95\n\n05\n\n12\n\n11\n\n48\n\n12\n\n60\n\n50\n\n29\n\n29\n\n40\n\n29\n\n17\n\n60\n\n10\n\n32\n\n12\n\n29\n\n50\n\n10\n\n05\n\n12\n\n03\n\n100\n\n05\n\n17\n\n29\n\n50\n\n40\n\n12\n\n62\n\n10\n\n63\n\n07\n\n48\n\n05\n\n100\n\n00\n\n33\n\n25\n\n61\n\n12\n\n29\n\n73\n\n12\n\n48\n\n12\n\n17\n\n00\n\n100\n\n10\n\n00\n\n01\n\n60\n\n34\n\n21\n\n90\n\n07\n\n95\n\n29\n\n33\n\n10\n\n100\n\n21\n\n32\n\n41\n\n55\n\n00\n\n29\n\n05\n\n01\n\n32\n\n50\n\n25\n\n00\n\n21\n\n100\n\n05\n\n12\n\n17\n\n40\n\n10\n\n32\n\n50\n\n48\n\n12\n\n40\n\n61\n\n01\n\n32\n\n05\n\n100\n\n90\n\n12\n\n35\n\n21\n\n24\n\n90\n\n12\n\n29\n\n12\n\n12\n\n60\n\n41\n\n12\n\n90\n\n100\n\n00\n\n00\n\n00\n\n10\n\n75\n\n60\n\n50\n\n62\n\n29\n\n34\n\n55\n\n17\n\n12\n\n00\n\n100\n\n29\n\n90\n\n23\n\n56\n\n50\n\n10\n\n10\n\n73\n\n21\n\n00\n\n40\n\n35\n\n00\n\n29\n\n100\n\n55\n\n21\n\n35\n\n29\n\n05\n\n63\n\n12\n\n90\n\n29\n\n10\n\n21\n\n00\n\n90\n\n55\n\n100\n\n23\n\n01\n\n29\n\n12\n\n07\n\n48\n\n07\n\n05\n\n32\n\n24\n\n10\n\n23\n\n21\n\n23\n\n100\n\n29\n\n40\n\n03\n\n48\n\n12\n\n95\n\n01\n\n50\n\n90\n\n75\n\n56\n\n35\n\n01\n\n29\n\n100\n\nP2\n\nP3\n\nP4\n\nP5\n\nP6\n\nP7\n\nP8\n\nP9\n\nP10\n\nP11\n\nP12\n\nP13\n\nP14\n\nP15\n\n42\n\n10\n\n72\n\n42\n\n23\n\n50\n\n32\n\n75\n\n15\n\n15\n\n35\n\n23\n\n13\n\n85\n\n100\n\n32\n\n32\n\n95\n\n05\n\n15\n\n11\n\n42\n\n15\n\n60\n\n50\n\n32\n\n32\n\n35\n\n32\n\n100\n\n32\n\n23\n\n60\n\n10\n\n32\n\n12\n\n32\n\n50\n\n10\n\n05\n\n12\n\n03\n\n32\n\n32\n\n100\n\n05\n\n23\n\n32\n\n50\n\n35\n\n15\n\n65\n\n10\n\n63\n\n07\n\n48\n\n95\n\n23\n\n05\n\n100\n\n00\n\n33\n\n25\n\n61\n\n15\n\n32\n\n72\n\n15\n\n42\n\n12\n\n100\n\n11\n\n30\n\n10\n\n30\n\n01\n\n12\n\n100\n\n35\n\n60\n\n01\n\n23\n\n56\n\n05\n\n23\n\n21\n\n75\n\n17\n\n35\n\n100\n\n23\n\n90\n\n95\n\n21\n\n55\n\n10\n\n53\n\n29\n\n56\n\n55\n\n90\n\n24\n\n01\n\n29\n\n23\n\n10\n\n00\n\n35\n\n95\n\n100\n\n75\n\n90\n\n30\n\n21\n\n05\n\n12\n\n29\n\n10\n\n30\n\n00\n\n15\n\n07\n\n48\n\n29\n\n90\n\n00\n\n100\n\n35\n\n30\n\n45\n\n03\n\n100\n\n24\n\n00\n\n00\n\n45\n\n90\n\n07\n\n42\n\n53\n\n21\n\n00\n\n12\n\n00\n\n15\n\n12\n\n40\n\nP5\n\n35\n\n35\n\n100\n\n21\n\n21\n\n63\n\n85\n\n72\n\n01\n\n15\n\n12\n\n90\n\n55\n\n70\n\n05\n\n30\n\nP4\n\nP1 2\n\n05\n\n45\n\n90\n\n10\n\n34\n\n10\n\n13\n\n10\n\n95\n\n30\n\n21\n\n100\n\n41\n\n30\n\n10\n\n30\n\nP3\n\nP1 1\n\n07\n\n00\n\n10\n\n01\n\n60\n\n65\n\n20\n\n42\n\n12\n\n90\n\n55\n\n01\n\n32\n\n15\n\n50\n\n50\n\nP2\n\nP10\n\n45\n\n21\n\n41\n\n100\n\n01\n\n15\n\n40\n\n100\n\n48\n\n15\n\n34\n\n32\n\n21\n\n61\n\n30\n\n60\n\nP1\n\n07\n\n70\n\n60\n\n21\n\n00\n\n40\n\n15\n\n10\n\nTable 2: Sequence similarities of the above proteins\n\nP1\n\n03\n\n63\n\n30\n\n01\n\n100\n\n25\n\n12\n\n15\n\nP15\n\n12\n\n10\n\n15\n\n00\n\n10\n\n50\n\n15\n\nP14\n40\n\n05\n\n65\n\n61\n\n10\n\n33\n\n32\n\n45\n\nP13\n\n30\n\n10\n\n15\n\n25\n\n100\n\n30\n\n75\n\nP9\n\n85\n\n30\n\n50\n\n40\n\n33\n\n00\n\n10\n\n11\n\nP8\n\n13\n\n50\n\n30\n\n50\n\n00\n\n20\n\n30\n\nP7\nP1\n\n20\n\n60\n\n12\n\n30\n\n100\n\n60\n\n15\n\n48\n\nP6\n\nP1\n\n40\n\n15\n\n32\n\n20\n\n05\n\n50\n\nP3\n\n100\nP1\n\n15\n\n45\n\n10\n\n05\n\n20\n\n05\n\nP2\n\n30\n\nP1\n\n15\n\n11\n\n60\n\n100\n\n20\n\n85\n\n30\n01\n\nP1\n\n75\n\n15\n\n20\n\n30\n\n95\n\n13\n\n100\n35\n\nP1\n\n30\n\n05\n\n30\n\n45\n\nP15\n\n17\n\n01\n23\n\n56\n\nP9\n\n50\n\n95\n\n100\n\n30\n\nP14\n\n40\n\n23\n21\n75\n\nP8\n\n20\n\n30\n\n70\n\nP13\n\n12\n\n35\n100\n\n23\n90\n\nP7\n\n45\n\n30\n\n30\n\nP1\n\n12\n\n21\n55\n10\n\n50\n\nP6\n\n70\n\n10\n\nP1 1\n\n75\n\n56\n55\n\n90\n24\n\n01\n\nP5\n\n10\n\n100\n\nP1 0\n\n29\n\n23\n100\n00\n\n32\n\n95\n\nP4\n\n45\n\nP9\n\n50\n\n75\n90\n\n30\n21\n05\n\n10\n\nP3\n\n45\n\nP8\n\n17\n\n10\n30\n00\n\n11\n\n07\n\n50\n\n100\n\nP7\n\n48\n\n00\n\n100\n35\n30\n\n40\n\n03\n\nP5\n\n00\n00\n\n55\n\n90\n\n05\n\nP2\n\nP6\n\n73\n\n00\n12\n00\n13\n\n12\n\nP4\n\n100\n\n01\n\n21\n\n60\n\nP1\n\nP5\n\n10\n\n90\n55\n73\n\n05\n\nP3\n\n00\n\n34\n12\n\n45\n\nP4\n\n48\n\n41\n35\n10\n\n95\n\nP2\n\n60\n60\n\n25\n\nP3\n\n100\n\n12\n50\n\n10\n\nP1\n\n10\n\n35\n\nP2\n\nTable 4: Connectivity similarities of the above proteins\n\nP1\n\n5\n\nP1\n\n30\n\n10\n\nP15\n\n45\n\nP14\n\n30\n\nP13\n\n70\n\nP12\n\n05\nP11\n\n20\n\nP10\n\n10\n\nTable 3: Interactivity similarities of the above proteins\n\n\fP15\n\n85\n\n32\n\n32\n\n40\n\n05\n\n12\n\n03\n\n63\n\n07\n\n48\n\n15\n\n45\n\n12\n\n34\n\n21\n\n90\n\n07\n\n95\n\n55\n\n02\n\n32\n\n05\n\n01\n\n21\n\n45\n\n15\n\n35\n\n53\n\n12\n\n35\n\n21\n\n24\n\n90\n\n90\n\n100\n\n02\n\n02\n\n02\n\n10\n\n75\n\n12\n\n02\n\n100\n\n32\n\n90\n\n23\n\n56\n\n35\n\n02\n\n32\n\n100\n\n55\n\n21\n\n35\n\n21\n\n02\n\n90\n\n55\n\n100\n\n23\n\n01\n\n24\n\n10\n\n23\n\n21\n\n23\n\n100\n\n32\n\n53\n\n90\n\n75\n\n56\n\n35\n\n01\n\n32\n\n100\n\n100\n13\n\n70\n\n90\n\n01\n\n30\n\n25\n32\n\n100\n\n35\n\n95\n\n01\nP14\n\n10\n10\n\n05\n\n28\n\nP13\n65\n01\n\n15\n\n07\n\n30\n\n10\n41\n\n32\n\n01\n48\n32\n\n45\n\n90\n\n100\n\n48\n60\n\n02\n\n23\n60\n01\n\n21\n\n21\n\n23\n\n40\n15\n\n55\n\n100\n15\n61\n\n10\n\n34\n\n21\n\nP12\n15\n\n41\n\n12\n\n55\nP11\n40\n\n01\n\n60\n\n48\n\n28\n32\n\n32\n\n45\n\n56\n12\n\n100\n\n01\n\n07\n\n21\n15\n\n21\n\n15\n\n23\n45\n21\n\n02\n\n63\n\n55\n15\n\n100\n\n70\n\n90\n75\n02\n\n10\n\n10\n\n100\nP10\n\n10\n\n32\n\n30\nP9\n25\n100\n\n65\n\n75\n\n33\n\n15\n\n90\n48\n02\n\n15\n\n10\n\n32\n\n61\n\n24\n32\n25\n\n40\n\n00\n\n10\n\n25\n\n21\n11\n60\n\n48\n\n00\n\n32\n15\n\n33\n\n28\nP8\n05\n\n32\n\n53\n48\n\n02\n\n28\n25\n\n25\n\n12\nP7\n\n100\n\n45\n\nP6\n05\n\n01\n\n05\n\n95\n100\n\n05\n\n25\n\n07\n32\n\n30\n\n95\n\n90\n32\n\n00\n\n45\n\n21\n70\n\n12\nP5\n\n48\nP4\n\n45\n\n03\n\n07\n\n40\n\n12\n\n12\n\n63\n\n32\n\n65\n\n05\n\n10\n\n32\n\n56\n\n10\n\n23\n\n48\n\n75\n\n90\n\n48\n\n10\n\n30\n\n60\n\n90\n\n00\n\n100\n\n32\n\n24\n\n00\n\n00\n\n15\n\n53\n\n21\n\n00\n\n12\n\n12\n\n28\n\n28\n\n100\n\n21\n\n45\n\n01\n\n12\n\n12\n\n90\n\n55\n\n32\n\n05\n\n45\n\n90\n\n10\n\n34\n\n11\n\n95\n\n30\n\n21\n\n100\n\n41\n\n30\n\n10\n\n07\n\n00\n\n10\n\n01\n\n60\n\n65\n\n15\n\n12\n\n90\n\n55\n\n01\n\n32\n\n12\n\n50\n\n60\n\n45\n\n21\n\n41\n\n100\n\n01\n\n12\n\n60\n\n05\n\n48\n\n12\n\n34\n\n32\n\n21\n\n61\n\n30\n\n12\n\n25\n\n07\n\n65\n\n60\n\n21\n\n00\n\n38\n\n12\n\n95\n\n03\n\n63\n\n30\n\n01\n\n100\n\n25\n\n12\n\n12\n\n32\n\n12\n\n10\n\n12\n\n00\n\n10\n\n50\n\n45\n\n32\n\n38\n\n05\n\n65\n\n61\n\n10\n\n33\n\n32\n\n75\n\n100\n\n03\n\n30\n\n10\n\n12\n\n25\n\n100\n\n30\n\n11\n\n32\n\n38\n\n85\n\n30\n\n50\n\n38\n\n33\n\n00\n\n10\n\n30\n\n32\n\n12\n\n13\n\n50\n\n30\n\n50\n\n00\n\n20\n\n12\n\n100\n\n30\n\nP15\n\n20\n\n60\n\n12\n\n30\n\n100\n\n60\n\n50\n\n10\n\n05\n\nP14\n\n38\n\n12\n\n32\n\n20\n\n05\n\n05\n\n45\n\n30\n\n100\n\nP13\n\n12\n\n45\n\n10\n\n05\n\n20\n\n20\n\nP3\n\n10\n\n32\n\nP12\n\n12\n\n11\n\n60\n\n100\n\n95\n\nP2\n\n50\n\n32\n\n01\n\nP11\n\n75\n\n12\n\n20\n\n30\n\n45\n\n85\n\n100\n\n35\n\nP10\n\n30\n\n05\n\n30\n\n30\n\nP1 5\n\n01\n\n23\n\n56\n\nP9\n\n50\n\n95\n\n100\n\n65\n\n13\n\n23\n\n21\n\n75\n\nP8\n\n20\n\n30\n\n30\n\nP14\n\n35\n100\n\n23\n\n92\n\nP7\n\n45\n\n30\n\n10\n\n25\n\n21\n55\n\n10\n\n53\n\nP6\n\n65\n\n100\n\nP1 3\n\n56\n55\n\n92\n\n24\n\n01\n\nP5\n\n10\n\n45\n\n40\n\n23\n100\n\n00\n\n35\n\n95\n\nP4\n\n45\n\nP1 2\n\n85\n\n75\n92\n\n32\n\n21\n\n05\n\n12\n\nP3\n\n100\n\n15\n\nP1 5\n\n10\n32\n00\n\n15\n\n07\n\n48\n\nP2\n\nP11\n\n13\n\n92\n00\n\n100\n\n35\n\n32\n\n42\n\n03\n\nP1\n\n15\n\nP14\n\n24\n00\n00\n\n42\n\n92\n\n07\n\n35\n\nP1 1\n\nP1 0\n\n20\n\n53\n21\n\n00\n12\n\n00\n\n15\n\n12\n\n85\n\nP10\n\n75\n\nP13\n\n35\n35\n100\n\n21\n\n21\n\n63\n\n32\n\nP9\n\nP9\n\n38\n\n01\n15\n\n12\n92\n\n55\n\n72\n\n05\n\n13\n\nP8\n\n32\n\nP12\n\n05\n42\n92\n\n10\n\n34\n\n10\n\n32\n\nP7\n\nP8\n\n95\n32\n\n21\n100\n41\n\n32\n\n10\n\n23\n\nP6\n\n48\n\n07\n00\n10\n\n01\n\n60\n\n65\n\n50\n\nP5\n\nP7\n\n92\n\n55\n01\n32\n\n15\n\n50\n\n35\n\nP4\n\n25\n\n21\n41\n\n100\n\n01\n\n15\n\n60\n\nP3\n\nP6\n\n34\n32\n21\n61\n\n32\n\n15\n\nP2\n\n45\n\n60\n\n21\n\n00\n\n35\n\n15\n\nP1\n\nP5\n\n01\n100\n25\n\n12\n\n15\n\nP15\n\n70\n\n00\n\n10\n50\n\n42\n\nP14\n\nP4\n\n10\n33\n32\n\n75\n\nP1 3\n\n10\n\n100\n32\n\n11\n\nP12\n\nP3\n\n00\n10\n\n32\n\nP11\n\n45\n\n23\n15\n\nP10\n\nP2\n\n60\n50\n\nP9\n\n100\n\n05\n\nP8\n\n6\n\nP1\n\n23\n\nP7\n\nTable 5: Cluster index similarities of the above proteins\n\nP1\n\nP6\n\nTable 6: Taxonomic and age diversity similarities of the\nabove proteins\n\n\fTable 7: Total probability of all proteins with respect\nto structure, sequence, connectivity, cluster index,\ninteractivity and taxonomic and age diversity\nP2\n\nP3\n\nP4\n\nP5\n\nP6\n\nP7\n\nP8\n\nP9\n\nP10\n\nP11\n\nP12\n\nP13\n\nP14\n\nP15\n\n6\n\n2.65\n\n0.70\n\n4.30\n\n2.60\n\n1.33\n\n3.08\n\n1.88\n\n4.45\n\n0.79\n\n0.74\n\n2.23\n\n1.15\n\n0.75\n\n5.20\n\nP1\n\nP1\n\nNow using the respective value for\nBond Factor. Let the sequence of entering\nproteins are p1, p2, p3, p4, p5, p6, p7, p8, p9,\np10, p11, p12, p13, p14, p15.\n\np1\n1.73\n\n5.65\n\n0.35\n\n0.74\n\n0.65\n\n2.75\n\n0.89\n\n3.70\n\n2.93\n\n1.88\n\n1.78\n\n2.38\n\n6\n\n1.83\n\n1.25\n\n3.80\n\n0.60\n\n1.92\n\n0.72\n\n1.83\n\n2.98\n\n0.60\n\n0.30\n\n0.72\n\n0.18\n\nP3\n\n1.93\n\n6\n\nP2\n\nFigure 1: Step1, Entry of p1\n\np1\n\n2.90\n\n0.40\n\n3.75\n\n0.62\n\n3.82\n\n0.79\n\n2.43\n\n2.88\n\n1.88\n\n1.26\n\n0.35\n\n6\n\nP4\n\np2\nFigure 2: Step2, Entry of p2\n\n1.45\n\n3.65\n\n0.81\n\n1.88\n\n4.23\n\n0.82\n\n2.65\n\n0.70\n\n6\n\n0.60\n\n0.02\n\n0.06\n\n3.60\n\n2.04\n\n1.26\n\n5.42\n\n0.42\n\n5.70\n\n6\n\n1.26\n\n1.92\n\n2.46\n\n3.30\n\n0.02\n\n1.83\n\n0.30\n\n0.06\n\nP7\n\n1.95\n\nP6\n\n0.02\n\n6\n\nP5\n\np1\np2\np3\n\n3.12\n\n1.97\n\n0.78\n\n2.17\n\n1.02\n\n0.52\n\n0.09\n\n6\n\nP8\n\nFigure 3: Step3, Entry of p3\n\n5.42\n\n1.44\n\n1.26\n\n2.03\n\n0.72\n\n5.42\n\n6\n\nP9\n\np1\np2\n\n4.50\n\n0.60\n\n0.02\n\n0.02\n\n0.02\n\n6\n\nP10\n\np4\n\np3\n\n3.36\n\n1.38\n\n5.42\n\n1.83\n\n6\n\nP11\n\nFigure 4: Step4, Entry of p4\n\n2.03\n\n1.26\n\n3.30\n\n6\n\nP12\n\np1\np2\n\n0.06\n\n1.38\n\n6\n\nP13\n\np4\n\np3\n\np5\n\n1.83\n\n6\n\nP14\n\n6\n\nP15\n\nFigure 5: Step5, Entry of p5\n\np1\n\nNow based on the total Bond Factor\nstated in Table 7, the proposed algorithm has\nbeen simulated with a view to generating a tree\nstructure using all 15 proteins leaving no scale\nfree protein.\nLet the sequence of entering proteins\nare p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, p11,\np12, p13, p14, p15.\n\np6\np4\n\np2\np3\n\np5\nFigure 6: Step6, Entry of p6\n\n7\n\n\fp1\n\np1\n\np9\n\nP7\np7\n\np2\n\np2\n\np4\n\np4\np3\n\np6\n\np3\n\np6\np10\n\np5\n\np5\n\np8\n\nFigure 7: Step7, Entry of p7\n\np11\n\nFigure 11: Step11, Entry of p11\n\np1\np1\n\nP7\n\np9\n\np2\np4\n\np7\n\np2\n\np6\n\np3\n\np4\np3\n\np5\n\np6\n\np8\n\np10\n\np5\np8\n\nFigure 8: Step8, Entry of p8\n\np11\n\np12\n\np\n\np\n\np\n\nFigure 12: Step12, Entry of p12\n\np\np1\n\np\np\n\np9\n\np\np7\n\np2\n\np\n\np4\n\np\n\np3\n\np10\n\np6\np5\n\nFigure 9: Step9, Entry of p9\n\np8\n\np13\n\np11\n\np12\n\np1\n\np9\n\np2\n\nFigure 13: Step13, Entry of p13\n\np7\n\np1\n\np9\n\np4\np3\n\np6\n\np7\n\np2\n\np10\n\np5\n\np4\np3\n\np8\n\np10\n\np6\np5\n\nFigure 10: Step10, Entry of p10\n\np8\n\np13\n\np11\n\np12\np14\nFigure 14: Step14, Entry of p14\n\n8\n\n\fp1\n\n14\nDo\n15\n{\n16\nproteinFile2 = Read a protein;\n17\nTreeNode = Parent [0];\n18\nWhile (Location is not fix)\n19\nDo\n20\nCurrentSelectedNode\n= TreeNode;\n21\nFor each node n of\nTreeNode\n22\nDo\n23\nCalculate\nTotalProbability\n= StructuralProbability (n,\nproteinFile2)\n+ SequentialProbability\n(n,\nproteinFile2) + InteractivityProbability (n,\nproteinFil e2)\n+ ClusterIndexProbability\n(n,proteinFile2) + ConnectivityProbability (n,\nproteinFile2)+\nTaxonomicAgeDiversityProbability\n(n,\nproteinfile2);\n\np9\n\np2\n\np10\n\np7\np4\n\np3\n\np6\np15\np5\np13\n\np8\np11\np12\np14\n\nFigure 15: Step15, Entry of p15\n\n3.2. PSEUDO CODE:\nThe simple pseudo code of the algorithm\nis given below\n\n// Use different functions to calculate the total\nBegin:\n// probability\n1\nProtein proteinFile1;\n// declare a\nprotein file variable\n2\nProtein proteinFile2;\n// declare a\nprotein file variable\n3\nproteinFile1= Read a protein;\n//\nRead a protein File\n4\nParent [0] = proteinFile1;\n//\nInitialize the parent array by file proteinFile1 as\nroot\n5\nTotalProbability = 0;\n//\ninitialize the total probability as zero\n6\nMaximumProbability = 0;\n7\nStructural probability;\n//\ndeclare variable for structural probability\n8\nSequential probability;\n//\ndeclare variable for sequential probability\n9\nInteractivity probability;\n//\ndeclare variable for interactivity probability\n10\nCluster index probability;\n//\ndeclare variable for cluster index probability\n11\nConnectivity probability;\n//\ndeclare variable for connectivity probability\nTaxonomic\nand\nage\ndiversity\nprobability; //\ndeclare\nvariable\nfor\ntaxonomic\n\n24\nIf\nTotalProbability > = MaximumProbability\n25\nThen\n26\nMaximumProbability = TotalProbability;\n27\nCurrentSelectedNode = n;\n28\n// End If\n29\n// End For\n30\nIf (all nodes of\nTreeNode\nare\nfinished\nand\nTreeNode\n=\nCurrentSelectedN\node)\n31\nThen\n32\nTreeNode ->\nChild = proteinFile2; // put the position of the\nprotein which\nwas\nnewly\nread\n33\nBreak; //\nout from inner while loop\n34\nElse\n35\nTreeNode =\nCurrentSelectedNode; // select next parent node\n36\n// End If\n37\n// End While\n38\n}\n39\nWhile (! End of proteins)\n\n//and age diversity probability\nTreeNode;\n// declare TreeNode as a node of tree\n13\nCurrentSelectedNode;\n// declare CurrentSelectedNode as a\nnode of Tree\n12\n\n9\n\n\f40\n\n// End While\n\n“Visualization and Graph-theoretic Analysis of a\nLarge-scale Protein Structural Interactome”,\nBMC Bioinformatics, 8 October 2003, 4,\n45:1471-2105, pp. 1-11.\n\nEnd;\n3.3. TIME COMPLEXITY OF THE\nPROPOSED ALGORITHM\n\n[2] Drew Lett1, Michael Hsing2 and Frederic\nPio,\n\"Interaction\nprofile-based\nprotein\nclassification of death domain\", 09 June 2004,\nBMC\nBioinformatics\n2004,\n5:75\ndoi:\n10.1186/1471-2105-5-75, pp.1\n\nWe considered only time complexity.\nThe T (A) is total time of compilation and\nexecution by the algorithm. The compile time\ndoesn’t depend on the instance characteristics.\nSo we just concern ourselves with the run time\nof the algorithm.\n\n[3] Lun Li, David Alderson¤, John C. Doyle¤,\nWalter Willingery, “Towards a Theory of ScaleFree Graphs: Definition, Properties, and\nImplications”, August 25, 2005, pp. 1-5\n\nThe time complexity of the proposed\nalgorithm\nWorst case: T (A) = O (n) where n= number of\nprotein file or node\nBest case: T (A) = O (l) where l = level of the\ntree\n\n[4] Francesc Comellas, “Recursive graphs with\nsmall-world scale-free properties”, Department\nde Matema`tica Aplicada IV, EPSC, Universitat\nPolite`cnica de Catalunya, Avinguda Canal\nOlı´mpic s/n, 08860 Castelldefels, Barcelona,\nCatalonia, Spain, Received 26 November 2002;\nrevised manuscript received 15 December 2003;\npublished 31 March 2004\n\n4. CONCLUSION\nOur algorithm for protein classification\nhas incorporated the major six properties of\nprotein, namely, a) Structure comparison\nb)Sequence Comparison c) Connectivity d)\nCluster Index e) Interactivity f) Taxonomic and\nage diversity. Integration of all properties in a\nsingle protein group technique provides a new\ndimension in protein grouping. Unlike PSIMAP\ntechnique this will leave any scale free protein\nthat to be created using this algorithm. The\nsimulation of the algorithm using dummy data\nhas been proved our assertion. Moreover, in term\nof time complexity if we consider huge protein\ndatabase then it will be more efficient comparing\nwith other existing protein grouping techniques.\n\n[5] Albert-László Barabási , Zoltán Dezs˝o ,\nErzsébet Ravasz , Soon-Hyung Yook and Zoltán\nOltvai, “Scale-free and hierarchical structures in\ncomplex networks” , Department of Physics,\nUniversity of Notre Dame, Notre Dame, IN\n46556, USA., Department of Pathology,\nNorthwestern University, Illinois 60611, USA\n[6] Dr.P.I.Haris, famous scientist and principal\nlecturer in Biochemistry of Leicester De Monfort\nUniversity, UK\n[7] John E., A Bioinformatics Researcher at\nBioinformatics.org, JhnErgl@hotmail.com\n\nHowever, the success of this algorithm\ndepends on the functions that are to be used to\ngenerate probabilistic value for each protein in\nthe proposed algorithm. But our study has\nrevealed that some of such functions based on\nthe properties of proteins are yet to be derived in\ndifferent bioinformatics research lab [7] such as\ncluster index [1, 17], connectivity and\ninteractivity. If the respective functions for\ncluster index, connectivity and interactivity are\nachieved then our algorithm will be the protein\ngrouping technique.\n\n[8] Ellis Horowitz, Sartaj Sahni, Sanguthevar\nRajasekaran , Fundamentals of computer\nalgorithm, 2000, Galgotia Publications Pvt. Ltd\n[9] McCraith S et al.: Genome-wide analysis of\nvaccinia virus protein-protein interactions. Proc\nNatl Acad Sci USA 2000,97(9):4879-4884.\n[10] Uetz P et al.: A comprehensive analysis of\nprotein-protein interactions in: Saccharomyces\ncerevisiae. Nature 2000,403(6770):623-7.\n[11] Walhout AJ et al.: Protein Interaction\nMapping in C. elegans Using Proteins Involved\nin Vulval Development. Science 1999,5450:116121.\n\nREFERENCES\n[1] Dan M Bolser, Panos Dafas, Richard\nHarrington, Jong Park and Michael Schroeder,\n10\n\n\f[12] Fromont-Racine M et al.: Genome-wide\nprotein interaction screens reveal functional\nnetworks involving Sm-like proteins. Yeast\n2000, 17(2):95-110.\n[13] Fromont-Racine M, Rain JC and Legrain P:\nToward a functional analysis of the yeast\ngenome through exhaustive two-hybrid screens.\nNat Genet 1997, 16(3):277-82.\n[14] Ito T et al.: Toward a protein-protein\ninteraction map of the budding yeast: A\ncomprehensive system to examine twohybrid\ninteractions in all possible combinations between\nthe yeast proteins. Proc Natl Acad Sci U S A\n2000, 97(3):1143-7.\n[15] Flajolet M et al.: A genomic approach of the\nhepatitis C virus generates a protein interaction\nmap. Gene 2000, 242(1– 2):369-79.\n[16] Rain JC et al.: The protein-protein\ninteraction map of Helicobacter pylori. Nature\n2001, 409(6817): 211-5.\n[17] Watts DJ and Strogatz SH: Collective\ndynamics of 'small-world' networks. Nature\n1998, 393(6684): 440-2.\n[18] Sungsam Gong1, Giseok Yoon2, Insoo\nJang3, Dan Bolser4, Panos Dafas5,Michael\nSchroeder6, Hansol Choi1, Yoobok Cho2,\nKyungsook Han7, Sunghoon Lee3,Hwanho\nChoi1, Michael Lappe8, Liisa Holm9, Sangsoo\nKim3, Donghoon Oh2 and Jonghwa Bhak1,\n\"PSIbase: a database of Protein Structural\nInteractome map (PSIMAP)\", March 3, 2005. pp.\n2.\n[19] Daeui Park1, Semin Lee2, Dan Bolser3,\nMichael\nSchroeder4,\nMichael\nLappe5,\nDonghoon Oh1 and Jong Bhak2, \"Comparative\ninteractomics analysis of protein family\ninteraction networks using PSIMAP (protein\nstructural interactome map)\"\n[20] Niklaus Wirth, Algorithm + Data Structure\n= Programs, Prentice-Hall of India Private\nLimited, New Delhi-110 001, 2001\n[21] www.rcsb.org, A group of biochemistry,\nlocated in the Department of Chemistry and\nChemical Biology at Rutgers, info@rcsb.org.\n\n11\n\n\f"
        ],
        [
         "41",
         "41",
         "cs.CE",
         "Computational Engineering",
         "1002.4067v1.pdf",
         "A Taxonomy of Causality-Based Biological Properties ∗\nC. Bodei1 , A. Bracciali1 , D. Chiarugi2 , and R. Gori1\n1 : Dipartimento di Informatica, Università di Pisa, Italy\n{chiara,braccia,gori}@di.unipi.it\n2 : Dipartimento di Scienze Matematiche e Informatiche, Università di Siena, Italy\nchiarugi3@unisi.it\n\nWe formally characterize a set of causality-based properties of metabolic networks. This set of properties aims at making precise several notions on the production of metabolites, which are familiar in\nthe biologists’ terminology. From a theoretical point of view, biochemical reactions are abstractly\nrepresented as causal implications and the produced metabolites as causal consequences of the implication representing the corresponding reaction. The fact that a reactant is produced is represented\nby means of the chain of reactions that have made it exist. Such representation abstracts away from\nquantities, stoichiometric and thermodynamic parameters and constitutes the basis for the characterization of our properties. Moreover, we propose an effective method for verifying our properties\nbased on an abstract model of system dynamics. This consists of a new abstract semantics for the\nsystem seen as a concurrent network and expressed using the Chemical Ground Form [6] calculus.\nWe illustrate an application of this framework to a portion of a real metabolic pathway.\n\n1\n\nIntroduction\n\nUnderstanding the relationships amongst the elements of biological interaction networks is a relevant\nproblem in Systems Biology. In the words of [23], “diagrams of interconnections represent a sort of\nstatic roadmaps, but what we really seek to know are the traffic patterns, why such patterns emerge, and\nhow we can control them”. Formal descriptions of interconnections and methodologies for performing\ntraffic simulations in silico can orientate in vitro experimentation.\nWe focus here on metabolic networks, i.e. the set of the cellular biochemical pathways involved in\nenergy management and in the synthesis of structural components. Biochemical pathways are typically\ncomposed of chains of enzymatically catalyzed chemical reactions and are interconnected in a complex\nway. This makes difficult to understand the overall emerging behaviour of a network, starting from the\ndetailed knowledge of the single reactions.\nAn interesting issue is the identification of the parts of a network whose integrity is crucial for certain\nfunctionalities. These “hot points” represent candidate drug targets for repressing undesired metabolic\nfunctions involved in pathological states, such as infectious diseases and cancer [9, 15]. Several properties characterizing different aspects of the network functionalities have been introduced in the biological\nliterature, often with slightly different versions for the same property. What formal methods can offer is\na way to make precise and classify properties, too often expressed only at an intuitive level.\nSince, broadly speaking, causality plays a key role in finding chains of reactions that connect the\nparts of a network, we base our understanding of properties in terms of causality relations. Following\nthe approach in [5], in order to give a formal characterization of causality-based properties, we interpret\n∗ This\n\nwork has been partially supported by the British-Crui Partnership Programme 2009.\n\nE. Merelli and P. Quaglia (Eds.)\nFrom Biology To Concurrency and back 2010 (FBTC’10)\nEPTCS 19, 2010, pp. 116–133, doi:10.4204/EPTCS.19.8\n\nc C. Bodei, A. Bracciali, D. Chiarugi & R. Gori\nThis work is licensed under the\nCreative Commons Attribution License.\n\n\fC. Bodei, A. Bracciali, D. Chiarugi & R. Gori\n\n117\n\nbiochemical reactions as “logical consequences“, where the source metabolites can cause, i.e. produce\nthe target ones. Furthermore, we adopt the notion of explanation of a certain metabolite. Given a set of\nreactions and initial conditions, an explanation represents the chain of reactions, causally dependent one\nfrom the another, that leads to the metabolite. Our approach therefore models the biochemical dynamics,\ncapturing causal dependencies, while abstracting away from other aspects, like quantities, stoichiometric\nand thermodynamic parameters.\nOn top of the causality notion, we formalize several properties from a potentially longer list. Beyond\nthe relevance of their biological meaning, these properties show how the few and simple ingredients we\npropose are sufficiently expressive to make precise several common notions, often intuitively used in\nbiology. Specifically, the set of properties we present concerns the role and the relations of metabolites\nand reactions within a metabolic network.\nWe propose an effective method for verifying the formalized causal properties, based on the construction once for all of an abstract representation of the dynamics of the biological system. The system\nis specified as a concurrent network in terms of the Chemical Ground Form calculus [7]. We opted for\nthe CGC for its extreme simplicity and well established theories and techniques, while it is, at the same\ntime, sufficiently concrete to capture our properties. For our verification purposes, we have defined a\nslightly different semantics from the one in [7, 10]. It is worth pointing out that our choice mainly strives\nfor simplicity. Other specification languages suitable for biological networks, e.g. [29, 34, 33, 8, 21],\ncould have been adopted as well, some perhaps even more expressive, but generally requiring higher\ncosts for model construction and verification procedures.\nOverall, we are interested in efficiently evaluating the impact of changes on working hypotheses,\nsuch as the variation of the initial conditions and of the sets of reactions, according to a what-if strategy.\nThe method we propose is meant to be exploited as a sort of preliminary in silico screening, aiming\nat determining the most promising experiments to be carried out in vitro. Finally, we believe that our\nframework should be palatable to biologists, since it is very close to the biochemical intuition of causality\nand to the spirit of many informal notions currently in use.\nRelated Work. Due to recent progress of wet-lab techniques, many metabolic networks are structurally well characterized and can be reconstructed for many organisms up to the genome-scale level\n(see e.g. [30]). However, approaches grounded on dynamical modeling, e.g. Metabolic Control Analysis\nor Metabolic Flux Analysis (see [16]), may encounter difficulties, mainly because part of the needed kinetic parameters are not known. In contrast, structure oriented analysis only requires information about\nthe topology of the investigated networks, which is often known. Even though this kind of approach may\nnot provide a detailed knowledge of the dynamics underlying the target phenomenon, it allows key properties of metabolic networks to be addressed, as demonstrated by the plethora of works in the literature.\nWe mention here [41] and [40], where the authors propose to exploit “elementary modes” or “extreme\npathways” to perform pathway analysis and to assess structural properties, such as structural robustness\nand redundancy. In [42], a method is presented that relies on the network structure for predicting robustness in gene regulation networks, while [2] reviews a group of works in which recurring patterns of\ninteraction in biochemical networks (a.k.a. network motifs) are identified and related to specific behaviors or network robustness. In [26] a novel method is used to target those nodes whose deletion causes\nthe failure of certain network functionalities.\nProcess algebras have been often used to abstractly model biological systems as concurrent systems,\ne.g. [37, 39, 38, 12, 13, 6, 36]. Closer to our approach is the work presented in [11], where the authors\napply a causal semantics of the π-calculus [27] in order to describe biochemical processes. We use\ninstead CGF, with a simpler semantics, but suitable for establishing the causal dependencies of interest.\n\n\f118\n\nA Taxonomy of Causality-Based Biological Properties\n\nOur results are close to those obtained by applying Control Flow Analysis (CFA), a quite efficient\nstatic technique, to process calculi used for modeling biological systems, e.g. [28, 29, 34, 33, 4]. In all the\ncases, an over-approximation of the behaviour of a system is offered. In particular, the analyses presented\nin [34, 33] capture causality information relevant for interpreting biological phenomena, and the authors\npropose a formalization of properties, like in our approach. Temporal and causal properties are also\naddressed in [21], where an Abstract Interpretation Analysis for systems specified in the BioAmbients\ncalculus is used to model the quantities of molecules involved in interactions.\nOur approach also shares some similarities with BIOCHAM [8] and the Pathway Logic [14].\nBIOCHAM is based on the Biochemical Abstract Machine, which offers a formal modeling environment for biochemical processes and qualitative descriptions of these processes. BIOCHAM is based on\na rule-centered language for specifying biochemical systems and, differently from our approach, it provides tools for querying temporal properties expressed in the Computation Tree Logic. Pathway Logic\nuses rewriting logic for modeling biological pathways and for enabling the symbolic analysis on them.\nIn a way similar to ours, biochemical reactions are rendered in terms of rules acting on molecules. Both\nthese approaches allow biochemical networks to be specified at a high level of abstraction. However,\nsome expressible features, e.g. the distinction among different classes of molecules or reactions, have\nappeared too detailed for the aim of tracking causality and for our quest for a skeletal language for\ncharacterizing causality-based relevant properties.\nAs discussed, several of the mentioned approaches may provide more detailed models and properties than ours, however they generally require computationally expensive verification techniques. Our\nproposal combines the formalization of properties with a light-weight, approximate in some regards,\ncomputational machinery.\nSynopsis. The metabolic network model is illustrated in §2, properties are formalized in §3 and the\nprocess-algebraic computational framework is introduced in §4. An example is discussed in §5.\n\n2\n\nA formal model of metabolic networks\n\nWe give an abstract representation of metabolic networks and of the corresponding biochemical reactions. More precisely, we abstract away from quantities, stoichiometric proportions, kinetic or thermodynamic parameters, that are involved in reactions, e.g. consider a standard biochemical reaction like:\naA + bB →r cC + dD\n\n(1)\n\nwhere A, B,C and D are the species involved, a, b, c and d are the corresponding stoichiometric coefficients, and r represents the rate at which reactants become products. We abstractly represent (1) as:\nA◦B → C◦D\n\n(2)\n\nWe focus on the fact that the presence of both A and B represents the possibility for C and D to be\nproduced or caused. Furthermore, we abstract from the dynamic evolution of the network, implicitly\nassuming that reactants are never consumed, that it is also also an abstraction over their quantities. As\na consequence (2) reads as A ◦ B → A ◦ B ◦ C ◦ D. Our model gives therefore an over-approximation of\nthe set of the actual pathways, possibly including some pathways that could be actually prevented, for\ninstance, by the lack of a suitable quantity of reactants or by an inadequate temperature.\nFor easing the computational machinery, we further decompose any rule causing more than one\nmetabolite into a set of rules with only one caused metabolite each, e.g. rule (2) becomes A ◦ B → C (3)\n\n\fC. Bodei, A. Bracciali, D. Chiarugi & R. Gori\n\n119\n\nplus A ◦ B → D (4). This transformation does not impact on causality: the set of metabolites producible\nby the original rule can be still produced by applying the new rules, as the premises are the same.\nFinally, following [20], the unlikelihood of reactions involving more than two species, leads us to\naddress only reactions with two causing metabolites at most. By summarizing, in the reactions we\nconsider, either two molecules produce a new molecule, or a molecules degrades to another one.\nDefinition 2.1 (Rules) Given a finite set of metabolites M, ranged over by over by A, Ai , B, C, D..., a\nrule is either in the form (1) A1 ◦ A2 → C, or (2) A → C\nThe description of causal relations within a metabolic network can be obtained by defining a set of\nreaction rules R that describe how new metabolites can be produced, and a set S of metabolites, initially\npresent in the network solution, which can be seen as premise-less rules.\nDefinition 2.2 (m network and Initial Solution) An m network R is a finite set of rules with non-empty\npremises. An initial solution S is a finite set of premise-less rules in the form → A.\nThe fact that a metabolite is caused by a network is made precise by means of the following definition\nrelating the metabolite to a chain of reactions that produce it.\nDefinition 2.3 (Explanation) ES,R (C) is an explanation for C ∈ M with respect to S and R if either\n• C ∈ S and ES,R (C) = C[ ], or\n• A1 ◦ A2 → C = r ∈ R, ∃ES,R (A1 ), ES,R (A2 ) and ES,R (C) = Cr [ES,R (A1 ), ES,R (A2 )], or\n• A → C = r ∈ R, ∃ES,R (A) and ES,R (C) = Cr [ES,R (A)].\nNote that a metabolite can be initially present in the solution or be produced anew from the network,\nor both. These cases can be distinguished by the structure of the relative explanations. For simplicity,\nhereafter in the following definitions we only report the case of rules in the form A1 ◦ A2 → C, by leaving\nout the simpler case of rules is in the form A → C, where ES,R (C) = Cr [ES,R (A)]. For observing the\nexplanation structure, we resort to the following auxiliary definition.\nDefinition 2.4 Given an explanation ES,R (C),\n• the set of metabolites required for C, written M (ES,R (C)), is defined as follows:\n\u001a\nC\nif ES,R (C) = C[ ]\nM (ES,R (C)) =\n{A1 , A2 } ∪ M (ES,R (A1 )) ∪ M (ES,R (A2 )) if ES,R (C) = Cr [ES,R (A1 ), ES,R (A2 )]\n• the set of reactions required for C, written R(ES,R (C)), is defined as follows:\n\u001a\nC\nif ES,R (C) = C[ ]\nR(ES,R (C)) =\n{r} ∪ R(ES,R (A1 )) ∪ R(ES,R (A2 )) if ES,R (C) = Cr [ES,R (A1 ), ES,R (A2 )]\nOf course, given S and R, there might be more explanation for the same metabolite C that corresponds\nto different ways to produce it. In turn, the explanation of another metabolite D that uses the metabolite C\nmore than once, could include different explanations for C at different points. For the sake of simplicity,\nwe assume to use only one explanation for each metabolite inside another explanation. For this reason,\nwe introduce the notion of a uniform explanation.\nDefinition 2.5 (Uniform Explanation) An explanation ES,R (C) for C ∈ M w.r.t. S and R is a uniform\nU (C)) if it is an explanation for C ∈ M and ∀D ∈ M (E U (C)), if E (D) and\nexplanation (written ES,R\nS,R\nS,R\n0\n0\nU\nU\nES,R (D) occur in ES,R (C), then ES,R (D) = ES,R (D), i.e. ES,R (C) does not contain two different explanation\nfor the same metabolite D.\n\n\f120\n\nA Taxonomy of Causality-Based Biological Properties\n\nSince we are going to observe the whole set of explanations for each metabolite, in order to characterize our properties, we do not loose generality, by restricting ourselves to uniform explanations. From\nnow on, we will then use only uniform explanations and therefore we will omit the superscript U. The\nfollowing result relates general and uniform explanations.\nU (C) for all C ∈ M.\nTheorem 2.6 Given S and R, we have that ∃ES,R (C) iff ∃ES,R\n\n3\n\nCausality-based properties\n\nSeveral properties regarding metabolic networks, which are widely accepted at an informal level, can be\nmade precise within our framework. Distinguishably, reasoning in terms of explanations adds an extra\nlevel of detail to the definition of the properties of interest, as well as having an explicit characterization\nof the network environment allows us to take into consideration the different conditions under which a\nnetwork may work. We present properties that can be interpreted in terms of our notions of causality\nand explanations and that, given the abstraction of our model, are qualitative properties. We group them\nin properties about reactions and about networks. The first ones allow us to interpret the results of\nperturbative experiments, due to variations of the initial solution S or of the rules in R, while network\nproperties have to do with robustness.\nReaction properties Often, the rules defining the reactions of a metabolic network correspond to enzymes that catalyze such reactions or to genes that code for such enzymes or for the proteins involved\nin reactions. Rules are hence the main object when studying a network behavior and it is quite natural trying to characterize their role in the production of metabolites. The next definition states when a\nrule has to be considered essential for the production of a given metabolite. A rule is essential if it is\nnot dispensable, i.e. the network, deprived of it (e.g. by knocking-out the corresponding gene), is not\nable to produce the metabolite. Generally, in the biological literature, the notion of essentiality has been\nexpressed informally and often referred to the elusive notion of viability of an organism, e.g. [19].\nDefinition 3.1 (Essentiality) Given R, a rule r ∈ R is essential in S for the metabolite C iff ∃ ES,R (C)\nand 6 ∃ ES,R\\r (C).\nNote that if a rule r is essential in S for the metabolite C, then all the explanations of C use r. From a\nbiological point of view, it can be significant to distinguish amongst two degrees of essentiality. In the\nfirst case, essential rules correspond to those reactions whose essentiality holds only in a given solution\nS. Characterizing these “hot points” in a biochemical network operating in a given solution, can be\nuseful when the studied networks are typically resident in a well defined environment. This is the case,\ne.g. , of drug development for cancer therapy, as malignant cells typically live in human blood or intercellular matrix. Essential rules in the metabolic network of malignant cells represent potential targets\nfor anti-cancer drugs designed for disrupting that network. Since cancerous cells always act in a unique\nenvironment, it is important to identify their “weak points” always considering an initial solution S\nresembling the composition of human blood or inter-cellular matrix. In contrast, when the target system\nis an organism capable of living in various environments (such as a bacteria), identifying a stronger kind\nof essentiality, where a rule is essential for all possible solutions S, turns out to be a better choice in order\nto find “universal” targets for inhibiting the production of a given metabolite. Note that verifying this\nsecond kind of essentiality for a certain metabolite C is straightforward, because it simple amounts to\nverifying whether there is only one rule (not having C in the premise) for producing C.\n\n\fC. Bodei, A. Bracciali, D. Chiarugi & R. Gori\n\n121\n\nAlso relationships between rules have been traditionally explored, as has been done with the notion\nof mutual essentiality, e.g. [46]. We say that two rules are mutually essential for C, when their individual\nexclusion does not prevent the production of C, i.e. neither of the two rules is essential, but their simultaneous exclusion does. Detecting mutually essential reactions can be useful, again, in drug research for\nidentifying multiple targets for drugs against parts of a network that represent functional alternatives for\nthe production of a given metabolite.\nDefinition 3.2 (Mutual essentiality) Given R, the rules r1, r2 ∈ R are mutually essential in S for the\nmetabolite C iff ∃ ES,R\\r1 (C) and ∃ ES,R\\r2 (C), while 6 ∃ ES,R\\{r1,r2} (C).\nMoreover, we establish that two explanations for a metabolite C are vicarious when they use different\nsets of rules, thus representing two different ways of producing C.\nDefinition 3.3 (Vicariate) Given R, and S, and a metabolite C, an explanation ES,R (C) is vicarious of\n0\n0\nES,R (C) iff R(ES,R (C)) 6= R(ES,R (C)).\nThis property is related to the previous one, e.g. if two rules r1 and r2 are mutually essential for C,\nthen ES,R\\r1 (C) is vicarious of ES,R\\r2 (C).\nFurthermore, we investigate the order in which different metabolites are produced, and in particular\nwe determine whether the production of a metabolite is a necessary condition (i.e. it is a checkpoint) for\nthe production of another one.\nDefinition 3.4 (Checkpoint) Given R and an initial solution S, B is necessary for C iff for all explanations of C ES,R (C), B ∈ M (ES,R (C)).\nIdentifying checkpoints offers some insights on the structure of metabolic networks. From a topological\npoint of view, checkpoint elements can be related to “bottlenecks” in molecular interaction networks\n[47]. As shown in [47] these elements, due to their strategical position in the network, are candidate for\nbeing essential as well as the reactions through which they are produced.\nSimilarly to the previous property, one can be interested in the order between rules and whether the\napplication of some rules of R it is a necessary condition for the application of other rules.\nDefinition 3.5 (Causality) Given R including r1 and r2 , and an initial solution S, let the metabolite\nC be the conclusion of rule r2 ∈ R. The rule r1 causes r2 (r1 v r2 ) iff for all explanations ES,R (C) =\nCr2 [ES,R (A1 ), ES,R (A2 )], either r1 ∈ R(ES,R (A1 )) or r1 ∈ R(ES,R (A2 )).\nNote that if r1 v r2 then the metabolite produced by rule r1 , say A, is necessary for the metabolite\nproduced by rule r2 , say B, while if A is necessary for B it can be the case that r1 6v r2 . Also this\nproperty can be exploited (eventually synergically with the checkpoint property) to gain topological\ninsights concerning the investigated network. For instance if a rule r causes a group of other rules it is\npossible to say that r acts as a bottleneck.\nThe next property is useful to reason on which metabolites can be omitted from the initial solution,\nwithout compromising the initial capability of the system to produce metabolites in many different ways.\nRoughly speaking a metabolite can be omitted from the initial solution because it not necessary in the\nproduction of a given C or it is necessary but the system is already able to produce it.\nIdentifying these metabolites can aid in metabolic engineering [45], e.g. when for optimizing resources usage is requested to characterize the minimal environment needed for a bioreactor. Note that\nthe so-called conditional mutants differ from the wild type (i.e. the microorganism possessing the genome\ncommonly found in nature) only for the minimal environment needed for their viability. The genome of\nconditional mutants do not code for an enzyme essential for its life and their survival is conditioned by\nthe presence in S of the metabolite produced by the missing reaction.\n\n\f122\n\nA Taxonomy of Causality-Based Biological Properties\n\nDefinition 3.6 (Redundancy) Given R and a metabolite C, an initial solution S is redundant for C 6∈ S\niff there exists at least a metabolite B ∈ S s.t. for all ES,R (C) for C, there exists ES\\{B},R (C) such that\nR(ES,R (C)) ⊆ R(ES\\{B},R (C)) .\nAccording to this definition, a given initial solution S can be redundant for a metabolite C in the\nwild-type but not redundant for the same metabolite in the conditional mutant. Of course, the previous\nproperty can be weakened obtaining another property that checks whether R is still able to produce C\nafter the exclusion of the reagent B from the initial solution. In other words, it addresses the impact of\nthe exclusion of some metabolites from the initial solution, offering straightforward applications to the\nresource optimization problem described above.\nDefinition 3.7 (Exclusion) Given R, a metabolite B ∈ S cannot be excluded for the production of\nmetabolite C 6∈ S iff 6 ∃ ES\\{B},R (C).\nNote that B cannot be excluded for the production of C if and only if for all explanations ES,R (C), B[]\ndoes occur in ES,R (C). The previous property is in some way related to the checkpoint property expressed\nbefore: if B belongs to S and is not necessary for the production of C, then we can harmlessly exclude it\nfrom the initial solution. However, in general, the two properties do not coincide (see Ex. 4.13).\nMetabolic network properties Informally, robustness can be defined as the capability of a whole network of resisting to damages. In the biological literature there is not a common agreement on what\n“robustness” exactly means [25]. One of the most used definition says: “robustness is a property that\nallows a system to maintain its functions against internal and external perturbations.” ([24]). A similar\ndefinition [44] is also widely used: “robustness, the ability to maintain performance in the face of perturbations and uncertainty, is a long-recognized key property of living systems”. Both definitions, however,\nresult to be not well assessed and therefore open to different possible interpretations. Moreover, the notion of robustness is related to the maintenance of a “function” or of “performance”. Both these concepts\nsubsume quantitative issues and their exact meaning change depending on the work considered. The\nuncertainty in definitions makes difficult both evaluating robustness effectively and comparing different\nnetworks addressing this property. A more reliable way to assess this notion consists in considering the\nqualitative features of the network in hand rather than its quantitative throughput [1, 43]. The notion of\nnetwork robustness can be linked to the overall error tolerance, seen as the capability of carrying information in spite of local failures which, in turn, depend critically on the topology of network wiring [1].\nIn our framework this corresponds to evaluate the resistance to failures in terms of the maintenance of\nthe capability of producing a given metabolite. This paradigm-shift allows us to propose the following\nformal notions of robustness.\nDefinition 3.8 (Strong Robustness) Given two m networks R1 and R2 , R2 is strongly more robust than\nR1 in S for C, written R1 \u001cS,C R2 iff\n∃ ES,R1 (C) ⇒ ∃ ES,R2 (C) ∧ R(ES,R1 (C)) = R(ES,R2 (C)).\nThis is quite a strong requirement, accounting to say that all the rules used for producing P in R1 , are\npresent in R2 and can be used as well. Of course, R2 may also allow more explanations, using different\nrules. This consideration leads us to formulate a weaker property, by requiring that R2 is able to produce\nthe same metabolite, without constraints on the rules to be used.\nDefinition 3.9 (Weak Robustness) Given two m networks R1 and R2 , R2 is weakly more robust than R1\nin S for C, written R1 ≺S,C R2 iff\n∃ ES,R1 (C) ⇒ ∃ ES,R2 (C)\n\n\fC. Bodei, A. Bracciali, D. Chiarugi & R. Gori\n\nR\nM\nS\nπ\nCGF\n\n::= 0 | A = M, R\n::= 0 | π λ .S + M\n::= 0 | A|S\n::= a | a | τ\n::= (R, S)\n\nReagents Environment\nMolecule\nSolution\nBasic Action\nChemical Ground Form\n\n123\n\n(empty, or a reagent and Reagents Env)\n(empty, or an interaction and Molecule)\n(empty, or a variable and Solution)\n(input, output, delay)\n(reagent environment with initial Solution)\n\nTable 1: Syntax of simplified CGF\n\n4\n\nVerification Methodology\n\nOur methodology is based on the construction of an abstract model of the biological system. The model is\nobtained by a new abstract semantics of the system, interpreted as a concurrency network and expressed\nusing the Chemical Ground Form (CGF) [7] calculus. We exploit the notion of path for verifying our\nproperties. The CGF is a fragment of the stochastic π-calculus [35, 32]. Since we abstract away from\nquantities, we resort to a simplified version of CGF, in which stochastic features, like action rates, are\ndiscarded. In particular, we abstract away from the version of CGF, presented in [10, 22], because we\nrepresent solutions as sets of reagents rather than multisets. The syntax of CGF is defined in Tab. 1. We\nconsider a set of Names (ranged over by a, b, c, . . .), a set of labels L (ranged over by λ , µ . . .), and a\nset Mol (ranged over by A,B,....) of variables (the reagents). A CGF specification is composed by a\n(finite) list of reagent definitions Ai = Mi , where Ai is a variable that stands for the name of a chemical\nspecies and Mi is a molecule that describes the interaction capabilities of the corresponding species. The\nenvironment R defines the reagents of a solution S. A molecule M may do nothing, or may change after\na delay (e.g. because of a molecular decay) or may interact with other reagents. A standard notation is\nadopted: τ represents a delay; a and a model interaction over a shared channel a (the input and output,\nrespectively). Together with the reagents definition, a CGF includes a solution S, that represents the\ninitial conditions and is described by a parallel composition of variables, i.e. a finite list of reagents. This\nmaps onto the initial solution from Def. 2.2.\nIn order to distinguish the actions that participate to a move, we label them. In a CGF (R, S), R is\nwell-labeled, if basic action labels are all distinct. We assume to have well-defined reagents environment\nand, given R, to have a definition for each variable A in R or in S. Moreover, given a label λ ∈ L , we\nuse the notation R.A.λ to indicate the process π λ .S, provided that A = . . . + π λ .S + . . . is the definition\nof A occurring in R. Finally, given a CGF (R, S), we denote with > the set of all molecules occurring in\neither S or in the rules of R. In the following, we will use Sol for the domain of sets of reagents. There\nis one transition rule for delay actions and one for synchronizations. Transition are of the form\nΘ, Ŝ,X\nc= L ∪ (L × L )\nS −−−→ S0 with S, S0 , Ŝ ∈ Sol, X ∈ Mol, Θ ∈ L\n\n• Θ reports the label(s) of the basic action(s), which participate to the move,\n• Ŝ ⊆ S0 reports the subset of the reagents of the initial solution directly involved in the current move\n• X reports the unique reagent produced by the move (i.e. by the corresponding reaction).\nThe Rule (Delay) models the move of a process τ λ .Q appearing in the definition of a reagent A. The\ntransition records the label λ , the singleton A if A belongs to the initial solution S0 and the reagent\nproduced by the reaction. The Rule (Sync) models the synchronization between two processes aλ .Q1\n\n\f124\n\nA Taxonomy of Causality-Based Biological Properties\n\nt1, t2= (β,γ),{B},A\nS0\n\nt1=(λ,μ),{A,B},D\n\nS1\n\nt1,t2,t3\n\nt3=ξ,{},C\n\nt1,t2,t3,t4,t5\nt4=(ψ,ν),{},E\nt5=(δ,η),{A},E\n\nS2\n\nS3\n\nFigure 1: LTS Graph of Ex. 4.1, where S0 = {A, B}, S1 = S0 ∪ {D}, S2 = S1 ∪ {C}, S3 = S2 ∪ {E}\nand aµ .0 occurring in the definition of A and B, resp. The transition records the label pair (λ , µ), together\nwith the part of S0 used in the rule (i.e. {A, B}∩S0 ) and the possibly new reagent produced by the reaction.\n(Delay)\n\nR.A.λ =τ λ .Q\nλ , {A}∩S0 ,Q\n\nS−\n−−−−−−→S∪{Q}\n\n(Sync)\n\nR.A.λ =aλ .Q\n\nR.B.µ=aµ .0\n\n(λ ,µ), {A,B}∩S0 , Q\n\nS−−−−−−−−−−→S∪{Q}\n\nWe denote with Tr((R, S0 )) = (S , →, S0 , R) the labeled transition system (LTS), obtained, starting from\nthe initial state S0 ∈ S , w.r.t. to environment R, and with Gr((R, S0 )) the corresponding graph. Since\nenvironments are well-labeled, different transitions leaving from the same state carry distinct labels.\nc. In our model, a reaction r : A ◦ B → D\nFor simplicity, we identify each reaction by a label Θ ∈ L\ncan be identified by (λ , µ) and rendered by the following reagent definitions:\nA = aλ .D\nB = aµ .0\n(λ ,µ),{A,B},D\n\nGiven an initial solution S0 = {A, B}, the system may perform the transition {A, B} −−−−−−−−→\n{A, B, D}, since Ŝ = {A, B} ∩ {A, B} and S0 = S0 ∪ {D}. If A and B are involved in other reactions, other\nactions can be added in their specifications, as in the example below, where we illustrate our approach\non a toy reaction network.\nExample 4.1 Consider the initial solution S0 = {A, B} and an m-network R, consisting of the rules\nreported below, on the left-hand side.\n(λ , µ) A ◦ B\n(δ , η) A ◦C\n(β , γ) D ◦ B\nξ\nD\n(ψ, ν) D ◦C\n\n→ D\n→ E\n→ A\n→ C\n→ E\n\nη\n\nA = aλ .D + b .0\nB = aµ .0 + d β .A\nγ\nD = τ ξ .C + cν .0 + d .0\nC = bδ .E + cψ .E\n\nThe corresponding CGF specification is above on the right, while the corresponding graph is in Fig.1.\nFor simplicity, in the presence of multiple self-loops, we collapse the self-loop arcs in a single one.\n• Starting from S0 = {A, B}, the only possible transition (here called t1 ) is the one that uses rule (λ , µ)\nand leads to the state S1 containing D. After this,\n• either we can fire transition t3 (rule ξ ), that leads to S2 , that includes C;\n• or, we can fire transition t2 (rule (β , γ)) that leads to S1 , where A is already present.\n• From S2 , both transitions t4 (rule (δ , η)) and t5 (rule (ψ, ν)) are possible, lead to S3 and produce E.\n• Intuitively, we can observe that some transitions, cause some others: t1 causes t2 , t3 t4 and t5 , t3 causes\nboth t4 and t5 , while t4 and t5 are independent from each other.\n\n\fC. Bodei, A. Bracciali, D. Chiarugi & R. Gori\n\n125\nt\n\nt\n\nt\n\n3\n4\n1\nS3 that corresponds to the\nS2 →\nS1 →\n• We have two paths reaching a state that includes E: S0 →\nt5\nt3\nt1\nexplanation E(δ ,η) [A[],Cξ [D(λ ,µ) [A[], B[]]]], and S0 → S1 → S2 → S3 , that corresponds to the explanation\nE(ψ,ν) [D(λ ,µ) [A[], B[]],Cξ [D(λ ,µ) [A[], B[]]].\n• Establishing which metabolites in S0 are used in each transition, can be useful to investigate their\nimpact on the production of the other metabolites. For instance, A is necessary for the production of C\nand E, because both the states including C and E are reached, using t1 (rule (λ , µ)), that requires A.\n\nIn the following, we are going to make precise the notions only informally introduced in Ex. 4.1. Note\nthat self-loops can correspond either to the application of a rule already applied or to the application of\na rule, that has not already been applied, but that produces a metabolite already present. Self-loops of\nthe first kind do not add any useful information from a causality point of view, in terms of the properties\nintroduced in the previous sections. Self-loops of the second kind can be instead useful, especially if\nwe are interested in checking the possibility of the system to produce a certain metabolite even if it is\nalready present in the initial solution. We first focus on the computation paths not including self-loops\nat all, that we call causally relevant paths. This notion is used to verify many properties of the previous\nsection.\nDefinition 4.2 (χ-path) A path p in Gr((R, S0 )) is a causally relevant path (χ-path) if\nΘ0 , Ŝ0 , X0\n\nΘ1 , Ŝ1 , X1\n\nΘm−1 , Ŝm−1 , Xm−1\n\np = S0 −−−−−→ S1 −−−−−→ S2 ...Sm−1 −−−−−−−−−→ Sm and Si 6= Si−1 for all i ∈ [1, m]\nWe say that p leads to C if C = Xm−1 (i.e. if Sm is the first state including C).\nTheorem 4.3 (Correspondence) Given a χ-path p in Gr((R, S0 )) that leads to C\nΘ0 , Ŝ0 , X0\n\nΘ1 , Ŝ1 , X1\n\nΘm−1 , Ŝm−1 ,Xm−1\n\np = S0 −−−−−→ S1 −−−−−→ S2 ....Sm−1 −−−−−−−−−→ Sm\nlet tr p () be the function, which for a given path p and reagent B is defined as follows:\n\n BΘ [tr p (A1 ),tr p (A2 )] if ∃ i ∈ [1, m]. Xi−1 = B, and Θ : A1 ◦ A2 → B\nis the rule applied in the transition ti ,\ntr p (B) =\n\nB[ ] if B ∈ S0 .\nWe obtain an explanation ES0 ,R (C) for C, as tr p (C), which uses the same rules of p.\nMoreover, given an explanation ES0 ,R (P) we can construct a set of corresponding paths, starting from\nthe subset of the initial solution used in the explanation, i.e. all the metabolites A such that A[] occurs in\nES0 ,R (P). We then proceed by exploring the explanation structure from innermost outermost and therefore\nfiring the transitions corresponding to the rules used in the explanation. Note that, serializing the possible\nparallelism of the explanation can give rise to a set of paths rather than to a unique path. For instance,\nif we start from an explanation CΘ1 [AΘ2 [B[], D[]], FΘ3 [E[], G[]]], corresponding to the application of rules\nΘ1 = A ◦ F → C, Θ2 = B ◦ D → A and Θ3 = E ◦ G → F, then we have two corresponding paths, where\nthe order in which the transitions occur is different. Note that paths obtained by ES0 ,R (P) are χ-paths.\nDefinition 4.4 (ρ-path) A path p in Gr((R, S0 )) is a relevant path (ρ-path) if\nΘ0 , Ŝ0 , X0\n\nΘ1 , Ŝ1 , X1\n\nΘm−1 , Ŝm−1 , Xm−1\n\np = S0 −−−−−→ S1 −−−−−→ S2 ...Sm−1 −−−−−−−−−→ Sm and ∀ j ∈ [1, m] S j = S j−1 ⇒\n(i) X j ∈ S0\nS\n(ii) {X j } ∩ ( 0≤i< j Xi ) = 0/\nS\n(iii) {X j } ∩ ( 0≤i< j Ŝi ) = 0/\nWe say that p leads to C if C = Xm−1 .\n\n(the produced metabolite was already in S0 ),\n(the produced metabolite was never produced before),\n(the produced metabolite was never required before).\n\n\f126\n\nA Taxonomy of Causality-Based Biological Properties\n\nIntuitively, conditions (i)-(iii) will aid us to determine which metabolites could harmlessly excluded from\nthe initial solution, identifying the metabolites that the system itself is able to produce before they are\nrequired. Note that ρ-paths as well as χ-paths are always finite: by definition, a self-loop transition can\nbe included in a ρ-path at most once. In particular, each χ-path is also a ρ-path.\nWe are now ready to characterize all the properties introduced in §3, in terms of χ- and ρ-paths. A\nrule Θ is essential for the metabolite C if every χ-path leading to C includes Θ, while the rules Θ1 and\nΘ2 are mutually essential for C if every χ-path leading to C includes at least one of the two rules.\n• A rule Θ, is essential in S0 for a reagent\n\nTheorem 4.5 (Essentiality and Mutual Essentiality)\n\nΘm−1 , Ŝm−1 , Xm−1\n\nΘ1 , Ŝ1 , X1\n\nΘ0 , Ŝ0 , X0\n\nC 6∈ S0 iff ∀ χ-path S0 −−−−−→ S1 −−−−−→ S2 .... −−−−−−−−−→ Sm in Gr((R, S0 )), leading to C,\nthere exists at least an i ∈ [0, m − 1] : Θi = Θ.\n• Two rules Θ1 and Θ2 are mutually essential in S0 for a reagent C 6∈ S0 iff\n– neither Θ1 nor Θ2 are essential in S0 for C, and\nΘ0 , Ŝ0 , X0\n\nΘ1 , Ŝ1 , X1\n\nΘm−1 , Ŝm−1 , Xm−1\n\n– ∀ χ-path S0 −−−−−→ S1 −−−−−→ S2 .... −−−−−−−−−→ Sm in Gr((R, S0 )), leading to C, there\nexists at least an i ∈ [0, m − 1] s.t. Θi = Θ1 or Θi = Θ2 .\nIn this context, two χ-paths leading to C represent vicarious explanations if the χ-paths resort to\ndifferent sets of rules.\nTheorem 4.6 (Vicariate) Given two χ-paths p1 and p2 in Gr((R, S0 )), leading to C\nΘ1 , Ŝ1 , X 1\n\nΘ1 , Ŝ1 , X 1\n\n1 ,X\nΘ1h−1 , Ŝh−1\nh−1\n\nΘ2 , Ŝ2 , X 2\n\nΘ2 , Ŝ2 , X 2\n\nΘ2 2\n\n0 0\n1 1\np1 = S0 −−0−−\n−→ S11 −−1−−\n−→ S21 .... −−−−−−−−−→ Sh1\nm −1\n\n2 ,X\n, Ŝk−1\nk−1\n\n0 0\n1 1\np2 = S0 −−0−−\n−→ S12 −−1−−\n−→ S22 .... −−−−−−−−−→ Sk2\nS\np1 is vicarious of p2 iff either h 6= k or there exists at least a j s.t. Θ1j 6∈ 0≤i<h {Θ2i }.\n\nTo prove checkpoint properties, we exploit the information recorded in Ŝ, in order to check whether\na certain metabolite B is necessary in the production of a reagent C.\nTheorem 4.7 (Checkpoint) Given R and an initial solution S0 , reagent B is necessary for the production\nΘ0 , Ŝ0 , X0\n\nΘ1 , Ŝ1 , X1\n\nΘm−1 , Ŝm−1 , Xm−1\n\nof C if for all χ-path S0 −−−−−→ S1 −−−−−→ ....Sm−1 −−−−−−−−−→ Sm in Gr((R, S0 )), leading to C, then\n(i) B ∈ Sm−1 and (ii) if B ∈ S0 , then B ∈ (Ŝ0 ∪ ... ∪ Ŝm−1 ).\nConditions (i) and (ii) amount to saying that there is a rule Θi that has B in its premises.\nExample 4.8 Consider the initial solution S0 = {A, B, D, O} and an m-network R, consisting of the rules\nreported below on the left, while the corresponding CGF specification is on the right.\n(λ , µ) A ◦ B\n(δ , η) C ◦ F\n(β , γ) D ◦ A\n(ξ , θ ) B ◦ D\n(ψ, ν) D ◦ H\n(σ , ρ) L ◦ O\n(φ , π) E ◦ H\n(o, ι) C ◦ D\n(α, ζ ) P ◦ O\n\n→\n→\n→\n→\n→\n→\n→\n→\n→\n\nC\nP\nF\nH\nE\nH\nL\nO\nE\n\nA = aλ .C + cγ .0\nB = aµ .0 + d ξ .H\nC = bδ .P + go .O\nθ\nD = cβ .F + d .0 + eψ .E + gι .0\nE = hπ .L\nη\nF = b .0\nπ\nH = eν .0 + h .0\nσ\nL = f .H\nρ\n\nζ\n\nO = f .0 + l .0\nP = l α .E\n\n\fC. Bodei, A. Bracciali, D. Chiarugi & R. Gori\n\n127\n\nt1,t2,t7,t8,t9\nt1,t2,t7,t8\nt9=(α,ζ),{O},E\n\nS13\n\nS8\nt7=(ο,ι),{D},O\nt1\n\nt1=(λ,μ),{A,B}C\n\nS1\n\nt3\n\nt8=(δ,η),{},P\nt1,t2,t7\nt2\n\nS4\n\nS9\n\nt3\n\nt8\n\nS14\n\nt4\nt1,t3,t7\n\nt2\n\nt1,t2,t3,t7,t8 t3\n\nt1,t2,t3,t7\nt3\n\nt1,t3,t4,t7\n\nt1,t2,t3,t4,t7\n\nt1\nS0\n\nt2=(β,γ),{A,D},F\nt3= (ξ,θ),{B,D},H\n\nS2\n\nS5\nt3\n\nt3\n\nt1\n\nS3\n\nt2\nt4=(ψν),{D},E\n\nt4\nS10\n\nt1\n\nt2\nt5\n\nt2,t3,t4\nS11\n\nt1\nt3,t4\nS6\n\nt2\n\nt6=(σ,ρ),{O},H\nt3,t4,t5\n\nt5=(φ,π),{},L\n\nt1,t2,t3,t4,t5,t6,t7,t8,t9\nt5\nS20\n\nt8\nt5\nt1,t2,t3,t4,t5,t6,t7\nt1,t3,t4,t5,t7,t6\n\nt1\n\nt4\n\nS18\n\nt8\n\nS15\n\nt3,t2\nS7\n\nt1,t2,t3,t4,t7,t8,t9\nt4\nt9\n\nt2\n\nt1\n\nS19\n\nS16\n\nt5\n\nt1\nt2,t3,t4,t5,t6\n\nt2\n\nS17\n\nS12\n\nFigure 2: LTS Graph of Ex. 4.8\nFigure 2 depicts the LTS semantics where S0 = {A, B, D, O}, S20 = >, and\nS1 = S0 ∪ {C}\nS7 = S3 ∪ {F}\nS13 = S8 ∪ {E}\nS19 = S16 ∪ {F}\n\nS2 = S0 ∪ {F}\nS8 = S4 ∪ {P}\nS14 = S8 ∪ {H}\n\nS3 = S0 ∪ {H}\nS9 = S4 ∪ {H}\nS15 = S9 ∪ {E}\n\nS4 = S1 ∪ {F}\nS10 = S5 ∪ {E\nS16 = S10 ∪ {L}\n\nS5 = S3 ∪ {C}\nS11 = S6 ∪ {F}\nS17 = S11 ∪ {L}\n\nS6 = S3 ∪ {E}\nS12 = S6 ∪ {L}\nS18 = S13 ∪ {H}\n\nWe can observe the following properties.\n• The production of H is necessary for that of L, indeed H ∩ S0 = 0/ and all the states containing L come\nafter states that include H.\n• Rule (ξ , θ ) is essential for the production of H. Actually, also rule (σ , ρ) is able to produce H, but it\nrequires the presence of L that in turn requires H to be produced, as discussed before.\n• Neither rule (ψ, ν), nor rule (α, ζ ) is essential for the production of E, because there exists\nt2\nt1\na χ-path p (p0 ) leading to E, which does not use rule (ψ, ν) ((α, ζ ), resp.): p = S0 →\nS2 →\nt9\nt8\nt3\nt4\nS8 →\nS3 →\nS7 . Nevertheless, rules (ψ, ν) and (α, ζ ) are mutually essenS4 →\nS13 , p0 = S0 →\ntial. As expected, p and p0 represent therefore two alternative ways of producing E. The corresponding explanations: E(α,ζ ) [P(δ ,η) [C(λ ,µ) [A[], B[]], F(β ,γ) [D[], A[]]], O[C(λ ,µ) [A[], B[]], D[]]]] (for p) and\nE(ψ,ν) [D[], H(ξ ,θ ) [B[], D[]]] (for p0 ) represent two vicarious explanations for E.\n• Note that if we exclude A from the initial solution, we cannot produce F, because rule (β , γ) could not\nbe applied, but we still have a way to produce E.\nWe now characterize the causality and the robustness properties in our computational framework.\nWe say that a rule causes another rule, whenever the second one is always preceded by the first one.\nDefinition 4.9 (Causality) Let Θ, Θ0 two rules used in Gr((R, S0 )). The rule Θ causes Θ0 (Θ v Θ0 ) in\nΘ0 , Ŝ0 , X0\n\nΘ1 , Ŝ1 , X1\n\nΘm−1 , Ŝm−1 , Xm−1\n\nGr((R, S0 )) iff for all χ-path in Gr((R, S0 )) p = S0 −−−−−→ S1 −−−−−→ S2 .... −−−−−−−−−→ Sm ,\n(Θ0 = Θ j ) ⇒ ∃Θi = Θ with i < j.\n\n\f128\n\nA Taxonomy of Causality-Based Biological Properties\nRobustness has to do with the capacity of a network to produce a certain metabolite.\n\nTheorem 4.10 (Strong and Weak Robustness) Given two environments R1 and R2 ,\n• R1 \u001cS,P R2 for C, iff for all χ-path p ∈ Gr((R1 , S0 )) leading to C, p ∈ Gr((R2 , S0 )) and leads to C.\n• R1 ≺S,P R2 for C iff for all χ-path p ∈ Gr((R1 , S0 )) leading to C, then there exists a χ-path p0 ∈\nGr((R2 , S0 )) leading to C.\nFinally, we characterize the redundancy and the exclusion properties. Both are related with the role\nof initial metabolites and the possibilities of the network to produce metabolites, in case of modifications\nof the initial set. To this aim we resort to ρ-paths and to the following notion, that given a ρ-path\np, computes the subset of metabolites U (p) of the initial solution strictly required to perform each\ntransition of the given path. Such information is obtained by collecting all the subsets Ŝi (i.e. the subsets\nof the initial solution S0 used by the transitions in p), and by removing those metabolites (in S0 ) that the\nsystem itself is able to produce along the path. Recall that since p is a ρ-path, we are guaranteed that the\ntransitions that produce these metabolites always come before the transitions that use them.\nΘ0 , Ŝ0 , X0\n\nΘ1 , Ŝ1 , X1\n\nΘm−1 , Ŝm−1 , Xm−1\n\nDefinition 4.11 Given a ρ-path in Gr((R, S0 )), p = S0 −−−−−→ S1 −−−−−→ S2 ...Sm−1 −−−−−−−−−→ Sm\nS\nS\nU (p) = ( 0≤i<m Ŝi \\ 0≤i<m {Xi })\nAn initial solution is redundant for the production of a metabolite C, whenever there exists at least\na component that is not required from the very beginning, in all the paths that lead to C. Moreover, to\nproduce a metabolite C, we can exclude a metabolite B from the initial solution S0 , if B is not required\nfrom the very beginning, in at least one path that leads to C.\nTheorem 4.12 (Redundancy and Exclusion) Given an environment R, an initial solution S0 , and a\nmetabolite C 6∈ S0 , let PC = {p | p is ρ-path in Gr((R, S0 )) that leads to C}. Then\n• S0 is redundant for C iff\n\nS\n\np∈PC U\n\n(p) ⊂ S0 .\n\n• a metabolite B can be excluded for the production of C iff ∃ a ρ-path p in PC s.t. B 6∈ U (p).\nExample 4.13 Consider again the network described in Ex. 4.8.\n• The initial solution S0 = {A, B, D, O} is redundant for the production of E. Consider indeed the ρ-path\nt9\nt8\nt7\nt1\nt1\nleading to E: p1 = S0 →\nS1 →\nS1 →\nS4 →\nS8 →\nS13 . Note that p1 is similar to the χ-path p, seen in\nEx. 4.8, except that it also includes the self-loop transition t7 (rule (o, ι)) on the state S1 . This transition\ncorresponds to a reaction that produces O, which is already in S0 , but that it is not required until this\npoint. Therefore O could safely be excluded from S0 , since O 6∈ U (p1 ). Similarly, we can prove that\nO 6∈ U (p) for all the other paths that reach S13 and all its successors.\nt3\nt4\n• Consider again the χ-path p0 = S0 →\nS3 →\nS7 , leading to E. Note that p0 is also a ρ-path and that\n0\nO 6∈ U (p ). The same result holds for all the paths reaching S13 and therefore the successor states.\nHence, by Theorem 4.12 we can conclude that the metabolite O could safely be excluded from the initial\nsolution without compromising the production of the metabolite E. If we are not interested in maintaining\nall the ways to produce E, but just the general ability of the system to produce it, we can exclude A, since\np0 is a ρ-path leading to E and A 6∈ U (p0 ).\n• Note that in this case, checkpoint and exclusion properties rely on the same information: we could have\ndetected indeed that A could be excluded from the fact that A was not necessary for the production of E.\nHowever, this is not true in general. Assume, e.g., to modify rule (ψ, ν) in order to require the presence\nof O, as (ψ, ν)0 : O + H → E. As a consequence, also the paths leading to state S7 require the presence\nof O, making also O necessary for the production of E. However, we can conclude that while S0 is not\n\n\fC. Bodei, A. Bracciali, D. Chiarugi & R. Gori\n(1)\nβ -D-Glucose ◦ AT P\n(2)\nβ -D-Glucose-6P\n(3)\nβ -D-Fructose-6P ◦ AT P\n(4)\nβ -D-Fructose-1,6bP\n(5)\nGlyceraldehyde-3-P\n(6)\nDihydroxyacetonephosphate\n(7)\nGlyceraldehyde-3-P ◦ NAD\n(8)\n1, 3 Bisphosphoglycerate ◦ ADP\n(9)\n3-Phosphoglycerate\n(10)\n2-Phosphoglycerate\n(11)\nPhosphoenol pyruvate ◦ ADP\n(12)\nβ -D-Glucose ◦ NADP+\n(13)\nD-Glucono-1,5-Lactone-6P\n(14)\n6-Phospo-D-Gluconate ◦ NADP+\n(15)\nRibulose-5-P\n(16)\nRibulose-5-P\n(17)\nD-Ribose-5P ◦ D-Xylulose-5P\n(18) D-sedoeptulose-7-P ◦ Glyceraldehyde-3-P\n(19)\nD-Erythrose-4P ◦ D-Xylulose-5-P\n\n129\n→\n→\n→\n→\n→\n→\n→\n→\n→\n→\n→\n→\n→\n→\n→\n→\n→\n→\n→\n\nβ -D-Glucose-6P ◦ ADP\nβ -D-Fructose-6P\nβ -D-Fructose-1,6bP ◦ ADP\nGlyceraldehyde-3-P ◦ Dihydroxyacetonephosphate\nDihydroxyacetonephosphate\nGlyceraldehyde-3-P\n1, 3 Bisphosphoglycerate ◦ NADH\n3-Phosphoglycerate ◦ AT P\n2-Phosphoglycerate\nPhosphoenol pyruvate\nPyruvate ◦ AT P\nD-Glucono-1,5-Lactone-6P ◦ NADPH\n6-Phospo-D-Gluconate\nRibulose-5-P ◦ NADPH\nD-Xylulose-5-P\nD-Ribose-5P\nGlyceraldehyde-3-P ◦ D-sedoeptulose-7-P\nD-Erythrose-4P ◦ D-Fructose-6-P\nGlyceraldehyde-3-P ◦ β -D-Fructose-6P\n\nTable 2: Rules of the Glycolytic Pathway and of the Pentose Phosphate Pathway\nredundant for the production of E in the modified system, considering p0 above, O could be excluded,\nsince throughout p0 the modified system is still able to produce E.\n• Finally note that (λ , µ) v (φ , π), (β , γ) v (φ , π) while neither (λ , µ) v (β , γ) nor (β , γ) v (λ , µ).\nIndeed, the transitions related to the application of rules (λ , µ) and (β , γ) (t1 and t2 resp.) are not\ncausally related, hence, they can be fired in any order.\n\n5\n\nProperties at work in a metabolic pathway\n\nA precise characterization of the structural role played by the single elements in the overall metabolic\nnetworks is relevant both for better understanding living systems and for developing treatments for pathological aspects. As an example consider the clinical studies of primary and metastatic cancers that have\nclearly demonstrated that human malignancies are characterized by an increased activity of glycolysis\nwhen compared to normal tissue [17]. This metabolic peculiarity suggests an inviting target for cancer treatment and various therapeutic strategies aiming at selectively disrupting glycolytic network of\nmalignant cells are under investigation [18].\nIn this light, we present a simplified glycolytic pathway embedded in a wider context comprising\nalso the Penthose Phosphate Pathway. Through these interconnected pathways the β -D-Glucose-6P is\noxidized yielding Pyruvate and energy (ATP) or Ribose and reducing equivalents (NADPH).\nThe pathway can be formalized as in Tab. 2. For lack of space we do not show here the corresponding\nLTS graph, however it should be clear how our properties, related with very important biological features,\ncan be verified using the method illustrated in § 4 (see in particular Ex. 4.8 and 4.13).\nReaction properties Suppose that our initial solution is Sα : {β -D-Glucose, AT P, NADP+ , NAD}; we\ncan verify the following properties.\n• The existence of the following causal chains of rules: (1) v (2) v ... v (11) and (12) v ... v (15).\n\n\f130\n\nA Taxonomy of Causality-Based Biological Properties\n\n• Rule (7) is essential for the production of the metabolite Pyruvate. Its exclusion interrupts all the\npossible paths reaching Pyruvate.\n• Rule (12) is essential for the production of various metabolites, e.g. NADPH and D-Erythrose-4P.\n• The metabolite D-Glucono-1,5-Lactone-6P is a checkpoint for the production of NADPH, produced\nboth by the rules (12) and (14), and for that of D-Xylulose-5-P, produced both by the rules (15) and (17).\nThe rule (12) which produces D-Glucono-1,5-Lactone-6P corresponds to the reaction catalyzed by the\nenzyme Glucose-6-Phosphate Dehydrogenase(G6PD), an enzymopathy commonly known as fauvism.\n• The metabolite Glyceraldehyde-3-P can be produced either by the χ-path composed by the transitions\ncorresponding to the rules: (12), (13), (14), (15), (16), (17), or by the χ-path composed by the transitions\ncorresponding to the rules: (1), (2), (3), (4). The two paths correspond to two vicarious explanations.\n• The metabolite NADP+ should be included in the initial solution in order to produce D-Xylulose-5-P,\nbut it can be excluded as far as the production of β -D-Fructose-1,6bP is concerned.\n• Finally, having the initial solution Sβ = Sα ∪ {Glyceraldehyde-3-P}, the rules (4) and (5) are mutually\nessential for Pyruvate as they must be both removed in order to suppress the Dihydroxyacetonephosphate\nproduction. Another example of mutually essential rules is given in [5].\nNetwork properties In order to illustrate our definition of robustness, we consider two pathways: the\npathway described above and another one, obtained from the first one by suppressing the two reactions,\none inverse of the other, represented by rules (5) and (6). This suppression corresponds to the inhibition\nof an enzyme, that is related to a severe disease, known as triosephosphate isomerase (TPI) deficiency,\nsee [31] for details. Considering the standard solution Sα , it is easy to verify that the glycolytic pathway results to be more robust than its variant related to the disease with respect to the production of\nPyruvate: only one of the explanations for Pyruvate existing in the original network is viable in the\nsecond one. This simple example well highlights the relevance that a study of robustness may have.\nQuite naturally, this notion can be extended in order to consider robustness with respect to different solutions or with respect to different metabolites of the same network. About the latter, intuitively, it turns\nout that there are at least two explanations for metabolites like Glyceraldehyde-3-P, 2-Phosphoglycerate\nand Pyruvate, given the solution Sα in the glycolytic pathway. Instead, only one explanation exists for\nmetabolites like β -D-Glucose-6P or. Therefore, the network results more robust for the production of\nGlyceraldehyde-3-P, 2-Phosphoglycerate and Pyruvate rather than for that of β -D-Glucose-6P. From a\ndrug research point of view, targeting the parts of the network involved in the production of the last two\nmetabolites may result more effective than targeting the others. Indeed, a drug targeting the reaction of\nhexokinase, leading to the production of β -D-Glucose-6P is under development [18].\n\n6\n\nConclusions\n\nWe have presented a taxonomy of biological properties of interest regarding metabolic network. Based\non a (formal) notion of causality, this taxonomy translates a bunch of properties in use within biologists\ninto a formal framework. We have also proposed a computational counterpart of the framework, which,\nallowing the automated verification of the properties, paves the way to the development of software\ntools supporting the analysis of metabolic networks. We have chosen a reading of causality and the\ncomputational mechanisms that rely on theories developed in concurrency and particularly suitable to\ndescribe causality in interactive behaviours and providing a wealth of analysis techniques. Definitions\ndo not depend on the computational framework, and this can be changed whenever another computational\nsupport may result more convenient for a specific domain or set of (causally-based) properties.\nFuture work regards the extension of the set of proposed properties, experimentation with case-\n\n\fC. Bodei, A. Bracciali, D. Chiarugi & R. Gori\n\n131\n\nstudies of interest for wet-lab research, possibly contrasting the framework with other analogous proposals especially as far as the trade-off between expressiveness and efficiency is concerned. Moreover, we\nwould like to attempt a characterization of properties of sets of interconnected signaling pathways, like\nthe ones involved in cancerogenesis, since the understanding of the structural features underlying their\ninteractions may provide useful hints for drug research. In this sense, it could be worth studing possible\nintegrations of our framework with the qualitative logical view adopted in [3] for signaling networks.\n\nReferences\n[1] R. Albert, H. Jeong & A. Barabasi (2000): Error and attack tolerance of complex networks. Nature 406.\n[2] U. Alon (2007): Network motifs: theory and experimental approaches. Nature reviews Genetics 8.\n[3] C. Baral, W. Kolch, C. Shankland & N. Tran (2005): Reasoning about the ERK signal transduction pathway\nusing BioSigNet-RR. In: Proc. of CMSB’05.\n[4] C. Bodei (2009): A Control Flow Analysis for Beta-binders with and without static compartments. Theoretical Computer Science 410(33-34).\n[5] C. Bodei, A. Bracciali & D. Chiarugi (2008): On deducing causality in metabolic networks. BMC Bioinformatics 9 (4).\n[6] L. Cardelli (2005): Brane calculi. In: Proc. of Computational Methods in Systems Biology (CMSB’04),\nLNCS 3082. Springer, pp. 257–280.\n[7] L. Cardelli (2008): On process rate semantics. Theoretical Computer Science 391(3), pp. 190–215.\n[8] N. Chabrier, F. Fages & S. Soliman (2005): The Biochemical abstract machine BIOCHAM. In: Proc. of\nCMSB’04, LNCS 3082. Springer, pp. 172–191.\n[9] R.G. Clyde, J.L. Bown, T.R. Hupp, N. Zhelev & J.W. Crawford (2006): The role of modelling in identifying\ndrug targets for diseases of the cell cycle. J R Soc Interface 3, pp. 617–627.\n[10] A. Coletta, R. Gori & F. Levi (2009): Approximating Probabilistic Behaviors of Biological Systems Using\nAbstract Interpretation. In: Proc. of FBTC’08, ENTCS 229(1). pp. 165–182.\n[11] M. Curti, P. Degano, C. Priami & C.T. Baldari (2004): Modeling biochemical pathways through enhanced\npi-calculus. Theoretical Computer Science 325(1), pp. 111–140.\n[12] V. Danos & J. Krivine (2005): Transactions in RCCS. In: Proc. of CONCUR’05, LNCS 3653. Springer.\n[13] V. Danos & C. Laneve (2003): Graphs for Core Molecular Biology. In: Proc. of CMSB’03, LNCS 2602.\nSpringer, pp. 34 – 46.\n[14] S. Eker, M. Knapp, P. Lincoln, K. Laderoute & C. Talcott (2002): Pathway Logic: Executable Models of\nBiological Network. In: Proc. of the Fourth International Workshop on Rewriting Logic and Its Applications\n(WRLA’02), ENTCS 71. Elsevier, pp. 144–161.\n[15] S. Fatumo, K. Plaimas, J.P. Mallm, G. Schramm, E. Adebiyi, M. Oswald, R. Eils & R. Knig (2009): Estimating novel potential drug targets of Plasmodium falciparum by analysing the metabolic network of knock-out\nstrains in silico. Infect. Genet. Evol. 9(3), pp. 351–8.\n[16] D.A. Fell (1997): Understanding the control of metabolism. Portland Press, London, United Kingdom.\n[17] S.S Gambhir (2002): Molecular imaging of cancer with positron emission tomorgraphy. Nat. Rev. Cancer 2,\npp. 891–899.\n[18] R.A. Gatenby & R.J. Gillies (2007): Glycolysis in cancer: A potential target for therapy. The International\nJournal of Biochemistry and Cell Biology 39, pp. 1358–1366.\n[19] S. Gerdes, R. Edwards, M. Kubal, M. Fonstein, R. Stevens & A. Osterman (2006): Essential genes on\nmetabolic maps. Current Opinion in Biotechnology 17, pp. 448–456.\n[20] G.T. Gillespie (2000): The chemical Langevin equation. Journal of Chemical Physics 113(1), pp. 297–306.\n\n\f132\n\nA Taxonomy of Causality-Based Biological Properties\n\n[21] R. Gori & F. Levi (2009): Abstract Interpretation based Verification of Temporal Properties for BioAmbients.\nTo appear in Info &Co.\n[22] R. Gori & F. Levi (2009): Abstract Interpretation for Probabilistic Termination of Biological Systems. In:\nProc. of MeCBIC’09, EPTCS 11.\n[23] H Kitano (2002): Systems Biology: a brief overview. Science 295(5560), pp. 1662–1664.\n[24] H. Kitano (2004): Biological robustness. Nat Rev Genet 5, pp. 826–837.\n[25] H. Kitano (2007): Towards a theory of biological robustness. Molecular Systems Biology 3, p. 137.\n[26] S. Klamt & E.D. Gilles (2002): Minimal cut sets in biochemical reaction networks. Nature 420.\n[27] R. Milner (1999): Communicating and mobile systems: the π-calculus. Cambridge University Press.\n[28] F. Nielson, H. Riis-Nielson, D. Schuch-Da-Rosa & C. Priami (2004): Static analysis for systems biology.\nIn: Proc. of workshop on Systeomatics-dynamic biological systems informatics. Computer Science Press,\nTrinity College Dublin,, pp. 1–6.\n[29] F. Nielson, H. Riis-Nielson, D. Schuch-Da-Rosa & C. Priami (2007): Control Flow Analysis for BioAmbients.\nENTCS 180(3), pp. 65–79.\n[30] Y-K. Oh, B.. Palsson, S.M. Park, C.H. Schilling & R. Mahadevan (2007): Genome-scale Reconstruction of\nMetabolic Network in Bacillus subtilis Based on High-throughput Phenotyping and Gene Essentiality Data.\nTJ Biol Chem 10, pp. 693–706.\n[31] F. Orosz, J. Olh & J. Ovdi (2006): Triosephosphate isomerase deficiency: facts and doubts. IUBMB life 12,\npp. 703–715.\n[32] A. Phillips & L. Cardelli (2004): A correct abstract machine for the stochastic pi-calculus. In: Proc. of\nBioconcur’04. ENTCS, Elsevier.\n[33] H. Pilegaard, F. Nielson & H. Riis Nielson (2008): Pathway analysis for BioAmbients. J. Log. Algebr.\nProgram. 77(1-2), pp. 92–130.\n[34] H. Pilegaard, H. Riis Nielson & F. Nielson (2006): Static Analysis of a Model of the LDL Degradation\nPathway. In: Simulation and Verification of Dynamic Systems.\n[35] C. Priami (1995): Stochastic π-calculus. The Computer Journal 38, pp. 578–589.\n[36] C. Priami & P. Quaglia (2005): Beta Binders for Biological Interactions. In: Proc. of CMSB’04, LNCS 3082.\nSpringer, pp. 20–33.\n[37] C. Priami, A. Regev, E. Shapiro & W. Silvermann (2001): Application of a stochastic name-passing calculus\nto representation and simulation of molecular processes. Inf. Process. Lett. 80(1), pp. 25–31.\n[38] A. Regev, E. Panina, W. Silverman, L. Cardelli & E. Shapiro (2004): Bioambients: An abstraction for\nbiological compartements. Theoretical Computer Science 325(1), pp. 141–167.\n[39] A. Regev, W. Silvermann & E. Shapiro (2001): Representation and Simulation of Biochemical Processes\nUsing the pi-Calculus Process Algebra. In: Pacific Symposium on Biocomputing. pp. 459–470.\n[40] C.H. Schilling, D. Letscher & B.O. Palsson (2000): Theory for the systemic definition of metabolic pathways\nand their use in interpreting metabolic function from a pathway-oriented perspective. J. Theor. Biol. 203.\n[41] S. Schuster, D. Fell & T. Dandekar (2000): A general definition of metabolic pathways useful for systematic\norganization and analysis of complex metabolic networks. Nat. Biotechnol 18, pp. 226–232.\n[42] S. Schuster, D. Fell & T. Dandekar (2002): Metabolic network structure determines key aspects of functionality and regulation. Nature 420, pp. 190–193.\n[43] B. Shargel, H. Sayama, I.R. Epstein & Y. Bar-Yam (2003): Optimization of Robustness and Connectivity in\nComplex Networks. Physical Review Letters 90.\n[44] J. Stelling, U. Sauer, Z. Szallasi, F.J. Doyle III & J. Doyle (2004): Robustness of cellular functions. Cell 118,\npp. 675–685.\n[45] J. Thykaer & J. Nielsen (2003): Metabolic engineering of beta-lactam production. Metabolic Eng. 5 (1).\n\n\fC. Bodei, A. Bracciali, D. Chiarugi & R. Gori\n\n133\n\n[46] B.J. Yu, B.H. Sung, J.Y. Lee, S.H. Son, M.S. Kim & S.C. Kim (2006): sucAB and sucCD are mutually\nessential genes in Escherichia coli. FEMS Microbiol Lett 254, pp. 245–250.\n[47] H. Yu, P.M. Kim, E. Sprecher, V. Trifonov & M. Gerstein (2009): The importance of bottlenecks in protein\nnetworks: correlation with gene essentiality and expression dynamics. PLOS Computational Biology 3 (4).\n\n\f"
        ],
        [
         "42",
         "42",
         "cs.CE",
         "Computational Engineering",
         "1310.0068v2.pdf",
         "Automatic estimation of the regularization parameter in 2-D\nfocusing gravity inversion: an application to the Safo manganese\nmine in northwest of Iran\nSaeed Vatankhaha , Vahid E Ardestania , Rosemary A Renautb,∗\na\n\narXiv:1310.0068v2 [cs.CE] 10 Jan 2014\n\nb\n\nInstitute of Geophysics, University of Tehran,Tehran, Iran\nSchool of Mathematical and Statistical Sciences, Arizona State University, Tempe, AZ 85287-1804, USA\n\nAbstract\nWe investigate the use of Tikhonov regularization with the minimum support stabilizer for\nunderdetermined 2-D inversion of gravity data. This stabilizer produces models with nonsmooth properties which is useful for identifying geologic structures with sharp boundaries.\nA very important aspect of using Tikhonov regularization is the choice of the regularization\nparameter that controls the trade off between the data fidelity and the stabilizing functional.\nThe L-curve and generalized cross validation techniques, which only require the relative\nsizes of the uncertainties in the observations are considered. Both criteria are applied in an\niterative process for which at each iteration a value for regularization parameter is estimated.\nSuitable values for the regularization parameter are successfully determined in both cases\nfor synthetic but practically relevant examples. Whenever the geologic situation permits,\nit is easier and more efficient to model the subsurface with a 2-D algorithm, rather than\nto apply a full 3-D approach. Then, because the problem is not large it is appropriate to\nuse the generalized singular value decomposition for solving the problem efficiently. The\nmethod is applied on a profile of gravity data acquired over the Safo mining camp in MakuIran, which is well known for manganese ores. The presented results demonstrate success in\nreconstructing the geometry and density distribution of the subsurface source.\nKeywords: Gravity inversion, manganese exploration, regularization parameter, L-curve\ncriterion, generalized cross validation, generalized singular value decomposition\nPACS: 93.30.Bz,, 93.85.Hj\n1. Introduction\nGravity inversion reconstructs models of subsurface density distribution using measured\ndata on the surface. There are two important ambiguities in the inversion of gravity data.\nCorresponding Author: Rosemary Renaut, 480 965 3795\nEmail addresses: svatan@ut.ac.ir (Saeed Vatankhah), ebrahim@ut.ac.ir (Vahid E Ardestani),\nrenaut@asu.edu ( Rosemary A Renaut)\n∗\n\nPreprint submitted to Journal of Geophysics and Engineering\n\nJanuary 13, 2014\n\n\fTheoretical ambiguity is caused by the nature of gravity; many equivalent sources in the subsurface can produce the same data at the surface. Algebraic ambiguity occurs when parameterization of the problem creates an underdetermined situation with more unknowns than\nobservations. Then, there is no unique density distribution which satisfies the observed data.\nFurther, the measurement process at the Earth’s surface is necessarily error-contaminated\nand such errors can introduce arbitrarily large changes in the reconstructed solutions; namely\nthe solutions are sensitive to errors in the measurements. Thus, the inversion of gravity data\nwith under sampling is a typical example of an ill-posed problem that requires the inclusion\nof a priori information in order to find a feasible reconstruction.\nTikhonov regularization is a well-known and well-studied method for stabilizing the solutions of ill-posed problems, (Hansen, 1998; Vogel, 2002, e.g.). The objective function of\nthe Tikhonov formulation includes a data fidelity (misfit term), and a stabilizing term that\ncontrols the growth of the solution with respect to a chosen weighted norm. The choice of the\nweighting for the regularization term impacts the properties of the solution. For example, a\nsmoothing stabilizer which employs the first or second derivative of the model parameters,\nsuch as used in (Li and Oldenburg, 1996, 1998, e.g.) produces smooth images of the subsurface density distribution. There are, however, situations in which the potential field sources\nare localized and have material properties that vary over relatively short distances. Then, a\nregularization that does not not penalize sharp boundaries should be used. Last and Kubik\n(1983) presented a compactness criteria for gravity inversion that seeks to minimize the volume of the causative body. This concept was developed by introducing minimum support\n(MS) and minimum gradient support (MGS) stabilizers Portniaguine and Zhdanov (1999),\nZhdanov ( 2002), which are applied iteratively, generating repeatedly updated weighted\nquadratic stabilizers.\nIn any regularization method, the trade off between the data fit and the regularization\nterm is controlled by a regularization parameter. Methods to find this regularization parameter, called parameter-choice methods, can be divided into two classes Hansen (1998): (i)\nthose that are based on knowledge of, or a good estimate of, the error in the observations,\nsuch as Morozov’s discrepancy principle (MDP), and (ii) those that, in contrast, seek to\nextract such information from the observations, such as the L-curve or generalized crossvalidation (GCV) methods. The use of the MDP is dominant in papers related to potential\nfield inversion,(Li and Oldenburg, 1996, 1998, e.g.), and the original paper for focusing inversion Portniaguine and Zhdanov (1999). In many practical applications, little knowledge\nabout the noise or error in the data measurements is available. The MDP then reduces to a\ntrial and error procedure for finding the optimal regularization parameter Li and Oldenburg\n(1999). Here we discuss the use of the L-curve and GCV methods for use in focusing inversion\nof gravity data in situations in which there is information about the relative magnitudes of\nthe standard deviations across the measured data Farquharson and Oldenburg (2004). Due\nto the iterative nature of the algorithm, the regularization parameter is determined each\niteration.\nDepending on the type of problem to be tackled, gravity inversion can be carried out\neither in two or three dimensions (2-D or 3-D). 2-D methods are suitable for the recovery\n\n2\n\n\fof geologic structures such as faults, dikes and rift zones for which the length of the source\nbody in one direction is much longer than its extension in other directions. Then, it may be\npossible to consider the gravitational sources as completely invariant in the direction parallel\nto the length direction. Additionally, 2-D sources are both easier to conceptualize and model\nthan their 3-D counterparts, Blakely (1996).\nThe outline of this paper is as follows. In Section 2 we review the derivation of the\nanalytic calculation of the gravity anomaly derived from a 2-D cell model and then present\nan overview of numerical methods for focusing inversion. The use, and rationale for the use,\nof the generalized singular value decomposition, Paige and Saunders ( 1981), for the solution\nis given in Section 2.2.1. The L-curve and GCV parameter-choice methods are discussed in\nSection 2.3. Results for synthetic data are illustrated in Section 3. The approach is applied\non a profile of gravity data acquired from the Safo mining camp in Maku-Iran in Section 4.\nFuture directions and conclusions follow in Section 5.\n2. Gravity modeling\n2.1. The theoretical model\nA simple 2-D model is obtained by dividing the subsurface under the survey area in\nto a large number of infinitely long horizontal prisms in the invariant y−direction, with\nvariations in densities only assumed for the x and z directions. The cross-section of the\nsubsurface under the gravity profile for the model is shown in Figure 1 in which the cells\nhave square cross section and unknown densities. The dimensions of the cells are equal to\nthe distances between two observation points and the unknown density is considered to be\nconstant for each block. This type of parameterization, originally used by Last and Kubik\n(1983), could indeed be improved. Yet, increasing the resolution of the models, and hence\nthe number of parameters, by dividing the subsurface into smaller cells, makes the problem\nmore ill-posed. Here the unknown density is considered to be constant for each block and the\ndata and model parameters are linearly related. The vertical component of the gravitational\nattraction gi of a two-dimensional body at the origin using Cartesian coordinates is given\nby, Blakely (1996),\nZ Z ′ ′ ′\nz dx dz\ngi = 2Γρ\n.\n(1)\nx′ 2 + z ′ 2\nHere Γ is the universal gravitational constant and the density ρ is assumed to be constant\nwithin the body. A solution of this integral for an ℓ-sided polygon is given by, Blakely (1996),\n\u0012\n\u0013\nℓ\nX\ngi\nrp+1\nνp\nlog\n= 2Γ\n− ψp (θp+1 − θp ) ,\nρj\n1 + ψp2\nrp\np=1\n\n(2)\n\nwhere ψp = (xp+1 − xp )/(zp+1 − zp ) and νp = xp − ψp zp . The variables rp , rp+1 , θp , and θp+1\nare as displayed for the upper side of a square block in Figure 1. The term on the right-hand\nside of (2), which quantifies the contribution to the ith datum of a unit density in the jth\n3\n\n\fFigure 1: The cross-section of the subsurface under the gravity profile. Gravity stations are located at the\ncenters of the blocks at the ground surface, indicated by the ∆ symbols. The cells are square and their\ndimensions are equal to the distances between two observation points. Each cell extends as an infinitely long\nprism in the invariant y−direction.\n\ncell, is denoted by the kernel weight Gij , and is valid only at station i for cell j. The total\nresponse for station i is obtained by summing over all cells giving\ngi =\n\nn\nX\n\nGij ρj ,\n\ni = 1, . . . , m,\n\nj=1\n\nm ≤ n,\n\n(3)\n\nwhich leads to the matrix equation\nd = Gm + e,\n\n(4)\n\nwhere we have used the standard notation that vector d is the set of measurements given by\nthe gi , m is the vector of unknown model parameters, here the densities ρj , and e represents\nthe error in the measurements. The purpose of the gravity inverse problem is to find a\ngeophysically plausible density model that reproduces d.\n2.2. Numerical approaches for focusing inversion\nThe conventional method for solving ill-posed inverse problems as described by (4) is\nbased on minimization of the parametric functional\nP α (m) = φ(d) + α2 S(m).\n\n(5)\n\nHere φ(d) measures the data fidelity, which is usually measured by the weighted discrepancy\nφ(d) = kWd (d − dobs )k22 ,\n\n(6)\n\nwhere d = Gm is the vector of predicted data, dobs contains the observations and Wd is a\ndata weighting matrix. Under the assumption that the noise e is Gaussian and uncorrelated,\n4\n\n\fWd = diag(1/σ1 , . . . , 1/σm ) where σi is the standard deviation of the noise in the ith datum. Following Farquharson and Oldenburg (2004), rather than always assuming that the\nabsolute magnitudes of the error are known, we assume that the relative magnitudes of the\nerror can be estimated. Then for an unknown σ0 each σi is reexpressed as σi = σ0 σ̃i and\nWd = diag(1/σ̃1 , . . . , 1/σ̃m ). In (5) S(m) is a stabilizing regularization functional and α is a\nregularization parameter.\nDifferent choices are possible for S(m). Here we use the minimum support (MS) stabilizer\nintroduced in Last and Kubik (1983) and used in Portniaguine and Zhdanov (1999). This\nstabilizer generates a compact image of a geophysical model with sharp boundaries and,\nfollowing Zhdanov ( 2002), is of the form:\nSMS (m) = (m − mapr )T (m̂ − m̂apr )2 + ǫ2 I)\n= (m − mapr )T We2 (m − mapr ),\n\n\u0001−1\n\n(m − mapr )\n\nWe = (m̂ − m̂apr )2 + ǫ2 I\n\n(7)\n\u0001−1/2\n\n,\n\n(8)\n\nwhere m̂ and m̂apr are diagonal matrices of the current model parameters m and an estimate\nof the model parameters mapr . If good prior knowledge of the properties of the subsurface\ndistribution exists, a full model of the expected physical properties can be used for mapr ,\notherwise it is often set to 0. In We , ǫ ≥ 0 is a focusing parameter that is introduced to\nprovide stability as m → mapr component wise. Small values for ǫ lead to compact models\nbut also increase the instability in the solution. For large ǫ the image will not be focused.\nIn general we are interested in the case where ǫ → 0. A trade-off curve method can be used\nto select ǫ, Zhdanov and Tolstaya (2004); Ajo et al (2007); Vatankhah et al (2013).\nIt is well known that in potential data inversion the reconstructed models tend to concentrate near the surface. The depth weighting matrix, Wdepth = 1/(zj + ζ)β introduced in\nLi and Oldenburg (1996, 1998); Pilkington (1997) can then be incorporated into the stabilizer term. Here zj is the mean depth of cell j and ζ > 0 is a small number imposed to\navoid singularity at the surface. The choice for β is important. Small β provide a shallow\nreconstruction for the solution and large values concentrate the solution at depth. For all\ninversions considered here β = 0.6 is selected. The hard constraint matrix Whard is also\nimportant for the inversion process. If field operation geological and geophysical information\nare able to provide the value of the density of cell j this information should be included\nin mapr . Then, Whard is the diagonal identity matrix but with (Whard )jj = 100 for those\ndiagonal entries j for which information on the density is known.\nCombining the diagonal weighting matrices, Whard , We and Wdepth , (5) is replaced by\nP α (m) = kWd (Gm − dobs )k2 + α2 kD(m − mapr )k2 ,\n\nD = We Whard Wdepth .\n\n(9)\n\nTo obtain solution m := arg minm P α (m) linear transformation of the original model parameters is introduced via m(α) = m − mapr . Then m(α) solves the normal equations\n(GT Wd2 G + α2 D T D)m(α) = GT Wd2 (dobs − Gmapr ) providing\nm = mapr + m(α).\n\n5\n\n(10)\n(11)\n\n\fWhen (9) is solved iteratively due to the dependence of We on k, we use mapr = m(k−1) ,\n(k)\nm(0) = 0 and D (k) = We Whard Wdepth with\nWe(k) = (m̂(k−1) − m̂(k−2) )2 + ǫ2 I\n\n\u0001−1/2\n\nk > 1,\n\nWe(1) = I.\n\n(12)\n\nThen m(k) is obtained from the iteratively regularized equation, with updated regularization\nparameter α(k) ,\n(GT Wd2 G + (α(k) )2 (D (k) )T D (k) )m(α(k) ) = GT Wd2 (dobs − Gm(k−1) ) yielding\nm(k) = m(k−1) + m(α(k) ).\n\n(13)\n(14)\n\nThis technique in which the weighting in We is frozen at each iteration, creating the possibility\nto solve using the standard Tikhonov update, was introduced in the context of focusing\ninversion in Zhdanov ( 2002). Because the MS stabilizer tends to produce the smallest\npossible anomalous domain we follow the approach of Portniaguine and Zhdanov (1999);\nBoulanger and Chouteau (2001) to produce a reliable image of the subsurface when using\nfocusing inversion. Based on geologic information, upper and lower bounds, mmin ≤ mj ≤\nmmax , can be determined for the model parameters. If during the iterative process a given\ndensity value falls outside the bounds, projection is employed to force the value back to the\nexceeded value, and a hard constraint is imposed at that cell via (Whard )jj = 100.\n2.2.1. Numerical Solution by the Generalized Singular Value Decomposition\nWe now discuss the numerical procedure for finding the solution to (13). For large\nscale problems, iterative methods such as conjugate gradients, or other Krylov methods\nshould be employed to find m(α(k) ), (Hansen, 1998, e.g.). For small scale problems it is\nfeasible to use the singular value decomposition (SVD) for the matrix G̃ = Wd G, when\nmatrix D is the identity. Otherwise the generalized singular value decomposition (GSVD),\nPaige and Saunders ( 1981), is needed. But again is effective to use for this problem because\nit facilitates efficient determination of the regularization parameter. We assume G̃ ∈ Rm×n ,\nm < n, D ∈ Rn×n and N (G̃) ∩ N (αD) = 0, where N (G̃) is the null space of matrix G̃.\nThen there exist orthogonal matrices U ∈ Rm×m , V ∈ Rn×n and a nonsingular matrix\nX ∈ Rn×n such that G̃ = UΛX T , D = V MX T where Λ of size m × n is zero except for\nentries 0 < Λ1,q+1 ≤ . . . Λm,n < 1 with q = n − m, M is diagonal of size n × n with entries\n1 = M1,1 = . . . Mq,q > Mq+1,q+1 ≥ M2,2 ≥ . . . Mn,n > 0. The generalized singular values\nof the matrix pair G̃, D are γi = λi /µi , where γ1 = · · · = γq = 0 < γq+1 ≤ · · · ≤ γn , and\nΛT Λ = diag(0, . . . 0, λ2q+1 , . . . , λ2n ), M T M = diag(1, . . . , 1, µ2q+1, . . . , µ2n ), and λ2i + µ2i = 1,\n∀i = 1 : n, i.e. M T M + ΛT Λ = In .\nUsing the GSVD, introducing ui as the ith column of matrix U and r̃(k) = Wd (dobs −\n\n6\n\n\fGm(k−1) ), we may immediately write the solution of (13) as\n(k)\n\nm(α ) =\n(k)\n\nm\n\nn\nX\n\nuTi−q r̃(k) T −1\nγi2\n(X )i ,\nγ 2 + (α(k) )2 λi\ni=q+1 i\n\n(k−1)\n\n=m\n\n+\n\nn\nX\n\ni=q+1\n\nfi\n\n(15)\n\nuTi−q r̃(k) T −1\nγi2\n(X )i , fi = 2\n, q < i ≤ n, fi = 0, 1 ≤ i ≤ q,\nλi\nγi + (α(k) )2\n(16)\n\nis the ith column of the inverse of the matrix X T and fi are the filter factors.\nwhere (X T )−1\ni\nTherefore the algorithm proceeds by first updating the matrix We at step k using (12),\ncalculating the GSVD for the matrix pair [G̃, D (k)] and then updating m(k) using (16) which\ndepends on α(k) .\nThree criteria are used to terminate the iterative procedure. Following Farquharson and Oldenburg\n(2004) the iteration is seen to have converged and is thus terminated when either (i) a suffi(k−1)\n(k)\n(k)\ncient decrease in the functional is observed,√P α\n− P α < τ (1 + P α ), or (ii) the change\nin the density satisfies km(k−1 − m(k) k < τ (1 + km(k) k). If neither of these conditions is\nsatisfied by an upper limit on the number of iterations, the procedure is terminated without convergence as measured in this manner. The parameter τ is taken as τ = .01 for the\ninversions considered here.\nThe remaining issue is the determination of the regularization parameter at each step of\nthe iteration. As noted, when a priori information in the form of the standard deviations\non the noise in the data is available, the MDP can be used to find α. Here, in the absence\nof the exact information on the error in the data we investigate the use of the L-curve and\nGCV methods to find α(k) for which the formulation using the GSVD is advantageous.\n2.3. Regularization Parameter Estimation\n2.3.1. The L-curve\nThe L-curve approach developed by Hansen (1992, 1998) for linear inverse problems is\na robust criterion for determining the regularization parameter. It is based on the tradeoff between the norm of the regularized solution and the norm of the corresponding fidelity\nterm residual as the regularization parameter varies. According to Hansen (1992, 1998) when\nthese two norms are plotted on a log-log scale, the curve has an L shape with an obvious\ncorner. This corner separates the flat and vertical parts of the curve where the solution is\ndominated by regularization errors and perturbation errors, respectively. Picking αopt as the\nα responsible for the corner point gives the optimal trade off between the two terms, and\nthe corresponding model is selected as the optimal solution. For α > αopt the regularized\nsolution does not change dramatically, while the residual does. In contrast, for α < αopt\nthe regularized solution increases rapidly with little decrease in the residual. Because of the\nrelation of αopt with the shape of the curve, Hansen (1998) recommends estimating αopt by\nfinding the maximum of the local curvature in the neighborhood of the dominant corner\nof the plot. Although the L-curve technique can be robust for problems generating welldefined corners, it may not work so well in other cases. For an underdetermined problem the\n7\n\n\frecovered model can change more slowly with the degree of regularization, Li and Oldenburg\n(1999), and the L-curve is thus smoother. This makes it difficult to find maximum point of\ncurvature of the curve. On the other hand, given the solution of the regularized problem in\nterms of the GSVD, as in (15), finding the L-curve is relatively efficient and is thus one of\nthe conventional ways to estimate αopt .\nIt was shown by Farquharson and Oldenburg (2004) that the L-curve choice for λ at\nearly iterations may be too small which may lead to inclusion of excessive structure in the\nmodel that needs to be eventually removed, hence requiring more iterations for the inversion.\nHence here we follow the approach suggested by Farquharson and Oldenburg (2004)\n\u0001 and\n(k)\n(k)\n(k−1)\n∗\nimpose a so-called cooling process in which α is given by α = max c α\n, α where\n∗\n0.01 ≤ c ≤ 0.5 and α is the point of maximum curvature of the L-curve. Moreover,\nchoosing a relatively large value for α(1) improves the performance of the algorithm. We use\nα(1) = max(γi )/mean(γi ) and c = 0.4 which work well for the presented inversion examples.\n2.3.2. Generalized Cross Validation\nThe major motivation of using the GCV to find an optimal value for α is that a good\nvalue should predict missing data values. Specifically, if an arbitrary measurement is removed\nfrom the data set, then the corresponding regularized solution should be able to predict the\nmissing observation. The choice of α should be independent of an orthogonal transformation\nof the data, Hansen (1998). The GCV functional is given by\nP\nk ni=q+1 (1 − fi )uTi−q r̃k2\nkG̃m(α) − r̃k2\nkG̃m(k) − d̃obs k2\nP\nP\n=\n, (17)\nGCV (α) =\n=\n(m − ni=q+1 fi )2\n(m − ni=q+1 fi )2\ntrace(Im − G̃G(α))2\n\nwhere the final expressions follow immediately from the GSVD, with G(α) = (G̃T G̃ +\nα2 D T D)−1 G̃T . Here the numerator is the squared residual norm and the denominator is\neffectively the square of the number of degrees of freedom, Hansen (1998). It can be shown\nthat the value of α which minimizes the expected value of the GCV function is near the minimizer of the expected value of the predictive mean-square error, kGm(α) − dexact k2 , Hansen\n(1998). Hence, finding the minimum for GCV (α) should lead to a reasonable estimate for α.\nWe have seen in our experiments that when the GCV does not fail, in which case it produces\na very small α, it usually leads to α which is slightly larger than the optimal value. Failure\noccurs when GCV (α) is almost flat near the optimal alpha, leading to numerical difficulties\nin computing its minimum. The cooling procedure, as described for the L-curve, is also applied to the GCV estimation of α(k) but with α∗ now chosen as the current minimum of the\nGCV function. For clarity we summarize the steps of the inversion that are applied every\niteration in Algorithm 1. Here we state this for the L-curve method. The GCV solutions are\nfound equivalently but at all steps using the L-curve instead the GCV method is applied.\nAlgorithm 1. The steps taken for the inversion at every iteration assuming the use of the\nL-curve to find the regularization parameter\n1. Calculate the GSVD for matrix pair [G̃, D].\n8\n\n\f2. Calculate the solutions, and the associated L-curve function, for a range of α , the\noptimal α is found using the L-curve.\n3. The cooling process is implemented for deciding whether the obtained α from step 2\nshould be used or not.\n4. With α from step 3, the model parameters are computed using equation (16).\n5. Density limits are implemented on model parameters, from step 4 , and then We and\nWhard are updated.\n6. Data misfit, S(m) and P α (m) are computed for model parameters obtained from step\n5.\n7. If the termination criteria are satisfied the iteration terminates. Otherwise, the a priori\ndensity model is set equal to the density model from step 5 and the iteration returns\nto step 1.\n3. Numerical Results: Simulated Model\nWe evaluated the use of the L-curve and GCV in focusing inversion, as described in\nSections 2.3.1 and 2.3.2, for several synthetic data examples. In these simulations data are\ncalculated at 50 stations with 10m spacing on the surface and the inversion is required at the\nsubsurface on a rectangular grid of 50 × 10 with cell size 10m, hence in this case m = 50 and\nn = 500. The iterations are initialized with m(0) = 0, Whard = We = I and bound constraints\non the density are set such that 0gr/cm3 ≤ mj ≤ 1gr/cm3 . The focusing parameter is fixed\nas ǫ = 0.02 for the inversions. It should be noted that the regularization parameter depends\non the choice for ǫ, a large value requires a larger value for α and generates a smoother\nmodel. In these simulations the maximum number of iterations is set to 20.\nThe synthetic gravity data are generated in each case for the rectangular body that has\ndensity contrast equal to 1gr/cm3 with an homogeneous background, Figure 2(a). In generating noise-contaminated data we use zero mean Gaussian noise with a standard deviation\nσ̃i = (η1 (dexact )i + η2 kdexact k), as indicated in Figure 2(b) for η1 = .03 and η2 = .001. In\neach case we calculated the χ2 measure of the actual noise in the noisy data and report the\ndata fidelity values, relative error k(mexact − m(K) )k2 /kmexact k2 , and final values α(K) for\neach inversion in Table 1.\nThe density models obtained for the first data set, Figure 2(b) are presented in Figures 5(a) and 5(b) for the L-curve and GCV inversions, respectively. In each case the geometry and density of the reconstructed models are close to those of the original model, although\nthe inversion using the L-curve criterion is more focused. This feature was present in all the\nexamples we have analyzed; inversion using GCV always provides a smoother reconstruction\nthan that obtained using the L-curve. Figures 4(a)-4(b) demonstrate the progression of the\nsolutions with iteration k for the data fidelity φ(d(k) ), the stabilizer S(m(k) ), the parametric\n(k)\nfunctional P α (m(k) ), and regularization parameter, α(k) , again for the L-curve and GCV\nrespectively for this first example. In our experience the behavior indicated is consistent\nwhen using the GCV to find the regularization parameter α; it generally decreases initially,\nbut then increases to converge to a fixed value by the maximum number of iterations. On\n9\n\n\f(a) Simulated Model\n\n(b) Gravity anomaly contaminated by uncorrelated noise\n\nFigure 2: In 2(a) the synthetic model of a body set in a grid of square cells each of size 10m, the density\ncontrast of the body is 1gr/cm3 . In 2(b) the gravity anomaly due to the synthetic model contaminated by\nuncorrelated noise with η1 = 0.03 and η2 = .001. The exact anomaly indicated by the solid line and the\ncontaminated data by the symbols.\n\n(a) L-curve\n\n(b) GCV\n\nFigure 3: Density model obtained from inverting the data of Figure 2(a) with MS stabilizer and noise level\nwith η1 = 0.03 and η2 = .001 using bounds on density 0gr/cm3 ≤ mj ≤ 1gr/cm3 . The regularization\nparameter was found using in (a) the L-curve and in (b) the GCV.\n\nthe other hand, for the L-curve the progression of α(k) is more erratic, generally oscillating\nin final iterations toward a converged value as shown in Figure 4(a). Manual intervention\nmay then be needed to force the overall convergence of the algorithm Ajo et al (2007). The\nmain problem for the L-curve, as mentioned in section section 2.3.1, is its smooth shape, that\nmakes it difficult to find the corner, namely the point of maximum curvature. To illustrate\nwe plot the L-curve for all iterations in Figure 5(a), showing that overall the approach is\nsuccessful, although at a given middle iteration the apparent corner is missed, Figure 5(c).\nStill the starting and final iterations in Figure 5(b) and Figure 5(d) do find useful corners.\nThe same formulation was used to generate two further synthetic data sets, with quantitative results shown also in Table 1. Noise generated using η1 = 0.01 and η = 0.05 was\nconsidered, with in both cases η2 = .001. The results of the inversions are illustrated in\nFigures 6 and 7, respectively. We note from Table 1 that the fidelity values at convergence\n(K)\n(K)\nare less than the initial χ2 measure of the noise, that always αL−curve < αGCV , but that\nthere is no fixed conclusion about the relation between the final relative errors and fidelity\nestimates by the L-curve and GCV inversions. We conclude that both the GCV and the\n10\n\n\f(a) L-curve\n\n(b) GCV\n\nFigure 4: The data fidelity φ(d), the stabilizer S(m), the parametric functional P α (m), and the regularization parameter, α ,all plotted against iteration k. The regularization parameter was found using in (a) the\nL-curve and in (b) the GCV.\n\nL-curve are successful in providing reasonable solutions. It should be noted that increasing\nthe focusing parameter ǫ can yield solutions which are not focused, especially as shown in\nFigure 7.\nTable 1: Relative error and final values of the regularization parameter for the inverted models.\n\nFigure\n3\n6\n7\n8\n9a-b\n9c-d\n\nχ2\nmeasure\n51.23\n47.26\n40.03\n51.23\n51.23\n51.23\n\nα(K)\nRelative\nL-curve GCV L-curve\n0.62\n7.41\n0.4270\n0.87\n14.15 0.3376\n0.50\n6.85\n0.4014\n0.63\n8.20\n0.7708\n27.04 429.38 0.4264\n27.23 441.86 0.4404\n\nError\nFidelity\nGCV L-curve\n0.4025 19.14\n0.3506 26.27\n0.4013 21.63\n0.6910 17.21\n0.4026 19.64\n0.4069 17.95\n\nφ(d)\nGCV\n21.93\n24.96\n13.61\n20.80\n16.77\n16.10\n\nTo assess both the impact of the choice of the bounds on the convergence properties for\nthe solution and choice of the regularization parameter α, we investigated two additional\nsituations. First we implemented the same problem as given in figure 2, with noise η1 = 0.03\nand η2 = 0.001, but inverted now with upper bounds on the density changed to 2gr/cm3 .\nThe results are illustrated in figure 8 and detailed as before in Table 1. We see that the solutions are more focused, as anticipated from the previous work of Portniaguine and Zhdanov\n(1999) but the relative error overall is increased and the fidelity of the solution is also decreased. On the other hand, it is of greater interest for the purposes of this study to observe\nthat the performance of the parameter choice techniques is independent of the upper bound,\n11\n\n\f(a)\n\n(b)\n\n(c)\n\n(d)\n\nFigure 5: The L-curve for all iterations with in each case the circle showing point with maximum curvature.\nIn figures 5(b)-5(d) we show the individual curves at iterations 1, 10 and the final stage.\n\nand the parameters are found stably independent of the constraint bounds. To determine\nthe necessity of using the MS instead of a smoothness stabilizer, we also considered the\nresults obtained using smoothness stabilizer, i.e. the We in (9) replaced with the approximation for the second derivative of the model parameters, for the situations in figures 3\nand 8. These results are also detailed in Table 1 and illustrated in figures 9(a)-9(b) and\n9(c)-9(d), respectively. They demonstrate the relative insensitivity to density limits of the\nsmoothness-stabilizer obtained solutions. On the other hand, the solutions lack the contrast\nthat is achieved using the MS regularization. Overall, the solutions obtained with GCV are\napparently more robust than those with the L-curve. It should be noted that using a non-ℓ2\nmeasure of the derivative in geophysical inversion leads to a strongly piecewice constant, or\nblocky, reconstruction with sharp jumps, see e.g. Farquharson and Oldenburg (1998).\n4. Numerical Results: Practical Data\n4.1. Geological Context\nThe data used for inversion was acquired over the Safo mining camp in Maku-Iran which\nis well known for manganese ores. Geologically this area is located in the Khoy ophiolite\n12\n\n\f(a) L-curve\n\n(b) GCV\n\nFigure 6: Density model obtained from inverting the data of Figure 2(b) with MS stabilizer: and noise level\nwith η1 = 0.01 and η2 = .001. The regularization parameter was found using in (a) the L-curve and in (b)\nthe GCV.\n\n(a) L-curve\n\n(b) GCV\n\nFigure 7: Density model obtained from inverting the data of Figure 2(b) with MS stabilizer and noise level\nwith η = 0.05 and η2 = .001. The regularization parameter was found using in (a) the L-curve and in (b)\nthe GCV.\n\nzone, in the northwest of Iran. Some manganese and iron-manganese deposits are found\nwithin sedimentary pelagic rocks and radiolarian cherts which are accompanied by Khoy\nophiolite, Imamalipour (2005). Most of these deposits have little reserve; the Safo deposit\nis the only viable area distinguished so far for mining. In the Safo deposit, depositions of\nmanganese have been found to occur in different horizons within pelagic rocks. Mineralogically, pyrolusite, bixibite, braunite and hematite are the main minerals present in ore, of\nwhich the pyrolusite is the dominant ore Imamalipour (2005), and Calcite with quartz and\nbarite present as minor phases. The banded, massive and disseminated textures are seen\nin orebodies. Manganese content varies from 7.4% to 69.1% in different regions of the area\nImamalipour (2005).\n4.2. Gravity anomaly\nThe area of the gravity survey extends between UTM coordinates [438276 438609] west\nand [4342971 4343187] north, Z38. The gravity survey was performed by the gravity branch\nof the institute of Geophysics, University of Tehran. The measurements were corrected for\neffects caused by instruments and tidal drift, latitude, free air and the Bouguer correction\nto yield the Bouguer gravity anomaly, Figure 10(a). The Bouguer anomaly displays extreme\nmagnitudes in the central of the area in the north-south direction, related to mineral oc13\n\n\f(a) L-curve\n\n(b) GCV\n\nFigure 8: Density model obtained from inverting the first data set η1 = 0.03 and η2 = .001 with MS stabilizer\nand density limits 0gr/cm3 ≤ mj ≤ 2gr/cm3 . The regularization parameter was found using in 8(a) the\nL-curve; and in 8(b) the GCV.\n\ncurrence which has high density contrast with the host rocks. This geologic structure is\ntherefore clearly suitable for using a 2-D algorithm. The residual anomaly was obtained\nby subtracting the regional anomaly from the Bouguer anomaly using a polynomial fitting\nmethod, Figure 10(b). One of the recommended steps in potential field inversion is upward\ncontinuation of data to a height of half the thickness of the shallowest cell which removes near\nsurface effects without noticeably degrading the data. Figure 11 shows upward continuation\nof the residual data up to 2.5m.\n4.3. Inversion result\nA profile of the anomaly (SA) consisting of 49 data measurements, sampled every 5m,\nis chosen for inversion. The subsurface is divided into 49 × 15 square cells of size 5m,\nhence in this case m = 49 and n = 735. Based on geological information Imamalipour\n(2005), the background density is set to 2.8gr/cm3 and the density limits for the inversion\nare 2.4gr/cm3 ≤ mj ≤ 4.7gr/cm3 . The maximum number of iterations was set to 20.\nEach datum is assigned a Gaussian error as in the simulated cases, here with η1 = .05 and\nη2 = .001. Figures 12a-b illustrate the reconstructed density model from the inversion of\nprofile SA using the L-curve and GCV methods for estimating the regularization parameter,\n(K)\n(K)\nyielding αL−curve = 0.65 and αGCV = 4.87 respectively. Figures 13(a)-13(b) illustrate the\nprofile of the anomaly (SA) which is used for the inversion, indicated by the stars, and\nthe resulting values obtained from reconstructed models in figures 12(a)-12(b), denoted in\neach case by the circles. Both solutions clearly represent the density contrast and geometry\nfor the occurrence of manganese ore, figures 12(a)-12(b). The horizontal extension of the\nobtained model is about 30m and the vertical extension shows a depth interval approximately\nbetween 5m and 35m. These results are close to those obtained by Borehole drilling on the\nsite; which show extension of manganese ores from 3−4m to 25−30m in the subsurface along\nthe north-south direction Noorizadeh (2010). The data fidelity, the stabilizer, the parametric\nfunctional and regularization parameter, with iteration k , are shown in Figures 14(a)-14(b).\nThe convergence histories have properties for the practical data that are similar to those for\nthe simulated data sets.\n\n14\n\n\f(a) L-curve\n\n(b) GCV\n\n(c) L-curve\n\n(d) GCV\n\nFigure 9: Density model obtained from inverting the first data set η1 = 0.03 and η2 = .001 with smoothness\nstabilizer. In 9(a) and 9(b) density limits are 0gr/cm3 ≤ mj ≤ 1gr/cm3 . In 9(c) and 9(d) density limits are\n0gr/cm3 ≤ mj ≤ 2gr/cm3 . The regularization parameter was found using in 9(a) and 9(c) the L-curve; and\nin 9(b) and 9(d) the GCV.\n\n5. Conclusions\nTikhonov regularization with the minimum support stabilizer has been demonstrated to\nyield non-smooth solutions and is thus an appropriate approach for recovery of geological\nstructures with sharp boundaries. The presented algorithm is flexible and allows variable\nweighting in the stabilizer, including depth weighting, a priori density ranges for the domain\nand the inclusion of hard constraints for the a priori information in the inversion process.\nThe L-curve criterion and GCV method for estimating the regularization parameter were\ndiscussed, and characteristics of each of them for obtaining a solution were introduced.\nNumerical tests using synthetic data have demonstrated feasibility of applying both methods\nin the iteratively reweighted algorithm. The regularization parameter is seen to converge\nas the number of iterations increases. Although GCV leads to inverse solutions which are\nslightly smoother than those obtained by the L-curve, both recovered models are close to\nthe original model. For this small-scale problem, it is shown that the GSVD can be used in\nthe algorithm, demonstrating the filtering of the solution. Moreover, this use of the GSVD,\nwhich might generally be assumed to be too expensive, is beneficial and worthwhile in the\ncontext of regularization parameter estimation as shown here. For large-scale problems\nit is anticipated that a randomized GSVD needs to be developed, along the lines of the\nrandomized SVD that was introduced in Liberty et al (2007). Finally the method was used\non a profile of gravity data acquired over the Safo manganese mine in the northwest of Iran.\nThe result shows a density distribution in subsurface from about 5m to 35m in depth and\nabout 30m horizontally. Future work will consider the inclusion of statistical weighting in\n15\n\n\f(a) L-curve\n\n(b) GCV\n\nFigure 10: Bouguer anomaly over the Safo manganese mine in 10(a) and the residual anomaly over the Safo\nmanganese mine in 10(b).\n\nFigure 11: Upward continuation of the residual anomaly to height 2.5m.\n\nthe solution and the use of regularization parameter estimation using statistical approaches,\nMead and Renaut (2009).\n6. Acknowledgements\nRosemary Renaut acknowledges the support of AFOSR grant 025717: “Development\nand Analysis of Non-Classical Numerical Approximation Methods”, and NSF grant DMS\n1216559: “Novel Numerical Approximation Techniques for Non-Standard Sampling Regimes”.\nWe would also like to thank the two anonymous referees who raised interesting questions\nthat lead us to include additional results and clarifications of the method.\n\n16\n\n\f(a) L-curve\n\n(b) GCV\n\nFigure 12: The density models obtained by inverting field gravity data (profile SA). The regularization\nparameter was found using in (a) the L-curve and in (b) the GCV.\n\n(a) L-curve\n\n(b) GCV\n\nFigure 13: The field gravity data (stars) and computed data for the reconstructed density model (circles).\nThe regularization parameter was found using in 13(a) the L-curve; in 13(b) the GCV.\n\nReferences\nReferences\nAjo-Franklin J B Minsley B J and Daley T M 2007 Applying compactness constraints to\ndifferential traveltime tomography Geophysics 72 R67-R75\nBlakely R J 1996 Potential Theory in Gravity & Magnetic Applications Cambridge University\nPress Cambridge United Kingdom\nBoulanger O and Chouteau M 2001 Constraints in 3D gravity inversion Geophysical Prospecting 49 2 265-280\nFarquharson C G and Oldenburg D W 1998 Non-linear inversion using general measure of\ndata misfit and model structure Geophysical Journal International 134 213-227\nFarquharson C G and Oldenburg D W 2004 A comparison of automatic techniques for estimating the regularization parameter in non-linear inverse problems Geophysical Journal\nInternational 156 411-425\n\n17\n\n\f(a) L-curve\n\n(b) GCV\n\nFigure 14: The data fidelity φ(d), the stabilizer S(m), the parametric functional P α (m), and the regularization parameter, α ,all plotted against iteration k. The regularization parameter was found using in (a)\nthe L-curve and in (b) the GCV.\n\nHansen P C 1992 Analysis of discrete ill-posed problems by means of the L-curve SIAM\nReview 34 561-580\nHansen P C 1998 Rank-Deficient and Discrete Ill-Posed Problems: Numerical Aspects of Linear Inversion SIAM Monographs on Mathematical Modeling and Computation 4 Philadelphia\nImamalipour A 2005 Geochemistry mineralogy and origin of Safo manganese deposit Ninth\nmeeting of Geological Society of Iran (in Persian)\nLast B. J. and Kubik K 1983 Compact gravity inversion Geophysics 48 713-721\nLi Y and Oldenburg D W 1996 3D inversion of magnetic data Geophysics 61 394-408\nLi Y and Oldenburg D W 1998 3D inversion of gravity data Geophysics 63 109-119\nLi Y and Oldenburg D W 1999 3D Inversion of DC resistivity data using an L-curve criterion\n69th Ann. Internat. Mtg. Soc. Expl. Geophys. Expanded Abstracts 251-254\nLiberty E Woolfe F Martinsson PG Rokhlin V and Tygert M 2007 Randomized algorithms for\nthe low-rank approximation of matrices Proceedings of the National Academy of Sciences\n104 20167-20172\nMead J L and Renaut R A 2009 A Newton root-finding algorithm for estimating the regularization parameter for solving ill-conditioned least squares problems Inverse Problems\n25 025002 doi: 10.1088/0266-5611/25/2/025002\nNoorizadeh A 2010 Geological report, Bore-hole results of Safo manganese mine Spadana\nMining Company\n18\n\n\fPaige C C and Saunders M A 1981 Towards a generalized singular value decomposition\nSIAM Journal on Numerical Analysis 18 3 398-405\nPilkington M 1997 3-D magnetic imaging using conjugate gradients Geophysics 62 1132-1142\nPortniaguine O and Zhdanov M S 1999 Focusing geophysical inversion images Geophysics\n64 874-887\nVatankhah S Ardestani E V and Ashtar J M 2013 A method for 2D inversion of gravity data\nJournal of Earth and Space Physics (accepted)\nVogel C R 2002 Computational Methods for Inverse Problems SIAM Frontiers in Applied\nMathematics SIAM Philadelphia U.S.A.\nZhdanov M S 2002 Geophysical Inverse Theory and Regularization Problems Elsevier Amsterdam\nZhdanov M S and Tolstaya E 2004 Minimum support nonlinear parameterization in the\nsolution of 3−D magnetotelluric inverse problem Inverse Problems 30 937-952\n\n19\n\n\f\f"
        ],
        [
         "43",
         "43",
         "cs.CE",
         "Computational Engineering",
         "0512063v1.pdf",
         "IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 52, NO. 3, MARCH 2006\n\n1\n\nComplex Random Vectors and ICA Models:\nIdentifiability, Uniqueness and Separability\n\narXiv:cs/0512063v1 [cs.IT] 15 Dec 2005\n\nJan Eriksson Member, IEEE and Visa Koivunen Senior Member, IEEE\n\nAbstract— In this paper the conditions for identifiability, separability and uniqueness of linear complex valued independent\ncomponent analysis (ICA) models are established. These results extend the well-known conditions for solving real-valued\nICA problems to complex-valued models. Relevant properties\nof complex random vectors are described in order to extend\nthe Darmois-Skitovich theorem for complex-valued models. This\ntheorem is used to construct a proof of a theorem for each of\nthe above ICA model concepts. Both circular and noncircular\ncomplex random vectors are covered. Examples clarifying the\nabove concepts are presented.\nIndex Terms— Blind methods, circularity, complex linear models, complex Darmois-Skitovich theorem, differential entropy,\nindependent component analysis (ICA), noncircular complex\nrandom vectors, properness.\n\nI. I NTRODUCTION\nIndependent component analysis (ICA) [1] is a relatively\nnew signal processing and data analysis technique. It may be\nused, for example, in blind source separation (BSS) and identifying or equalizing instantaneous multiple-input multipleoutput (I-MIMO) models. It has found applications, e.g., in\nwireless communications, biomedical signal processing and\ndata mining (see [2] for references). In instantaneous complexvalued ICA problem\n~x = A~s,\n(1)\nthe goal is to recover the original source signal vectors ~s from\nthe observation vectors ~x blindly without explicit knowledge\nof the sources or the linear mixing system A. ICA is based\non the crucial assumption that the underlying unknown source\nsignals are statistically independent. Recent textbooks provide\nan interesting tutorial material and a partial review on ICA\n[2], [3].\nThe theorems for linear combinations of real-valued random\nvectors and theoretical conditions on separation for realvalued signals are now well-known [1], [4], [5]. Even though\nalgorithms for separation of complex-valued signals have been\ndeveloped, for example [1], [6], the conditions when the\nseparation is possible have not been established. Also recent\npapers, e.g., [7]–[10], proposing ICA algorithms for complexvalued data ignore this important issue.\nIn this paper we construct theorems stating the conditions\nfor identifiability, separability, and uniqueness of complexvalued linear ICA models. These results extend the theorems\nManuscript received March xx, 2004; revised December xx, 2005. This\nwork was supported in part by the Academy of Finland and GETA Graduate\nSchool.\nThe authors are with the SMARAD CoE, Signal Processing Laboratory,\nDepartment of Electrical Engineering, Helsinki University of Technology,\nFIN-02015 HUT, Finland (e-mail: {jan.eriksson,visa.koivunen}@hut.fi).\n\nproved for the real-valued instantaneous ICA model [1], [5]\nto the complex case. Both circular (proper) and noncircular\ncomplex random vectors are covered by the theorems. These\nconditions depend not only on the probabilistic structure of the\nsources but also the linear space structure of the mixing. In\norder to prove the theorems, the celebrated Darmois-Skitovich\ntheorem [4] needs to be extended to linear combinations\nof complex random variables. A good number of statistical\nproperties of circular and noncircular complex vectors have\nto be considered in the process of constructing the proof.\nThis is due to the special operator structure that may be\nused for complex random vectors. In addition, the second\norder statistical properties of noncircular complex vectors may\nnot be defined using the covariance matrix alone [11]–[13].\nGeneral complex Gaussian random vectors is an important\nclass of random vectors that need to be addressed in detail.\nThere are relatively few papers where noncircular complex\nrandom vectors are studied [11]–[16]. Hence, many of the\nkey results needed in proving the theorems are included in\nthis paper and presented in a unified manner. This also allows\na direct derivation of some fundamental information-theoretic\nquantities like the entropy of a complex normal random vector.\nThe paper is organized as follows. In Section II relevant\nproperties that distinguish complex random vectors from real\nrandom vectors are described in detail. Especially, the correlation structure is used to study complex normal random vectors.\nThese properties are needed in proving the Darmois-Skitovich\ntheorem for the complex case. This theorem plays a key role\nin establishing the conditions for identifiablity, separability\nand uniqueness of complex linear ICA models in Section III.\nFinally, some concluding remarks are given. Most of the proofs\nare presented in appendices.\nII. R ELEVANT P ROPERTIES\n\nOF\nVECTORS\n\nC OMPLEX\n\nRANDOM\n\nThe traditional probability theory is concerned with realvalued random variables (r.v.s) and random vectors (r.vc.s).\nThe theory has been generalized to various algebraic structures. Main studies are in the frameworks of locally compact\nspaces and complete separable metric spaces (see, e.g., [17]–\n[20] and references therein). However, the most natural extension from the engineering point of view is the complex Hilbert\nspace. It seems to have gained relatively little attention. Some\nresults on complex normal r.vc.s can be found in [21], [22].\nThe second-order structure of complex r.vc.s has been studied\nin [11]–[13], [15], and a general framework for higher-order\nstatistics can be found from [23]. Some research has been\nconducted on complex elliptically symmetric distributions [24]\n\n\f2\n\nand on complex stable distributions [25]. Polya’s theorem to\ncomplex case is presented in [26]. The only systematic Hilbert\nspace approach known to the authors is [14]. This may be\ndue to the fact that the additive structure of the complex\nHilbert space is the same as that of the real Hilbert space.\nHowever, the multiplicative structure and the operator structure\nare different giving r.vc.s in a complex Hilbert space distinct\nproperties. Even though many results from the general abstract\ntheory apply directly to the complex Hilbert space case, the\nsystematic treatment considering both the additive and the\nmultiplicative structure seems to be missing.\nIn Section II-B the finite dimensional Hilbert space is\nreviewed by constructing an isomorphism into a real-valued\nHilbert space. This isomorphism shows essentially the difference between the real and complex Hilbert spaces. In\nSection II-C some basic properties of r.vc.s in the complex\nHilbert space are stated, the second-order structure of complex\nr.vc.s is studied in Section II-D. Complex normal r.vc.s are\nstudied is Section II-E and, finally the complex DarmoisSkitovich theorem is proved in Section II-F.\nA. Notation\nLet us begin with some definitions and notations. We have\nused typewriter font for all random objects, e.g. x, in order to\ndistinguish them from deterministic ones, e.g. x. For random\nvectors, e.g. ~x, we have used the vec symbol in order to\nseparate them from scalar random variables. For deterministic\nobjects, the bold face lower case letters are used for vectors,\ne.g. z, and the bold face upper case letters are used for\nmatrices, e.g. W .\nThe modulus√of a complex\np number z = zR + zI ∈ C is∗\n2 + z 2 , where the superscript\ndenoted |z| = z ∗ z = zR\nI\n√\ndenotes the complex conjugate, z ∗ = zR − zI , and  = −1\nis the imaginary unit. Recall that any nonzero complex number\nz can be given in polar form z = αeθ , where α > 0, θ ∈ R.\nThe number θ is called an argument of the complex number\nz, and the argument θ = Arg(z) such that −π ≤ θ < π is\ncalled the principal argument. The real part of a p-dimensional\ncomplex vector (z1 z2 · · · zp )T = z ∈ Cp , where T is\nthe ordinary transpose, is denoted by zR and the imaginary\npart by zI . The Euclidean norm of a vector z is denoted\nk z k2 = hz, zi = zH z, where h·, ·i is the inner product and\nthe superscript H denotes the conjugate transpose, i.e., the\nHermitian adjoint. A complex matrix C ∈ Cp×p is termed\n[27] symmetric if C T = C and Hermitian if C H = C.\nFurthermore, the matrix C is orthogonal if C T C = CC T =\nI p and unitary if C H C = CC H = I p , where I p denotes the\np × p identity matrix.\nB. Complex Hilbert space isomorphism\nLet C = C R + C I ∈ Cm×p and z = zR + zI ∈ Cp . We\nuse the following notations\n\u0013\n\u0012 \u0013\n\u0012\nzR\nC R −C I\n(2)\nand zR =\nCR =\nzI\nCI CR\nfor the associated 2m × 2p real matrix and 2p-variate real\nvector, respectively. The mapping z 7→ zR gives naturally a\n\nIEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 52, NO. 3, MARCH 2006\n\ngroup isomorphism between the additive Abelian groups Cp\nand R2p . In the case m = p = 1, the mapping given by\nC 7→ C R defines a field isomorphism (e.g., [14], [22]) between\nthe complex numbers and a subset of real two dimensional\nmatrices. Therefore, one can construct real structures where\nthe role of complex multiplication is played by the special\nmatrices.\nNow consider the mapping\nCz 7→ (Cz)R = C R zR .\n\n(3)\n\nIt is continuous and therefore preserves the topological properties, i.e., it is a homeomorphism [19]. Let diag(z) (as in\nMatlab) denote the diagonal matrix with components of z in\nits main diagonal and zeros elsewhere. Since Cp is a vector\nspace, where the scalar multiplication for c ∈ C is given by\n \ncz1\n\u0001\n .. \ncz ,  .  = diag( c · · · c )z,\n(4)\nczp\nthe mapping (3) defines a vector space isomorphism between\nthe standard p-dimensional complex vector space and a 2pdimensional real-valued vector space given by the mapping. It\nis important to realize that this associated real-valued vector\nspace is not isomorphic to the standard real vector space R2p .\nFurthermore, by equating zH\n1 with C in (3) it is easily verified\nH\nthat the mapping C → R2 : zH\n1 z2 7→ (z1 )R (z2 )R associates\n2p\na (complex) inner product for R . Therefore, the mapping\n(3) is also a Hilbert space isomorphism. Again, it should be\nemphasized that the inner product given by the mapping is\nnot the standard Euclidean inner product in R2p . However,\nthe vector norms, and hence metrics, are equivalent in both.\nThe following properties are easily established.\nLemma 1: Let C ∈ Cp×p and z ∈ Cp .\n\u0001\n\u0001\n(i) | det C |2 = det C R .\n\u00012\n(ii) C is Hermitian\niff C R is \u0001symmetric. Then\n\u0001\n\u0001 det C =\ndet C R and 2 × rank C = rank C R .\n(iii) C is nonsingular iff C R is nonsingular.\n(iv) C is unitary iff C R is orthogonal.\n(v) zH Cz = zTR C R zR\n(vi) C is Hermitian positive definite iff C R is symmetric\npositive definitive.\n(vii) Any polynomial with complex coefficients in variables\nzR can be equivalently given in variables (z, z∗ ).\nProof: These properties are direct consequences of the\nisomorphism, see, e.g., [22], [24]. The last property follows\n∗\nfrom the identities zR = 12 (z + z∗ ) and zI = −\n2 (z − z ).\n∗\nSince the variables (z, z ) in Lemma 1(vii) are dependent,\nwe call such complex polynomials wide sense polynomials.\nThe idea of using also the complex conjugate variable has\nturned out to be highly useful in, e.g., complex parameter\nestimation [28] and blind channel equalization [16].\nC. Complex random vectors\nA p-variate complex random vector (r.vc.) ~x is defined as\nan r.vc. of the form\n~x = ~xR + ~xI ,\n\n(5)\n\n\fERIKSSON AND KOIVUNEN: COMPLEX RANDOM VECTORS AND ICA MODELS\n\nwhere ~xR and ~xI are p-variate real r.vc.s, i.e., ~xR and ~xI are\nmeasurable functions from a probability space to Rp . This\nis equivalent for ~x to be measurable from the probability\nspace into Cp due to the separability of the complex space.\nTherefore, the probabilistic structure of the r.vc.s in Cp and the\nprobabilistic structure of the r.vc.s in R2p is the same. However, the operator structure is different as it is evident from\nthe previous section. This gives distinct properties to the r.vc.s\nwith complex values, and justifies studying them separately.\nThroughout this paper all complex r.vc.s are assumed to be\nfull. This means that the support of the induced measure of a\np-dimensional r.vc. is not contained in any lower dimensional\ncomplex subspace.\nSince the probabilistic structures of r.vc.s in Cp and in R2p\nare the same, also the operator structure of r.vc.s in Cp can be\nstudied by first using the isomorphism (3) and then applying\nthe concepts associated with the real r.vc.s. However, we define\nthese associated concepts directly on Cp , since this approach\nis notationally more convenient.\nThe expectation E[·] of a complex r.vc. ~x is defined as\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003\n(6)\nE~x ~x = E~xR ~xR +  E~xI ~xI ,\n\nand the distribution function F~x is given as F~x (z) , F~xR (zR ),\nwhere z = (z1 , . . . , zp )T ∈ Cp and F~xR denotes the distribution function of real-valued r.vc. ~xR . Then for independent\nr.v.s (s1 , . . . , sp )T = ~s, we have\nF~s (z) = F~sR (zR ) =\n\np\nY\n\nF(sk )R ((zk )R ) =\n\nk=1\n\np\nY\n\nFsk (zk ). (7)\n\nk=1\n\nThe same way we define the probability density function f~x\n(if it exists) of a p-dimensional complex r.vc. ~x as f~x (z) ,\nf~xR (zR ), and the characteristic function (c.f.) [14] as\n\u0002\n\u0001\u0003\nϕ~x (z) , ϕ~xR (zR ) = E~xR exp hzR ,~xR i\n\u0002\n\b\n\u0001\u0003\n(8)\n= E~x exp  Re hz,~xi .\n\nIt follows directly from Eq. (7) that for independent complex\nr.v.s (s1 , . . . , sp )T = ~s,\nϕ~s (z) =\n\np\nY\n\nϕsk (zk ).\n\n(9)\n\nk=1\n\nUsing a standard property of real c.f.s and the properties of\nthe isomorphism (3), we have a useful relation for the c.f. of\nan r.vc. ~x and the c.f. of the linearly transformed r.vc. C~x.\nNamely, for any complex matrix C, we have\nϕC~x (z) =ϕ(C~x)R (zR ) = ϕC R~xR (zR ) = ϕ~xR ((C R )T zR )\n=ϕ~xR ((C H )R zR ) = ϕ~xR ((C H z)R ) = ϕ~x (C H z).\n(10)\nFinally, a c.f. ϕ~x (z) is called analytic if ϕ~xR (zR ) is an analytic\nc.f. [29], i.e., the real c.f. ϕ~xR (zR ) has a regular extension\ndefined on C2p in some neighborhood of the origin.\nD. Second-order statistics of complex random vectors\nAn r.vc. ~x has\norder or weak second order [14]\n\u0002 finite second\n\u0003\nstatistics if E~x |h~x, zi|2 < ∞ for all z ∈ Cp . This is clearly\nequivalent to the existence of finite second order statistics for\n\n3\n\nboth real r.vc.s ~xR and ~xI . All r.vc.s in this section are assumed\nto have finite second order statistics. Such r.vc.s are in general\ncalled second-order complex r.vc.s.\nThe second-order statistics between two real r.vc.s may be\ndescribed by\nmatrix. The complex covariance\n\u0003\n\u0002 the covariance\nmatrix cov ~x1 ,~x2 of two complex r.vc.s ~x1 and ~x2 may be\ndefined as\n\u0002 \u0003 \u0003\n\u0002 \u0003\n\u0002\n\u0003\n\u0002\ncov ~x1 ,~x2 , E~x1 ,~x2 (~x1 − E~x1 ~x1 )(~x2 − E~x2 ~x2 )H . (11)\nHowever, considering the real representations of the complex\nr.vc.s, it can be seen that the complex covariance matrix does\nnot give complete second order description.\n\u0003For that we define\n\u0002\nthe pseudo-covariance matrix1 pcov ~x1 ,~x2 [11] as\n\u0002 \u0003 \u0003\n\u0002 \u0003\n\u0002\n\u0003\n\u0002\npcov ~x1 ,~x2 , E~x1 ,~x2 (~x1 − E~x1 ~x1 )(~x2 − E~x2 ~x2 )T\n\u0003\n\u0002\n= cov ~x1 ,~x∗2 .\n(12)\n\nTwo complex r.vc.s ~x1 and ~x2 are uncorrelated\nif real r.vc.s\n\u0002\n\u0003\n(~x1 )R and (~x2 )R are uncorrelated, i.e., cov (~x1 )R , (~x2 )R =\n02p×2p , where 02p×2p denotes the 2p × 2p matrix of zeros.\nThen, by using the properties from the previous section, the\nfollowing lemma [11] follows directly.\nLemma 2:\nr.vc.s\n\u0003 ~x2 are uncorrelated if and\n\u0003\n\u0002 ~x1 and\n\u0002 Complex\nonly if cov ~x1 ,~x2 = pcov ~x1 ,~x2 = 0p×p .\nAs it is the case with real r.vc.s, the internal correlation\nstructure of a single r.vc. ~x may be of interest in addition\n\u0002 \u0003\nto correlation\nbetween\n\u0002 \u0003\n\u0002 \u0003two r.vc.s. \u0002Then\u0003 we define cov ~x ,\ncov ~x,~x and pcov ~x , pcov ~x,~x , and call them the\ncovariance matrix and the pseudo-covariance matrix of an\nr.vc. ~x, respectively.\nIt is easily seen that the covariance\n\u0002 \u0003\nmatrix cov ~x is Hermitian and the pseudo-covariance matrix\nis symmetric. Since all\n\u0002 \u0003 r.vc.s are assumed to be full, the\ncovariance matrix cov ~x is also positive definite. R.vc. ~x is\nsaid to have uncorrelated components if all its marginal r.v.s\nxk and xl , k 6= l, are uncorrelated. The following lemma is a\nsimple consequence of Lemma 2.\nLemma 3: A complex r.vc. ~x has uncorrelated components\nif and only if its covariance matrix and pseudo-covariance\nmatrix are diagonal.\n\u0002 \u0003\nAn r.vc. ~x is said to be\u0002 spatially\nwhite, if cov ~x = σ 2 I p\n\u0003\nfor some σ 2 > 0. If pcov ~x = 0p×p , then the r.vc. is called\nsecond order circular (or circularly symmetric). Some authors\nprefer the term proper [11], [14]. Circular r.vc.s have gained\nmost of the attention in the literature of complex r.vc.s. This is\nlikely due to the fact that all the second order information of\ncircular r.vc.s is contained in the covariance matrix, which, on\nthe other hand, behaves like the covariance matrix for the real\nr.vc.s. However, in this paper we need the complete secondorder description to be derived next. Our approach is to our\nbest knowledge novel, mainly based on the following theorem.\nFor alternative characterizations, see [12]–[14].\nTheorem 1: Any full complex p-dimensional r.vc. ~x with\nfinite second order statistics can be transformed by using\na nonsingular square matrix C such that the r.vc. ~s =\n(s1 , . . . , sp )T = C~x has the following properties:\n1 The pseudo-covariance matrix is called the relation matrix in [12] and the\ncomplementary covariance matrix in [13].\n\n\f4\n\nIEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 52, NO. 3, MARCH 2006\n\n\u0002 \u0003\n(i) cov ~s\u0002 \u0003= I p\n\u0002 \u0003\n\u0002 \u0003\n(ii) pcov ~s = diag(λ ~s ), where λ ~s = (λ1 , . . . , λp )T\ndenotes a vector such that λ1 ≥ · \u0002· · ≥ \u0003λp .\n\u0002 \u0003\nProof:\ncov C~x = C cov ~x C H\n\u0002 It\u0003is easily verified\n\u0002 \u0003 that\nand pcov C~x = C pcov ~x C T . By Corollary 4.6.12(b) in\n[27], if a matrix A is Hermitian and positive definite and a\nmatrix B is symmetric, then there exists a nonsingular matrix\nC such that CAC H = I p and CBC T is a diagonal matrix\nwith nonnegative diagonal entries. Since the covariance matrix\nis Hermitian and positive definitive and the pseudo-covariance\nmatrix is symmetric, the proof is completed by noticing that\nthe diagonal entries can be ordered by permutating the rows\nof C.\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003\nSince\n\u0002 cov x\u0003I and pcov x =\n\u0002 \u0003 cov x \u0002 =\u0003 cov xR +\ncov xR − cov xI + 2 cov xR , xI for any complex\nr.v. \u0002 x \b = xR \b+ x\n\u0003 I , it follows that in \u0002Theorem\n\b\n\u0003 1\ncov\u0002Re\bsk ,\u0003Im sk = 0 and 1 ≥ λk = cov Re sk −\n≥ 0, k = 1, . . . , p. The r.vc.s satisfying the\ncov Im sk\nproperties of Theorem 1 have a special structure, and they are\nhere called strongly uncorrelated.\nAny strongly uncorrelated\n\u0002 \u0003\nr.vc. is white with cov ~s = I p , but the converse is not true.\nIn general, for a given r.vc. ~x, the strongly uncorrelated r.vc. ~s\nand the strong-uncorrelating transform C given by Theorem 1\nare not unique. However, we have the following.\u0002 \u0003\nTheorem 2: For a given r.vc. ~x, the vector λ ~s in Theorem 1 is unique.\nProof: Suppose there exist two nonsingular transformations C 1 and C 2 such that r.vc.s ~s1 = C 1~x and ~s2 = C 2~x\nsatisfy the properties in Theorem 1. Let C 1 = U 1 Λ1 V H\n1\nand C 2 = U 2 Λ2 V H\n2 be the singular value decompositions\n(SVD) \u0002(see\nmatrices. Now \u0002I p\u0003 =\n\u0003 [27]) of the \u0002transform\n\u0003 H\nC 1 cov ~x C H\n=\nC\ncov\n~\nx\nC\n,\nand\n2\n1\n2\n\u0002 \u0003therefore cov ~x =\n−2 H\n−2 H\nV 1 Λ1 V 1 = V 2 Λ2 V 2 . Since cov ~x is positive definite,\nH\nit follows V 1 Λ1 V H\n1 = V 2 Λ2 V 2 . Now\n\u0002 \u0003\n\u0002\n\u0003\npcov ~s1 =U 1 Λ1 V H\nx V ∗1 Λ1 U T1\n1 pcov ~\n\u0002 \u0003 ∗\nH\n=U 1 (V H\nx V 1 Λ1 (V T1 V ∗1 )U T1\n1 V 1 )Λ1 V 1 pcov ~\n\u0002\n\u0003\nH\n=U 1 V H\nx (V ∗1 Λ1 V T1 )V ∗1 U T1\n1 (V 1 Λ1 V 1 ) pcov ~\n\u0002 \u0003 ∗\nH\n=U 1 V H\nx (V 2 Λ2 V T2 )V ∗1 U T1\n1 (V 2 Λ2 V 2 ) pcov ~\n\u0002 \u0003\nH\nH\n=U 1 V H\nx\n1 V 2 (U 2 U 2 )Λ2 V 2 pcov ~\nV ∗2 Λ2 (U T2 U ∗2 )V T2 V ∗1 U T1\n\n\u0002 \u0003\nH\nH\n=U 1 V H\nx\n1 V 2 U 2 (U 2 Λ2 V 2 pcov ~\n\nV ∗2 Λ2 U T2 )U ∗2 V T2 V ∗1 U T1\n\u0002 \u0003 ∗ T ∗ T\nH\ns2 U 2 V 2 V 1 U 1 ,\n=U 1 V H\n1 V 2 U 2 pcov ~\n\n(13)\n\u0002 \u0003\n\u0002 \u0003\nis unitary, pcov ~s1 and pcov ~s2\nand since U 1 V\nhave \u0002the\u0003 same singular\n\u0002 \u0003 values. Since by the assumption\npcov ~s1 and\u0002 pcov\n\u0003 ~s2 are\n\u0002 \u0003diagonal with sorted entries, it\nfollows pcov ~s1 = pcov ~s2 .\nRemark 1: The proof of Theorem 2 gives a way to construct\na strong-uncorrelating transform C as follows:\n\u0002 \u0003− 1\n(i) Find the usual whitening transform D = cov\u0002 ~x\u0003 2 , i.e.,\nthe inverse of the matrix square root of cov ~x .\n(ii) Any symmetric matrix B has a special form of SVD\nknown as Takagi’s factorization (see [27]). The factorizaH\nH\n1 V 2U 2\n\ntion is given as B = U ΛU T , where U is unitary and Λ\nis a diagonal matrix with real nondecreasing nonnegative\nmain diagonal entries. An example of\u0002 the \u0003factorization\nis given in Eq. (13). Hence, find pcov D~x = U ΛU T .\n(iii) Set C = U H D.\n\u0002 \u0003\nNotice also that the vector λ ~s contains the singular values\nof the pseudo-covariance matrix of a white r.vc. with unit\nvariances.\nThe previous theorems lead to a useful characterization of\nsecond-order complex r.vc.s. \u0002 \u0003\n\u0002 \u0003\nDefinition 1: The vector λ ~x , λ ~s = (λ1 , . . . , λp )T in\nTheorem 1 is called the circularity spectrum of an r.vc. ~x. An\nelement of the circularity spectrum corresponding to an r.v. is\ncalled a circularity coefficient.\nAny r.vc. ~x is clearly second order circular \u0002if\u0003and only if\nits circularity spectrum is a zero vector, i.e., λ ~x = 0p×1 .\nCorollary 1: If the circularity spectrum of an r.vc. has\ndistinct elements, all rows corresponding to nonzero circularity\ncoefficients of the strong-uncorrelating transform are unique\nup to multiplication of the row by −1. A row corresponding\nto the zero coefficient is unique up to multiplication of the\nrow by eθ , θ ∈ R.\nProof: The left unitary factor in the SVD of a block\nmatrix with distinct singular values is determined up to right\nmultiplication by the matrix Λ = diag(eθ1 , . . . , eθp ) and the\nright unitary factor is determined by the left unitary factor [27].\nIn the special form for a symmetric matrix (Takagi’s factorization), θk = 0 or θk = π for the values of k corresponding\nH\nto nonzero singular values. Therefore, U 1 V H\n1 V 2 U 2 = Λ in\nEq. (13), and\nH\nH\nC 1 =U 1 Λ1 V H\n1 = U 1 V 1 (V 1 Λ1 V 1 )\nH\nH\n=ΛU 2 V H\n2 (V 2 Λ2 V 2 ) = ΛU 2 Λ2 V 2 = ΛC 2\n\n(14)\n\nby the proof of Theorem 2.\nSome properties of the circularity coefficient are listed in\nthe following lemma, whose proof is given in Appendix I.\nLemma 4: Let x and y be uncorrelated second-order complex r.v.s. Then\n\u0002\u0003\n\u0002 \u0003\n\u0002 \u0003\n| pcov x |\n\u0002\n\u0003 ≤ 1 for any nonzero\n(i) 0 ≤ λ cx = λ x =\ncov x\n\nconstant\nc ∈ C,\n\u0002 \u0003\n(ii) λ x = 1 if and only if x = c(sR + α) for some unit\nvariance real r.v. sR and deterministic constants 0 6= c ∈\nC, α ∈ R,\n\u0002\u0003\n\u0002\u0003\n\u0002\n\u0003\n\u0002 \u0003 \u0002 \u0003\n| pcov x +pcov y |\n\u0002\u0003\n\u0002\u0003\n(iii) λ x + y =\n≤ max{λ x , λ y }\ncov x +cov y\n\u0002 \u0003\n\u0002 \u0003\nwith the \u0002equality\nif and only\n\u0003\n\u0002 \u0003 if λ\u0002 x\u0003 = λ y and\nArg(pcov x ) = Arg(pcov y ) if λ x 6= 0.\n\nE. Complex normal random vectors\nThere are no commonly agreed definitions of what is meant\nby complex normal r.vc.s. It is natural to require that a r.vc.\n~x is normal (Gaussian) if the real r.vc. ~xR is multivariate\nnormal. Such r.vc.s are generally called wide sense normal\nr.vc.s [14]. Since the real complex normal r.vc. is completely\ncharacterized by its mean vector and covariance, the results\nfrom the previous section show that a wide sense complex\n\n\fERIKSSON AND KOIVUNEN: COMPLEX RANDOM VECTORS AND ICA MODELS\n\nnormal r.vc. is completely specified by its mean, covariance\nmatrix, and pseudo-covariance matrix.\nHowever, all wide sense normal r.vc.s do not possess all the\nproperties that real normal r.vc.s do. Only a special subclass\nof wide sense normal r.vc.s has a density function similar to\nthe real r.vc.s [21], [22], maximizes the entropy [11], or has\nthe 2-stability property (Polya’s characterization) [26]. Such\nr.vc.s are called narrow sense normal r.vc.s [14]. They are\nwide sense normal r.vc.s such that the real and imaginary parts\nof any linear projection of the r.vc. are independent and have\nequal variances. This condition is equivalent to the requirement\nthat a wide sense normal r.vc. is second order circular (see,\ne.g., [11]).\nIn order to establish the properties of the complex ICA\nmodel of Eq. (1), neither wide sense normal in its full generality nor narrow sense normal is adequate, and a more specific\ncharacterization of complex normal r.vc.s is needed. This is\ndone next. From now on, we will use the term “complex\nnormal” to mean wide sense complex normal r.vc.\nThe main result is the following decomposition theorem for\ncomplex normal random vectors.\nTheorem 3: An r.vc. ~n is complex normal with circularity\nspectrum λ if and only if\n~n = C(~\nηR + ~\nηI ) + µ\n\n(15)\n\nfor some nonsingular matrix C, a complex constant vector µ,\nand multinormal\nreal independent r.vc.s ~ηR ∼ N 0p×1\u0001, 12 I p +\n\u0001\n1\nand ~ηI ∼\nN 0p×1 , 21 I p − 12 diag(λ)\n2 diag(λ)\n\u0002 \u0003\n\u0002\n\u0003\n\u0002 \u0003. Also\nH\nT\ncov ~n = CC , pcov ~n = C diag(λ)C , and E~n ~n = µ.\nProof: \u0002It is\nthat the r.vc.\n\u0003 obvious H\n\u0002 \u0003 ~n in Eq. (15) is complex\nnormal,\ncov\n~\nn\n=\nCC\n,\npcov\n~n = C diag(λ)C T , and\n\u0002 \u0003\nE~n ~n = µ. Thus, it remains to show that any complex normal\nr.vc. can be given the form (15).\nLet ~n be a complex normal r.vc. Without loss of generality\nassume it is zero mean. By Theorem\n\u0002 \u0003 1, there exists a\u0002 nonsin\u0003\ngular matrix D such that cov D~n = I p and pcov\nD~n =\n\u0001\n1\n1\ndiag(λ). Let ~ηR ∼ N 0p×1\n\u0001 , 2 I p + 2 diag(λ) and ~ηI ∼\n1\n1\nN 0\u0002 p×1 , 2 I p \u0003− 2 diag(λ) be real independent r.vc.s. Now\ncov ~ηR +\u0002 ~\nηI = 12\u0003I p + 21 diag(λ) + 12 I p − 12 diag(λ) = I p\nηR + ~\nηI have\nand pcov ~ηR + ~\nηI = diag(λ). Hence D~n and ~\nthe same second order structure. Since a zero mean complex\nnormal r.vc. is completely characterized by the covariance and\nthe pseudo-covariance matrices, it follows D~n = ~ηR +~\nηI , and\nthe claim follows by setting C = D−1 .\nA complex normal r.vc. ~η such that C = I p and µ = 0p×1\nin the representation (15), i.e., ~\nη=~\nηR +~\nηI , is called standard\ncomplex normal with the circularity spectrum λ. Clearly any\ncentered and strongly uncorrelated complex normal r.vc. is\nstandard. Also, it is seen that any complex normal r.vc. may\nbe alternatively specified by the mean, the circularity spectrum,\nand the (inverse of) strong-uncorrelating matrix C.\nThe previous decomposition allows the derivation of differential entropy of a complex normal r.vc. in a closed form.\nEntropy h(~n) of an r.vc. ~x is defined as the entropy [30] of the\nreal r.vc. ~xR . The following result has been implicitly derived\nin [31] without reference to circularity coefficients.\nCorollary 2: The differential entropy h(~n) of a zero-mean\ncomplex normal r.vc. ~n with the circularity coefficients λk 6=\n\n5\n\n1, k = 1, . . . , p, is given by\np\n\u0002 \u0003\u0001 1X\nlog(1 − λ2k ).\n(16)\nh(~n) = log det(πe cov ~n ) +\n2\nk=1\nProof: Let ~n = C~η \u0002be \u0003the decomposition\ngiven by\nQp\nTheorem 3. Now det(2 cov ~ηR ) = k=1 (1 − λ2k ), and the\ndifferential entropy of real-valued normal r.vc. [30] simplifies\nas\n\u0002 \u0003\u0001\n1\nh(~n) = log det(2πe cov ~nR )\n2\n\u0002\n\u0003\u0001\n1\n= log det(2πe cov C R ~ηR )\n2\n\u0001\n\u0002 \u0003\n1\n= log det(2πeC R cov ~ηR C TR )\n2\n\u0001 1\n\u0002 \u0003\u0001\n1\n= log det(πeC R C TR ) + log det(2 cov ~ηR )\n2\n2\np\nY\n\u0001 1\n\u0001\n1\n= log (πe)2p det((CC H )R ) + log\n(1 − λ2k )\n2\n2\nk=1\n\np\n\u0002 \u0003 \u0001 1X\n1\n= log (πe)2p det(cov ~n R ) +\nlog(1 − λ2k )\n2\n2\nk=1\n\np\n1X\n\n\u0002 \u0003 \u0001\n1\n= log (πe)2p det(cov ~n )2 +\n2\n2\n\nk=1\n\nlog(1 − λ2k )\n\np\n\u0002 \u0003 \u0001 1X\nlog(1 − λ2k )\n= log det(πe cov ~n ) +\n2\nk=1\n\n(17)\n\nby the properties of Lemma 1.\nSince the summation term on the right of Eq. (16) is\nalways nonpositive and the entropy of real r.vc.s with the\ngiven covariance is maximized for Gaussian r.vc.s [30], it\nmay be seen that the entropy of complex r.vc.s with the given\ncovariance is maximized for a narrow sense complex normal\nr.vc. [11], i.e., for a complex normal r.vc. with zero pseudocovariance. Theorem 3 allows also an easy derivation of the\nc.f. of a complex normal r.vc. [12], [14].\nCorollary 3: The c.f. of a complex normal r.vc. ~n is given\nby\n\u0002 \u0003\n\u0002 \u0003\n1 \b\n1\nϕ~n (z) = exp − zH cov ~n z − Re zH pcov ~n z∗\n4 \b\n4\n\u0002 \u0003 \u0001\n+  Re zH E~n ~n\n\u0002 \u0003\n\u0002 \u0003\n1 \b\n= exp − Re hz, cov ~n z + pcov ~n z∗ i\n4 \b\n\u0002 \u0003 \u0001\n+  Re hz, E~n ~n i .\n(18)\nProof: By Theorem 3, ~n = C(~ηR + ~\nηI ) + µ. Let z =\nzR + zI ∈ Cp , and ~η = ~ηR + ~\nηI . Now\n\u0002 \u0003 \u0001\n1\nϕη~ (z) =ϕη~R (zR ) = exp − (zTR cov ~ηR zR )\n2\n1 T\n= exp − (zR (I p + diag(λ))zR\n4\n\u0001\n+ zTI (I p − diag(λ))zI )\n(19)\n1\n= exp − (zTR zR + zTI zI + zTR diag(λ)zR\n4\n\u0001\n− zTI diag(λ)zI )\n\b\n\u0001\n1\n= exp − (zH z + Re zT diag(λ)z ) ,\n4\n\n\f6\n\nIEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 52, NO. 3, MARCH 2006\n\nand by Eq. (10)\n\b\n\u0001\nϕ~n (z) =ϕC~η+µ (z) = ϕC~η (z) exp  Re hz, µi\n\b\n\u0001\n=ϕη~ (C H z) exp  Re hz, µi\n\b\n\u0001\n1\n= exp − (zH CC H z + Re zT C ∗ diag(λ)C H z )\n4\b\n\u0001\nexp  Re zH µ\n\b\n1\n= exp − (zH CC H z + Re zH C diag(λ)C T z∗ )\n\b4\n\u0001\n+  Re zH µ .\n(20)\nCorollary 3 shows in particular that the second characteristic function ψ~x , log ϕ~x of a complex r.vc. ~x is a second-order\nwide sense polynomial in variables (z, z∗ ). Theorem 3 can be\nalso used to derive the density function of a complex normal\nr.vc. However, unlike the c.f., the density function of a wide\nsense normal r.vc. does not appear to have a simple form. See\n[12] for expressions for the density function in terms of the\ncovariance and the pseudo-covariance matrices. The following\nexample essentially shows that in some cases the distribution\nof a standard complex normal r.vc. is invariant to orthogonal\ntransformations.\nExample 1: Let the components of ~n be uncorrelated complex normal r.v.s with the same circularity coefficient λ.\nNow for a diagonal matrix Λ the r.vc. Λ~n is standard complex normal with the circularity spectrum (λ · · ·\u0002 λ)T , \u0003and\nfor any\northonormal matrix O, cov\u0002 OΛ~n\u0003 =\n\u0002 (real-valued)\n\u0003 H\nT\nO cov Λ~\nn\nO\n=\nOI\n= I p and pcov OΛ~n =\npO\n\u0002 \u0003 T\nO pcov Λ~n O = O(λI p )O T = λI p . Therefore, the r.vc.\nOΛ~n is also standard complex normal.\n\nr.v.s)\nx1 =\n\nn\nX\n\nαk sk and x2 =\n\nn\nX\n\nβk s k ,\n\n(21)\n\nk=1\n\nk=1\n\nwhere αk , βk ∈ C, k = 1, . . . , n, are independent, then r.v.s\nsk for which αk βk 6= 0 are complex normal.\nSketch of the proof: The complete proof is given in Appendix II and it follows the proof of the real-valued DarmoisSkitovich theorem (see [4]) with appropriate extensions to\ncomplex field. The idea is to consider two forms of the\nlogarithm of the joint c.f. of x1 and x2 following from\nindependence. This functional equation is only satisfied for\nwide sense polynomials showing that the r.v. x1 is complex\nnormal. This is only possible if r.v.s sk are complex normal.\nAlthough narrow sense complex normal r.v.s had to be\nadmitted to the complex Darmois-Skitovich theorem, it may\nstill appear in the view of Corollary 1 that complex normal\nr.v.s appearing in the theorem can not be completely arbitrary.\nThat is, it may appear that some of the circularity coefficients\nof normal r.v.s should be equal. It is true if n = 2. However,\nit is not generally true as it is shown in the next example.\nExample 2: Let ~η1 = (n1 , n2 , n3 )T be \u0002standard\ncomplex\n\u0003\n1 1 1 T\n=\n(\nnormal r.vc. with the circularity\nspectrum\nλ\n~\nη\n1\n3, 5, 8) .\n\u0001\n1\n3 5 4\nThen ~η2 = 5√\n~\nη\nis\nalso\nstandard\ncomplex\nnormal\n1\n3\n−5\n4\n2\n\u0002 \u0003\nr.vc. with the circularity spectrum λ ~η2 = ( 15 , 15 )T . Thus\nmarginals of ~η2 are independent, and the Darmois-Skitovich\ntheorem applies. However, the circularity spectrum of ~η1 is\ndistinct. Notice also that by Example 1, the r.vc. obtained from\n~η2 by multiplying with any orthogonal matrix is also standard\ncomplex normal r.vc. with the same circularity spectrum.\nIII. C OMPLEX ICA M ODELS\n\nF. Darmois-Skitovich theorem for complex random variables\nOne of the main characterization theorems for real r.v.s\nis the well-known Darmois-Skitovich theorem (see [4]). The\ntheorem is fundamental for proving the identifiability of real\nICA models [1], [5]. Here we extend the theorem to complex\nr.v.s.\nThe proofs of the complex Darmois-Skitovich theorem and\nthe proof of a closely related characterization theorem (Theorem 5 in Section 5) are both based on a complex functional\nequation (Lemma 5 in Appendix II). The functional equation is\nan extension of the corresponding equation for real variables\n(see, e.g., Lemma 1.5.1 in [4]) to complex variables. Using\nthe mapping (3) Lemma 5 may be easily seen to be a direct\nconsequence of the real multivariate theorem [32] (see also\n[4], [33]). A direct proof is given in Appendix II for the sake\nof completeness.\nThe complex extension of Darmois-Skitovich theorem has\nexactly the same form as the real theorem with the wide\nsense complex normal r.v.s taking the role of real normal r.v.s.\nHence, this theorem is an example where the analogy [22]\nbetween theories of narrow sense complex normal r.v.s and\nreal normal r.v.s is broken.\nTheorem 4 (Complex Darmois-Skitovich): Let sk , . . . , sn\nbe mutually independent complex r.v.s. If the linear forms (the\n\nIn this section, we show that complex ICA is actually a\nwell-defined concept, and we establish theoretical conditions\nsimilar to the real-valued case [5]. In Section III-A the main\ndefinitions along with some illustrative examples are given.\nAlso a crucial characterization theorem giving a connection between vector coefficients and complex normal r.v.s is proved.\nFinally, in sections III-B, III-C, and III-D the conditions for\nseparability, identifiability, and uniqueness of complex ICA\nmodels, respectively, are derived.\nA. Definitions and problem statement\nA general linear instantaneous complex-valued ICA model\nmay be described by the equation\n~x = A~s,\nT\n\n(22)\n\nwhere (s1 , . . . , sm ) = ~s are unknown complex-valued independent non-degenerate r.v.s, i.e., sources, A is a complex\nconstant p × m unknown mixing matrix, p ≥ 2, and ~x =\n(x1 , . . . , xp )T are mixtures, i.e., the observed complex r.vc.\n(sensor array output). The couple (A,~s) is called a representation of r.vc. ~x. If no column in the mixing matrix A is collinear\nwith another column in the matrix, i.e., all columns are pairwise linearly independent, the representation is called reduced.\nAll representations are assumed to be reduced throughout\n\n\fERIKSSON AND KOIVUNEN: COMPLEX RANDOM VECTORS AND ICA MODELS\n\nthis paper. Furthermore, a reduced representation for the r.vc.\n~x in the model (22) is called proper, if it satisfies all the\nassumptions made about the model.\nThe model of Eq. (22) is defined to be\n(i) identifiable, or the mixing matrix is (essentially) unique,\nif in every proper representations (A,~s) and (B,~r) of\n~x, every column of complex matrix A is collinear with\na column of complex matrix B and vice versa,\n(ii) unique if the model is identifiable and furthermore the\nsource r.vc.s ~s and ~r in different proper representations\nhave the same distribution for some permutation up to\nchanges of location and complex scale, and\n(iii) separable, if for every complex matrix W such that W~x\nhas m independent components, we have ΛP~s = W~x\nfor some diagonal matrix Λ with nonzero diagonals and\npermutation matrix P . Moreover, such a matrix W has\nto always exist.\nIt is completely possible for the model (22) to be identifiable\nbut not unique nor separable as it is shown in the next example.\nExample 3: As an example of a model which is identifiable\nbut is not separable nor unique, consider independent nonnormal r.v.s sk , k = 1, . . . 4. Let η1 , η2 , and η3 be independent\nstandard normal r.v.s with the same circularity coefficient.\nThen also r.v.s η1 + η2 and η1 − η2 are independent. Now\n\ns1\n\n0 1 1 \n s2 \n\n1 1 −1\ns3 + η1 \ns4 + η2\n\n\ns\n+\nη\n1\n1 + η2\n\u0013\n\n1 1 \ns2 + η1 − η2  ,\n\n1 −1 \ns3\ns4\n(23)\n\n\u0013 \u0012\n\u0012\n1\ns1 + s3 + s4 + η1 + η2\n=\n0\ns2 + s3 − s4 + η1 − η2\n\n=\n\n\u0012\n\n1 0\n0 1\n\n\u0013\n\n\n\nwhich shows that the corresponding model can not be unique.\nHowever, it is identifiable. R.v.s of the form s + n, where n\nis a normal r.v. independent of s, are said to have a normal\ncomponent.\nIt follows from the reduction assumption that the number\nof columns, i.e., the number of sources or the model order,\nis the same in every proper representation of ~x in identifiable\nmodels. If W is a separating matrix, then linear manifolds\u0001 of\nΛP and W\n\u0001 must coincide, and therefore p ≥ rank W =\nrank ΛP = m, i.e., there has to be at least as many mixtures\nas sources in a separable model. This fact also emphasizes that\nidentifiability of the model (22) depends also on the linear\noperator structure, and since the linear operators defined on\nR2p and Cp are not isomorphic, one can not simply consider\nreal-valued model with twice the observation dimension when\nstudying the complex ICA model (22). This is illustrated in\nthe following example.\nExample 4: By simply considering real-valued models with\ntwice the dimension, it may actually seem that the complex\nseparation is possible only under very strict conditions. Indeed,\nlet rk , k = 1 . . . , 4, be independent real-valued r.v.s, and let\nA1 , A2 , B 1 , and B 2 be 2 × 2 nonsingular real matrices.\n\n7\n\nDefine ~s1 = A1 (r1 r2 )T and ~s2 = A2 (r3 r4 )T . Now ~s1 and\n~s2 are independent, but so are also ~y1 and ~y2 ,\n\u0013 \u0012 −1\n\u0013\u0012 \u0013\n\u0012 \u0013 \u0012\nA1\n02×2\n~s1\nB 1 02×2\n~y1\n, (24)\nP\n=\n~s2\n02×2 B 2\n~y2\n02×2 A−1\n2\nfor any permutation matrix P . However, ~y1 and ~y2 are\nmixtures of ~s1 and ~s2 for many permutations P .\nThe previous example is easily generalized to the ICA\nmodels that have multidimensional independent sources, i.e.,\none is looking for independent multidimensional subspaces.\nThe example shows that such models can not be identified\nor separated without additional constraints on the internal\ndependency structure of the sources or the allowed mixing\nmatrices.\nSince linear operators in complex and real spaces are\nnot isomorphic, the classes of separable source r.v.s are not\nthe same. That is, some source r.v.s considered in complex\nmixtures can be separated although their real-valued representations in real mixtures can not. This is shown in the next\nexample.\nExample 5: Let η1 , . . . , η2m be independent standard zero\nmean unit variance real Gaussian r.v.s. Define\n√\n1 √\n1\n( mη1 + ηm+1 ), √ ( m − 1η2 + ηm+2 ),\n~η = √\nm\nm+1\n\u0001\n1\n. . ., √ (ηm + η2m ) .\n2\n(25)\nNow it is easily seen that ~η is a\u0002 standard\nnormal r.vc. with the\n\u0003\nm−2\nT\ndistinct circularity spectrum λ ~η = ( m−1\nm+1 , m , . . . , 0) . If\n~ηR is taken as the source r.vc. in the real-valued ICA model,\ni.e., ~y = B~ηR and B is a 2p × 2m real-valued matrix, p ≥ m,\nthe model is not separable [5]. However, the complex model\ninvolving ~η itself, i.e., ~x = A~η and A is a p × m complexvalued matrix, is separable by Corollary 1.\nThe following characterization theorem is the base of the\nidentifiablility and uniqueness theorems. It is an extension of\na real theorem [4, Theorem 10.3.1] to the complex case. The\nidea of the proof is similar to the proof of Darmois-Skitovich\ntheorem, and the proof given follows loosely that of the real\ncounterpart with appropriate complex extensions.\nTheorem 5: Let (A,~s) and (B,~r) be two reduced representations of a p-dimensional complex r.vc. ~x, where A and B\nare constant complex matrices of dimensions p × m and p × n,\nrespectively, and ~s = (s1 , . . . , sm )T and ~r = (r1 , . . . , rn )T\nare complex r.vc.s with independent components. Then the\nfollowing properties hold.\n(i) If the kth column of A is not collinear with any column\nof B, then the r.v. sk is complex normal.\n(ii) If the kth column of A is collinear with the lth column\nof B, then the logarithms of the c.f.s of r.v.s sk and rl\ndiffer by a wide sense polynomial in a neighborhood of\nthe origin.\nProof:\n(i) By Lemma 7 (see Appendix III), there exists a 2 × p\nmatrix C such that the kth column of D 1 = CA is\nnot collinear with any other column of D 1 , or with any\ncolumn of D2 = CB. Then C~x = D1~s = D 2~r, and\n\n\f8\n\nIEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 52, NO. 3, MARCH 2006\n\napplying Lemma 8(i) (see Appendix III) it is seen that\nthe r.v. sk is complex normal.\n(ii) By definitions the kth column of A, say α, is collinear\nonly with the lth column of B, say β. Therefore by\nLemma 7 (see Appendix III), there exists a 2 × p matrix\nC such that the kth column of D1 = CA is not collinear\nwith any other columns of D1 , or with any column of\nD 2 = CB except possibly the lth. Furthermore, since\nCα = C(cβ) = c(Cβ) for some c ∈ C, it is seen that\n(D 1 ,~s) and (D 2 ,~r) are reduced representations of C~x\nsuch that Lemma 8(ii) gives the claim.\n\nB. Separability\nICA is commonly used as a Blind Source Separationmethod, where the problem is to extract the original signals\nfrom the observed linear mixture. Therefore, separability of\nthe ICA model is an important issue. The separability theorem\nfor the complex ICA model below may be surprising, since it\nallows also separation of some complex normal mixtures.\nTheorem 6 (Separability): The model of Eq. (22) is separable if and only if the complex mixing matrix A is of full\ncolumn rank and there are no two complex normal source r.v.s\nwith the same circularity coefficient.\nProof:\u0001 Suppose the\u0001 model is separable. Since m =\nrank W A ≤ rank A ≤ m, the mixing matrix A is\nof full column rank m. If there were two complex normal\nsource r.v.s with the same circularity coefficient, by Example 1\nin Section II-E, there would exist matrices that produce m\nindependent components but which are not diagonal matrices\nfor any permutation of the columns.\nTo the other direction, suppose the mixing matrix A is of\nfull column rank and there are no two complex normal source\nr.v.s with the same circularity coefficient. Now A# , where the\nsuperscript # denotes the Moore-Penrose generalized inverse\n[27], is a separating matrix. Suppose W is a matrix such\nthat W~x has m independent components. If W A is not of\nthe form ΛP , then there exist at least two columns such\nthat they both contain at least two nonzero elements. By\nLemma 10 (see Appendix III) there can not exist only one\nsuch column since the sources are nondegenerate. Assume\nwithout loss of generality that the first l columns βk , k =\n1, . . . , l ≤ m, of W A are columns with at least two nonzero\nelements, and denote the corresponding matrix of rank l by\nB = (β 1 · · · βl ). By Theorem 4 the r.v. sk corresponding\nto the column β k , k = 1, . . . , l, is complex normal, and\nwe assume, without loss of generality, that the r.vc. ~η1 =\n(s1 · · · sl )T is standard complex normal. By Theorem 10\n(see Appendix II) all components of ~n2 = B~\nη1 are complex\nnormal, and by Lemma 9 (see Appendix III) all components\nof ~n2 are independent. Choose any l rows of B such that\nthe corresponding submatrix B̂ is of rank l, and B̂ contains\na row with two nonzero elements. Since B̂ is not diagonal\nfor any permutation by construction, ~\nη1 is standard, and ~n2\nhas independent components, it follows from Corollary 1 that\nη1 can not have a distinct circularity spectrum, which is a\n~\ncontradiction. Therefore, W A is of the form ΛP , and the\nmodel is separable.\n\nRemark 2: If the source ~s has finite\n\u0002 \u0003 second order statistics and the circularity spectrum λ ~s is distinct, then the\nseparation can be achieved by simply performing the stronguncorrelating transform by Corollary 1. In this case, there is\nno additional restrictions on the distribution of the source r.v.s,\nand therefore some normal r.v.s can be also separated. An\nexample of such a mixture is seen in Example 5.\nC. Identifiability\nIdentifiability considers reconstruction of the mixing matrix.\nThis is useful in some problems, where the immediate interest\nmay not be in the sources themselves but in how they were\nmixed (e.g., channel matrix in MIMO communications).\nTheorem 7 (Identifiability): The model of eq. (22) is identifiable, if\n(i) no source r.v. is complex normal, or\n(ii) A is of full column rank and there are no two complex\nnormal source r.v.s with the same circularity coefficient.\nProof:\n(i) Since there are no complex normal r.v.s, by Theorem 5(i),\nevery column has to be collinear with exactly a column\nin another proper representation, i.e., the model is identifiable.\n(ii) Let(A,~s) and (B,~r) be proper representations of ~x.\nSince the model is separable by Theorem 6 and A#\nis a separating matrix, A# B = P Λ for a permutation\nmatrix P and a diagonal matrix Λ. By the uniqueness\nof the generalized inverse, it follows AP Λ = B.\nThere is a striking contrast between the two cases in\nTheorem 7. Namely, if there are more sources than mixtures\nnot a single normal r.v. is allowed whereas in the other case\nall source r.v.s can be normal. The following example shows\nthe reason why we can not allow a single normal r.v. for\nidentifiability when there are more sources than sensors.\nExample 6: Consider independent non-normal r.v.s s1 , s2 ,\nand standard normal r.v.s η1 and η2 with the same circularity\ncoefficient. Now\n\n\n\u0013 \u0012\n\u0012\n\u0013\ns1\n1 1 0 \ns1 + s2 + 2η1\ns2 + 2η1 \n=\n~x =\n1 0 1\ns1 + 2η2\n2η2\n\n\n\u0012\n\u0013 s1 + η1 + η2\n1 1 1 \n,\ns2\n=\n1 0 −1\nη1 − η2\n(26)\nand the last column shows that the model is not identifiable.\nIt is evident from the previous example and from the\nseparation theorem that another identifiability condition could\nbe formulated by essentially allowing a single normal r.v. and\nnot allowing other source r.v.s to have normal components\nwith the same circularity coefficient. However, this condition\nis unnecessarily complicated. Therefore, it is not stated in a\nformal manner.\n\n\fERIKSSON AND KOIVUNEN: COMPLEX RANDOM VECTORS AND ICA MODELS\n\nD. Uniqueness\nUniqueness considers the case where one is interested not\nonly in the mixing matrix but also in the distribution of the\nsources.\nTheorem 8 (Uniqueness): The model of Eq. (22) is unique\nif either of the following properties hold.\n(i) The model is separable.\n(ii) All c.f.s of source r.v.s are analytic (or all c.f.s are\nnon-vanishing), and none of the c.f.s has an exponential\nfactor with a wide sense polynomial of degree at least\ntwo, i.e., no source r.v. has the c.f. ϕ such that ϕ(z) =\nϕ1 (z) exp(P(z, z ∗)) for a c.f. ϕ1 (z) and for some wide\nsense polynomial P(z, z ∗) of degree at least two.\nProof:\n(i) Let(A,~s) and (B,~r) be proper representations of ~x. By\nTheorem 7(ii) the model is identifiable, and therefore\nAP Λ = B for a permutation matrix P and a diagonal\nmatrix Λ. Now ~s = A#~x = A# B~r = P Λ~r.\n(ii) There can not be any complex normal r.v.s, and therefore\nthe model is identifiable by Theorem 7(i). Now the\nlogarithms of the c.f.s of the source variables in two\nproper representations differ by a wide sense polynomial\nby Theorem 5(ii). However, by the assumption this wide\nsense polynomial can be at most of degree 1, i.e., the\nsource variables have the same distribution up to changes\nof location and complex scale.\nA nonunique but identifiable mixture was described in Example 3. By slightly restricting the allowed mixing matrices,\nit is possible in the real case to obtain more classes of unique\nmodels [5]. Further work is needed to determine if those\ntheorems can be extended to the complex case.\nIV. C ONCLUSION\nIn this paper conditions for separability, identifiablity, and\nuniqueness of complex-valued linear ICA models are established. Both circular and noncircular complex random vectors\nare covered by the results. So far these conditions have\nbeen known for real random vectors only. The conditions for\nidentifiablity, and uniqueness are sufficient and the separability\ncondition is also found to be necessary. In order to show these\nresults, a proof of complex extension of the Darmois-Skitovich\nTheorem is constructed. Some second-order properties and\ncharacterizations of linear forms of complex random vectors\nare reviewed and new results found in the process of proving\nthe theorem. As a by-product of establishing the conditions,\na theorem on differential entropy for complex normal random\nvectors is proved and a slightly surprising result about separating complex Gaussian sources is found.\nACKNOWLEDGMENT\nThe authors wish to thank the anonymous reviewers for their\nvaluable comments and suggestions.\n\n9\n\nA PPENDIX I\nP ROOF OF L EMMA 4\nProof of Lemma 4: By Theorem 1 there exist nonzero\nconstants a, b ∈ C such that r.v.s s = ax and r = by are\nstrongly uncorrelated.\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003\n(i) Since\n\u0003 sR \u0002+ \u0003cov sI = 1,\u0002 0 \u0003≤ λ x = aλ s =\n\u0002 cov\ncov sR − cov sI = 1 − 2 cov\u0002 sI\u0003 ≤ 1. \u0002Also\n\u0003 c (cx)\n\u0002 =\n\u0003\ns, and thus by uniqueness λ cx = λ s = λ x .\nFurthermore\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003 | pcov s |\n| pcov ax |\n\u0002\n\u0003\n\u0002\n\u0003\n=\nλ x = pcov s =\ncov s\ncov ax\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003\n|a2 pcov x |\n|a2 || pcov x |\n| pcov x |\n\u0002 \u0003 =\n\u0002 \u0003 =\n\u0002 \u0003 .\n= 2\n|a| cov x\n|a|2 cov x\ncov x\n(27)\n\u0002 \u0003\n\u0002 \u0003\n\u0002 \u0003\n(ii) λ x = 1 −\u00022\u0003cov sI \u0002=\u00031 if and only if cov sI = 0.\n(iii) Suppose λ x ≥ λ y . Using the first part of the\nlemma for an r.v. x + y, uncorrelateness, and the triangle\ninequality, we have\n\u0002 \u0003\n\u0002 \u0003\n\u0002\n\u0003\n\u0002\n\u0003 | pcov x + y |\n| pcov x + pcov y |\n\u0002\n\u0003 =\n\u0002 \u0003\n\u0002 \u0003\nλ x+y =\ncov x + y\ncov x + cov y\n\u00021 \u0003\n\u00021 \u0003\n| pcov a s + pcov b r |\n\u0002 \u0003\n\u0002 \u0003\n=\ncov a1 s + cov 1b r\n\u0002\n\u0003\n\u0002 \u0003\n| a12 pcov s + b12 pcov r |\n=\n1\n1\n|a|2 + |b|2\n\u0002\n\u0003\n\u0002\n\u0003\n| a12 λ x + b12 λ y |\n=\n1\n1\n|a|2 + |b|2\n\u0002\n\u0003\n\u0002 \u0003\n1\n1\n\u0002 \u0003\n|a|2 λ x + |b|2 λ y\n≤\n≤λ x ,\n1\n1\n|a|2 + |b|2\n(28)\nwhich proves the inequality.\nIf both r.v.s x and y are second order circular, then clearly\nthe equality holds in (28). Now suppose the condition for\nthe\n\u0002 \u0003equality\n\u0002 \u0003holds in the noncircular\n\u0002 \u0003 case, and let \u0002λ \u0003=\nλ x = λ y and θ = Arg(pcov x ) = Arg(pcov y ).\nThen\n\u0002 \u0003\n\u0002 \u0003\n\u0002\n\u0003 | pcov x + pcov y |\n\u0002 \u0003\n\u0002 \u0003\nλ x+y =\ncov x + cov y\n\u0002 \u0003\n\u0002 \u0003\n|λ cov x eθ + λ cov y eθ |\n\u0002 \u0003\n\u0002 \u0003\n(29)\n=\ncov x + cov y\n\u0002\n\u0003\n\u0002\n\u0003\n|λeθ || cov x + cov y |\n\u0002 \u0003\n\u0002 \u0003\n= λ.\n=\ncov x + cov y\n\nTo the other direction,\n\u0002 the\n\u0003 last\n\u0002 \u0003inequality \u0002in \u0003(28) holds\nwith the equality iff λ x = λ y . If now λ x 6= 0, then\nthe triangle inequality in (28) holds with the equality iff\n\u0002 \u0003\n\u0002 \u0003\nb2 pcov s\npcov x\nb2\n\u0002 \u0003=\n\u0002 \u0003.\n(30)\n0< 2 = 2\na\na pcov r\npcov y\n\u0002 \u0003\n\u0002 \u0003\nHence Arg(pcov\ny ) by the polar\n\u0002 \u0003 x ) = Arg(pcov\n\u0002 \u0003\nforms of pcov x and pcov y .\n\n\f10\n\nIEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 52, NO. 3, MARCH 2006\n\nP ROOF\n\nOF\n\nA PPENDIX II\nTHE COMPLEX DARMOIS -S KITOVICH\n\nTHEOREM\n\n\u0002\u0003\nwhere ∆ · is the general difference operator defined by\n1\u0002\n\u0003\n∆ f (z) =f (z + a) − f (z)\n\nAND RELATED THEOREMS\n\na\n\nThe following theorem is a direct consequence of the\nmultivariate version of the real Marcinkiewicz theorem. The\ntheorem shows essentially that a complex normal r.v. is the\nonly r.v. whose second c.f. is a wide sense polynomial.\nTheorem 9 (Complex Marcinkiewicz): If in some neighborhood of zero the c.f. ϕx of a complex r.v. x admits the\nrepresentation\n\u0001\nϕx (z) = exp P(z, z ∗) ,\n(31)\n\nwhere P is a wide sense polynomial, then the r.v. x is complex\nnormal.\nProof: Fix\u0001z0 ∈ C, and define a c.f. ϕ0 (t) , ϕx (tz0 ) =\nexp P(tz0 , tz0∗ ) for t ∈ R. Then for some ε > 0, log ϕ0 (t)\nis a polynomial in t, |t| < ε. Therefore, by a version of αdecomposition theorem (see [34, Theorem 7.4.2]) the relation\nis valid for all t and ϕ0 (t) is normal. Since z0 is assumed to\nbe arbitrary, it follows that the equation (31) is valid for all\nz. By the last property of Lemma 1, P(z, z ∗) is a polynomial\nin zR , and the claim follows from the multivarite (bivariate)\nMarcinkiewicz’s theorem (e.g., [29, Theorem 3.4.3]).\nAlso the well-known Cramer’s theorem has a direct complex\ncounterpart.\nTheorem 10 (Complex Cramer): If s1 and s2 are independent r.v.s such that s1 + s2 is a complex normal r.v., then each\nof the r.v.s s1 and s2 is complex normal.\nProof: This is a direct corollary to the real multivariate\nCramer’s theorem (e.g., [34, Theorem 6.3.2]).\nLemma 5: Consider the equation, assumed valid for\n|z1 |, |z2 | < ε,\np\nX\n\nψk (z1 + ck z2 ) = h1 (z1 ) + h2 (z2 ),\n\n(32)\n\nk=1\n\nwhere ψk , k = 1, . . . , p, h1 , and h2 are continuous complexvalued functions of complex variables and the nonzero complex numbers ck , k = 1, . . . , p, are distinct. Then all the\nfunctions in (32) are wide sense polynomials in (z, z ∗ ) of\ndegree not exceeding p.\n(1)\nProof: Let dk = (1 − cckp )b1 . Now, for small enough b1 ,\nwe have\np\nX\n\nk=1\n\np\n\nX\nb1\n(1)\nψk (z1 + b1 + ck (z2 − )) =\nψk (z1 + dk + ck z2 )\ncp\nk=1\n\n=h1 (z1 + b1 ) + h2 (z2 −\nby substituting (z1 + b1 ) for z1 and (z2 −\nSubtracting (32) from (33), we obtain\np−1\nX\n\nb1\ncp )\n\nb1\n)\ncp\n(33)\n\nfor z2 in (32).\n\n(1)\n\nb1\n\n∆\n\na0 ,...,an\n\nand\n\u0002\n\u0003\nf (z) =\n\nn\n\n∆\n\na0 ,...,an−1\n\n(35)\n\n\u0002\n\u0003\nf (z + an ) − f (z)\n\nfor any constants ak ∈ C. Equation (34) is of the same form\nas (32) except the number of the terms in the sum is lower. Let\n(2)\nck\n)b2 . Again by substituting and subtracting,\ndk = (1 − cp−1\nwe obtain from (34) the equation\np−2\nX\n\n2\n\n∆\n\n(1)\n\n(2)\n\nk=1 dk ,dk\n\n\u0002\n\n2 \u0002\n\u0003\n\u0003\nψk (z1 +ck z2 ) = ∆ h1 (z1 ) +\nb1 ,b2\n\n2\n\n∆\n−b1\ncp\n\n−b2\np−1\n\n,c\n\n\u0002\n\u0003\nh2 (z2 ) .\n(36)\n\nContinuing the process, we end up with the equation\np−1\n\n∆\n\n(1)\n(p−1)\nd1 ,...,d1\n\n\u0002\n\u0003\nψ1 (z1 + c1 z2 )\n\np−1\n\n=\n\n∆\n\nb1 ,...,bp−1\n\n\u0002\n\u0003\nh1 (z1 ) +\n\n(37)\n\np−1\n\n∆\n−b1\ncp\n\n,...,\n\n−bp−1\nc2\n\n\u0002\n\n\u0003\nh2 (z2 ) .\n\nThis is the generalized Cauchy’s equation\nfor\n\u0002\n\u0003 complex ∗variables [35] showing that ∆p−1\nfor\n(1)\n(p−1) ψ1 (z) = az + bz\nd1 ,...,d1\nsome constants a, b ∈ C. Since coefficients bk are arbitrary\nin the neighborhood of zero, and by continuity, the difference\noperator structure [36] shows that ψ1 (z) is a wide sense polynomial in (z, z ∗ ) of degree not exceeding p. By renumbering,\nthe same is obtained for ψk (z), k = 1, . . . , p, and thus also\nfor h1 (z) and h2 (z).\nProof of Theorem 4: The joint c.f. of (x1 , x2 )T is given\nas\n\u0001\nϕx1 ,x2 z1 , z2\n\u0002\n\b\n\u0001\u0003\n= Ex1 ,x2 exp  Re h(z1 , z2 )T , (x1 , x2 )T i\nn\nX\n\u0002\n\b\n\u0001\u0003\n= Ex1 ,x2 exp  Re h(z1 , z2 )T ,\n(αk sk , βk sk )T i\nk=1\n\n\u0002\n= Ex1 ,x2 exp \n=\n=\n\nn\nY\n\nk=1\nn\nY\n\nn\nX\n\nk=1\n\n\b\n\u0001\u0003\nRe (αk z1 + βk z2 )sk\n\n(38)\n\n\u0002\n\b\n\u0001\u0003\nEsk exp  Re (αk z1 + βk z2 )sk\nϕsk (αk z1 + βk z2 ),\n\nk=1\n\nz1 , z2 ∈ C, by independence of r.v.s sk , k = 1, . . . , n. On the\nother hand, by independence of x1 and x2 , we have\n\u0001\nϕx1 ,x2 z1 , z2 =ϕx1 (z1 )ϕx2 (z2 )\nn\nn\nY\nY\n(39)\n=\nϕsk (αk z1 )\nϕsk (βk z2 ).\nk=1\n\nk=1\n\nThus by combining equations (38) and (39), we get\n\n1 \u0002\n1 \u0002\n1\u0002\n\u0003\n\u0003\n\u0003\n∆ ψk (z1 + ck z2 ) = ∆ h1 (z1 ) + ∆ h2 (z2 ) , (34)\n\nk=1 dk\n\nn+1\n\n−b1\ncp\n\nn\nY\n\nk=1\n\nϕsk (αk z1 + βk z2 ) =\n\nn\nY\n\nk=1\n\nϕsk (αk z1 )\n\nn\nY\n\nk=1\n\nϕsk (βk z2 ). (40)\n\n\fERIKSSON AND KOIVUNEN: COMPLEX RANDOM VECTORS AND ICA MODELS\n\nAs always, there exists a neighborhood of zero such that all\nc.f.s in Eq. (40) are nonzero. Let rk = α∗k sk and ck = βk /αk\nfor αk 6= 0, and ck = βk for αk = 0. Then, by Eq. (10), we\ncan rewrite Eq. (40) for some positive ε > |z1 |, |z2 | by setting\nψk = log ϕrk as\nl\nX\n\nψk (z1 + ck z2 ) =\n\nk=1\n\nl\nX\n\nψk (z1 ) +\n\nk=1\n\nl\nX\n\nψk (ck z2 ),\n\n(41)\n\nk=1\n\nwhere it is assumed without loss of generality that l first r.v.s\nrk , k = 1, . . . , l, are such that αk βk 6= 0, and therefore\ncomponents ψk , k > l, cancel out. By combining functions\nψk with the equal arguments to a single function ψ̃ and\nrenumbering, Eq. (41) may be rewritten as\nq\nX\n\nψ̃k (z1 + ck z2 ) =\n\nk=1\n\nl\nX\n\nψk (z1 ) +\n\nk=1\n\nq\nX\n\nψ̃k (ck z2 )\n\n(42)\n\nk=1\n\nsuch\nPl that numbers ck , k = 1, . . . , q ≤ l, are distinct. Therefore,\nsense polynomial by Lemma 5. By\nk=1 ψk (z1 ) is a wide\nPl\nTheorem 9, the r.v.\nk=1 rk is complex normal. Thus by\nTheorem 10 each r.v. rk , and hence each r.v. sk , k = 1, . . . , l,\nis complex normal.\n\nA DDITIONAL\n\nA PPENDIX III\nCHARACTERIZATION LEMMAS\n\nLemma 6: Let α1 , . . . , αm be given nonzero vectors of an\ninner product space. Then there exist a vector β, which is not\northogonal to any of the given vectors.\nProof: Suppose β is not orthogonal to any αl , l =\n1, . . . , k − 1, but is orthogonal to αk . Then a scalar c ∈ C can\nbe chosen such that hβ, αl i =\n6 −chαk , αl i for all l ≤ k. Now\nthe vector β̂ = β + cαk is not orthogonal to any αl , l ≤ k.\nSince α1 is nonzero, β 1 = α1 is not orthogonal to α1 .\nChoose β 2 = β 1 + c2 α2 , where c2 is a scalar as above if β 1\nis orthogonal to α2 , and c2 = 0 otherwise. By iterating the\nprocedure m − 1 times, it is seen that βm is a required type\nof vector.\nLemma 7: Let α1 , . . . , αm be given p-dimensional nonzero\ncomplex vectors such that α1 is not collinear with any αk ,\nk 6= 1. Then there exists a 2 × p matrix C such that Cα1 is\nnot collinear with any Cαk , k 6= 1.\nProof: Denote αk = (αk1 , . . . , αkp )T , k = 1, . . . , m.\nWithout loss of generality we assume that the coefficients αk1 ,\nk = 1, . . . , m, are either zero or one. Furthermore, we may\ntake α11 = 1 by permutating the original indices.\nSuppose α1 is not collinear with αk , i.e., α1 6= αk , for\nany k 6= 1. Define\n\u0013\n\u0012\n1\n0 ··· 0\n,\n(43)\nC=\nβ 1 β2 · · · β p\nwhere β = (β1 , . . . , βp )T is a vector such that\nhβ, (α1 − αk )∗ i =\n6 0,\n\nk = 2 . . . , m.\n\n11\n\nCα1 can be collinear with another vector Cαk only if αk1 =\n1. But then the difference\n\u0012\n\u0013 \u0012\n\u0013 \u0012\n\u0013\n1\n1\n0\nCα1 − Cαk =\n−\n=\nhβ, (α1 − αk )∗ i\nβ T α1\nβ T αk\n(45)\nis not zero by construction. Thus Cα1 is not collinear with\nany Cαk , k 6= 1, and C is a required type of matrix.\nLemma 8: Let (A,~s) and (B,~r) be two reduced representations of a 2-dimensional complex r.vc. ~x, where A and B\nare constant complex matrices of dimensions 2 × m and 2 × n\nrespectively, and ~s = (s1 , . . . , sm )T and ~r = (r1 , . . . , rn )T\nare complex r.vc.s with independent components. Then the\nfollowing properties hold.\n(i) If the kth column of A is not collinear with any column\nof B, then the r.v. sk is complex normal.\n(ii) If the kth column of A is collinear with the lth column\nof B, then the logarithms of the c.f.s of sk and rl differ\nby a wide sense polynomial in a neighborhood of the\norigin.\nProof:\n(i) Without loss of generality we assume that matrices A\nand B are scaled such that the first rows consist only of\nzeros and ones. This amounts only to the scale of r.v.s\nsl and r.v.s rl . Furthermore, since the components of ~x\ncan be interchanged if necessary, the first entry of the\nkth column of A can be taken to be one.\nAs always, there exists a neighborhood ε > 0 of zero\nsuch that all c.f.s are nonzero, and the logarithms of\nc.f.s are well-defined. Therefore for z = (z1 , z2 )T ∈ C2 ,\n|z1 | < ε, |z2 | < ε, we have using the properties (10) and\n(9) that\nlog ϕ~x (z) = log ϕ~s (AH z) = log ϕ~r (B H z)\nm\nX\nlog ϕsl (α∗1l z1 + α∗2l z2 )\n=\n=\n\nl=1\nn\nX\n\n∗\n∗\nlog ϕrl (β1l\nz1 + β2l\nz2 ),\n\n(46)\n(47)\n\nl=1\n\nwhere A = (αql ), B = (βql ). Let q be the number of\ndifferent noncollinear columns with nonzero coefficients\nin A and B other than the kth column of A. Now\nsubstituting (47) from (46), and combining the terms with\nequal nonzero coefficient arguments to functions hl , and\nwith one zero coefficient to f and g, respectively, we get\nan equation of the form\nlog ϕsk (z1 + α∗2k z2 ) +\n\nq\nX\n\nhl (z1 + γl z2 ) = f (z1 ) + g(z2 )\n\nl=1\n\n(48)\n\nif α2k 6= 0, and of the form\nq\nX\n\nhl (z1 + γl z2 ) = log ϕsk (z1 ) + g(z2 )\n\n(49)\n\nl=1\n\n(44)\n\nBy Lemma 6 such a vector β exists. Now vectors Cαk are\nagain such that the first component is either zero or one. Thus\n\nif α2k = 0. Numbers α2k , γ1 , . . . , γq are now distinct,\nand then by Lemma 5, log ϕsk must be a wide sense\npolynomial in (z, z ∗ ) of degree not exceeding q. Thus\nby Theorem 9, the r.v. sk is complex normal.\n\n\f12\n\nIEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 52, NO. 3, MARCH 2006\n\n(ii) By definitions of representations, kth column of A is\ncollinear only with the lth column of B. Thus one of the\nh’s in the proof of part (i) is the difference the logarithms\nof the c.f.s of sk and rl , and the claim follows from\nLemma 5.\nLemma 9: Suppose independent complex r.v.s s1 and s2\nare independent of complex normal r.v.s n1 and n2 . If s1 + n1\nis independent of s2 +n2 , then also n1 and n2 are independent.\nProof: Since the r.vc. (s1 , s2 )T is independent of the\nr.vc. (n1 , n2 )T , the joint c.f. can be written as\n\u0001\nϕs1 +n1 ,s2 +n2 z1 , z2\n\u0001\n\u0001\n(50)\n=ϕs1 ,s2 z1 , z2 ϕn1 ,n2 z1 , z2\n\u0001\n=ϕs1 (z1 )ϕs2 (z2 )ϕn1 ,n2 z1 , z2 .\n\nOn the other hand, using the independence of s1 + n1 and\ns2 + n2 , we have\n\u0001\nϕs1 +n1 ,s2 +n2 z1 , z2 =ϕs1 +n1 (z1 )ϕs2 +n2 (z2 )\n(51)\n=ϕs1 (z1 )ϕn1 (z1 )ϕs2 (z2 )ϕn2 (z2 ),\nand therefore\n\u0001\nϕs1 (z1 )ϕs2 (z2 )ϕn1 ,n2 z1 , z2\n=ϕs1 (z1 )ϕs2 (z2 )ϕn1 (z1 )ϕn2 (z2 ).\n\n(52)\n\nThen, in some neighborhood of zero, all c.f.s in (52) are\nnonzero, and we have\n\u0001\nϕn1 ,n2 z1 , z2 = ϕn1 (z1 )ϕn2 (z2 )\n(53)\n\nin the neighborhood. By the α-decomposition theorem [34,\nTheorem 7.4.2], the equation if valid for all z1 and z2 , i.e., n1\nand n2 are independent.\nLemma 10: If complex r.v.s n and s are independent and\nn+s is independent of n, then n is degenerate (i.e., a constant).\nProof: By Theorem 4 the r.v. n is complex normal. As\nin the proof of Lemma 9, it follows that the equation\nϕn (z1 + z2 ) = ϕn (z1 )ϕn (z2 )\n\n(54)\n\nis satisfied in a neighborhood of zero. This is only possible if\nn is a degenerate complex normal r.v., i.e., a complex normal\nr.v. with zero variance.\nR EFERENCES\n[1] P. Comon, “Independent component analysis, a new concept?” Signal\nProcessing, vol. 36, no. 3, pp. 287–314, Apr. 1994.\n[2] A. Hyvärinen, J. Karhunen, and E. Oja, Independent Component Analysis. John Wiley & Sons, 2001.\n[3] A. Cichocki and S. Amari, Adaptive Blind Signal and Image Processing:\nLearning Algorithms and Applications. John Wiley & Sons, 2002.\n[4] A. Kagan, Y. Linnik, and C. Rao, Characterization Problems in Mathematical Statistics, ser. Probability and Mathematical Statistics. New\nYork, NY: John Wiley & Sons, 1973.\n[5] J. Eriksson and V. Koivunen, “Identifiability, separability and uniqueness\nof linear ICA models,” IEEE Signal Processing Lett., vol. 11, no. 7, pp.\n601–604, July 2004.\n[6] J.-F. Cardoso, “An efficient technique for the blind separation of complex\nsources,” in Proc. HOS’93, South Lake Tahoe, CA, June 1993, pp. 275–\n279.\n[7] E. Bingham and A. Hyvärinen, “A fast fixed-point algorithm for independent component analysis of complex valued signals,” Int. J. Neural\nSystems, vol. 10, no. 1, pp. 1–8, Feb. 2000.\n\n[8] V. Calhoun and T. Adali, “Complex infomax: convergence and approximation of infomax with complex nonlinearities,” in Proc. NNSP 2002,\nMartigny, Switzerland, Sept. 2002, pp. 307–316.\n[9] S. Fiori, “Extended hebbian learning for blind separation of complexvalued sources,” IEEE Trans. Circuits Syst. II, vol. 50, no. 4, pp. 195–\n202, Apr. 2003.\n[10] J. Anemüller, T. Sejnowski, and S. Makeig, “Complex independent\ncomponent analysis of frequency-domain electroencephalographic data,”\nNeural Networks, vol. 16, no. 9, pp. 1311–1323, Nov. 2003.\n[11] F. Neeser and J. Massey, “Proper complex random processes with\napplications to information theory,” IEEE Trans. Inform. Theory, vol. 39,\nno. 4, pp. 1293–1302, July 1993.\n[12] B. Picinbono, “Second-order complex random vectors and normal distributions,” IEEE Trans. Signal Processing, vol. 44, no. 10, pp. 2637–2640,\nOct. 1996.\n[13] P. Schreier and L. Scharf, “Second-order analysis of improper complex\nrandom vectors and processes,” IEEE Trans. Signal Processing, vol. 51,\nno. 3, pp. 714–725, Mar. 2003.\n[14] N. Vakhania and N. Kandelaki, “Random vectors with values in complex\nHilbert spaces,” Theory Probab. Appl., vol. 41, no. 1, pp. 116–131, Feb.\n1996.\n[15] B. Picinbono and P. Bondon, “Second-order statistics of complex signals,” IEEE Trans. Signal Processing, vol. 45, no. 2, pp. 411–420, Feb.\n1997.\n[16] O. Grellier, P. Comon, B. Mourrain, and P. Trebuchet, “Analytical blind\nchannel identification,” IEEE Trans. Signal Processing, vol. 50, no. 9,\npp. 2196–2207, Sept. 2002.\n[17] N. Vakhania, V. Tarieladze, and S. Chobanyan, Probability Distributions\non Banach Spaces. Dordrecht, Netherlands: Reidel, 1987.\n[18] I. Ruzsa and G. Székely, Algebraic Propbability Theory. John Wiley\n& Sons, 1988.\n[19] R. Dudley, Real analysis and probability. Chapman & Hall, 1989.\n[20] G. Feldman, Arithmetic of Probability Distributions, and Characterization Problems on Abelian Groups, ser. Translations of mathematical\nmonographs. Providence, RI: AMS, 1993, vol. 116.\n[21] R. Wooding, “The multivariate distribution of complex normal variables,” Biometrika, vol. 43, no. 1/2, pp. 212–215, June 1956.\n[22] N. Goodman, “Statistical analysis based on certain multivariate complex\nGaussian distribution (An introduction),” Ann. Math. Stat., vol. 34, no. 1,\npp. 152–177, Mar. 1963.\n[23] P. Amblard, M. Gaeta, and J. Lacoume, “Statistics for complex variables\nand signals – Part I: Variables,” Signal Processing, vol. 53, pp. 1–13,\n1996.\n[24] P. Krishnaiah and J. Lin, “Complex elliptically symmetric distributions,”\nCommun. Statist. A, vol. 15, no. 12, pp. 3693–3718, 1986.\n[25] W. Hudson and J. Veeh, “Complex stable sums of complex stable\nrandom variables,” J. Mult. Anal., vol. 77, pp. 229–238, 2001.\n[26] N. Vakhania, “Polya’s characterization theorem for complex random\nvariables,” J. Complexity, vol. 13, pp. 480–488, 1997.\n[27] R. Horn and C. Johnson, Matrix Analysis. New York, NY: Cambridge\nUniversity Press, 1985.\n[28] B. Picinbono and P. Chevalier, “Widely linear estimation with complex\ndata,” IEEE Trans. Signal Processing, vol. 43, no. 8, pp. 2030–2033,\nAug. 1995.\n[29] R. Cuppens, Decomposition of Multivariate Probabilities, ser. Probability and Mathematical Statistics. Academic Press, 1975, vol. 29.\n[30] T. Cover and J. Thomas, Elements of Information Theory. John Wiley\n& Sons, 1991.\n[31] G. Tauböck, “Rotationally variant complex channels,” in Proc. 23rd\nSymp. on Inform. Theory in the Benelux, Louvain-la-Neuve, Belgium,\nMay 2002.\n[32] S. Ghurye and I. Olkin, “A characterization of the multivariate normal\ndistribution,” Ann. Math. Stat., vol. 33, no. 2, pp. 533–541, June 1962.\n[33] A. Mathai and G. Pederzoli, Characterizations of the Normal Probability\nLaw. New Delhi, India: Wiley Eastern Limited, 1977.\n[34] Y. Linnik and I. Ostrovskiǐ, Decomposition of Random Variables and\nVectors, ser. Translation of Mathematical Monographs. AMS, 1977,\nvol. 48.\n[35] J. Aczél and J. Dhombres, Functional Equations in Several Variables,\nser. Encyclopedia of Mathematics and its Applications. Cambridge,\nGreat Britain: Cambridge University Press, 1989, vol. 31.\n[36] J. Aczél, Lectures on Functional Equations and their Applications, ser.\nMathematics in Science and Engineering. New York, NY: Academic\nPress, 1966, vol. 19.\n\n\fERIKSSON AND KOIVUNEN: COMPLEX RANDOM VECTORS AND ICA MODELS\n\nJan Eriksson (M’04) received the M.Sc. degree\nin mathematics from University of Turku, Finland,\nin 2000, and the D.Sc.(Tech) degree (with honors)\nin signal processing from Helsinki University of\nTechnology (HUT), Finland, in 2004. He is currently\nworking as a postdoctoral researcher of Academy of\nFinland.\nHis research interest are in blind signal processing, stochastic modeling, digital communication, and\ninformation theory.\n\nVisa Koivunen (Senior Member, IEEE) received his\nD.Sc. (Tech) degree with honors from the University\nof Oulu, Dept. of Electrical Engineering. From 1992\nto 1995 he was a visiting researcher at the University\nof Pennsylvania, Philadelphia, USA. Year 1996 he\nheld a faculty position at the Department of Electrical Engineering, University of Oulu, Finland. From\n1997 to 1999 he was an Associate Professor at the\nSignal Processing Labroratory, Tampere University\nof Technology. Since 1999 he has been a Professor\nof Signal Processing at the Department of Electrical\nand Communications Engineering, Helsinki University of Technology (HUT),\nFinland. He is one of the Principal Investigators in SMARAD Center of\nExcellence in Radio and Communications Engineering nominated by the\nAcademy of Finland. Since year 2003 he has been also adjunct professor\nat the University of Pennsylvania, Philadelphia, USA.\nDr. Koivunen’s research interest include statistical, communications and\nsensor array signal processing. He received the best paper award (co-authored\nby C. Ribeiro and A. Richter) from IEEE PIMRC 2005 for his work on MIMO\nchannel propagation parameter estimation. He has published more than 170\npapers in international scientific conferences and journals. He has served as\nan associate editor for IEEE Signal Processing Letters. He is a member of the\neditorial board for the Signal Processing journal. He is also a member of the\nIEEE Signal Processing for Communication Technical Committee (SPCOMTC).\n\n13\n\n\f"
        ],
        [
         "44",
         "44",
         "cs.CE",
         "Computational Engineering",
         "0412031v1.pdf",
         "УДК 004.9:66.013.512\nОСОБЕННОСТИ КОМПЛЕКСНОЙ САПР\nРЕКОНСТРУКЦИИ ПРОМЫШЛЕННЫХ\nПРЕДПРИЯТИЙ\nВ.В. Мигунов1\nОбсуждаются особенности разработки проектов реконструкции\nдействующего предприятия силами его проектно-конструкторского\nподразделения: конечным результатом являются чертежи по\nстандартам ЕСКД и СПДС; большое число малых проектов для\nразличных имеющихся объектов; разнообразие марок чертежей в\nсоставе проекта; большой бумажный архив. На примере САПР\nTechnoCAD GlassX изложены модели и методы разработки\nкомплексной САПР с удобной единой средой проектирования, с\nнастройкой на профиль работ, с использованием общих частей\nпроекта при наличии ряда проблемно-ориентированных подсистем.\n\nВведение. САПР в реконструкции предприятий\nПроекты реконструкции промышленных предприятий охватывают\nмного небольших объектов с различных сторон − технологической,\nмонтажной, строительной, электротехнической, санитарно-технической и\nдр. согласно требованиям системы проектной документации для\nстроительства (СПДС). В этих условиях для целей автоматизации\nприходится набирать несколько различных САПР [Орельяна, 2001] с\nестественными сложностями в освоении их интерфейса и неудобством\nработы в разных средах. Обычно такие проекты выполняются силами\nнебольших проектно-конструкторских подразделений (ПКО) самого\nпредприятия, что делает невозможной специализацию рабочих мест и\nусиливает эти трудности. В чертежах есть общие части: строительная\nподоснова, технологическая схема. Они должны передаваться от одного\nпроектировщика к другому и допускать доработку на всех рабочих местах.\nРезультатом работы ПКО является проектно-сметная документация для\nстроительно-монтажных работ (СМР), в основном чертежи различных\nмарок СПДС. Электронное представление чертежей не дает значимого\nэффекта при выполнении почти не автоматизируемых СМР, и\n1\n\n420088, Казань, ул. Губкина, 50, ЦЭСИ Р при КМ РТ, vmigunov@csp.kazan.ru\n\n\fэффективность САПР реконструкции не распространяется на их\nпроизводство. В самом ПКО, как правило, большая часть чертежей\nхранится в бумажном архиве, и лишь по мере проектирования\nреконструкции переводится в электронную форму, что также снижает\nэффективность применения САПР. В результате наблюдается отставание в\nразвитии САПР реконструкции предприятий от САПР в других сферах.\nЗадача автоматизации проектирования реконструкции промышленного\nпредприятия требует программного обеспечения, ориентированного на\nвыпуск чертежей по требованиям ЕСКД и СПДС, работающего\nодновременно с растровой и векторной графикой, в едином интерфейсе\nпользователя автоматизирующего подготовку чертежей различных марок\nСПДС в едином графическом формате. Теории таких систем практически\nнет. Из 692 диссертаций по специальности 05.13.12 (САПР), защищенных\nв период 1988 − 2002 г.г. с передачей авторефератов в РГБ, ни одна не\nпосвящена этой проблеме (выборка с http://aleph.rsl.ru, 10.02.2004).\nНиже на примере отечественной САПР TechnoCAD GlassX [Мигунов,\n2004] изложены некоторые модели и методы разработки комплексной\nСАПР, обеспечивающей удобную единую среду проектирования с\nнастройкой на профиль работ, с использованием общих частей проекта\nпри наличии ряда проблемно-ориентированных подсистем. Состав\nподсистем сформирован на основе практических потребностей\nдействующего ПКО ОАО \"Казаньоргсинтез\" в период 1994 − 2004 г.г.\n\nМодель чертежа\nОбъектом разработки является двумерный чертеж − совокупность\nплоских геометрических элементов. Временный выход из плоских\nпредставлений допускается в специализированных расширениях (СР),\nтаких как проектирование аксонометрических схем трубопроводных\nсистем (АСТС) или молниезащиты зданий и сооружений (МЗС). Габариты\nчертежа достигают 30м для технологических схем. Элементы с размером\nна бумаге менее 0.3 мм, трудно различимые невооруженным взглядом, не\nпринимаются в чертеж. Применяются две системы координат: натурная и\nбумажная, с единицами измерения мм, с началом в середине чертежа.\nКоординаты хранятся в четырехбайтных числах: при 23 битах в мантиссе\nразличимы точки 15000мм и 15000.001мм. На время геометрических\nпостроений эти числа преобразуются в рабочий 8-байтный формат, что\nпри 52 битах мантиссы повышает разрешение еще в 500 млн. раз. В есть\nэлементы, хранящие вместе с геометрической частью комплекты\nпараметров для реализации специализированных операций (модуль).\nОбычно один файл чертежа содержит один лист (одну основную надпись).\n\n\fРабота с растрами\nЗначительная часть чертежей выполняется \"поверх\" прежних чертежей\nреконструируемых объектов внесением в них фрагментарных изменений.\nНеизменяемая часть чертежа остается в растровом виде, нужные для\nстыковки с реконструируемой частью элементы обводятся на растровой\nподложке, а новая часть вычерчивается сразу в векторном виде. Для\nобводки растра применяются средства подгонки по задаваемому\nприближению геометрического элемента. В чертеже одновременно\nприсутствуют и выводятся на печать и векторная, и растровая части.\nДостаточно работать с монохромными растрами с разрешением 75 − 300\nточек на дюйм, что обеспечивает нормальное качество при печати.\nИспользуется склейка растров. Вывод растра на экран производится по\nчастям, без ожидания вывода частей, находящихся за экраном.\n\nВыполнение требований ЕСКД и СПДС\nИспользование иностранных графических ядер приводит к\nнеобходимости в специальных надстройках для выполнения требований\nЕСКД и СПДС, таких, как auto.ЕСКД и auto.СПДС [РПК, 2004].\nЕстественно выполнять эти требования не факультативно, а в той или иной\nстепени принудительно: за счет специальной модели чертежа, не\nдопускающей нестандартных вариантов (жесткое принуждение), и за счет\nбольшего удобства применения стандартных решений (мягкое\nпринуждение). Жестко ограничиваются выбираемые варианты: типов\nлиний, масштабов, размера и угла наклона чертежного шрифта, величины\nстрелок, засечек, перехода выносных линий за размерные, проекций,\nбланков табличных конструкторских документов (ТКД). Мягкое\nпринуждение реализовано в выборе формата чертежа (пользовательский\nзадавать дольше); в штриховании (пользовательский стиль сделать\nсложнее); в графических библиотеках: в комплекте GlassX содержатся\nстандартные основные и дополнительные надписи, условные обозначения\nарматуры, элементов трубопроводов и др. (пользовательские библиотеки\nсоздавать сложнее, чем выбрать из имеющихся); специальный тип\nгеометрического элемента \"Магистраль\" упрощает вычерчивание\nусловных обозначений трасс кабелей, проводок и др.; все СР ускоряют\nпроектирование, но позволяют создавать чертеж только по стандартам.\n\nЭлектронные номенклатурные каталоги\nЭлектронные каталоги изделий, выпускаемых промышленностью\n(ЭНК) включают сведения из ГОСТов, ОСТов, ТУ, каталогов\nпроизводителей оборудования, труб, арматуры, элементов трубопроводов,\n\n\fприборов и исполнительных механизмов. Они охватывают (пока) задачи\nспецифицирования по трем профилям работ для чертежей марок: МТ\n(монтажно-технологическая часть проекта, охват 111 бумажных\nкаталогов), А.. (автоматизация, 14 каталогов); ОВ, ВК, НВК, ТС\n(отопление, вентиляция, водоснабжение, канализация, теплоснабжение,\n102 каталога). Все каталоги для целей САПР удалось представить в единой\nформе таблиц с данными и текстовых правил генерации обозначения,\nнаименования и других полей спецификаций в едином пользовательском\nинтерфейсе. Графическая часть бумажных каталогов в ЭНК не является\nнеобходимой. В данные в таблицах можно помещать списки допустимых\nвариантов (встроенные меню), а в правила - внешние меню и формы ввода.\nПравила задают сложение фрагментов строк, которые могут быть\nконстантой, значением поля в таблице данных, результатом выбора из\nвнешнего меню, результатом ввода числового или строкового значения, а\nтакже временной переменной, в которой сохранено введенное, выбранное\nили сгенерированное ранее для другой графы ТКД значение. Правила и\nвстроенные меню запускаются в работу, когда проектировщик выбрал\nподходящее изделие в таблице данных. В ТКД числовые значения\nприводятся к нужным единицам измерения автоматически, а в графах\nтаблиц данных сохраняются единицы измерения из бумажных каталогов.\nДля ускорения выбора из 68213 строк 265 таблиц имеются интерактивные\nи автоматические фильтры по профилю работ, по исходному каталогу, по\nсимволу измеряемой прибором величины, по признаку первичный −\nвторичный прибор, по принадлежности изделий группам в иерархической\nклассификации (\"Оборудование/Насос\"), по интервалам температур,\nдавлений и др., по значениям в любой графе таблицы данных.\n\nСпецифицирование\nМодель ТКД для автоматизации специфицирования с использованием\nЭНК реализуется путем совместного хранения в элементе чертежа типа\n\"Табличный\nмодуль\"\nвидимых\nгеометрических\nэлементов\nи\nпараметрического представления (ПП) ТКД, достаточного для генерации\nего изображения. Первично ПП в виде массива записей. Нулевая запись −\nшапка таблицы. Структура записей одинакова и задается иерархией\nделения блоков (прямоугольных фрагментов ТКД) на части по вертикали и\nгоризонтали, с признаками видимости этого деления в шапке и в области\nданных, на фиксированное или произвольное число частей, вплоть до\nнеделимых блоков − ячеек таблицы. Таким образом задаются ТКД с\nразделами или без них. Видимые в шапке ячейки имеют свойство \"текст в\nшапке\" − заголовки граф. У блока может быть ключевое слово для поиска\nв ЭНК (\"Трубы\" − выбор в ЭНК труб). Ячейкам можно задать единицы\n\n\fизмерения. В ТКД заносятся пересчитанные значения, например, давления\nв МПа, хотя в ЭНК встречаются и кгс/кв.см, и м вод.ст.\nГенерация спецификаций, заказных спецификаций, таблиц колодцев,\nмонтажных ведомостей трубопроводов и др. достигается путем хранения\nневидимых специфицирующих свойств в модулях чертежа: АСТС,\nприборы и исполнительные механизмы, профили наружных сетей\nводоснабжения и канализации (ПНС), позиционные обозначения. Задается\nобласть сбора информации (текущий чертеж или несколько чертежей на\nдиске, типы модулей), и производится автоматическое заполнение ТКД.\nДля выделения разделов и оформления ТКД имеются автоматизированные\nоперации фасовки строк, упорядочивания по задаваемому списку граф,\nсливания одинаковых и выделения общих частей наименований. \"Буфер\nизделий\" позволяет переносить группу строк с одноименными\nспецифицирующими свойствами между различными ТКД. Можно\nпродолжать ТКД влево или вправо кусками нужной высоты, повторяя\nшапку или строки с номерами граф, менять типы линий, шрифт текста в\nграфе или ячейке и др. с мгновенной перегенерацией изображения.\n\nМодульная технология создания специализированных расширений\nСР эффективны, когда трудоемкость ввода данных в СР существенно\nниже трудоемкости непосредственного черчения и расчетов. Например,\nкогда нужны трудоемкие расчеты или нормативные требования\nпорождают много графических изображений по малому объему данных. В\nобоих случаях наиболее эффективна генерация чертежа по\nпараметрическому представлению. Каждое СР реализуется в своем\nосновном меню как группа операций, в ходе которых создается ПП\nобъекта проектирования. На экране постоянно поддерживается\nизображение, соответствующее текущему ПП. Затем геометрическая часть\nи ПП помещаются в чертеж как модуль соответствующего типа. Как\nэлемент чертежа, модуль может выбираться, сдвигаться, удаляться,\nпомещаться в графическую библиотеку, к нему возможны привязки и др.\nТип модуля задает структуру его ПП: списки объектов и общие\nпараметры, часть из которых носят смысл установок. Списки объектов −\nмассивы, за каждым их элементом закрепляются все сведения об объекте,\nвключающие в зависимости от его сути геометрические характеристики,\nпризнаки ориентации, специфицирующие свойства, цвет, тип линии,\nтексты, ссылки на объекты других списков по номерам и др. Например,\nтексты обычно привязаны к другим элементам чертежа, и списки текстов\nимеют ссылки на них. Совокупность списков объектов − реляционная база\nданных со ссылочной целостностью. Специализированные структуры\n\n\fданных в ПП сильно повышают возможную степень автоматизации работ.\nХранение ПП на диске создает удобные библиотеки прототипов.\nНа основе модульной технологии созданы СР по проектированию:\nстроительной подосновы, схем автоматизации, АСТС, ПНС, ТКД, МЗС.\nНаиболее критично выделение части проекта, подготовка которой будет\nавтоматизироваться в СР. С одной стороны, желательно охватить широкий\nкруг задач. С другой стороны, задачи должны допускать общее внутреннее\nпредставление. В некоторых случаях индивидуальность задачи столь\nсильна, что выбор части проекта становится тривиальным (МЗС). В то же\nвремя АСТС очень близки в таких частях проекта, как чертежи\nтехнологических трубопроводов, схемы водопровода и канализации,\nотопления, теплоснабжения, вентиляции, кондиционирования воздуха.\n\nИнтерфейс с пользователем\nСостав опций главного меню определяется выбором текущего профиля\nработ из 5 вариантов: \"Монтажно-технологический\", \"КИП и автоматика\",\n\"Строительный\", \"Электротехнический\", \"Производственные регламенты\"\n(не для ПКО). В соответствии с этим профилем доступны различные\nнаборы операций, комплекты таблиц ЭНК, меняются свойства части меню\nи форм ввода, установки по умолчанию и др. Контекстная справка по\nтекущему режиму работы всегда доступна, содержит 12 тыс. строк в 1300\nразделах. Текст справки начинается с содержательной характеристики\nситуации и заканчивается описанием правил работы в текущем состоянии\nпользовательского интерфейса (для начинающих пользователей).\n\nЗаключение\nИзложенные модели и методы разработки комплексной САПР\nреконструкции промышленных предприятий не охватывают всего\nразнообразия чертежей в проектах реконструкции, но являются\nэлементами технологии разработки САПР для них, прошедшими\nапробацию на практике в ходе десятилетнего развития системы\nTechnoCAD GlassX. Подробнее о ней: http://technocad.narod.ru.\n\nСписок литературы\n[Орельяна, 2001] Орельяна И.О. Автоматизация при реконструкции и развитии\nпромышленных объектов в России // CADmaster. 2001. № 3.\n[Мигунов, 2004] Мигунов В.В. TechnoCAD GlassX − отечественная САПР\nреконструкции предприятия // САПР и графика. 2004. № 4.\n[РПК, 2004] Русская Промышленная Компания. Новости. \"Приложение auto.ЕСКД\nтеперь работает под AutoCAD 2005\", \"Теперь auto.СПДС работает и под AutoCAD\n2005\".  http://www.cad.ru, 25.04.2004.\n\n\f"
        ],
        [
         "45",
         "45",
         "cs.CE",
         "Computational Engineering",
         "1802.06013v1.pdf",
         "A HYBRIDIZABLE DISCONTINUOUS GALERKIN METHOD FOR TWO-PHASE FLOW IN\nHETEROGENEOUS POROUS MEDIA\nMAURICE S. FABIEN∗ , MATTHEW G. KNEPLEY\n\n† , AND\n\nBÉATRICE M. RIVIÈRE∗\n\narXiv:1802.06013v1 [cs.CE] 16 Feb 2018\n\nAbstract. We present a new method for simulating incompressible immiscible two-phase flow in porous media. The semi-implicit\nmethod decouples the wetting phase pressure and saturation equations. The equations are discretized using a hybridizable discontinuous\nGalerkin (HDG) method. The proposed method is of high order, conserves global/local mass balance, and the number of globally coupled\ndegrees of freedom is significantly reduced compared to standard interior penalty discontinuous Galerkin methods. Several numerical\nexamples illustrate the accuracy and robustness of the method. These examples include verification of convergence rates by manufactured\nsolutions, common 1D benchmarks and realistic discontinuous permeability fields.\n\n1. Introduction. Multiphase flows in porous media are fundamental processes in geophysics. For instance,\nthey characterize enhanced oil recovery [51], hydrogeology [34, 9], as well as CO2 sequestration in geological formations [11]. The equations that govern two-phase flow form a coupled system of nonlinear partial differential\nequations. Numerous techniques have been proposed to resolve the nonlinearity inherent in the system, for instance, implicit-explicit (IMPES), semi-sequential, semi-implicit, and fully implicit methods [17].\nThe selection of spatial discretization is also a critical decision in the solution process. Accuracy, local conservation,\nmass balance, and efficiency of implementation are all important features. With respect to the wetting phase pressure equation, incorrect approximations to the phase velocity can cause oscillations and instability when used in\nthe convection-dominated transport equation satisfied by the wetting phase saturation. Compatible discretizations\n(as defined in [26]) for flow and transport maintains local and global mass conservation which provides stability\nand accuracy in the numerical methods.\nIn this work we discretize the pressure and saturation equations by the Hybridizable Discontinuous Galerkin (HDG)\nmethod. The HDG method has several interesting properties. In particular, discrete analogs of global conservation\nfor flow, and local conservation for transport are satisfied. This postprocessing is available since the normal component of the numerical flux for the HDG method is single valued [18]. Classical discontinuous Galerkin (DG) methods\nconstruct the velocity from the pressure, which means that the velocity (and subsequent its H(div) postprocessing)\nconverge sub-optimally [8].\nAn explosion of interest in discontinuous Galerkin methods has occurred since the 1990s. DG methods have a\nnumber of attractive features, e.g. local mass conservation, hp–adaptation, their ability to handle nonconforming\nmeshes, and the fact they are well-suited for parallelism. One main disadvantage is that DG methods in general\nhave more degrees of freedom than their continuous counterparts. This difficulty is exacerbated by the fact that\nat each time step, multiple linear solvers may be required for complex porous media problems like two-phase flow\n(e.g. nonlinear solver, iterative coupling). The hybridizable discontinuous Galerkin method addresses this concern\n[18, 5, 19]. A global system solely in terms of the approximate trace of the saturation variable can be obtained. To\ndo this, we prescribe a specific numerical flux for the approximate saturation variable, such that we can express it\nand the approximation to the saturation, in terms of an additional unknown defined on the skeleton of the mesh.\nTo ensure that the numerical trace is single valued, we require that the normal component of the numerical flux\nacross the element boundaries is continuous.\nFor higher approximation orders, hybridization can significantly reduce the number of degrees of freedom [54]. In\naddition, the HDG method boasts optimal orders accuracy k + 1 for all approximate variables (including the velocity) if k is the polynomial order. The method possesses a local post-processing that can enhance the accuracy of\nthe scalar variable (with an order of accuracy k + 2), and retains favorable aspects of DG methods (e.g. local mass\nconservation, ability to handle unstructured meshes, etc.). The piecewise constant case is convergent for the Local\nDiscontinuous Galerkin (LDG)-HDG ([19]), as such, in the same framework, we can compare a finite volume type\nmethod to higher order schemes. In addition, the piecewise constant case discretization is useful for multigrid [32];\nas this system is much smaller and acts as a cheap coarse grid correction that smooths stubborn low frequencies\nin the multigrird process The importance of hybridization cannot be overstated; at each time step one is required\nto perform multiple linear solves due to linearization (e.g. Newton’s method) or iterative coupling (high order\ntime stepping, velocity extrapolation). This observation is even more important as one considers three dimensional\nmultiphase flow problems.\nDG methods have been applied to two-phase flow problems in porous media over the last ten years. The most\ncommon techniques for resolving the nonlinearity present in the governing equations are IMPES (splitting pressure/saturation: implicit pressure, explicit saturation), time lagging (splitting pressure/saturation: implicit pressure, time lagged-implicit saturation), semi-implicit (splitting pressure/saturation:implicit pressure, implicit saturation), and fully implicit algorithms.\nIMPES methods are appealing computationally since the saturation equation is treated explicitly. However,\n∗ Department\n† Department\n\nof Computational and Applied Mathematics, Rice University, Houston, TX.\nof Computer Science and Engineering, State University of New York at Buffalo, Buffalo, NY.\n1\n\n\f2\n\nM. S. FABIEN, M. G. KNEPLEY, AND B. M. RIVIÈRE\n\nthere are a number of drawbacks to this approach. Such a method requires slope limiting, which may be complicated\nto formulate as no theory in 2D/3D exists. Furthermore, although the saturation is treated explicitly, the time\nstep may have to be taken prohibitively small in the presence of highly varying permeability. DG methods using\nIMPES were pursued by authors in [2, 6, 38]. It is found that slope limiting is needed, as the saturation equation is\nof hyperbolic nature, and high order approximations lead to overshoot and undershoot that must be mitigated. In\n[42, 31], the authors use a time lagging approach, which splits the pressure and saturation equations, but linearizes\nthe saturation equation by time lagging its coefficients. Slope limiting is needed in this approach as well, even\nthough the time stepping is implicit (backward Euler). A hybridized mixed finite element method coupled with\nDG (IMPES) is considered in [36]. Slope limiting is required to stabilize the approximation of the hyperbolic\nsaturation equation. Another method is examined in [37], with two different algorithms, namely, hybrid mixed\nfinite element (HMFE)-DG (IMPES) and HMFE-DG (Picard-time lag). Here the authors use iterative coupling\nwhen time lagging, and slope limiters are still required for stability. All of the above IMPES/time lag methods use\npiecewise linear approximations for the saturation equation, as slope limiting reduces the accuracy to first order.\nFully implicit methods are the most computationally demanding, since they do not relax the nonlinearity in any\nway. DG fully implicit approaches were explored in [30, 29, 7]. No slope limiters are required, but the resulting\nlinear system that is to be solved at each time step is very large. Polynomial orders up to four are considered in\n[30] on very coarse meshes, and up to degree two are considered in [7].\nTo the best of our knowledge, our work presented in this paper is the first to explore high order HDG methods\nin a semi-implicit algorithm for multiphase flow (up to degree eight). By splitting the pressure equation from the\nsaturation equation, but treating both equations implicitly, we show that we do not need slope limiters. Highly\naccurate approximations are available in a computationally efficient manner as hybridization significantly reduces\nthe total number of degrees of freedom. Primal DG methods are less accurate for the total velocity, as they need to\nreconstruct it by computing explicitely the gradient of the pressure approximation. In contrast, the HDG method\nsimultaneously obtains approximations for the wetting phase saturation, the gradient of wetting phase saturation,\nand the total velocity, that converge at the rate k + 1 in the L2 norm. Any issues with loss of mass balance do not\ngive rise to instabilities or spurious oscillations as the HDG forms a pair of compatible discretizations. Furthermore,\nlocal superconvergent postprocessing is available whenever an enhanced solution is required for the wetting phase\npressure or saturation.\nA brief outline of the paper is described. Section 2 presents the model problem. The HDG method is described\nin Section 3 for both pressure and saturation equations. Numerical results are shown in Section 4. Conclusions\nfollow.\n2. Model problem. The equations that govern two-phase flow in a porous medium are given by a coupled\nsystem of nonlinear partial differential equations. In this paper, we assume that the phases are incompressible and\nimmiscible, which allows for the coupled system to be expressed in a pressure–velocity–saturation formulation. That\nis, a system of partial differential equations for which the primary variables are the pressure and the saturation of\nthe wetting phase and the total velocity:\n(1)\n(2)\n(3)\n\nut = −λt K∇pw − λo K∇pc ,\n−∇ · ut = 0,\n\u0012\n\u0013\n\u0012\n\u0013\n∂(φsw )\nλo λw\nλw\n+∇·\nK∇pc = −∇ ·\nut ,\n∂t\nλt\nλt\n\nin\nin\n\nΩ × (0, T ),\nΩ × (0, T ),\n\nin\n\nΩ × (0, T ).\n\nThe domain Ω is the porous medium in consideration, and (0, T ) is the time interval. The wetting (aqueous)\nphase pressure and saturation are denoted by pw and sw , and similarly the non-wetting (oil) phase pressure and\nsaturation are denoted by po and so . The capillary pressure, pc , is the difference between the phase pressures.\nThe total velocity is denoted by ut ; it is the sum of the phase velocities. The phase mobilities are denoted by λw\nand λo . Capillary pressure and phase mobilities are nonlinear functions of the wetting phase saturation [14]. The\nderivative of the capillary pressure is negative and we will rewrite ∇pc as −|p0c |∇sw in the rest of the paper. The\ntotal mobility is denoted by λt . The sum of the phase saturations is equal to one. In summary, we have\npc = po − pw ,\n\nλt = λw + λo ,\n\nsw + so = 1.\n\nPermeability and porosity of the porous medium are denoted by φ and K respectively. The permeability may vary\nover several orders in magnitude in heterogeneous media.\nEquations (1)-(3) are completed by initial condition for the saturation (sw = s0w ) and by various boundary conditions. We decompose the boundary of the domain, ∂Ω, into disjoint sets:\n(4)\n\n∂Ω = ΓpD ∪ ΓpN = ΓsD ∪ ΓsN ,\n\nΓpD ∩ ΓpN = ΓsD ∩ ΓsN = ∅.\n\n\fA HYBRIDIZABLE DISCONTINUOUS GALERKIN METHOD FOR TWO-PHASE FLOW IN HETEROGENEOUS POROUS MEDIA\n3\n\nWe consider Dirichlet and Neumann boundary conditions for both wetting phase pressure and saturation, and total\nvelocity.\n(5)\n(6)\n(7)\n(8)\n\non ΓpD × (0, T ),\non ΓpN × (0, T ),\non ΓsD × (0, T ),\n\npw = pD ,\nut · n = 0,\nsw = sD ,\n−K\n\nλw λo 0\np ∇sw · n = 0,\nλt c\n\non\n\nΓsN × (0, T ).\n\nOur primary unknowns are the wetting phase pressure, saturation and total velocity. Other choices of primary\nunknowns can be made, for instance wetting phase pressure and non-wetting phase pressure, global pressure and\nwetting phase saturation [12]. We use the formulation given by equations (1), (2), and (3), as this formulation\nweakens the nonlinearity while retaining a middle ground in terms of computational efficiency, when compared to\nIMPES type methods (which require slope limiting) and fully implicit methods (more computationally expensive).\n3. The HDG method for two-phase. Let Eh be a regular mesh of the domain Ω, made of triangular\nelements denoted by E. Let Γh denote the set of interior and boundary edges. We also denote\n∂Eh = {∂E,\n\nE ∈ Eh }.\n\nFor any set O, the short-hand notation (·, ·)O is used for the L2 inner-product over the elements of O. For instance,\nif O = Γh , we have\nXZ\n(w, µ)Γh =\nwµ.\ne∈Γh\n\ne\n\n3.1. Pressure and velocity approximation. Let Qk (E) denote the space of polynomials of degree k in\neach coordinate direction. The HDG discrete spaces are introduced:\n\n(9)\n\nWh = {w ∈ L2 (Ω) : w|E ∈ Qk (E),\nVh = Wh × Wh ,\n\n∀E ∈ Eh },\n\nMh = {µ ∈ L2 (Γh ) : µ|e ∈ Qk (e), ∀e ∈ Γh },\nMh (0) = {µ ∈ Mh : µ = 0, on ΓpD }.\n\nThe HDG discretization of equations (1) and (2) seeks (uth , pwh , pbwh ) ∈ Vh × Wh × Mh (0) such that\n(10)\n(11)\n(12)\n\n−1\n0\n(λ−1\nuth , v)Eh − (pwh , ∇ · v)Eh + (b\npwh , v · n)∂Eh = −(Πh pD , v · n)ΓpD − (λo λw λ−1\nt K\nt |pc (sw )|∇sw , v)Eh\n\n−(uth , ∇w)Eh + (b\nuth · n, w)∂Eh = 0,\n([[b\nuth · n]], µ)Γh = 0,\n\nfor all (v, w, µ) ∈ Vh × Wh × Mh (0), where Πh denotes the L2 -projection onto Mh . The numerical traces are given\nas follows:\n\u001a\npbwh ,\non Γh \\ΓpD\npewh =\nΠh pD ,\non ΓpD ,\nb th = uth + τ (pwh − pewh )n,\nu\nwhere τ is a piecewise constant stabilization defined on element boundaries. In our work we set [46]\nτ |e = K|E min λt (swh (x)),\nx∈e\n\n∀e ∈ ∂E,\n\nThe HDG system written in matrix form can be expressed as\n   \n\nU\nRu\nA −B T C T\n\n   \nD\nE  P  =  Rp  ,\nB\nC\nG\nH\nPb\nRpb\nand isolating interior unknowns gives\n(13)\n\n\" #\nU\nP\n\n\"\n=\n\nA\n\n−B T\n\nB\n\nD\n\n#−1\n\n\" #\nRu\nRp\n\n\" # !\nCT b\n−\nP .\nE\n\n\f4\n\nM. S. FABIEN, M. G. KNEPLEY, AND B. M. RIVIÈRE\n\nDue to the discontinuity of the approximations uth and pwh , the inverted matrix in equation (13) can be performed\nin an element by element manner. The equation that enforces continuity of the normal component of the numerical\ntrace of the total velocity is\n(14)\n\nCU + GP + H Pb = Rpb.\n\nWe can condense the interior unknowns to obtain a globally coupled system only defined in terms of Pb, the wetting\nphase pressure on the mesh skeleton:\n\u0014\n\u0015−1 \u0014 \u0015\n\u0014\n\u0015−1 \u0014 \u0015\nT\nRu\nC T , F = R − [C G] A −B T\nHPb = F, H = H − [C G] A −B\np\nb\nRp .\nB\nD\nE\nB\nD\nWe note that the expressions for H and F can be obtained at the element level. The HDG method (10)-(12) has a\nnumber of appealing features, notably:\n• the property that allows the element-by-element elimination of interior degrees of freedom, resulting in a\nsignificantly smaller fully coupled problem with its only unknowns on the mesh skeleton (this property is\ncalled static condensation),\n• the approximations for uth , pwh , and pbwh all converge at the optimal rate of k + 1,\n• a local element-by-element postprocessing for pwh results in a new approximation p∗wh that converges at\nthe rate of k + 2 [19],\n• the numerical trace of uth has its normal component continuous, which is a critical property for flows in\nheterogeneous media.\n3.2. Saturation approximation. The transport equation (3) is typically convection dominated, with possibly\ndegenerate parabolic nature. DG methods have shown promising results for miscible displacement and multiphase\nflows. As such, a natural extension is to consider HDG methods because at each time step one may be required to\nperform multiple linear solves due to linearization (e.g. Newton’s method) or iterative coupling (high order time\nstepping, velocity extrapolation). In this section, we present an approximation for the wetting phase saturation by\na hybridizable discontinuous Galerkin method. We rewrite the transport equation (3) in first order form:\nq − ∇sw = 0,\n∂(φsw )\n+ ∇ · (Fc (sw ) + Fv (q, sw )) = 0,\n∂t\n\n(15)\n(16)\n\nwhere Fc and Fv denote the convective and viscous terms:\nFc (ut , sw ) =\n\nλw (sw )\nut ,\nλt (sw )\n\nFv (q, sw ) = −\n\nλo (sw )λw (sw )\nK|p0c (sw )|q.\nλt (sw )\n\nThe continuous-in-time HDG discretization of (15)-(16) seeks (qh , swh , sbwh ) ∈ Wh × Vh × Mh such that\n(qh , v)Eh + (swh , ∇ · v)Eh − (b\nswh , v · n)Γh \\∂Ω = (Πh sD , v · n)Γs\n\n(17)\n\n(18)\n\u0012\n\u0013\n\u0010\n\u0011\n∂(φswh )\nb c (b\nb v (qh , swh , sbwh )) · n, w\n,w\n− (Fc (uth , swh ) + Fv (qh , swh ), ∇w)Eh + (F\nuth , swh , sbwh ) + F\n= 0,\n∂t\n∂Eh\nEh\n\u0010\n\u0011\nb c (b\nb v (qh , swh , sbwh )) · n, µ\n(19)\n(F\nuth , swh , sbwh ) + F\n= 0,\nΓh \\∂Ω\n\nb c and F\nb v respectively.\nfor all (v, w, µ) ∈ Vh × Wh × Mh . The numerical convective and viscous fluxes are denoted F\nb\nFor Fc , we use a Lax-Friedrich like numerical flux, in which we evaluate the analytical flux function with sbwh and\npenalize the jump between sbwh and swh . In other words, we have\nb c (b\nF\nuth , swh , sbwh ) = Fc (b\nuth , sbwh ) + τc (swh − sbwh )n,\nwith τc > 0 as a stablization parameter [46, 49, 33]. In practice, given an edge e on an element E, we choose\nτc |e = max (|Fc (uth (x), swh (x)) · ne |, 0).\nx∈e\n\nAs we are using a LDG inspired HDG, we use the LDG flux ([4, 24]) for the diffusive numerical flux:\nb v (q, swh , sbwh ) = Fv (q, sbwh ) + τv (swh − sbwh )n,\nF\nwith τv > 0 as a stablization parameter [46, 49, 33]. For an edge e on an element E, we choose\nτv |e = min (Fv (qh , swh (x)), 1).\nx∈e\n\nD\n\n\fA HYBRIDIZABLE DISCONTINUOUS GALERKIN METHOD FOR TWO-PHASE FLOW IN HETEROGENEOUS POROUS MEDIA\n5\n\nA further discussion on the selection and analysis of stabilization parameters can be found in [16]. We mention\nbriefly that the HDG stabilization factor does not depend on the polynomial order or mesh size, unlike other DG\nmethods.\nTo discretize the temporal dimension we employ first order implicit Euler. We remark that high order BDF and\nimplicit Runge Kutta methods have been used successfully for HDG methods [39, 47]. The above system describes\nthe HDG discretization. It is nonlinear, so at each time step we linearize it using Newton’s method. The first time\nstep requires a sufficiently accurate initial guess. To obtain this initial guess, we take one step of an Anderson\naccelerated Picard iteration [1]. For all other time steps the initial guess is taken as the wetting phase saturation\nfrom the previous time step.\nA key feature of the HDG method is its ability to utilize static condensation. At a given time step our system in\nmatrix form can be written as\n\n   \n\"\n#\nAqq Aqs Bq\nQ\nRq\nAqq Aqs\n\n   \n, B = [Bq , Bs ]T , C = [Cq , Cs ],\nAsq Ass Bs   S  = Rs  , A =\nA\nA\nsq\nss\nCq\nCs D\nSb\nRsb\nand condensing out the interior degrees of freedom Q and S gives\nH Sb = R,\n\nH = D − CA−1 B,\n\n\u0014 \u0015\nR\nR = Rsb − CA−1 Rq .\ns\n\nThus, one can assemble a globally coupled system that only depends on the trace unknowns sbwh . We note that\nthis assembly can be performed in an element by element manner due to the local nature of HDG method. After\nthe wetting phase saturation is solved for on the mesh skeleton, one may recover the volume space approximations\nthrough a element by element procedure. Indeed, the Shur complement yields\n\" # \"\n#−1 \u0012 \" #\n\u0013\nQ\nAqq Aqs\nRq\n=\n− B Sb .\nS\nAsq Ass\nRs\nb S and Q are actually Newton increments, and not necessarily the desired solution, so\nIt should be realized that S,\none would have to introduce the following variables:\nb\nSbm+1 = Sbm + S,\nS m+1 = S m + S\nQm+1 = Qm + Q,\nb S and Q denote the mth Newton iteration. To further accelerate convergence of Newton’s\nwhere superscripts on S,\nmethod, a sufficient initial guess is required. Anderson acceleration is one strategy that can be used. It is also feasible\nto consider using a damped Newton iteration (for a HDG specific example see [59]) in conjunction with Anderson\nacceleration to improve the convergence. For our numerical experiments, one step of an Anderson accelerated Picard\niteration allowed Newton’s method to converge quadratically.\nThe HDG method for the saturation has the same properties as mentioned in subsection 3.1 for the pressure-velocity\nsystem. In particular, we reiterate that the approximations for ∇swh , swh , and sbwh all converge at the optimal rate\nof k + 1.\n3.3. Postprocessing the saturation variable. The postprocessing procedure used in this work is explained\nhere, and is inspired by the one established in [48]. The element-by-element postprocessing of the saturation swh is\ndenoted by s∗wh ; and results in a new piecewise discontinuous polynomial approximation of degree k + 1 such that\n(20)\n(21)\n\n(∇s∗wh , ∇w)E = (qh , ∇w)E ,\n(s∗wh , 1)E = (swh , 1)E ,\n\n∀w ∈ Qk+1 (E),\n\nfor all E ∈ Eh , where we assume that qh and swh are known. Numerical evidence (see section 4.3) shows that this\npostprocessing converges for k > 0, at the rate of k + 2 in the L2 -norm, and k + 1 in H 1 -norm. We mention that\nvarious postprocessings exist, some of which have their resulting approximation not satisfying the original PDE in\nany sense [20, 21, 56, 19]. Moreover, to guarantee superconvergence on Cartesian meshes, it is necessary to use a\nslightly larger finite element space than the standard tensor product space [23]. The postprocessing does not need\nto occur at every time step, it can be activated at whenever an enhanced solution is desired. Superconvergence of\nthis postprocessing (steady-state or otherwise) is reliant on both swh and ∇swh converging optimally at the rate of\nk + 1 in the L2 -norm [48]. The system in equation (21) is element local, and as such, is completely data parallel.\nFurthermore, it is cheaper to compute than a fully coupled linear system [32].\n\n\f6\n\nM. S. FABIEN, M. G. KNEPLEY, AND B. M. RIVIÈRE\n\n3.4. Semi-implict HDG algorithm. The semi-implicit HDG two-phase flow algorithm can be found in\nAlgorithm 1. Let tn denote the time at the nth time step, so that tn+1 = tn + ∆t, where ∆t is a given time\nspacing. The variable nsteps is the total number of time steps required. In addition, a superscript of n means that\nthe variable is evaluated at tn , e.g., swh (x, tn ) := snwh . Similarly, for the Newton steps, a superscript of m denotes\nthe mth Newton iteration.\nAlgorithm 1 Semi-implicit HDG two-phase flow.\n1:\n2:\n3:\n4:\n5:\n6:\n7:\n8:\n9:\n10:\n11:\n\nfor n = 0 to (nsteps − 1) do\nGiven qhn and snwh , solve equations (10), (11), and (12) for unth ,\npnwh , and pbnwh .\nn n\nSet (qhm , sm\nbm\nbnwh ),\nwh , s\nwh ) = (qh , swh , s\nSet r = 1,\nwhile r ≥ tol do\nn\nUsing (qhm , sm\nbm\nwh , s\nwh ) and uth , solve equation (19) for\nm+1 m+1 m+1\n(qh , swh , sbwh ).\nm\nSet r = max{kqhm+1 − qhm kL2 (Ω) , ksm+1\nwh − swh kL2 (Ω) ,\nm+1\nm\nkb\nswh − sbwh kL2 (Ω) },\nm+1 m+1 m+1\nSet (qhm , sm\nbm\n, swh , sbwh ),\nwh , s\nwh ) = (qh\nend while\nbm+1\nSet (qhn , snwh , sbnwh ) = (qhm+1 , sm+1\nwh , s\nwh ),\nend for\n\n4. Numerical experiments. In this section, several numerical experiments are examined. We validate our\nmethod on two benchmark problems, the McWhorter and Buckley-Leverett problems. The McWhorter problem\nmodels counter-current two-phase flow where capillary forces are present [45]. The Buckley-Leverett problem is a\nwell-known example of 1D hyperbolic transport [15]. Both the McWhorter and Buckley-Leverett problems have\nanalytic or semi-analytic solutions. We verify that the correct convergence rates are obtained in 2D by using the\nmethod of manufactured solutions. Finally we test our approach on heterogeneous porous media, where no analytic\nsolution is known.\n4.1. McWhorter problem. The McWhorter problem is a one dimensional example of counter-current twophase flow where capillary forces are present. The governing equation is nonlinear, parabolic, and may be degenerate\nin the total velocity:\n\u0012\n\u0013\n∂\nλw (sw )λo (sw )\n∂sw\n∂sw\n−\n|p0c |K\n= 0, in Ω × (0, T ),\n(22)\nΦ\n∂t\n∂x λw (sw ) + λo (sw )\n∂x\nwhere we invoke Brooks–Corey relative permeabilities and capillary pressure:\n(23)\n\nλw (sw ) =\n\ns4w\n,\nµw\n\nλo (sw ) =\n\n(1 − sw )2 (1 − s2w )\n,\nµo\n\npc (sw ) = pd s−1/2\n.\nw\n\nA semi-analytical solution can be obtained for this problem (see [45]). We fix the following parameters: entry\npressure pd = 5000 (Pa), porosity Φ = 0.3, permeability K = 10−8 (m2 ), viscosities µo = µw = 10−3 (Pa.s). The\ndomain is the interval (0, 1.6). At the left and right boundaries we prescribe sD = 0.9 and sD = 0.1 respectively,\nand the initial condition is taken as s0w = 0.1. The HDG method is used to discretize the problem in space, and\nimplicit Euler is utilized in time. We use implicit time stepping as the McWhorter problem is parabolic, and\nexplicit time marching schemes have a severe time step restriction for this class of problems. In one dimension,\nstatic condensation for the HDG method always results in a matrix that is tridiagonal, for any polynomial order\nk ≥ 0, since the intersection of two adjacent elements is a single point.\nIn Fig. 1 wetting phase saturation profiles at different times are displayed using a high order approximation of k = 8\non a mesh with 32 elements. Overshoot and undershoot remain bounded due to the use of Newton’s method. A\npolynomial refinement study is performed in Fig. 2. A relatively coarse mesh of 64 elements is used and we run\nthe simulation to T = 80. The polynomial order varies: k = 0, 1 and k = 4. We note that the saturation front for\nthe case k = 0 lags behind the front of the semi-analytical solution. As the polynomial order increases, so does the\naccuracy. We show in Fig. 3 a zoom-in view of the saturation profile in the neighborhood of x = 0.5 for a geometric\nsequence of polynomial orders, k ∈ {0, 1, 2, 4, 8, 16}. We observe convergence of the solution with polynomial\ndegree refinement. Even though the solution has poor regularity, high order polynomial approximations bestow an\nadvantage without any use of slope limiting. Similarly, if we fix the polynomial order and solve the problem on\nsuccessively refined meshes, we obtain improved accuracy in the numerical solution. Fig. 4 shows the saturation\nprofiles on meshes with 16, 32, 64 and 128 elements. The polynomial approximation varies: k ∈ {0, 1, 2, 4}. We\nobserve that the piecewise constant (k = 0) solution gives a poor approximation in comparison to the high order\n\n\fA HYBRIDIZABLE DISCONTINUOUS GALERKIN METHOD FOR TWO-PHASE FLOW IN HETEROGENEOUS POROUS MEDIA\n7\n\n0.9\nT=40\nT=120\nT=240\nT=480\nT=960\n\n0.8\n0.7\n0.6\n\nSw\n\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\n1.4\n\n1.6\n\nx\n\nFig. 1: Saturation profiles at different times for the McWhorter problem. Piecewise octic polynomials are used on\na coarse mesh with 32 elements.\n\n0.9\n\n0.9\nAnalytic\nk=0\n\n0.8\n\n0.9\nAnalytic\nk=1\n\n0.8\n\n0.6\n\n0.5\n\n0.5\n\n0.5\n\nSw\n\n0.7\n\n0.6\n\nSw\n\n0.7\n\n0.6\n\nSw\n\n0.7\n\n0.4\n\n0.4\n\n0.4\n\n0.3\n\n0.3\n\n0.3\n\n0.2\n\n0.2\n\n0.2\n\n0.1\n\n0.1\n\n0.1\n\n0\n\n0\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\n1.4\n\n1.6\n\nAnalytic\nk=4\n\n0.8\n\n0\n0\n\n0.2\n\n0.4\n\n0.6\n\nx\n\n0.8\n\n1\n\n1.2\n\n1.4\n\n1.6\n\n0\n\nx\n\n(a) k = 0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1.2\n\n1.4\n\n1.6\n\nx\n\n(b) k = 1\n\n(c) k = 4\n\nFig. 2: Polynomial order refinement study. Clearly as we increase the polynomial order the solution converges.\n\n0.26\nAnalytic\nk=0\nk=1\nk=2\nk=4\nk=8\nk=16\n\n0.24\n0.22\n0.2\n\nSw\n\n0.18\n0.16\n0.14\n0.12\n0.1\n0.08\n0.45\n\n0.46\n\n0.47\n\n0.48\n\n0.49\n\n0.5\n\n0.51\n\n0.52\n\n0.53\n\n0.54\n\nx\n\nFig. 3: Zoom-in view of saturation profiles for polynomial orders from k = 0 to k = 16 for the McWhorter problem.\n\npolynomials; there is a visible gap around x = 0.4 between the piecewise constant approximation and the analytic\nsolution. The figure shows that as we refine the mesh, the approximation does improve. However, to match the\naccuracy of the higher order approximations, one would have to use a mesh that is very fine. On a mesh with\n128 elements, the picewise constant solution is visibly of lesser quality when compared to the piecewise quartic\napproximation on a coarse mesh of 16 elements.\nThe HDG method brings with it the ability to apply a simple postprocessing to the wetting phase saturation swh\n\n\f8\n\nM. S. FABIEN, M. G. KNEPLEY, AND B. M. RIVIÈRE\n0.9\n\n0.9\nAnalytic\n16 elements\n32 elements\n64 elements\n128 elements\n\n0.8\n0.7\n\n0.7\n0.6\n\nSw\n\n0.6\n\nSw\n\nAnalytic\n16 elements\n32 elements\n64 elements\n128 elements\n\n0.8\n\n0.5\n\n0.5\n\n0.4\n\n0.4\n\n0.3\n\n0.3\n\n0.2\n\n0.2\n0.1\n\n0.1\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0\n\n0.1\n\n0.2\n\nx\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nx\n\n(a) k = 0\n\n(b) k = 1\n\n0.9\n\n0.9\nAnalytic\n16 elements\n32 elements\n64 elements\n128 elements\n\n0.8\n0.7\n\nAnalytic\n16 elements\n32 elements\n64 elements\n128 elements\n\n0.8\n0.7\n\n0.5\n\nSw\n\n0.6\n\n0.5\n\nSw\n\n0.6\n\n0.4\n\n0.4\n\n0.3\n\n0.3\n\n0.2\n\n0.2\n\n0.1\n\n0.1\n\n0\n\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\nx\n\n(c) k = 4\n\n0.4\n\n0.5\n\n0.6\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nx\n\n(d) k = 8\n\nFig. 4: Mesh refinement study for different polynomial degrees. As we refine the mesh or increase the polynomial\norder the solution converges.\n\nto obtain a new approximation s∗wh that converges at a rate of k + 2 in the L2 -norm. As discussed in Section 3.3,\nthe postprocessing takes the form of a simple diffusion problem that is element local, as such it is completely data\nparallel. In [32] a work-precision study is performed which illustrates that the postprocessing is cheaper than solving\na refined globally coupled problem. Fig. 5 shows that the local postprocessing removes the spurious oscillations that\noccur near x = 0.5. We note that even though the oscillations remain bounded, mesh refinement is not as effective as\nthe postprocessing with respect to eliminating the oscillations, see Fig. 4. In other words, the cheap postprocessing\nmay be useful in situations where a more accurate solution is desired without further mesh refinement. We point\nout that the postprocessing is able to recover an enhanced approximation using an even coarser mesh of 16 elements\n(Fig. 5).\n4.2. Buckely–Leverett problem. The Buckely–Leverett problem is a popular one-dimensional model problem that is used to validate numerical methods for two-phase flows with zero capillary pressure [15]. The equations (1)-(3) simplify to:\n\u0012\n\u0013\n∂sw\n∂\nλw (sw )\n(24)\nΦ\n+\nut = 0.\n∂t\n∂x λw (sw ) + λo (sw )\nThe computational domain is taken to be Ω = (0, 300) (m). The relative permeabilities are the same as in Section 4.1.\nThe porosity is set to Φ = 0.2, the velocity u = 3 · 10−7 (m/s) and viscosities µw = µn = 1 (Pa s). The initial\nsaturation is equal to zero and the Dirichlet boundary condition at the left boundary, x = 0, is sD = 1. As\nequation (24) is hyperbolic, we use the standard fourth order Runge-Kutta explicit time stepping. This choice is\nmade as implicit methods may add a large amount of diffusion smearing the concentration front. The selected\ninitial condition propagates into a solution that develops a shock, and higher order discontinuous Galerkin methods\nrequire slope limiting to handle overshoot and undershoot [25]. We employ a standard minmod limiter [53]. The\ntime step is chosen to satisfy a CFL condition, ∆t = 0.5Φh/ut , which yields ∆t = 1.929 days. The polynomial\ndegree is set equal to one. The saturation profile is displayed in Fig. 6 for t = 500, 1000, and 1500 days. We\n\n\fA HYBRIDIZABLE DISCONTINUOUS GALERKIN METHOD FOR TWO-PHASE FLOW IN HETEROGENEOUS POROUS MEDIA\n9\n0.9\nAnalytic\nPostprocessed approximation (16 elements)\nk=1 (32 elements)\nPostprocessed approximation (32 elements)\n\n0.8\n0.7\n0.6\n\nSw\n\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nx\n\nFig. 5: Effect of postprocessing k = 1 approximation on a mesh with 16 elements and 32 elements, compared to\nthe k = 1 approximation on a mesh with 32 elements. The spurious oscillations are eliminated, and a much coarser\nmesh may be used instead of mesh refinement.\n\nT = 500 days\nT = 1000 days\nT = 1500 days\n\n1\n\n0.8\n\nSw\n\n0.6\n\n0.4\n\n0.2\n\n0\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\nx\n\nFig. 6: Saturation profiles at different times for the Buckley-Leverett problem using piecewise linears and a mesh\nof 256 elements.\n\nthen perform a mesh refinement study and fix the final time T = 1500 days. Fig. 7 shows the saturation profiles\nusing either piecewise constant or piecewise linear approximations on successively refined meshes. We also include\na zoom-in view for the piecewise linear solution in Fig. 7(c). As expected, accuracy is increased as we refine the\nmesh and increase the polynomial degree.\n4.3. Convergence rates. We take Ω to be the unit square, and consider the following manufactured solutions:\n(25)\n\nsw = 0.5(t + 1) − (7.0)xy(1 − x)(1 − y)e(−x\npw = (t + 1) + xy tanh(1 − x) tanh(1 − y).\n\n2\n\n−y 2 )\n\n,\n\nThe function sw is designed so that 0 < sw < 1. Dirichlet boundary conditions are imposed for both wetting phase\npressure and wetting phase saturation. Capillary pressure and relative permeabilities are defined in Section 4.1,\nwith entry pressure pd = 1000. The viscosities are µo = 10−2 and µw = 10−3 . For simplicity, the medium is\nassumed to be homogeneous, with permeability K = 10−4 . The mesh is made of N × N square elements. The\ntime-step is chosen as ∆t = 1/(N k+1 ), and the final time is T = 1. In Table 1 we verify that the numerical scheme\ngenerates approximations to the saturation that agree with the expected convergence rates. We vary the mesh sizes\nand the polynomial degree. We compute the errors in the L2 norm for the saturation and its gradient, at the final\ntime. We observe that swh converges at the optimal rate of O(hk+1 ). The HDG method also has the property that\nthe gradient qh , also converges at the optimal rate of O(hk+1 ). Primal DG methods require one to reconstruct the\ngradient from the variable swh , which results in a loss of accuracy. The accuracy of qh is important, because the\ngradient of capillary pressure in the Darcy flow requires the gradient of the wetting phase saturation. We also show\nin Table 1 the numerical rates obtained for the quantity (swh − s∗wh ) in the L2 norm; convergence of order O(hk+2 )\nis observed. Convergence rates for the numerical scheme of the Darcy system (1)-(2) are presented in Table 2. It is\n\n\f10\n\nM. S. FABIEN, M. G. KNEPLEY, AND B. M. RIVIÈRE\n\n64 elements\n128 elements\n256 elements\nExact\n\n1\n\n64 elements\n128 elements\n256 elements\nExact\n\n1\n\n0.8\n\n0.6\n\n0.6\n\nSw\n\nSw\n\n0.8\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n0\n\n0\n0\n\n50\n\n100\n\n150\n\n200\n\n250\n\n300\n\n0\n\n50\n\n100\n\nx\n\n150\n\n200\n\n250\n\n300\n\nx\n\n(a) k = 0\n\n(b) k = 1\n\n0.7\n\n64 elements\n128 elements\n256 elements\nExact\n\n0.6\n\nSw\n\n0.5\n0.4\n0.3\n0.2\n0.1\n0\n220\n\n225\n\n230\n\n235\n\n240\n\n245\n\nx\n\n(c) k = 1 (zoom-in)\n\nFig. 7: Buckely-Leverett problem.\n\nevident that both the wetting phase pressure and total velocity converge at the optimal rate of O(hk+1 ) in the L2\nnorm. As our algorithm is semi-implicit, having an accurate total velocity is of critical importance as it is inserted\ninto the saturation equation (3).\nIn Fig. 10 we compare the degree of freedom growth for classical primal DG methods to that of HDG, in a log-log plot.\nWe assume a uniform Cartesian mesh in two dimension. For piecewise quartic approximation, DG methods have\nroughly 2.5 times more unknowns than HDG. Increasing the polynomial order to k = 8, DG methods have roughly\n4.5 times more unknowns than HDG. Fig. 11 examines the total number of nonzero entries in the discretization\nmatrix for DG compared to the statically condensed HDG. For k = 5, the DG method has about 4.3 times more\nnonzero entries as the HDG method. This ratio increases to 6.4 for the case k = 8.\n4.4. 2D heterogeneous medium. A heterogeneous test problem is explored here, with a similar set-up as\nin [29]. The permeability is set as K = 5 · 10−9 (m2 ) in the domain Ω = [0, 100] × [0, 100] (m), except in the region\n[37.5, 100] × [37.5, 62.5], where K = 5 · 10−13 (m2 ). Capillary pressure, relative permeabilities and viscosities are\nthe same as in Section 4.3. At the boundary corresponding to x = 0, we set pD = 3 · 106 , and sD = 0.85. At the\nright boundary (x = 100), we enforce pD = 1 · 106 and sD = 0.2. The remaining boundaries are set as no flow. A\nrelatively coarse uniform mesh of 256 square elements is used. The wetting phase floods the domain from the left\nboundary, and due to a pressure gradient flows from left to right. As the wetting phase reaches the region of lower\npermeability, it is unable to invade, and must flow around it.\nIn Fig. 12 we plot the wetting phase saturation at 700 days for different polynomial orders. The semi implicit\nmethod allows for large time steps, in our case we fix ∆t = 8 days and use implicit Euler in time. Overshoot and\nundershoot are visible for k = 1 and k = 2, and remain bounded. As the polynomial order increases, overshoot and\nundershoot are no longer visible, and the saturation front is sharper. Pressure contours are displayed in Fig. 12.\nThe pressure contours do not appear to vary greatly by altering the polynomial order, however, the saturation\ncurves are much more distinctive.\n\n\fA HYBRIDIZABLE DISCONTINUOUS GALERKIN METHOD FOR TWO-PHASE FLOW IN HETEROGENEOUS POROUS MEDIA\n11\n\n6\n\n12\nk=1\nk=2\nk=3\nk=4\nk=5\nk=6\nk=7\nk=8\n\n5\n\n4\n\n3\n\n2\n\n1\n\n0\n\nHDG degree of freedom growth\n\nx 10\n\nk=1\nk=2\nk=3\nk=4\nk=5\nk=6\nk=7\nk=8\n\n10\n\n8\n\n6\n\n4\n\n2\n\n0\n\n1\n\n2\n\n3\n4\nTotal number of elements\n\n5\n\n6\n\n0\n\n7\n\n0\n\n1\n\n2\n\n4\n\nx 10\n\nFig. 8: DG degrees of freedom\n\n3\n4\nTotal number of elements\n\n5\n\n6\n\nFig. 9: HDG degrees of freedom\n\nFig. 10: (Left) Classical DG degree of freedom growth. (Right) HDG degree of freedom growth.\n\n8\nRatio of nonzeros (DG vs HDG)\n\n7\n6\n\nRatio of nonzeros\n\nDegrees of freedom\n\n5\n\nDG degree of freedom growth\n\nx 10\n\nDegrees of freedom\n\n6\n\n5\n4\n3\n2\n1\n0\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\nPolynomial order\n\nFig. 11: Ratio of nonzeros in discretization matrix for DG vs HDG.\n\n7\n4\n\nx 10\n\n\f12\n\nM. S. FABIEN, M. G. KNEPLEY, AND B. M. RIVIÈRE\n\n(a) k = 1\n\n(b) k = 2\n\n(c) k = 4\n\n(d) k = 8\n\nFig. 12: Wetting phase saturation contour at T = 700 days for different polynomial orders. As the order increases,\novershoot and undershoot diminish.\n\nTo examine the convergence of our method as we increase the polynomial degree, we plot the wetting phase\nsaturation along the line y = 50 in Fig. 14. Notice that this line intersects the region where the permeability drops\nby four orders of magnitude. The spurious oscillations remain bounded and decrease with higher polynomial order.\nFurthermore, as the polynomial order increases we see that the solution converges. A similar plot for the vertical\nline x = 50 can be found in Fig. 15. The conclusions remain the same in this case; higher order polynomials are\nbeneficial.\n4.5. 2D heterogeneous medium with realistic permeability. All parameters are the same as in subsection 4.4, except for the domain that is larger (Ω = [0, 1000]2 ) and for the permeability field. For all simulations in\nthis section we choose k = 4. The simulation is driven by boundary conditions, as described in the previous section.\nRealistic permeability data is extracted from horizontal slices of the SPE10 CSP model 2 [55]. We take (32 × 32),\n(32 × 32), and (64 × 64) subsets of the data for vertical layers 5, 44, and 68, respectively. Figs. 16 (b), (d), and\n(f) display the utilized highly heterogeneous permeability fields in log scale. There are large channels in layer 44\nwhereas there are many small less-connected channels in layer 68.\nInitially the reservoir has a wetting phase saturation of 0.2. Uniform meshes with quadrilateral elements are adopted:\n1024 elements for Figs. 16 (a), (c) and 4096 elements for Fig. 16 (e). We plot the wetting phase saturation after 150\ndays, which can be seen in Figs. 16 (a), (c), and (e). It is evident that the wetting phase saturation avoids regions\nof low permeability which act as a barrier. The permeability layers we selected range across the Tarbert and Upper\nNess formations, which demonstrates that the method is robust for permeabilities with different characteristics.\n5. Conclusions. In this paper we presented a new method based on the hybridizable discontinuous Galerkin\nmethod for incompressible immiscible two-phase flow in porous media. Numerical examples in 1D and 2D show\nthat the method is high order accurate, and robust, even in the case of realistic discontinuous highly varying\npermeability. Furthermore, we are able to take advantage of the HDG method, which is locally conservative, high\norder, and allows for a significant reduction in the total number of degrees of freedom through static condensation.\nStatic condensation enables us to consider polynomial orders that would be otherwise intractable for classical primal\n\n\fA HYBRIDIZABLE DISCONTINUOUS GALERKIN METHOD FOR TWO-PHASE FLOW IN HETEROGENEOUS POROUS MEDIA\n13\n\n(a) k = 1\n\n(b) k = 2\n\n(c) k = 4\n\n(d) k = 8\n\nFig. 13: Pressure contour plots at T = 700 days for various polynomial orders.\n\nDG discretizations. The method does not require penalization or slope limiters, and the stabilization factor does\nnot depend on the polynomial order or mesh size.\nAcknowledgments. This work was partially supported by NSF-DMS 1318348. Fabien would like to acknowledge support from the Ken Kennedy Institute Graduate Fellowship Endowment, and Rice Graduate Education for\nMinorities (RGEM).\n\n\f14\n\nM. S. FABIEN, M. G. KNEPLEY, AND B. M. RIVIÈRE\n\nProfile along y = 50\n\n0.9\n\nk=1\nk=2\nk=4\nk=8\n\n0.8\n\nk=1\nk=2\nk=4\nk=8\n\n0.8\n\n0.7\n\nWetting phase saturation\n\nWetting phase saturation\n\nProfile along y = 50 (zoom-in)\n\n0.9\n\n0.6\n0.5\n0.4\n\n0.7\n\n0.6\n\n0.5\n\n0.3\n0.4\n0.2\n0.1\n\n0.3\n0\n\n20\n\n0.25\n\n40\n\n60\n\n80\n\n5\n\n10\n\n15\n\n20\n\n25\n\nx\n\nx\n\nProfile along y = 50 (zoom-in)\n\nProfile along y = 50 (zoom-in)\nk=1\nk=2\nk=4\nk=8\n\n0.24\n\n30\n\n0.55\n\n0.23\n0.22\n0.21\n0.2\n\n35\n\nk=1\nk=2\nk=4\nk=8\n\n0.6\n\nWetting phase saturation\n\nWetting phase saturation\n\n0\n\n100\n\n0.5\n0.45\n0.4\n0.35\n0.3\n\n0.19\n0.25\n0.18\n\n0.2\n\n0.17\n30\n\n35\n\n40\n\n45\n\n50\n\nx\n\n55\n\n60\n\n65\n\n0.15\n25\n\n30\n\n35\n\n40\n\n45\n\nx\n\nFig. 14: Inspection of the saturation profile along the line y = 50. The approximation converges as the polynomial\norder increases.\n\n\fA HYBRIDIZABLE DISCONTINUOUS GALERKIN METHOD FOR TWO-PHASE FLOW IN HETEROGENEOUS POROUS MEDIA\n15\n\nProfile along x = 50\n\n0.6\n0.55\n\nk=1\nk=2\nk=4\nk=8\n\n0.55\nk=1\nk=2\nk=4\nk=8\n\n0.45\n\n0.5\n\nWetting phase saturation\n\n0.5\n\nWetting phase saturation\n\nProfile along x = 50 (zoom-in)\n\n0.6\n\n0.4\n0.35\n0.3\n\n0.45\n0.4\n0.35\n0.3\n\n0.25\n\n0.25\n\n0.2\n\n0.2\n\n0.15\n0\n\n20\n\n40\n\n60\n\n80\n\n0.15\n10\n\n100\n\n15\n\n20\n\ny\n\n40\n\n0.55\n\nWetting phase saturation\n\nWetting phase saturation\n\n0.24\n\n35\n\nProfile along x = 50 (zoom-in)\n\n0.6\n\nk=1\nk=2\nk=4\nk=8\n\n0.25\n\n30\n\ny\n\nProfile along x = 50 (zoom-in)\n\n0.26\n\n25\n\n0.23\n0.22\n0.21\n0.2\n\n0.5\nk=1\nk=2\nk=4\nk=8\n\n0.45\n0.4\n0.35\n0.3\n\n0.19\n0.25\n0.18\n0.2\n0.17\n35\n\n40\n\n45\n\n50\n\ny\n\n55\n\n60\n\n65\n\n60\n\n65\n\n70\n\n75\n\n80\n\ny\n\nFig. 15: Inspection of the saturation profile along the line x = 50. The approximation converges as the polynomial\norder increases.\n\n\f16\n\nM. S. FABIEN, M. G. KNEPLEY, AND B. M. RIVIÈRE\n\n(a) Wetting phase saturation at t = 150 days\n\n(b) Layer 5 (32 × 32 grid, log scale)\n\n(c) Wetting phase saturation at t = 150 days\n\n(d) Layer 44 (32 × 32 grid, log scale)\n\n(e) Wetting phase saturation at t = 150 days\n\n(f) Layer 68 (64 × 64 grid, log scale)\n\nFig. 16: Two-phase flow in highly heterogeneous media. Uniform meshes with quadrilateral elements, and discontinuous piecewise quartic basis functions.\n\n\fA HYBRIDIZABLE DISCONTINUOUS GALERKIN METHOD FOR TWO-PHASE FLOW IN HETEROGENEOUS POROUS MEDIA\n17\nREFERENCES\n[1] D. G. Anderson, Iterative procedures for nonlinear integral equations, Journal of the ACM (JACM), 12 (1965), pp. 547–560.\n[2] T. Arbogast, M. Juntunen, J. Pool, and M. F. Wheeler, A discontinuous Galerkin method for two-phase flow in a porous\nmedium enforcing h (div) velocityand continuous capillary pressure, Computational Geosciences, 17 (2013), pp. 1055–1078.\n[3] D. N. Arnold and F. Brezzi, Mixed and nonconforming finite element methods: implementation, postprocessing and error\nestimates, RAIRO-Modélisation mathématique et analyse numérique, 19 (1985), pp. 7–32.\n[4] D. N. Arnold, B. C. F. Brezzi, and L. D. Marini, Unified analysis of discontinuous galerkin methods for elliptic problems,\nSIAM J. Numerical Analysis, 39 (2002), pp. 1749–1779.\n[5] J. G. B. Cockburn and R. D. Lazarov, Unified hybridization of discontinuous galerkin, mixed, and continuous galerkin methods\nfor second order elliptic problems, SIAM J. Numerical Analysis, 47 (2009), pp. 1319–1365.\n[6] P. Bastian, Higher order discontinuous Galerkin methods for flow and transport in porous media, in Challenges in Scientific\nComputing-CISC 2002, Springer, 2003, pp. 1–22.\n[7] P. Bastian, A fully-coupled discontinuous Galerkin method for two-phase flow in porous media with discontinuous capillary\npressure, Computational Geosciences, 18 (2014), pp. 779–796.\n[8] P. Bastian and B. Rivière, Superconvergence and h (div) projection for discontinuous galerkin methods, International journal\nfor numerical methods in fluids, 42 (2003), pp. 1043–1057.\n[9] J. Bear and A. Cheng, Modeling Groundwater Flow and Contaminant Transport, Theory and Applications of Transport in\nPorous Media, Springer Netherlands, 2010.\n[10] J. P. Berrut and L. N. Trefethen, Barycentric Lagrange interpolation, SIAM Review, 46 (2004), pp. 501–517.\n[11] A. Bielinski, Numerical simulation of CO2 sequestration in geological formations, PhD dissertation, Institut fur Wasserbau der\nUniversit at Stuttgar, 2007.\n[12] T. I. Bjørnarå and E. Aker, Comparing equations for two-phase fluid flow in porous media, in COMSOL conference, Hannover,\n2008.\n[13] F. Brezzi, J. Douglas, and L. D. Marini, Two families of mixed finite elements for second order elliptic problems, Numerische\nMathematik, 47 (1985), pp. 217–235.\n[14] R. Brooks and T. Corey, Hydraulic properties of porous media, Hydrology Papers, Colorado State University, 24 (1964).\n[15] S. E. Buckley, M. Leverett, et al., Mechanism of fluid displacement in sands, Transactions of the AIME, 146 (1942), pp. 107–\n116.\n[16] T. Bui-Thanh, From Godunov to a unified hybridized discontinuous Galerkin framework for partial differential equations, Journal\nof Computational Physics, 295 (2015), pp. 114–146.\n[17] Z. Chen, G. Huan, and Y. Ma, Computational methods for multiphase flows in porous media, SIAM, 2006.\n[18] B. Cockburn, J. G. B. Dong, M. Restelli, and R. Sacco, A hybridizable discontinuous Galerkin method for steady-state\nconvection-diffusion-reaction problems, SIAM J. Scientific Computing, 31 (2009), pp. 3827–3846.\n[19] B. Cockburn, B. Dong, and J. Guzmán, A superconvergent LDG-hybridizable Galerkin method for second-order elliptic problems,\nMath. Comput., 77 (2008), pp. 1887–1916.\n[20] B. Cockburn, B. Dong, J. Guzmán, M. Restelli, and R. Sacco, A hybridizable discontinuous galerkin method for steady-state\nconvection-diffusion-reaction problems, SIAM Journal on Scientific Computing, 31 (2009), pp. 3827–3846.\n[21] B. Cockburn, B. Dong, J. Guzmán, M. Restelli, and R. Sacco, Superconvergent and optimally convergent ldg-hybridizable\ndiscontinuous galerkin methods for convection-diffusion-reaction problems, SIAM J. Sci. Comput, 31 (2009), pp. 3827–3846.\n[22] B. Cockburn and J. Gopalakrishnan, A characterization of hybridized mixed methods for second order elliptic problems, SIAM\nJournal on Numerical Analysis, 42 (2004), pp. 283–301.\n[23] B. Cockburn, W. Qiu, and K. Shi, Conditions for superconvergence of hdg methods for second-order elliptic problems, Mathematics of Computation, 81 (2012), pp. 1327–1353.\n[24] B. Cockburn and C.-W. Shu, The local discontinuous galerkin method for time-dependent convection-diffusion systems, SIAM\nJournal on Numerical Analysis, 35 (1998), pp. 2440–2463.\n[25] B. Cockburn and C.-W. Shu, The Runge–Kutta discontinuous Galerkin method for conservation laws V: multidimensional\nsystems, Journal of Computational Physics, 141 (1998), pp. 199–224.\n[26] C. Dawson, S. Sun, and M. F. Wheeler, Compatible algorithms for coupled flow and transport, Computer Methods in Applied\nMechanics and Engineering, 193 (2004), pp. 2565–2580.\n[27] J. Douglas Jr, R. E. Ewing, and M. F. Wheeler, The approximation of the pressure by a mixed method in the simulation of\nmiscible displacement, RAIRO-Analyse numérique, 17 (1983), pp. 17–33.\n[28] M. Dubiner, Spectral methods on triangles and other domains, Journal of Scientific Computing, 6 (1991), pp. 345–390.\n[29] Y. Epshteyn and B. Riviere, On the solution of incompressible two-phase flow by ap-version discontinuous Galerkin method,\nInternational Journal for Numerical Methods in Biomedical Engineering, 22 (2006), pp. 741–751.\n[30] Y. Epshteyn and B. Rivière, Fully implicit discontinuous finite element methods for two-phase flow, Applied Numerical Mathematics, 57 (2007), pp. 383–401.\n[31] A. Ern, I. Mozolevski, and L. Schuh, Discontinuous Galerkin approximation of two-phase flows in heterogeneous porous media\nwith discontinuous capillary pressures, Computer methods in applied mechanics and engineering, 199 (2010), pp. 1491–1501.\n[32] M. S. Fabien, M. G. Knepley, R. T. Mills, and B. M. Riviere, Heterogeneous computing for a hybridizable discontinuous\nGalerkin geometric multigrid method, ArXiv e-prints, (2017), arXiv:1705.09907.\n[33] G. Fu, W. Qiu, and W. Zhang, An analysis of hdg methods for convection-dominated diffusion problems, ESAIM: Mathematical\nModelling and Numerical Analysis, 49 (2015), pp. 225–256.\n[34] R. Helmig, Multiphase Flow and Transport Processes in the Subsurface: A Contribution to the Modeling of Hydrosystems,\nEnvironmental engineering, Springer, 1997, https://books.google.com/books?id=4ZVRAAAAMAAJ.\n[35] G. M. Homsy, Viscous fingering in porous media, Annual review of fluid mechanics, 19 (1987), pp. 271–311.\n[36] H. Hoteit and A. Firoozabadi, Numerical modeling of two-phase flow in heterogeneous permeable media with different capillarity\npressures, Advances in Water Resources, 31 (2008), pp. 56–73.\n[37] J. Hou, J. Chen, S. Sun, and Z. Chen, Adaptive mixed-hybrid and penalty discontinuous Galerkin method for two-phase flow in\nheterogeneous media, Journal of Computational and Applied Mathematics, 307 (2016), pp. 262–283.\n[38] M. Jamei and H. Ghafouri, A novel discontinuous Galerkin model for two-phase flow in porous media using an improved IMPES\n\n\f18\n\nM. S. FABIEN, M. G. KNEPLEY, AND B. M. RIVIÈRE\n\nmethod, International Journal of Numerical Methods for Heat & Fluid Flow, 26 (2016), pp. 284–306.\n[39] A. Jaust and J. Schütz, A temporally adaptive hybridized discontinuous Galerkin method for time-dependent compressible flows,\nComputers & Fluids, 98 (2014), pp. 177–185.\n[40] G. Karniadakis and S. J. Sherwin, Spectral/hp Element Methods for CFD, Numerical mathematics and scientific computation,\nOxford University Press, 1999.\n[41] J. Killough, C. Kossack, et al., Fifth comparative solution project: evaluation of miscible flood simulators, in SPE Symposium\non Reservoir Simulation, Society of Petroleum Engineers, 1987.\n[42] W. Klieber and B. Riviere, Adaptive simulations of two-phase flow by discontinuous Galerkin methods, Computer methods in\napplied mechanics and engineering, 196 (2006), pp. 404–419.\n[43] T. Koornwinder, Two-variable analogues of the classical orthogonal polynomials, in Theory and application of special functions\n(Proc. Advanced Sem., Math. Res. Center, Univ. Wisconsin, Madison, Wis., 1975), Academic Press New York, 1975, pp. 435–\n495.\n[44] R. Lantz et al., Rigorous calculation of miscible displacement using immiscible reservoir simulators, Society of Petroleum\nEngineers Journal, 10 (1970), pp. 192–202.\n[45] D. B. McWhorter and D. K. Sunada, Exact integral solutions for two-phase flow, Water Resources Research, 26 (1990),\npp. 399–413.\n[46] N. Nguyen, J. Peraire, and B. Cockburn, An implicit high-order hybridizable discontinuous galerkin method for linear convectiondiffusion equations, Journal of Computational Physics, 228 (2009), pp. 3232 – 3254.\n[47] N. C. Nguyen and J. Peraire, Hybridizable discontinuous Galerkin methods for partial differential equations in continuum\nmechanics, Journal of Computational Physics, 231 (2012), pp. 5955–5988.\n[48] N. C. Nguyen, J. Peraire, and B. Cockburn, An implicit high-order hybridizable discontinuous galerkin method for nonlinear\nconvection–diffusion equations, Journal of Computational Physics, 228 (2009), pp. 8841–8855.\n[49] N. C. Nguyen, J. Peraire, and B. Cockburn, A class of embedded discontinuous galerkin methods for computational fluid\ndynamics, Journal of Computational Physics, 302 (2015), pp. 674–692.\n[50] R. G. Owens, Spectral approximations on the triangle, Proceedings of the Royal Society of London A: Mathematical, Physical\nand Engineering Sciences, 454 (1998), pp. 857–872.\n[51] D. Peaceman, Fundamentals of Numerical Reservoir Simulation, Developments in Petroleum Science, Elsevier Science, 2000,\nhttps://books.google.com/books?id=-DujQRDF4kwC.\n[52] J. Proriol, Sur une famille de polynomes á deux variables orthogonaux dans un triangle, Comptes rendus hebdomadaires des\nséances de l’Académie des sciences, 245 (1957), pp. 2459–2461.\n[53] P. L. Roe, Characteristic-based schemes for the Euler equations, Annual review of fluid mechanics, 18 (1986), pp. 337–365.\n[54] A. Samii, C. Michoski, and C. Dawson, A parallel and adaptive hybridized discontinuous Galerkin method for anisotropic\nnonhomogeneous diffusion, Computer Methods in Applied Mechanics and Engineering, 304 (2016), pp. 118–139.\n[55] http://www.spe.org/web/csp/datasets/set02.htm. Accessed: 2017-05-27.\n[56] R. Stenberg, Postprocessing schemes for some mixed finite elements, ESAIM: Mathematical Modelling and Numerical Analysis,\n25 (1991), pp. 151–167.\n[57] M. Todd, W. Longstaff, et al., The development, testing, and application of a numerical simulator for predicting miscible\nflood performance, Journal of Petroleum Technology, 24 (1972), pp. 874–882.\n[58] H. Wang, H. K. Dahle, R. E. Ewing, M. S. Espedal, R. C. Sharpley, and S. Man, An ELLAM scheme for advection-diffusion\nequations in two dimensions, SIAM Journal on Scientific Computing, 20 (1999), pp. 2160–2194.\n[59] M. Woopen, T. Ludescher, and G. May, A hybridized discontinuous Galerkin method for turbulent compressible flow, in 44th\nAIAA Fluid Dynamics Conference, 2014, p. 2783.\n\n\fA HYBRIDIZABLE DISCONTINUOUS GALERKIN METHOD FOR TWO-PHASE FLOW IN HETEROGENEOUS POROUS MEDIA\n19\n\nkswh − sw kL2 (Ω)\nN\nError\nRate\n4 1.05e-03\n8 2.48e-04 2.089\n16 6.08e-05 2.026\n32 1.53e-05 1.992\n64 3.85e-06 1.987\n\nks∗wh − sw kL2 (Ω)\nError\nRate\n6.84e-04\n1.12e-04 2.607\n1.63e-05 2.777\n2.22e-06 2.878\n2.90e-07 2.936\n\nkqh − ∇sw kL2 (Ω)\nError\nRate\n7.28e-03\n2.30e-03 1.661\n6.61e-04 1.799\n1.78e-04 1.889\n4.65e-05 1.941\n\n2\n\n2\n4\n8\n16\n32\n\n3.94e-04\n5.69e-05\n7.39e-06\n9.48e-07\n1.20e-07\n\n2.792\n2.945\n2.963\n2.981\n\n2.59e-04\n2.28e-05\n1.65e-06\n1.12e-07\n7.61e-09\n\n3.503\n3.786\n3.876\n3.879\n\n2.51e-03\n4.37e-04\n6.36e-05\n8.66e-06\n1.17e-06\n\n2.520\n2.781\n2.877\n2.887\n\n3\n\n2\n4\n8\n16\n32\n\n5.52e-05\n3.35e-06\n2.25e-07\n1.47e-08\n9.48e-10\n\n4.042\n3.894\n3.934\n3.954\n\n2.94e-05\n1.01e-06\n3.59e-08\n1.20e-09\n4.014e-11\n\n4.863\n4.817\n4.902\n4.901\n\n3.95e-04\n2.81e-05\n2.00e-06\n1.34e-07\n8.58e-09\n\n3.814\n3.814\n3.900\n3.963\n\n4\n\n2\n4\n8\n16\n32\n\n4.90e-06\n2.51e-07\n9.58e-09\n3.20e-10\n1.05e-11\n\n4.284\n4.716\n4.904\n4.916\n\n1.42e-06\n3.48e-08\n6.09e-10\n9.99e-12\n1.69e-13\n\n5.350\n5.838\n5.930\n5.882\n\n2.80e-05\n1.34e-06\n4.87e-08\n1.62e-09\n5.40e-11\n\n4.380\n4.787\n4.905\n4.906\n\n5\n\n2\n4\n8\n16\n\n7.49e-07\n1.98e-08\n3.20e-10\n5.19e-12\n\n5.240\n5.952\n5.946\n\n1.78e-07\n1.56e-09\n1.15e-11\n9.07e-14\n\n6.827\n7.085\n6.991\n\n4.01e-06\n8.39e-08\n1.34e-09\n2.22e-11\n\n5.581\n5.959\n5.924\n\nk\n1\n\nTable 1: Errors and convergence rates for the saturation, swh , the gradient, qh , and its post-processed approximation, s∗wh , on a Cartesian mesh of N × N elements.\n\n\f20\n\nM. S. FABIEN, M. G. KNEPLEY, AND B. M. RIVIÈRE\n\nkpwh − pw kL2 (Ω)\nError\nRate\n1.5217e-01\n6.0507e-02 1.330\n2.0184e-02 1.583\n6.7061e-03 1.589\n2.4355e-03 1.461\n9.9662e-04 1.289\n\nkuth − ut kL2 (Ω)\nError\nRate\n3.2169e-03\n1.7914e-03 8.445e-01\n8.1952e-04\n1.128\n3.6293e-04\n1.175\n1.6977e-04\n1.096\n8.2987e-05\n1.032\n\nk\n0\n\nN\n2\n4\n8\n16\n32\n64\n\n2\n\n2\n4\n8\n16\n32\n\n3.90e-03\n5.40e-04\n7.34e-05\n9.83e-06\n1.28e-06\n\n2.851\n2.880\n2.901\n2.936\n\n4.68e-04\n6.77e-05\n9.61e-06\n1.35e-06\n1.86e-07\n\n2.791\n2.816\n2.826\n2.863\n\n4\n\n2\n4\n8\n16\n32\n\n7.88e-05\n4.65e-06\n1.87e-07\n6.32e-09\n2.03e-10\n\n4.082\n4.630\n4.893\n4.955\n\n8.99e-06\n5.20e-07\n2.17e-08\n7.60e-10\n2.52e-11\n\n4.112\n4.578\n4.840\n4.911\n\nkpwh − pw kL2 (Ω)\nN\nError\nRate\n2 2.16e-02\n4 7.49e-03 1.529\n8 2.23e-03 1.746\n16 6.16e-04 1.856\n32 1.62e-04 1.920\n64 4.19e-05 1.956\n\nkuth − ut kL2 (Ω)\nError\nRate\n2.04e-03\n7.90e-04 1.369\n2.39e-04 1.720\n6.75e-05 1.827\n1.83e-05 1.881\n4.84e-06 1.919\n\n3\n\n2\n4\n8\n16\n32\n\n2.54e-04\n2.84e-05\n2.56e-06\n1.82e-07\n1.19e-08\n\n3.159\n3.474\n3.814\n3.930\n\n4.17e-05\n3.78e-06\n3.16e-07\n2.28e-08\n1.54e-09\n\n3.462\n3.581\n3.791\n3.885\n\n5\n\n2\n4\n8\n16\n32\n\n1.38e-05\n4.38e-07\n7.08e-09\n1.13e-10\n1.82e-12\n\n4.978\n5.953\n5.958\n5.966\n\n1.63e-06\n5.51e-08\n9.59e-10\n1.64e-11\n2.73e-13\n\n4.888\n5.843\n5.871\n5.905\n\nk\n1\n\nTable 2: Errors and convergence rates for pwh and uth , on a Cartesian mesh of N × N elements.\n\n\f"
        ],
        [
         "46",
         "46",
         "cs.CE",
         "Computational Engineering",
         "1407.4650v1.pdf",
         "Protein Folding in the Hexagonal Prism Lattice\nwith Diagonals\n\narXiv:1407.4650v1 [cs.CE] 17 Jul 2014\n\nDipan Lal Shaw, M. Sohel Rahman, A. S. M. Shohidull Islam, and Shuvasish\nKarmaker\n1\n\n2\n\nAℓEDA Group, CSE, BUET, Bangladesh.\nDepartment of CSE, BUET, Dhaka, Bangladesh\n\nAbstract. Predicting protein secondary structure using lattice model\nis one of the most studied computational problem in bioinformatics.\nHere secondary structure or three dimensional structure of protein is\npredicted from its amino acid sequence. Secondary structure refers to\nlocal sub-structures of protein. Mostly founded secondary structures are\nalpha helix and beta sheets. Since, it is a problem of great potential\ncomplexity many simplified energy model have been proposed in literature on basis of interaction of amino acid residue in protein. Here we\nuse well researched Hydrophobic-Polar (HP) energy model. In this paper, we proposed hexagonal prism lattice with diagonal that can overcome the problems of other lattice structure, e.g., parity problem. We\ngive two approximation algorithm for protein folding on this lattice. Our\nfirst algorithm leads us to similar structure of helix structure which is\ncommonly found in protein structure. This motivated us to find next\nalgorithm which improves the algorithm ratio of 79 .\n\n1\n\nIntroduction\n\nProtein structure prediction is one of the most studied computational problems\nin bioinformatics. By using simplified and abstract models, many approximate\nsolutions for this problem have been given in the literature. There exist a variety\nof models attempting to simplify the problem by abstracting only the “essential\nphysical properties” of real proteins. A lattice model for folding amino acids is\nrepresented by connected beads in two dimensional lattices or three dimensional\ncubic lattices and considers a simplified energy function.\nWe can categorize the lattice structure models into two different classes: Simplified Lattice Models (e.g. [13]) and Realistic Lattice Models [5]. One of the widely\nused simplified lattice model is the HP model which was first introduced by Dill\n[13]. In HP model, there are only two types of beads: H represents a hydrophobic\nor non-polar bead and P represents a polar or hydrophilic one. The main force\nin the folding process is the hydrophobic-hydrophobic force, i.e., H-H contacts.\nFor optimal embedding, our main goal in this model is to maximize the H-H\ncontacts.\nThe protein folding problem in HP model is NP-hard [4]. Hart and Istrail gave\nthe first 4-approximation algorithm for the problem on the 2D square lattice\n\n\f[6]. Later on, Newman [17] improved the approximation ratio to 3 considering the conformation as a folded loop. A 83 -approximation algorithm for the\nproblem on the 3D square lattice was given by Hart and Istrail [6]. In [2], the\nauthors introduced square lattice with diagonals and presented algorithms that\n26\nfor the two-dimensional and 85 for the threegive an approximation ratio of 15\ndimensional lattice. Later, Newman and Ruhl improved this based on different\ngeometric ideas; they achieved an improved approximation ratio of 0.37501 [18].\nTo remove the parity problem of the square and cubic lattices Agarwala et al.\nfirst proposed the triangular lattice in [1]. There, they gave a 11\n6 approximation\nalgorithm. For a more generalized version, namely, the 3D FCC lattice, Agarwala\net al. [1] gave an approximation algorithm having an approximation ratio of 53 .\nTo alleviate the problem of sharp turns, Jiang and Zhu introduced the hexagonal\nlattice model and gave an approximation algorithm with approximation ratio 6\n[12]. A linear time approximation algorithm for protein folding in the HP side\nchain model on the extended cubic lattice having an approximation ratio of 0.84\nwas presented by Heun [7].\nA number of heuristic and meta-heuristic techniques have also been applied to\ntackle the protein folding problem in the literature. A genetic algorithm for the\nprotein folding problem in the HP model in 2D square lattice was proposed in\n[20]. In [8,9], a hybrid genetic algorithm was presented for the HP model in 2D\ntriangular lattice and 3D FCC lattice. The authors in [16] first proposed the\npull move set for the rectangular lattices, which was used in the HP model under a variety of local search methods. They also showed the completeness and\nreversibility of the pull move set for the rectangular grid lattices. In [3], the\nauthors extended the idea of the pull move set in the local search approach for\nfinding an optimal embedding in 2D triangular grid and the FCC lattice in 3D.\nIn this paper, we introduce the hexagonal prism lattices with diagonals for protein folding. Our prior work on hexagonal lattice model with diagonals gives\nan approximation ratio of 53 for primary protein structure [19]. The motivation\nfor introducing hexagonal prism lattice comes from the secondary structure of\na protein as follows. The secondary structure of a protein suggests that, in real\nprotein folding, sharp turn does not occur frequently. Hexagonal model alleviates this sharp turn problem [12]. On the other hand, in the cubic lattice HP\nmodel there is a serious shortcoming, namely, the parity problem as follows.\nDue to a grid structure in a cubic lattice, contact can be established between\ntwo hydrophobic atoms only if they both are either on even positions or on odd\npositions of the sequence. To address this parity problem, we propose idea of\nthis new lattice model, i.e., hexagonal prism lattice model with diagonals. In\nthis model contacts may exist through diagonals (see Fig. 1). Notably, these\nissues have also been partially alleviated in the cubic lattice with diagonals and\ntriangular lattice. To this end, our new model opens a new avenue for further\nresearch for this long standing problem. We present two novel approximation\nalgorithms for long structure protein folding on this lattice. Our first algorithm\nprovide 2 approximation ratio for k > 13 where k is the number of sequences of\nH’s in the HP string. Our next algorithm improves the approximation ratio to\n\n\f9\n7\n\nfor k > 132 where k is the number of sequences of H’s in the HP string. This\nalgorithm is based on a strategy of partitioning the entire protein sequence into\ntwo pieces. Since now Alireza Hadj Khodabakhshi et al. used hexagonal prism\nlattice more successfully for finding inverse protein folding, which is due to [15].\nThe rest of the paper is organized as follows. In Section ‘??’, we introduce the\nhexagonal prism lattice with diagonals and define some related notions. Section\n‘??’ describes our algorithms and relevant results. We briefly conclude in Section\n‘??.\n\n2\n\nPreliminaries\n\nIn this section, we present the required notions and notations to describe the\nhexagonal prism lattice model with diagonals.\nDefinition 1. The three-dimensional hexagonal prism lattice with diagonals is\nan infinite graph G = (V, E) in the Euclidian Space with vertex set V = R3\nand edge set E = {(x, x′ )|x, x′ ∈ R3 , |x − x′ | ≤ 2}, where |.| denotes the\nEuclidean norm. The hexagonal prism lattice is composed by stacking multiple two-dimensional hexagonal lattices with diagonals on top of each other. On\na hexagonal prism lattice with diagonals each two-dimensional hexagonal lattice\nwith diagonals is called a layer. The edges connecting the two layers are called\nlayer edges. An edge e ≡ (x, x′ ) ∈ E is a non-diagonal or non-diagonal layer\nedge iff |x − x′ | = 1; otherwise it is a diagonal edge or diagonal-layer edge.\nWe use the well known notion of neighbourhood or adjacency of graph theory:\ntwo vertices are adjacent/neighbour to each other if they are connected through\nan edge. In this connection, the difference between the usual hexagonal prism\nmodel and our propose model lies in the fact that a vertex in the former has 5\nneighbours, whereas in the latter it has additional 15 neighbours, i.e., a total of\n20 neighbours (see Fig. 1).\n\nFig. 1: A hexagonal prism lattice with diag-\n\nonals. Different layers are indicated using\nblack and red color. Connecting edges between layers are indicated using green color.\n\nFig. 2: Crossing between binding\n\nedges; this situation is forbidden\nin a valid conformation.\n\nAlthough the lattice is defined as an infinite graph, we will be concerned with\nonly a finite sub-graph of it for each conformation of a protein. The input to\n\n\fthe protein folding problem is a finite string p over the alphabet {P, H} where\np = {P }∗ b1 {P }+ b2 {P }+ ...{P }+ bk {P }∗ . Here bi ∈ {H}+ f or 1 ≤ i ≤ k and\nPk\nlet n = i=1 |bi |. Here, H denotes non-polar and P denotes polar amino acids\nrespectively. Often, in what follows, the input string in our problem will be refer\nto as an HP string. An H-run in an HP string denotes the consecutive H’s and\na P-run denotes consecutive P’s. So, the total number of H-runs is k and total\nnumber of H is n. An H-run of even (odd) length is said to be an even H-run\n(odd H-run). We will now define the valid embeddings and conformation of a\nprotein into this lattice. An embedding is a self-avoiding walk inside the grid.\nDefinition 2. Let p = p1 . . . pt be an HP string of length t and let G = (V, E)\nbe a lattice. An embedding of p into G is a mapping function f : {1, . . . , t} → V\nfrom the positions of the string to the vertices of the lattice. It assigns adjacent\npositions in p to adjacent vertices in G, (f (i), f (i + 1)) ∈ E for all 1 ≤ i ≤ t − 1.\nThe edges (f (i), f (i + 1)) ∈ E for 1 ≤ i ≤ t − 1 are called binding edges. An\nembedding of p into G is called a conformation, if no two binding edges cross\neach other (see Fig. 2).\n\nHydrophobic Residue\n\nC\n\nD\n\nPolar Residue\n\nB\n\nE\n\nBinding Edge\n\nA\n\nF\n\nFig. 4: (C,D) and (B,C) are alter-\n\nnating edges; (A,C), (C,F) and\n(C,E) are loss edges.\nFig. 3: Conformation of PHPHHH-\n\nPHPHPHPHPHHH on the lattice.\nIn a conformation, a vertex occupied by an H (P) will often be referred to\nas an H-vertex (a P-vertex). Fig. 3 shows an example of a conformation. Edges\ncoloured blue are binding edges and all other edges between residues are nonbinding edges. Throughout the paper, the H-vertices are indicated by filled circle\nand the P-vertices are indicated by blank circles.\nDefinition 3. Given a conformation φ, an edge (x, x′ ) of G is called a contact\nedge, if it is not a binding edge, but there exist i, j ∈ {1, . . . , t} such that\nf (i) = x, f (j) = x′ , and pi = pj = H. The vertices of the lattice which are not\noccupied by an H or a P are called unused vertices. A binding edge connecting\nan H with a P is called an alternating edge. Loss edge is a non-binding edge\nincident to an H that is not a contact edge (see Fig. 4).\nNow, we define the neighbourhood of an edge in the lattice.\n\n\fDefinition 4. Let e = (x, y) be any edge in G. We define the neighbourhood\nN (e) of e as the intersection of the neighbours of its endpoints x and y.\n\n3\n3.1\n\nOur Approaches\nUpper Bound\n\nWe will deduce a bound based on a simple counting argument: we will count\nthe number of neighbours of a vertex in the lattice. We start with the following\nuseful lemmas.\ny\n\ny\n\nx\n\nx\n(b)\n\n(a)\n\nx\n\nx\n\ny\ny\n(c)\n\n(d)\n\nFig. 5: (a)12 neighbourhood of the non-diagonal edge (x, y) (b)4 neighbourhood\n\nof the diagonal edge (x, y) (c)2 neighbourhood of layer-diagonal edge (x, y) (d)6\nneighbourhood of layer non-diagonal edge (x, y).\n\nLemma 1. Let p be an HP string and G = (V, E) is a hexagonal lattice with\ndiagonals. If p has a conformation in G, then any H in p can have at most 18\ncontact edges.\nProof: Every vertex in the lattice G has exactly 20 neighbours comprising 3\nnon-diagonal neighbours, 9 diagonal neighbours in one layer, 4 neighbour from\nupper layer and 4 neighbour from lower layer (see Fig. 1). In this conformation,\nevery H-vertex has exactly two binding edges. Hence 18 edges remain, which\ncould potentially be contact edges. And hence the result follows. ⊓\n⊔\nLemma 2. Let p be an input string for the problem and φ be a conformation\nof p. Let e =(x, y) be a loss edge with respect to φ. Then there are at most four\nalternating edges in N (e).\nProof: From Fig. 5 if e is a non-diagonal edge, then N (e) contain 12 vertices;\nif e is a diagonal edge, then N (e) contain 4 vertices; if e is a layer-diagonal edge,\nthen N (e) contain 2 vertices; if e is a layer non-diagonal edge, then N (e) contain\n6 vertices. Again, each of x and y can be incident to at most two binding edges.\nSo, there are at most four binding edges in N (e). It follows immediately that\nthere can be at most four alternating edges adjacent to e. ⊓\n⊔\nNow we are ready to present the upper bound.\n\n\fLemma 3. For a given HP string p, the the total number of contacts in a conformation φ is at most 18n − 12 k, where k is the total number of H-runs and n\nis the total number of H.\nProof : From Lemma 1, we know that the number of contacts is at most\n18n. In a confirmation one loss edge incident to H means that it would lose\none contact edge. In what follows we will show that there will be at least 12 k\nloss edges in φ. Since every H-run is preceded and followed by a total of two\nalternating edges, it is sufficient to prove that, for each alternating edge in φ for\np, we have 41 loss edge on average.\nFrom Lemma 2 we know that, for every loss edge there will be at most four\nalternating edges in its neighbourhood. Alternatively, we can say that, for every\nfour alternating edges there will be at least one loss edge, assuming that the\nalternating edges are in the neighbourhood of that loss edge. Clearly, if the\nalternating edges are not within the neighbourhood then the number of loss\nedges will increase. So, for every alternating edge there will be at least 41 loss\nedge. There are a total of 2k alternating edges. So, the total number of loss edges\n⊔\nwill be, 14 × 2 × k = 21 k. Hence, the result follows. ⊓\n3.2\n\nAlgorithms and lower bounds\n\nIn this section, we present two novel approximation algorithms for the problem.\n\n36\n\nZ\n\n30\n\n35\n\n23\n\n29\n\n17\n\n34\n22\n\n33\n\n28\n\n31\n\n24\n\n25\n32\n\n16\n\n18\n\n15\n9\n\n26\n\n21\n\n27\n10\n\n14\n\n8\n\n3\n19\n20\n\n13\n\n4\n\n1\n\n2\n\n2\n11\n\n12\n\n5\n\n6\n\n8\n\n1\n\n7\n\n1\n\n1\n\nFig. 6: Folding of HP string H 4P H P H 1 by Algorithm HelixArrangement.\nDotted black line represent the lattice, solid line represent binding edge of protein, blue dashed line shows 9 contacts of a H. Binding edges are numbered\nsequentially. z indicates the direction of side layers of Upper layer.\n\nAlgorithm HelixArrangement Idea of first algorithm is to arrange all H’s\nof the input string in helix structure. The main difference between conventional\nhelix structure, here we arrange P’s of input string outside of the main helix\nstructure. Fig. 6 shows the way we arrange H’s and P’s.\n\n\fAlgorithm HelixArrangement\nInput: An HP string p.\n1. Arrange the H’s as follows:\n(a) Starting from a layer arrange the first six H’s in a hexagon. Let, called\nthis base hexagon.\n(b) Using the layer diagonal edge climb to upper layer. In this layer arrange\nnext six H’s in a hexagon which is parallel to base hexagon.\n(c) repeat step (b) until end of string p. The hexagon where the process\nended, let called that top hexagon.\n2. Intermediate P-runs are arranged in the outer side of hexagon in a layer(see\nFig. 6)\nApproximation ratio for Algorithm HelixArrangement Except the H’s\nof base hexagon and top hexagon a H can achieve at least 9 contacts. A H from\nits layer achieve 3 contacts, from its immediate upper layer 3 contacts and from\nits immediate lower layer 3 contacts. H’s of base hexagon miss the contacts from\nlower layer and H’s of top hexagon miss the contacts from upper layer. So, there\nis in total 12 H in base hexagon and top hexagon which miss in total 12 ∗ 3 or 36\ncontacts. Note that, it is possible that top hexagon is not filled 6 H’s. But it\ndoes not change any computation, because there is still 6 H’s in top hexagon\nand lower layer hexagon of top hexagon, which miss 3 contacts.\nNow, if we consider the P’s arrangement, we will achieve two contacts for\nevery alternating edge. If there is k alternating edge we will achieve 2k contacts.\nSo, for n H’s total number of contacts(C) can be achieved as follows: C ≥ 9n −\n36 + 2k\nHence we get the following approximation ratio A1 :\nA1 =\n\n18n − 21 k\n(9n − 36 + 2k)\n\n(1)\n\nFrom Equation 1 it can be seen that for large n, A1 tends to reach 18\n9 or 2. So\nwe compute the value of k so that our approximation ratio is at most 2 as shown\n18n− k\n2\nbelow. (9n−36+2k)\n≤ 18\n9\n⇒ 81k ≥ 18 × 30 × 2\n⇒ k ≥ 40\n3 ≈ 13\nSo, if the total number of H-runs is greater than 13, then Algorithm HelixArrangement will achieve an approximation ratio of 2.\nTheorem 1. For any given HP string, Algorithm HelixArrangement gives a 2\napproximation ratio for k > 13, where k is the total number of H-runs and n is\nthe total number of H. ⊓\n⊔\nAlgorithm LayerArrangement The idea of second algorithm is to arrange\nall H’s occurring in the input string along the two layers. We arrange the H’s\nin the prefix of the string up to the ⌊ n2 ⌋-th H on the upper layer and arrange\n\n\fz\n\nFig. 7: Folding of HP string H 3 P 6 H 2 P 2 H 4 P 7 H 1 3P 5 H 5 P 6 H 4 P 2 H 5 by Algo-\n\nrithm LayerArrangement only in Upper layer. Z indicates the direction of side\nlayers of Upper layer\nthe rest of those on the lower layer. In a layer, H-runs are arranged in a spiral\nmanner. Then we arrange the P’s between the H’s outside these two layers. The\narrangements of the P-runs outside the two layers are shown in Fig. 7. Within\na layer the arrangement is done in chains (see Fig. 7). The arrangement in the\nupper (lower) layer can be further divided into nine regions, namely, the left region, the right region, the up region, the down region, the inside-left region, the\ninside-right region, the inside-up region, the inside-down region and the middle\nregion (see Fig. 8).\nAlgorithm LayerArrangement\nInput: An HP string p.\n1. Set f = ⌊ n2 ⌋.\n2. Suppose F denotes the position in p after the f -th H. Denote by pref F (p)\nthe prefix of p up to position F and by suff F (p) the suffix, that starts right\nafter it. Now,\n(a) Arrange the H’s in pref F (p) in the upper layer as follows:\ni. Let, i and j are two integers that divide m1 with reminder 0, such\nthat |i − j| is minimal for all i and j. Let, r = min(i, j), which\nis number of the chains in a layer. Let s = ⌊ fr ⌋, which is number\nof residues in a chain. Suppose, S1 , S2 , S3 ... denote the position in p\nafter the s-th,2s-th,3s-th... H respectively. So, Si (p)= pSi−1 , ..., pSi −1\nfor i = 1, 2, 3.... Here S0 is starting position.\nii. Now arrange Si (p) in chain one by one from top to bottom for i =\n1, 2, 3....\niii. Intermediate P-runs are arranged in the upper-side layers of the upper layer (see Fig. 7)\n(b) Arrange the H’s in suff F (p) along the lower layer following the same\nstrategy spelled out in Step 2(a); intermediate P-runs are arranged in\nthe lower-side layer of the lower layer (see Fig. 7).\n\n\fUp Region\nInside Up Region\nRight Region\nInside Right Region\n\nMiddle Region\nInside Left Region\nLeft Region\nInside Down Region\nDown Region\n\nz\n\nFig. 8: Divided into 9 region\n\nApproximation ratio for Algorithm LayerArrangement Now we focus\non deducing an approximation ratio for Algorithm LayerArrangement. Suppose\nthat m1 = ⌊ n2 ⌋. So, according to Algorithm LayerArrangement, the upper (lower)\nlayer will contain m1 (m1 or m1 + 1) H’s. We consider two cases, namely, where\nm1 is odd, i.e., m1 = 2x + 1 and m1 is even, i.e., m1 = 2x, with an integer x > 0.\nNow, let, i and j are two integers that divide m1 with reminder 0, such that\n|i − j| is minimal for all i and j. Let, r = min(i, j), which is number of the chains\nin a layer. Now, let, s = m1 /r which is number of residues in a chain. The chains\nare arranged spirally in a layer.\nIn what follows, we will use vw-upper layer (vw-lower layer) to denote a\nparticular region of the upper (lower) layer. So, vw could be one of the 9 options,\nnamely, lR (left region), rR (right region), uR (up region), dR (down region),\nil R (inside-left region), ir R (inside-right region), iu R (inside-up region), id R\n(inside-down region) and mR (middle region). We also use φCA to refer to the\nconformation given by Algorithm LayerArrangement.\nThe analysis of this case will be easy to understand with the help of Fig. 8.\nIn φCA , every vertex in the lR-up layer and rR-up layer has at least 8 contacts.\nEvery vertex in the il R-upper layer and the ir R-upper layer has at least 12\ncontacts. For each of lR-upper layer, rR-upper layer, il R-upper layer and the\nir R-upper layer, there are r−2 such vertices (see Fig. 8). Every vertex in the uRupper layer and the dR-upper layer has at least 6 contacts. There are s+3\n2 such\nvertices for each of the uR-upper layer and the dR-upper layer. Every vertex in\nthe iu R-upper layer and the id R-upper layer has at least 11 contacts. There are\n( s−3\n2 ) such vertices for each of the iu R-upper layer and the id R-upper layer. So\nthere remain (rs − 2r − 2s − 4) vertices in upper layer which fall to mR-upper\nlayer, where every vertex achieved 14 contacts.\nSo, the total number of contacts (C ) of all the vertices of the upper layer\ncan be computed as follows:\n\n\fs−3\nC ≥ 2×8×(r−2)+2×12×(r−2)+2×6× s+3\n2 +2×11×( 2 )+14×(2x−2r−2s−4)\n⇒ C ≥ 16r − 32 + 24r − 48 + 6s + 18 + 11s − 33 + 14sr − 28r − 28s − 56\n⇒ C ≥ 14sr + 12r − 11s − 151\n⇒ C ≥ 14m1 + 12r − 11s − 151\n⇒ C ≥ 7n + 12r − 11s − 151\nSince the upper layer is symmetric to the lower layer, both layer will have the\nsame number of vertices if n = 2m1 . So all the vertices of the lower layer will\nalso have at least C contacts. So the total number of contacts will be at least 2C\nor 14n + 24r − 22s − 302.\nIf n = 2m1 + 1, then let n1 = n − 1. This n1 vertices will have at least\n14n1 +24r−22s−302 contacts. The remaining vertex will have at least 2 contacts.\nSo the total number of contacts will be at least 14(n − 1) + 24r − 22s − 302 + 2\nor 14n + 24r − 22s − 314.So, combining the two cases, we get that the total\nnumber of contacts is at least 14n + 24r − 22s − 314. Now we need to take\nthe alternating edges into our consideration. For every alternating edge we get\ntwo extra contacts for the two vertices (each having one). So, for n H’s and k\nalternating edges we get a total of at least 14n + 24r − 22s − 314 + 2k contacts.\nHence we get the following approximation ratio A2 :\n\nA2 =\n\n18n − 12 k\n(14n + 24r − 22s − 314 + 2k)\n\n(2)\n\n18\n. So we\nFrom Equation 2 it can be seen that for large n, A2 tends to reach 14\n18\ncompute the value of k so that our approximation ratio is at most 14 as shown\nbelow.\n18n− k\n18\n2\n(14n+24r−22s−314+2k) ≤ 14\n18\n⇒ 14 × 18n − k2 ≤ (14n+24r−22s−314+2k)\n⇒ 252n − 7k ≤ 252n + 432r − 396s − (314 × 18) + 36k\n⇒ 43k ≥ 36(11s − 12r) + (314 × 18) ⇒ k ≥ 36(11s−12r)+(314×18)\n43\nNow, from this case if 11s = 12r, k ≥ (314×18)\n≈ 132\n43\nSo, if the total number of H-runs is greater than 132, then Algorithm LayerAr18\nor 97 for 11s = 12r. Note\nrangement will achieve an approximation ratio of 14\nthat, the value of k is dependent on n and the HP string. We now deduce the\nexpected value of k for a given HP string. This problem can be mapped into the\nproblem of Integer P artitioning as defined below. Notably, similar mapping\nhas recently been utilized in [10][11][19] for deriving an expected approximation\nratio of another algorithm.\n\nProblem 1. Given an integer Y , the problem of Integer Partitioning aims to\nprovide all possible ways of writing Y , as a sum of positive integers.\nNote that the ways that differ only in the order of their summands are considered\nto be the same partition. A summand in a partition is called a part. Now, if we\nconsider n as the input of Problem 1 (i.e., Y ) then each length of H-runs can\nbe viewed as parts of the partition. So if we can find the expected number of\npartitions we could in turn get the expected value of k. Kessler and Livingston\n\n\f[14] showed that to get an integer partition of an integer Y , expected number of\nrequired parts is:\nr\nr\n3Y\nπ\n× (log Y + 2γ − 2 log\n),\n2π\n6\nwhere γ is the famous Euler’s constant. For our problem Y = n. If we denote\nE[P ] as the expected number of H-runs then,\nr\nr\n6 √\nπ\n1\nE[P ] =\n× n × ( log n + γ − log\n).\nπ\n2\n6\nq\np\n1\nNow, as ( 12 log n + γ − log π6 ) ≤ ( 2π\n3 × 2 log n) for n ≥ 5, we can say that\nE[P ] ≤\n\n√\nn × log n.\n\n√\n√ So the expected value of k is less than or equal to n×log n which implies that\nn × log n ≥ 132 or n ≥ 500. Now, if 11s > 12r, lower bound of k increases, as a\nresult expected lower bound of n will increases. On the other side, if 11s < 12r,\nexpected lower bound of n will decreases. The above findings are summarized in\nthe following theorems.\nTheorem 2. For any given HP string, Algorithm ChainArrangement gives a\n9\n7 approximation ratio for k > 132, where k is the total number of H-runs and\n11s = 12r where, n = 2rs and n is the total number of H. ⊓\n⊔\nTheorem 3. For any given HP string, Algorithm ChainArrangement is expected\nto achieve an approximation ratio of 97 for n ≥ 500 and 11s = 12r where, n = 2rs\nand n is the total number of H. ⊓\n⊔\n\n4\n\nConclusion\n\nOne vertex of SC (Simple Cubic) lattice have 6 neighbour, FCC (Face Centered\nCubic) or BCC (Body Centered Cubic) lattice have 14 neighbour. On the other\nhand, one vertex of hand hexagonal prism lattice with diagonal have 20 neighbour which property leads us to find better approximation ratio. On the other\nhand this lattice model remove some well known problems of protein folding in\nSC lattice e.g., parity problem. Considering such properties of this lattice surely\ntell us that better approximation algorithm could be developed. Also heuristics\nalgorithm can be applied on this lattice, which can surely lead us to better result.\n\nReferences\n1. R. Agarwala, S. Batzogloa, V. Dancik, S. Decatur, S. Hannenhalli, M. Farach,\nS.Muthukrishnan, and S. Skiena. Local rules for protein folding on a triangular\nlattice and generalized hydrophobicity in the hp model. Journal of Computational\nBiology, 4(3):276–296, 1997.\n\n\f2. H. Bockenhauer and D. Bongartz. Protein folding in the hp model on grid lattices\nwith diagonals. In Discrete Applied Mathematics, volume 155, pages 230–256, 2007.\n3. H.-J. Böckenhauer, A. Z. M. D. Ullah, L. Kapsokalivas, and K. Steinhöfel. A local\nmove set for protein folding in triangular lattice models. In WABI, pages 369–381,\n2008.\n4. P. Crescenzi, D. Goldman, C. Papadimitriou, A. Piccolboni, and M.Yannakakis.\nOn the complexity of protein folding. Journal of Computational Biology, 5(3),\n1998.\n5. Y. Duan and P. A. Kollman. Computational protein folding: From lattice to all\natom. IBM research journal, 1998.\n6. W. Hart and S. Istrail. Fast protein folding in the hydrophobic-hydrophilic model\nwithin three-eighths of optimal. Journal of Computational Biology, 3(1):53–96,\n1996.\n7. V. Heun. Approximate protein folding in the hp side chain model on extended\ncubic lattices. In ESA, pages 212–223, 1999.\n8. T. Hoque, M. Chetty, and L. S. Dooley. A hybrid genetic algorithm for 2d fcc\nhydrophobic-hydrophilic lattice model to predict protein folding. In Australian\nConference on Artificial Intelligence, pages 867–876, 2006.\n9. T. Hoque, M. Chetty, and A. Sattar. Protein folding prediction in 3d fcc hp lattice\nmodel using genetic algorithm. In IEEE Congress on Evolutionary Computation,\npages 4138–4145, 2007.\n10. A. S. M. S. Islam and M. S. Rahman. On the protein folding problem in 2dtriangular lattices. Algorithms for Molecular Biology, 8:30, 2013.\n11. A. S. M. S. Islam and M. S. Rahman. Protein folding in 2d-triangular lattice\nrevisited. In IWOCA, 2013.\n12. M. Jiang and B. Zhu. Protein folding on the hexagonal lattice in the hp model. J.\nBioinformatics and Computational Biology, 3(1):19–34, 2005.\n13. K.A.DilL. Theory for the folding and stability of globular proteins. 24:1501–1509,\n1985.\n14. I. Kessler and M. Livingston. The expected number of parts in a partition of n.\nMonatshefte für Mathematik, 81(3):203–212, 1976.\n15. A. H. Khodabakhshi, J. Manuch, A. Rafiey, and A. Gupta. Inverse protein folding\nin 3d hexagonal prism lattice under hp model. In BIOCOMP, pages 619–625, 2008.\n16. N. Lesh, M. Mitzenmacher, and S. Whitesides. A complete and effective move set\nfor simplified protein folding. In 7th Annual International Conference on Research\nin Computational Molecular Biology (RECOMB) 2003, pages 188–195. ACM Press,\n2003.\n17. A. Newman. A new algorithm for protein folding in the hp model. In Symposium\non Discrete Algorithms, (SODA), pages 876–884, 2002.\n18. A. Newman and M. Ruhl. Combinatorial problems on strings with applications\nto protein folding. In LATIN, volume 2976 of Lecture Notes in Computer Science,\npages 369–378. Springer, 2004.\n19. D. Shaw, A. S. M. S. Islam, M. S. Rahman, and M. Hasan. Protein folding in\nhp model on hexagonal lattices with diagonals. BMC Bioinformatics, 15(S-2):S7,\n2014.\n20. R. Unger and J. Moult. Genetic algorithms for protein folding simulations. Journal\nof Molecular Biology, 231:75–81, 1993.\n\n\f"
        ],
        [
         "47",
         "47",
         "cs.CE",
         "Computational Engineering",
         "1509.04729v1.pdf",
         "Integrating Research Data Management into\nGeographical Information Systems\nChristian T. Jacobs, Alexandros Avdis,\nSimon L. Mouradian, and Matthew D. Piggott\n\narXiv:1509.04729v1 [cs.DL] 15 Sep 2015\n\nDepartment of Earth Science and Engineering, South Kensington Campus,\nImperial College London, London SW7 2AZ, United Kingdom\n{c.jacobs10,a.avdis,simon.mouradian06,m.d.piggott}@imperial.ac.uk\nhttp://www.imperial.ac.uk/engineering/departments/earth-science\n\nAbstract. Ocean modelling requires the production of high-fidelity computational meshes upon which to solve the equations of motion. The\nproduction of such meshes by hand is often infeasible, considering the\ncomplexity of the bathymetry and coastlines. The use of Geographical\nInformation Systems (GIS) is therefore a key component to discretising\nthe region of interest and producing a mesh appropriate to resolve the\ndynamics. However, all data associated with the production of a mesh\nmust be provided in order to contribute to the overall recomputability\nof the subsequent simulation. This work presents the integration of research data management in QMesh, a tool for generating meshes using\nGIS. The tool uses the PyRDM library to provide a quick and easy way\nfor scientists to publish meshes, and all data required to regenerate them,\nto persistent online repositories. These repositories are assigned unique\nidentifiers to enable proper citation of the meshes in journal articles.\nKeywords: Geographical Information Systems, Research Data Management, Digital Curation, Reproducibility, Digital Object Identifier,\nOnline Repositories\n\n1\n\nIntroduction\n\nComputer simulations of ocean dynamics are becoming ever more important to\npredict the effects of global-scale hazards such as tsunamis [13], the influence of\nmarine renewable energy turbines on sediment transport [20], and the dispersal\nrange of nuclear contaminants [6], to name just a few applications. The underlying numerical model behind such simulations often requires a mesh upon which\nthe equations describing the flow dynamics are solved, thereby transitioning from\na continuous description of the region of interest (also known as the domain) to a\ndiscrete one. An example focussing on the area around the Orkney and Shetland\nIsles is shown in Figure 1. A mesh for ocean simulations must be of high enough\nquality to resolve the intricate coastlines and bathymetry [12]. However, creating\nsuch a mesh manually is infeasible for large-scale, high-resolution simulations.\nGeographical Information Systems (GIS) offer an effective way of processing\nbathymetry and coastline data to create a geometry with which to work [19].\n\n\f2\n\nData Management in Geographical Information Systems\n\nFig. 1. An example of an unstructured computational mesh which discretises the marine area around the North-East coast of Scotland. The resolution is highest around\nthe Scottish coastline and around the Orkney and Shetland Isles.\n\nA method of producing a computational mesh from this geometry is then required to perform a simulation on it. QMesh [2] is a software package currently\nbeing developed at Imperial College London for this purpose. QMesh reads in a\ngeometry defined in the QGIS Geographical Information System software [21],\nand then converts the geometry into a readable format for the Gmsh mesh generation software [11], which in turn generates the mesh to provide a discrete\nrepresentation of the domain. Ocean simulations may then be performed with a\ncomputational fluid dynamics package.\nPublications that are dependant on numerical simulation often provide details of the simulation setups to improve reproducibility and indeed recomputability. However, while a description of the domain may also be given, the\nmesh that discretises this domain is rarely provided as a supplementary material.\nThis lack of data availability has also been highlighted in many other areas of\nscience [27], [1], [26]. Furthermore, citations to the software used to produce the\nmesh typically only refer to a generic user manual and contain no information\nabout which version was used. For the purpose of recomputability and reproducibility, it is crucial that researchers provide all the data files, as well as the\nprecise version of the software’s source code used to produce the output in the\nfirst place [8], [5]. In the case of this work, the input data is the geographical\ninformation defining the domain, the output data is the computational mesh,\nand the software is QMesh (and its dependencies).\n\n\fData Management in Geographical Information Systems\n\n3\n\nDespite the need for a more open research environment where software and\ndatasets are shared freely, the level of motivation amongst researchers to do\nthis is generally quite low. This is in part due to the extra effort and time\nrequired to gather and publish the data [18], whilst typically gaining little from\nthe process. To encourage the sharing of data and improve its reproducibility\nand recomputability, it is therefore important to make the publication process\nmore straight forward and swift. This can be effected by the development of\nresearch data management tools that readily capture the datasets involved and\ninformation about the software being used [25], [18].\nThis paper describes the integration of a research data management tool,\nwhich uses the PyRDM library [14], into the QMesh software. The tool automates the publication of the QMesh source code, as well as the input and\noutput data for a specified QGIS project, to online, citable and persistent repositories such as those provided by Figshare (figshare.com), Zenodo (zenodo.org)\nand DSpace-based (dspace.org) hosting services. The tool has both a command\nline and a graphical user interface, and allows users to publish the software and\ndata at the ‘push of a button’, thereby facilitating sharing and a more open\nresearch environment. In contrast to other software tools that also facilitate\nthe publication of code and datasets, such as Fidgit [24], rfigshare [4], and dvn\n[17], the QMesh publishing tool incorporates application-specific knowledge to\nprovide a greater amount of automation. For example, the tool is able to parse\nQGIS project files to automatically determine the relevant input data to publish,\nrather than the user having to specify the data files manually. Furthermore, this\nwork represents a novel application of research data management and curation\nsoftware within a GIS environment.\nSection 2 describes in greater detail the extensions made to the QMesh software to automate the publication process for the software itself, the input files\n(for a given QGIS project) and any output files (i.e. the computational mesh).\nSection 3 presents a realistic example of a scientific workflow involving production of a mesh of a UK coastal region. The data files are read in to QGIS and\na mesh is produced. Both the QGIS data and mesh are subsequently published\nto an online repository provided by Figshare, and a DOI is assigned which can\nbe used to properly cite the data in journal articles. Finally, some concluding\nremarks are made in Section 4.\n\n2\n\nIntegration with QMesh\n\nQMesh features a command line interface (CLI), as well as a graphical user interface (GUI) via a QGIS plugin through which users can select relevant geometry\nobjects and produce a mesh. The integration of research data management techniques into QMesh was achieved by adding a PyRDM-based publishing tool to\nboth of these interfaces.\nThe tool provides the option of publishing the QMesh software source code\nand data required to reproduce the mesh to separate online repositories. Users\nare presented with a simple interface and only have to provide a minimal amount\n\n\f4\n\nData Management in Geographical Information Systems\n\nof information; this is illustrated in Figure 2. The publication process itself is\nhandled by the PyRDM library [14] which communicates with an online repository hosting service via its Application Programming Interface (API). The publication process results in a Digital Object Identifier (DOI) [7] being assigned to\nthe repository, with which users can properly cite their research outputs.\n\nFig. 2. The QMesh publisher tool, which is part of the QMesh QGIS plugin. Users\nchoose the online repository service that they wish to use; by default this is set to\nFigshare. In addition to the input data files associated with the QGIS project, users\nmay also publish the output data file (i.e. the resulting computational mesh) produced\nby QMesh, if they so desire. By default, the publication is made public unless the\nuser decides otherwise; in the case of private publication, a DOI is still assigned to the\nrepository, but will not be made active/‘live’ until the repository is made public.\n\nThe publication of data is handled separately to the publication of the QMesh\nsoftware. In the former case, when a suitable mesh has been produced and is\nready to be published, users simply have to provide the QMesh publishing tool\nwith the location of the QGIS project file on the computer’s file system when\nusing the CLI. When using the GUI, this location is provided automatically when\nthe project is opened in QGIS. The tool then searches for the <datasource> tags\nin the XML-based project file to determine the location of all the files that the\nproject comprises; these may include shape files that define various layers in the\n\n\fData Management in Geographical Information Systems\n\n5\n\ngeometry, data files in NetCDF format [23] which define the bathymetry of the\nocean, and a multitude of other data formats. Optionally, the location of the\nGmsh mesh file may also be provided, thereby publishing the resultant output\ndata along with the files required to produce it. The locations of all these data\nfiles, including the QGIS project file itself, are then provided to PyRDM which\nautomatically creates a repository on the hosting service and uploads the files\nvia the service’s API. The service then returns a publication ID and a DOI,\nwhich is presented to the user for citation purposes. This process is illustrated\nin Figure 3.\nThe publication of software involves a similar process, but can currently only\nbe accomplished via the CLI. The user only has to provide the QMesh publishing\ntool with the location of the software’s source code on the computer’s file system.\nThe PyRDM library then handles the rest; it determines the exact version of\nQMesh currently in use using the Git version control system (git-scm.com) [22],\nand then checks to see whether that version has been published already1 . If it\nhas, PyRDM retrieves the existing DOI for re-use. If it has not, then PyRDM\npublishes the source code in a similar fashion to the case of publishing data, as\nshown in Figure 3. Note that publications in journals would need to reference\nboth the software repository’s DOI and the data repository’s DOI. There is\ncurrently no explicit link that is made between the software and data repositories,\nunless specified manually.\nAs demonstrated by Figure 3, the QMesh publishing tool requires minimal\nuser interaction and is largely automated by the PyRDM library. This is important for encouraging the sharing of software and data files, in order to achieve a\nmore open research environment.\n\n3\n\nWorkflow Example\n\nTo demonstrate an example of a scientific workflow involving mesh generation\nusing GIS, the Orkney and Shetland Isles considered in [2] and [3] are used.\nThe researcher first has to describe the geography of the domain in QGIS and\nthen decide on the area they wish to create a mesh for. The QGIS project for the\nOrkney and Shetland Isles comprises a number of geometrical layers which define\nthe coastlines (and potentially coastal engineering structures such as marine\npower turbines), in addition to a NetCDF file which defines the bathymetry of\nthe ocean floor, and another NetCDF file which defines the desired resolution\nthroughout the mesh. These files are shown in Figure 4 beside the area that will\nbe meshed.\nThe mesh that QMesh produces for this domain (shown in Figure 1) is then\nused by the researcher in their marine simulations. Once the researcher is ready\nto publish their results, they upload the data files associated with the production\n1\n\nRepository searching is only available when using the Figshare repository service,\ndue to API limitations explained later in Section 4. PyRDM will publish the software\nregardless of whether it has been published before when Zenodo or a DSpace-based\nservice is chosen.\n\n\f6\n\nData Management in Geographical Information Systems\n\nFig. 3. The processes behind publishing the QGIS data files (left) and QMesh software\nsource code (right) to Figshare.\n\nFig. 4. Screenshot of the UK region visualised in QGIS. The solid dark purple line\ndefines the area that will be meshed (in this case it contains the Orkney and Shetland\nIsles). The different files that make up the layers of the geometry are specified in the\ncolumn on the left-hand side.\n\n\fData Management in Geographical Information Systems\n\n7\n\nof the simulation’s mesh to an online repository using the QMesh publishing tool\nshown in Figure 2 (the CLI may also be used instead of the graphical interface).\nIn this example, it uploads all the files previously mentioned to Figshare. Once\nuploaded, the files can be downloaded from the Figshare website (see Figure 5)\nand a DOI is presented to the researcher to share with colleagues and for use in\njournal publications (see Figure 6).\n\nFig. 5. A screenshot of the resulting repository on the Figshare website, with the files\nreadily available to download. The QMesh publishing tool automatically assigns a title\nand tags to the repository based on the QGIS project’s name.\n\nThe version of the QMesh software’s source code that is used should also\nbe published, in a separate repository to the data. However, it should be noted\nthat publishing the QMesh source code may not be enough to reproduce the\nexact same mesh without also knowing the versions of its dependencies. For\nexample, different versions of Gmsh may produce slightly different meshes as a\nresult of algorithmic improvements within the software. It is therefore important\nthat such information be recorded in some way to further improve the degree of\nreproducibility. For example, ideally Gmsh would also have a similar system for\npublishing the current version of its source code in use.\n\n\f8\n\nData Management in Geographical Information Systems\n\nFig. 6. A Figshare publication ID and a DOI are assigned to each repository, and\npresented to the researcher once the publication process is complete.\n\n4\n\nDiscussion and Conclusions\n\nThroughout the production of the PyRDM-based publishing tool for QMesh,\nseveral issues were encountered which largely stemmed from a lack of standardisation and support in the repository hosting services’ APIs. For example, in\norder for PyRDM to attribute authors to the software repository on Figshare,\nall authors of QMesh must provide their Figshare author IDs in the AUTHORS\nfile that is part of the QMesh source code. Unfortunately, another different set\nof author IDs would need to be provided when using a different repository service such as Zenodo, which is inconvenient and requires all authors of QMesh\nto have accounts across all the supported services. A more standardised way of\nidentifying and attributing authors to research software and data would be to\nuse ORCID (orcid.org) researcher IDs. Figshare has recently added support for\nauthenticating with ORCID IDs via its web interface [9], and it is hoped that\nORCID authentication via the Figshare API will also be added for the benefit\nof PyRDM. Another example, this time involving lack of API support, is the\ncurrent inability to search for an existing repository with the Zenodo API. Further developments are necessary in this area to enrich the publication process\nand improve automation.\nThe production of meshes can involve proprietary and/or private data which\ncannot be published openly, but at the same time sharing all research output\nis becoming a common requirement imposed by research funders. The QMesh\npublishing tool comes with the option of publishing the data to private repositories. However, with some services the private storage space is rather limited,\nand typically not large enough to store high quality mesh files for realistic ocean\nsimulations. For example, the free private storage space offered by Figshare is 1\nGB at the time of writing this paper, with a 250 MB individual file size limit2 .\nFurthermore, only a maximum of 5 collaborators can be given access to a private\nrepository. In contrast, the integration of Figshare for Institutions [10] offers a\nmore suitable platform for larger-scale research data management. This project\nenables researchers at an institution to publish to private repositories hosted\nin the cloud. This is considerably more sustainable for GIS projects and mesh\n2\n\nhttp://figshare.com/pricing\n\n\fData Management in Geographical Information Systems\n\n9\n\ngeneration that can involve very large file sizes, both public and private data,\nand collaboration amongst many researchers and research groups.\nIn conclusion, the integration of a publishing tool in a Geographical Information System has helped to mitigate one of the reasons why researchers tend not\nto publish their software and data; that is, it is time-consuming to do so with\nlittle reward. The new QMesh publishing tool makes publishing a computational\nmesh and associated data files easy and largely effortless through the addition\nof a significant amount of automation. Furthermore, the use of online repository\nservices enable more formal citation of all research outputs through the use of\nDOIs. However, it is the responsibility of the scientific community to encourage\nand provide incentives for the openness and public availability of this software\nand data, in order to overcome the barrier of lack of motivation to publish.\nAcknowledgments CTJ was funded by an internal grant entitled “Research\ndata management: Where software meets data” from the Research Office at\nImperial College London. Part of the work presented in this paper is based\non work first presented in poster form at the International Digital Curation\nConference (IDCC) in February 2015 [16], and in a PyRDM project report [15].\nThe authors would like to thank the two anonymous reviewers of this paper for\ntheir feedback.\n\nReferences\n1. Alsheikh-Ali, A.A., Qureshi, W., Al-Mallah, M.H., Ioannidis, J.P.A.: Public Availability of Published Research Data in High-Impact Journals. PLoS ONE 6(9),\ne24357 (2011)\n2. Avdis, A., Hill, J., Jacobs, C.T., Kramer, S.C., Candy, A.S., Gorman, G.J., Piggott, M.D.: Efficient unstructured mesh generation for renewable tidal energy using\nGeographical Information Systems (In Preparation)\n3. Avdis, A., Jacobs, C.T., Hill, J., Piggott, M.D., Gorman, G.J.: Shoreline and\nBathymetry Approximation in Mesh Generation for Tidal Renewable Simulations.\nIn: Proceedings of the 11th European Wave and Tidal Energy Conference (Accepted)\n4. Boettiger, C., Chamberlain, S., Ram, K., Hart, E.: rfigshare: an R interface to\nfigshare.com. (2014), http://CRAN.R-project.org/package=rfigshare, r package\nversion 0.3-1\n5. Buckheit, J.B., Donoho, D.L.: WaveLab and Reproducible Research. In: Antoniadis, A., Oppenheim, G. (eds.) Wavelets and Statistics, Lecture Notes in Statistics, vol. 103, pp. 55–81. Springer, New York (1995)\n6. Choi, Y., Kida, S., Takahashi, K.: The impact of oceanic circulation and phase\ntransfer on the dispersion of radionuclides released from the Fukushima Dai-ichi\nNuclear Power Plant. Biogeosciences 10, 4911–4925 (2013)\n7. Davidson, L.A., Douglas, K.: Digital Object Identifiers: Promise and Problems for\nScholarly Publishing. Journal of Electronic Publishing 4(2) (1998)\n8. de Leeuw, J.: Reproducible Research: the Bottom Line. Department of Statistics Papers, University of California (2001), http://escholarship.org/uc/item/\n9050x4r4\n\n\f10\n\nData Management in Geographical Information Systems\n\n9. Figshare: figshare ORCID integration. Figshare blog, http://figshare.com/blog\n(2013)\n10. Figshare: Loughborough University, figshare, Arkivum and Symplectic announce pioneering research data management solution. Figshare blog,\nhttp://figshare.com/blog (2014)\n11. Geuzaine, C., Remacle, J.F.: Gmsh: A 3-D finite element mesh generator with builtin pre- and post-processing facilities. International Journal for Numerical Methods\nin Engineering 79(11), 1309–1331 (2009)\n12. Gorman, G.J., Piggott, M.D., Wells, M.R., Pain, C.C., Allison, P.A.: A systematic\napproach to unstructured mesh generation for ocean modelling using GMT and\nTerreno. Computers & Geosciences 34(12), 1721–1731 (2008)\n13. Hill, J., Collins, G.S., Avdis, A., Kramer, S.C., Piggott, M.D.: How does multiscale\nmodelling and inclusion of realistic palaeobathymetry affect numerical simulation\nof the Storegga Slide tsunami. Ocean Modelling 83, 11–25 (2014)\n14. Jacobs, C.T., Avdis, A., Gorman, G.J., Piggott, M.D.: PyRDM: A Python-based\nlibrary for automating the management and online publication of scientific software\nand data. Journal of Open Research Software 2(1), e28 (2014)\n15. Jacobs, C.T., Avdis, A., Gorman, G.J., Piggott, M.D.: RDM Green Shoots Project\nReport: Research data management: Where software meets data (2014), http:\n//dx.doi.org/10.6084/m9.figshare.1269127\n16. Jacobs, C.T., Avdis, A., Gorman, G.J., Piggott, M.D.: PyRDM: A library to facilitate the automated publication of software and data in computational science.\nPoster presentation at the 10th International Digital Curation Conference (2015),\nhttp://dx.doi.org/10.6084/m9.figshare.1318710\n17. Leeper, T.J.: Archiving Reproducible Research with R and Dataverse. The R Journal 6(1) (2014), http://journal.r-project.org/archive/2014-1/leeper.pdf\n18. LeVeque, R.J., Mitchell, I.M., Stodden, V.: Reproducible Research for Scientific\nComputing: Tools and Strategies for Changing the Culture. Computing in Science\n& Engineering 14(4), 13–17 (2012)\n19. Li, R.: Data Models for Marine and Coastal Geographic Information Systems,\nchap. 3. CRC Press (2000)\n20. Martin-Short, R., Hill, J., Kramer, S.C., Avdis, A., Allison, P.A., Piggott, M.D.:\nTidal resource extraction in the Pentland Firth, UK: potential impacts on flow\nregime and sediment transport in the Inner Sound of Stroma. Renewable Energy\n76, 596–607 (2015)\n21. QGIS Development Team: QGIS Geographic Information System. Open Source\nGeospatial Foundation (2009), http://qgis.osgeo.org\n22. Ram, K.: Git can facilitate greater reproducibility and increased transparency in\nscience. Source Code for Biology and Medicine 8(7) (2013)\n23. Rew, R.K., Davis, G.P.: NetCDF: an interface for scientific data access. IEEE\nComputer Graphics and Applications 10(4), 76–82 (1990)\n24. Smith, A.: Fidgit - DOIs for code. figshare (2013), http://dx.doi.org/10.6084/\nm9.figshare.828487\n25. Stodden, V., Bailey, D., Borwein, J., LeVeque, R.J., Rider, W., Stein, W.: Setting the Default to Reproducible: Reproducibility in Computational and Experimental Mathematics. Tech. rep., Institute for Computational and Experimental Research in Mathematics (ICERM) (2013), http://www.davidhbailey.com/\ndhbpapers/icerm-report.pdf\n26. Vines, T.H., Andrew, R.L., Bock, D.G., Franklin, M.T., Gilbert, K.J., Kane, N.C.,\nMoore, J.S., Moyers, B.T., Renaut, S., Rennison, D.J., Veen, T., Yeaman, S.: Man-\n\n\fData Management in Geographical Information Systems\n\n11\n\ndated data archiving greatly improves access to research data. The FASEB Journal\n27(4), 1304–1308 (2013)\n27. Whitlock, M.C., McPeek, M.A., Rausher, M.D., Rieseberg, L., Moore, A.J.: Data\nArchiving. The American Naturalist 175(2), 145–146 (2010)\n\n\f"
        ],
        [
         "48",
         "48",
         "cs.CE",
         "Computational Engineering",
         "1601.00863v3.pdf",
         "arXiv:1601.00863v3 [math.OC] 14 Aug 2016\n\nCoordinate friendly structures, algorithms and\napplications∗\nZhimin Peng, Tianyu Wu, Yangyang Xu, Ming Yan, and\nWotao Yin\n\nThis paper focuses on coordinate update methods, which are useful\nfor solving problems involving large or high-dimensional datasets.\nThey decompose a problem into simple subproblems, where each\nupdates one, or a small block of, variables while fixing others. These\nmethods can deal with linear and nonlinear mappings, smooth and\nnonsmooth functions, as well as convex and nonconvex problems.\nIn addition, they are easy to parallelize.\nThe great performance of coordinate update methods depends\non solving simple subproblems. To derive simple subproblems for\nseveral new classes of applications, this paper systematically studies coordinate friendly operators that perform low-cost coordinate\nupdates.\nBased on the discovered coordinate friendly operators, as well\nas operator splitting techniques, we obtain new coordinate update\nalgorithms for a variety of problems in machine learning, image\nprocessing, as well as sub-areas of optimization. Several problems\nare treated with coordinate update for the first time in history.\nThe obtained algorithms are scalable to large instances through\nparallel and even asynchronous computing. We present numerical\nexamples to illustrate how effective these algorithms are.\nKeywords and phrases: coordinate update, fixed point, operator splitting, primal-dual splitting, parallel, asynchronous.\n\n∗\n\nThis work is supported by NSF Grants DMS-1317602 and ECCS-1462398.\n\n1\n\n\f2\n\n1. Introduction\nThis paper studies coordinate update methods, which reduce a large problem to smaller subproblems and are useful for solving large-sized problems.\nThese methods handle both linear and nonlinear maps, smooth and nonsmooth functions, and convex and nonconvex problems. The common special\nexamples of these methods are the Jacobian and Gauss-Seidel algorithms for\nsolving a linear system of equations, and they are also commonly used for\nsolving differential equations (e.g., domain decomposition) and optimization\nproblems (e.g., coordinate descent).\nAfter coordinate update methods were initially introduced in each topic\narea, their evolution had been slow until recently, when data-driven applications (e.g., in signal processing, image processing, and statistical and\nmachine learning) impose strong demand for scalable numerical solutions;\nconsequently, numerical methods of small footprints, including coordinate\nupdate methods, become increasingly popular. These methods are generally\napplicable to many problems involving large or high-dimensional datasets.\nCoordinate update methods generate simple subproblems that update\none variable, or a small block of variables, while fixing others. The variables\ncan be updated in the cyclic, random, or greedy orders, which can be selected to adapt to the problem. The subproblems that perform coordinate\nupdates also have different forms. Coordinate updates can be applied either\nsequentially on a single thread or concurrently on multiple threads, or even\nin an asynchronous parallel fashion. They have been demonstrated to give\nrise to very powerful and scalable algorithms.\nClearly, the strong performance of coordinate update methods relies on\nsolving simple subproblems. The cost of each subproblem must be proportional to how many coordinates it updates. When there are totally m coordinates, the cost of updating one coordinate should not exceed the average per-coordinate cost of the full update (made to all the coordinates\nat once). Otherwise, coordinate update is not computationally worthy. For\nexample, let f : Rm → R\u0001be a C 2 function, and consider the Newton update\n−1\nxk+1 ← xk − ∇2 f (xk ) ∇f (xk ). Since updating each xi (keeping others fixed) still requires forming the Hessian matrix ∇2 f (x) (at least O(m2 )\noperations) and factorizing it (O(m3 ) operations), there is little to save in\ncomputation compared to updating all the components of x at once; hence,\nthe Netwon’s method is generally not amenable to coordinate update.\nThe recent coordinate-update literature has introduced new algorithms.\nHowever, they are primarily applied to a few, albeit important, classes of\nproblems that arise in machine learning. For many complicated problems,\n\n\fCoordinate friendly structures, algorithms, and applications\n\n3\n\nit remains open whether simple subproblems can be obtained. We provide\npositive answers to several new classes of applications and introduce their\ncoordinate update algorithms. Therefore, the focus of this paper is to build\na set of tools for deriving simple subproblems and extending coordinate\nupdates to new territories of applications.\nWe will frame each application into an equivalent fixed-point problem\n(1)\n\nx=Tx\n\nby specifying the operator T : H → H, where x = (x1 , . . . , xm ) ∈ H, and\nH = H1 × · · · × Hm is a Hilbert space. In many cases, the operator T itself\nrepresents an iteration:\n(2)\n\nxk+1 = T xk\n\nsuch that the limit of the sequence {xk } exists and is a fixed point of T ,\nwhich is also a solution to the application or from which a solution to the\napplication can be obtained. We call the scheme (2) a full update, as opposed\nto updating one xi at a time. The scheme (2) has a number of interesting\nspecial cases including methods of gradient descent, gradient projection,\nproximal gradient, operator splitting, and many others.\nWe study the structures of T that make the following coordinate update\nalgorithm computationally worthy\n(3)\n\nxk+1\n= xki − ηk (xk − T xk )i ,\ni\n\nwhere ηk is a step size and i ∈ [m] := {1, . . . , m} is arbitrary. Specifically, the\n1\n, or lower, of that of performing (2). We\ncost of performing (3) is roughly m\ncall such T a Coordinate Friendly (CF) operator, which we will formally\ndefine.\nThis paper will explore a variety of CF operators. Single CF operators\ninclude linear maps, projections to certain simple sets, proximal maps and\ngradients of (nearly) separable functions, as well as gradients of sparsely\nsupported functions. There are many more composite CF operators, which\nare built from single CF and non-CF operators under a set of rules. The fact\nthat some of these operators are CF is not obvious.\nThese CF operators let us derive powerful coordinate update algorithms\nfor a variety of applications including, but not limited to, linear and secondorder cone programming, variational image processing, support vector machine, empirical risk minimization, portfolio optimization, distributed computing, and nonnegative matrix factorization. For each application, we present\n\n\f4\nan algorithm in the form of (2) so that its coordinate update (3) is efficient.\nIn this way we obtain new coordinate update algorithms for these applications, some of which are treated with coordinate update for the first time.\nThe developed coordinate update algorithms are easy to parallelize. In\naddition, the work in this paper gives rise to parallel and asynchronous extensions to existing algorithms including the Alternating Direction Method\nof Multipliers (ADMM), primal-dual splitting algorithms, and others.\nThe paper is organized as follows. §1.1 reviews the existing frameworks\nof coordinate update algorithms. §2 defines the CF operator and discusses\ndifferent classes of CF operators. §3 introduces a set of rules to obtain composite CF operators and applies the results to operator splitting methods.\n§4 is dedicated to primal-dual splitting methods with CF operators, where\nexisting ones are reviewed and a new one is introduced. Applying the results\nof previous sections, §5 obtains novel coordinate update algorithms for a variety of applications, some of which have been tested with their numerical\nresults presented in §6.\nThroughout this paper, all functions f, g, h are proper closed convex and\ncan take the extended value ∞, and all sets X, Y, Z are nonempty closed\nconvex. The indicator function ιX (x) returns 0 if x ∈ X, and ∞ elsewhere.\nFor a positive integer m, we let [m] := {1, . . . , m}.\n1.1. Coordinate Update Algorithmic Frameworks\nThis subsection reviews the sequential and parallel algorithmic frameworks\nfor coordinate updates, as well as the relevant literature.\nThe general framework of coordinate update is\n1. set k ← 0 and initialize x0 ∈ H = H1 × · · · × Hm\n2. while not converged do\n3.\nselect an index ik ∈ [m];\n4.\nupdate xk+1\nfor i = ik while keeping xk+1\n= xki , ∀ i 6= ik ;\ni\ni\n5. k ← k + 1;\nNext we review the index rules and the methods to update xi .\n1.1.1. Sequential Update. In this framework, there is a sequence of\ncoordinate indices i1 , i2 , . . . chosen according to one of the following rules:\ncyclic, cyclic permutation, random, and greedy rules. At iteration k, only\nthe ik th coordinate is updated:\n(\nxk+1\n= xki − ηk (xk − T xk )i , i = ik ,\ni\nk+1\nxi = xki ,\nfor all i 6= ik .\n\n\fCoordinate friendly structures, algorithms, and applications\n\n5\n\nSequential updates have been applied to many problems such as the GaussSeidel iteration for solving a linear system of equations, alternating projection [75, 4] for finding a point in the intersection of two sets, ADMM [31, 30]\nfor solving monotropic programs, and Douglas-Rachford Splitting (DRS) [26]\nfor finding a zero to the sum of two operators.\nIn optimization, coordinate descent algorithms, at each iteration, minimize the function f (x1 , . . . , xm ) by fixing all but one variable xi . Let\nxi− := (x1 , . . . , xi−1 ),\n\nxi+ = (xi+1 , . . . , xm )\n\ncollect all but the ith coordinate of x. Coordinate descent solves one of the\nfollowing subproblems:\n(4a)\n\n(T xk )i = arg min f (xki− , xi , xki+ ),\nxi\n\n(4b)\n(4c)\n(4d)\n\n1\nkxi − xki k2 ,\n2ηk\nxi\n1\n(T xk )i = arg min h∇i f (xk ), xi i +\nkxi − xki k2 ,\n2ηk\nxi\n1\n(T xk )i = arg min h∇i f diff (xk ), xi i + fiprox (xi ) +\nkxi − xki k2 ,\n2ηk\nxi\n\n(T xk )i = arg min f (xki− , xi , xki+ ) +\n\nwhich are called direct update, proximal update, gradient update, and proxgradient update, respectively. The last update applies to the function\nf (x) = f diff (x) +\n\nm\nX\n\nfiprox (xi ),\n\ni=1\nprox\nwhere f diff is differentiable and each\nis proximable (its proximal map\n\u0001 fi\ntakes O dim(xi ) polylog(dim(xi )) operations to compute).\nSequential-update literature. Coordinate descent algorithms date\nback to the 1950s [35], when the cyclic index rule was used. Its convergence\nhas been established under a variety of cases, for both convex and nonconvex\nobjective functions; see [77, 85, 57, 33, 45, 70, 32, 72, 58, 8, 36, 78]. Proximal\nupdates are studied in [32, 1] and developed into prox-gradient updates\nin [74, 73, 13] and mixed updates in [81].\nThe random index rule first appeared in [48] and then [61, 44]. Recently, [82, 80] compared the convergence speeds of cyclic and stochastic\nupdate-orders. The gradient update has been relaxed to stochastic gradient\nupdate for large-scale problems in [21, 83].\n\n\f6\nThe greedy index rule leads to fewer iterations but is often impractical\nsince it requires a lot of effort to calculate scores for all the coordinates.\nHowever, there are cases where calculating the scores is inexpensive [11, 41,\n79] and the save in the total number of iterations significantly outweighs the\nextra calculation [74, 25, 55, 49].\nA simple example. We present the coordinate update algorithms under different index rules for solving a simple least squares problem:\n1\nminimize f (x) := kAx − bk2 ,\nx\n2\nwhere A ∈ Rp×m and b ∈ Rp are Gaussian random. Our goal is to numerically demonstrate the advantages of coordinate updates over the full update\nof gradient descent:\nxk+1 = xk − ηk A> (Axk − b).\nThe four tested index rules are: cyclic, cyclic permutation, random, and\ngreedy under the Gauss-Southwell1 rule. Note that because this example is\nvery special, the comparisons of different index rules are far from conclusive.\nIn the full update, the step size ηk is set to the theoretical upper bound\n2\n2 , where kAk2 denotes the matrix operator norm and equals the largest\nkAk2\nsingular value of A. For each coordinate update to xi , the step size ηk is set\nto (A>1A)ii . All of the full and coordinate updates have the same per-epoch\ncomplexity, so we plot the objective errors in Figure 1.\n1.1.2. Parallel Update. As one of their main advantages, coordinate\nupdate algorithms are easy to parallelize. In this subsection, we discuss both\nsynchronous (sync) and asynchronous (async) parallel updates.\nSync-parallel (Jacobi) update specifies a sequence of index subsets\nI1 , I2 , . . . ⊆ [m], and at each iteration k, the coordinates in Ik are updated\nin parallel by multiple agents:\n(\nxk+1\n= xki − ηk (xk − T xk )i , i ∈ Ik ,\ni\nxk+1\n= xki ,\ni 6∈ Ik .\ni\nSynchronization across all agents ensures that all xi in Ik are updated and\nalso written to the memory before the next iteration starts. Note that, if\n1\n\nit selects ik = arg maxi k∇i f (xk )k.\n\n\fCoordinate friendly structures, algorithms, and applications\n\n7\n\n5\n\n10\n\nfull update\ngreedy\nrandom\ncyclic\ncyclic permutation\n\n0\n\nfk − f∗\n\n10\n\n−5\n\n10\n\n−10\n\n10\n\n−15\n\n10\n\n0\n\n20\n\n40\n60\nEpochs\n\n80\n\n100\n\nFigure 1: Gradient descent: the coordinate updates are faster than the full\nupdate since the former can take larger steps at each step.\n\nIk = [m] for all k, then all the coordinates are updated and, thus, each\niteration reduces to the full update: xk+1 = xk − ηk (xk − T xk ).\nAsync-parallel update. In this setting, a set of agents still perform\nparallel updates, but synchronization is eliminated or weakened. Hence, each\nagent continuously applies (5), which reads x from and writes xi back to\nthe shared memory (or through communicating with other agents without\nshared memory):\n(\n\u0001\nxk+1\n= xki − ηk (I − T )xk−dk i , i = ik ,\ni\n(5)\nxk+1\n= xki ,\nfor all i 6= ik .\ni\nUnlike before, k increases whenever any agent completes an update.\nThe lack of synchronization often results in computation with out-ofdate information. During the computation of the kth update, other agents\nmake dk updates to x in the shared memory; when the kth update is written,\nits input is already dk iterations out of date. This number is referred to as the\nasynchronous delay. In (5), the agent reads xk−dk and commits the update\nto xkik . Here we have assumed consistent reading, i.e., xk−dk lying in the set\n{xj }kj=1 . This requires implementing a memory lock. Removing the lock can\nlead to inconsistent reading, which still has convergence guarantees; see [54,\nSection 1.2] for more details.\nSynchronization across all agents means that all agents will wait for the\nlast (slowest) agent to complete. Async-parallel updates eliminate such idle\ntime, spread out memory access and communication, and thus often run\n\n\f8\nAgent 1\nAgent 2\n\nidle\n\nAgent 2\n\nidle\n\nAgent 3\nk: 0\n\nAgent 1\n\nidle\n\nAgent 3\n\nidle\n\n1\n\n(a) sync-parallel computing\n\n2\n\nk: 0\n\n1\n\n2\n\n3 4 5 6 7 8 9 10\n\n(b) async-parallel computing\n\nFigure 2: Sync-parallel computing (left) versus async-parallel computing\n(right). On the left, all the agents must wait at idle (white boxes) until\nthe slowest agent has finished.\n\nmuch faster. However, async-parallel is more difficult to analyze because of\nthe asynchronous delay.\nParallel-update literature. Async-parallel methods can be traced\nback to [17] for systems of linear equations. For function minimization, [12]\nintroduced an async-parallel gradient projection method. Convergence rates\nare obtained in [69]. Recently, [14, 62] developed parallel randomized methods.\nFor fixed-point problems, async-parallel methods date back to [3] in\n1978. In the pre-2010 methods [2, 10, 6, 27] and the review [29], each agent\nupdates its own subset of coordinates. Convergence is established under the\nP -contraction condition and its variants [10]. Papers [6, 7] show convergence\nfor async-parallel iterations with simultaneous reading and writing to the\nsame set of components. Unbounded but stochastic delays are considered\nin [67].\nRecently, random coordinate selection appeared in [19] for fixed-point\nproblems. The works [47, 59, 43, 42, 37] introduced async-parallel stochastic\nmethods for function minimization. For fixed-point problems, [54] introduced\nasync-parallel stochastic methods, as well as several applications.\n1.2. Contributions of This Paper\nThe paper systematically discusses the CF properties found in both single and composite operators underlying many interesting applications. We\nintroduce approaches to recognize CF operators and develop coordinateupdate algorithms based on them. We provide a variety of applications to\nillustrate our approaches. In particular, we obtain new coordinate-update\nalgorithms for image deblurring, portfolio optimization, second-order cone\nprogramming, as well as matrix decomposition. Our analysis also provides\n\n\fCoordinate friendly structures, algorithms, and applications\n\n9\n\nguidance to the implementation of coordinate-update algorithms by specifying how to compute certain operators and maintain certain quantities in\nmemory. We also provide numerical results to illustrate the efficiency of the\nproposed coordinate update algorithms.\nThis paper does not focus on the convergence perspective of coordinate\nupdate algorithms, though a convergence proof is provided in the appendix\nfor a new primal-dual coordinate update algorithm. In general, in fixed-point\nalgorithms, the iterate convergence is ensured by the monotonic decrease of\nthe distance between the iterates and the solution set, while in minimization problems, the objective value convergence is ensured by the monotonic\ndecrease of a certain energy function. The reader is referred to the existing\nliterature for details.\nThe structural properties of operators discussed in this paper are irrelevant to the convergence-related properties such as nonexpansiveness (for an\noperator) or convexity (for a set or function). Hence, the algorithms developed can be still applied to nonconvex problems.\n\n2. Coordinate Friendly Operators\n2.1. Notation\nFor convenience, we do not distinguish a coordinate from a block of coordinates throughout this paper. We assume our variable x consists of m\ncoordinates:\nx = (x1 , . . . , xm ) ∈ H := H1 × · · · × Hm\n\nand xi ∈ Hi , i = 1, . . . , m.\n\nFor simplicity, we assume that H1 , . . . , Hm are finite-dimensional real Hilbert\nspaces, though most results hold for general Hilbert spaces. A function maps\nfrom H to R, the set of real numbers, and an operator maps from H to G,\nwhere the definition of G depends on the context.\nOur discussion often involves two points x, x+ ∈ H that differ over one\ncoordinate: there exists an index i ∈ [m] and a point δ ∈ H supported on\nHi , such that\n(6)\n\nx+ = x + δ.\n\n+\nNote that x+\nj = xj for all j 6= i. Hence, x = (x1 , . . . , xi + δi , . . . , xm ).\n\nDefinition 1 (number of operations). We let M [a 7→ b] denote the number\nof basic operations that it takes to compute the quantity b from the input a.\n\n\f10\nFor example, M [x 7→ (T x)i ] denotes the number of operations to compute the ith component of T x given x. We explore the possibility to compute\n(T x)i with much fewer operations than what is needed to first compute T x\nand then take its ith component.\n2.2. Single Coordinate Friendly Operators\nThis subsection studies a few classes of CF operators and then formally\ndefines the CF operator. We motivate the first class through an example.\nIn the example below, we let Ai,: and A:,j be the ith row and jth column\n>\nof a matrix A, respectively. Let A> be the transpose of A and A>\ni,: be (A )i,: ,\ni.e., the ith row of the transpose of A.\nExample 1 (least squares I). Consider the least squares problem\n(7)\n\n1\nminimize f (x) := kAx − bk2 ,\nx\n2\n\nwhere A ∈ Rp×m and b ∈ Rp . In this example, assume that m = Θ(p),\nnamely, m and p are of the same order. We compare the full update of\ngradient descent to its coordinate update.2 The full update is referred to as\nthe iteration xk+1 = T xk where T is given by\n(8)\n\nT x := x − η∇f (x) = x − ηA> Ax + ηA> b.\n\nAssuming that A> A and A> b are already computed, we have M [x 7→ T x] =\nO(m2 ). The coordinate update at the kth iteration performs\n= (T xk )ik = xkik − η∇ik f (xk ),\nxk+1\nik\nand xk+1\n= xkj , ∀j 6= ik , where ik is some selected coordinate.\nj\n\u0001\nSince for all i, ∇i f (xk ) = A> (Ax − b) i = (A> A)i,: · x − (A> b)i , we\n1\nhave M [x 7→ (T x)i ] = O(m) and thus M [x 7→ (T x)i ] = O( m\nM [x 7→ T x]).\nTherefore, the coordinate gradient descent is computationally worthy.\nThe operator T in the above example is a special Type-I CF operator.\nDefinition 2 (Type-I CF). For an operator T : H → H, let M [x 7→ (T x)i ]\nbe the number of operations for computing the ith coordinate of T x given x\n2\nAlthough gradient descent is seldom used to solve least squares, it often appears\nas a part in first-order algorithms for problems involving a least squares term.\n\n\fCoordinate friendly structures, algorithms, and applications\n\n11\n\nand M [x 7→ T x] the number of operations for computing T x given x. We\nsay T is Type-I CF (denoted as F1 ) if for any x ∈ H and i ∈ [m], it holds\n\u0012\n\u0013\n1\nM [x 7→ (T x)i ] = O\nM [x 7→ T x] .\nm\nExample 2 (least squares II). We can implement the coordinate update in\nExample 1 in a different manner by maintaining the result T xk in the memory. This approach works when m = Θ(p) or p \u001d m. The full update (8) is\nunchanged. At each coordinate update, from the maintained quantity T xk , we\nimmediately obtain xk+1\n= (T xk )ik . But we need to update T xk to T xk+1 .\nik\nk+1\nk\nSince x\nand x differ only over the coordinate ik , this update can be\ncomputed as\nk\n>\nT xk+1 = T xk + xk+1 − xk − η(xk+1\nik − xik )(A A):,ik ,\n\nwhich is a scalar-vector multiplication followed by vector addition, taking\nonly O(m) operations. Computing T xk+1 from scratch involves a matrixvector multiplication, taking O(M [x 7→ T (x)]) = O(m2 ) operations. Therefore,\n\u0012\nh\ni\ni\u0013\nh\n1\nk\nk k+1\nk+1\nk+1\nk+1\n=O\nM x\n7→ T x\n.\nM {x , T x , x } 7→ T x\nm\nThe operator T in the above example is a special Type-II CF operator.\nDefinition 3 (Type-II CF). An operator T is called Type-II\nCF (denoted\n\u0001\nas F2 ) if, for any i, x and x+ := x1 , . . . , (T x)i , . . . , xm , the following holds\n(9)\n\n\u0002\n\u0003\nM {x, T x, x+ } 7→ T x+ = O\n\n\u0012\n\n\u0013\n\u0002\n\u0003\n1\nM x+ 7→ T x+ .\nm\n\nThe next example illustrates an efficient coordinate update by maintaining certain quantity other than T x.\nExample 3 (least squares III). For the case p \u001c m, we should avoid\npre-computing the relative large matrix A> A, and it is cheaper to compute\nA> (Ax) than (A> A)x. Therefore, we change the implementations of both\nthe full and coordinate updates in Example 1. In particular, the full update\nxk+1 = T xk = xk − η∇f (xk ) = xk − ηA> (Axk − b),\n\u0002\n\u0003\npre-multiplies xk by A and then A> . Hence, M xk 7→ T (xk ) = O(mp).\n\n\f12\nWe change the coordinate update to maintain the intermediate quantity\nIn the first step, the coordinate update computes\n\nAxk .\n\n(T xk )ik = xkik − η(A> (Axk ) − A> b)ik ,\nk\nk+1\nby pre-multiplying Axk by A>\nik ,: . Then, the second step updates Ax to Ax\nk+1\nk\nk\nby adding (xik − xik )A:,ik to Ax . Both steps take O(p) operations, so\n\u0012\nh\ni\u0013\nh\ni\n1\nk\nk\nk\nk\nk+1\nk+1\n.\nM x 7→ T x\nM {x , Ax } 7→ {x , Ax } = O(p) = O\nm\n\nCombining Type-I and Type-II CF operators with the last example, we\narrive at the following CF definition.\nDefinition 4 (CF operator). We say that an\u0001operator T : H → H is CF if,\nfor any i, x and x+ := x1 , . . . , (T x)i , . . . , xm , the following holds\n\u0012\n\u0013\n\u0002\n\u0003\n1\n+\n+\n(10)\nM {x, M(x)} 7→ {x , M(x )} = O\nM [x 7→ T x] ,\nm\nwhere M(x) is some quantity maintained in the memory to facilitate each\ncoordinate update and refreshed to M(x+ ). M(x) can be empty, i.e., except\nx, no other varying quantity is maintained.\nThe left-hand side of (10) measures the cost of performing one coordinate update (including the cost of updating M(x) to M(x+ )) while the\nright-hand side measures the average per-coordinate cost of updating all\nthe coordinates together. When (10) holds, T is amenable to coordinate\nupdates.\nBy definition, a Type-I CF operator T is CF without maintaining any\nquantity, i.e., M(x) = ∅.\nA Type-II CF operator T satisfies (10) with M(x) = T x, so it is also\nCF. Indeed, given any x and i, we can compute x+ by immediately letting\n+\nx+\ni = (T x)i (at O(1) cost) and keeping xj = xj , ∀j 6= i; then, by (9), we\n+\nupdate T x to T x at a low cost. Formally, letting M(x) = T x,\n\u0002\n\u0003\nM {x, M(x)} 7→ {x+ , M(x+ )}\n\u0002\n\u0003\n\u0002\n\u0003\n≤M {x, T x} 7→ x+ + M {x, T x, x+ } 7→ T x+\n\u0012\n\u0013\n\u0002 +\n\u0003\n1\n(9)\n+\n= O(1) + O\nM x 7→ T x\nm\n\u0012\n\u0013\n1\n=O\nM [x 7→ T x] .\nm\n\n\fCoordinate friendly structures, algorithms, and applications\n\n13\n\nIn general, the set of CF operators is much larger than the union of\nType-I and Type-II CF operators.\nAnother important subclass of CF operators are operators T : H → H\nwhere (T x)i only depends on one, or a few, entries among x1 , . . . , xm . Based\non how many input coordinates they depend on, we partition them into three\nsubclasses.\nDefinition 5 (separable operator). Consider T := {T | T : H → H}. We\nhave the partition T = C1 ∪ C2 ∪ C3 , where\n• separable operator: T ∈ C1 if, for any index i, there exists Ti : Hi → Hi\nsuch that (T x)i = Ti xi , that is, (T x)i only depends on xi .\n• nearly-separable operator: T ∈ C2 if, for any index i, there exists Ti\nand index set Ii such that (T x)i = Ti ({xj }j∈Ii ) with |Ii | \u001c m, that is,\neach (T x)i depends on a few coordinates of x.\n• non-separable operator: C3 := T \\ (C1 ∪ C2 ). If T ∈ C3 , there exists\nsome i such that (T x)i depends on many coordinates of x.\nThroughout the paper, we assume the coordinate update of a (nearly-)\nseparable operator costs roughly the same for all coordinates. Under this\nassumption, separable operators are both Type-I CF and Type-II CF, and\nnearly-separable operators are Type-I CF.3\n2.3. Examples of CF Operators\nIn this subsection, we give examples of CF operators arising in different\nareas including linear algebra, optimization, and machine learning.\nExample 4 ((block) diagonal matrix). Consider the diagonal matrix\n\n\na1,1\n\nA=\n0\n\n0\n..\n\n\n\nm×m\n.\n∈R\n\n.\nam,m\n\nClearly T : x 7→ Ax is separable.\n3\n\nNot all nearly-separable operators are Type-II CF. Indeed, consider a sparse\nmatrix A ∈ Rm×m whose non-zero entries are only located in the last column. Let\nT x = Ax and x+ = x+δm . As x+ and x differ over the last entry, T x+ = T x+(x+\nm−\nxm )A:,m takes m operations. Therefore, we have M [{x, T x, x+ } 7→ T x+ ] = O(m).\n+\n+\nSince T x+ = x+\nm A:,m takes m operations, we also have M [x 7→ T x ] = O(m).\nTherefore, (9) is violated, and there is no benefit from maintaining T x.\n\n\f14\nExample 5 (gradient and proximal maps of a separable function). Consider\na separable function\nm\nX\nf (x) =\nfi (xi ).\ni=1\n\nThen, both ∇f and proxγf are separable, in particular,\n(∇f (x))i = ∇fi (xi )\n\nand\n\n(proxγf (x))i = proxγfi (xi ).\n\nHere, proxγf (x) (γ > 0) is the proximal operator that we define in Definition 10 in Appendix A.\nExample 6 (projection to box constraints). Consider the “box” set B :=\n{x : ai ≤ xi ≤ bi , i ∈ [m]} ⊂ Rm . Then, the projection operator projB is\nseparable. Indeed,\n\n\u0001\nprojB (x) i = max(bi , min(ai , xi )).\nExample 7 (sparse matrices). If every row of the matrix A ∈ Rm×m is\nsparse, T : x 7→ Ax is nearly-separable.\nExamples of sparse matrices arise from various finite difference schemes\nfor differential equations, problems defined on sparse graphs. When most\npairs of a set of random variables are conditionally independent, their inverse\ncovariance matrix is sparse.\nExample 8 (sum of sparsely supported functions). Let E be a class of\nindex sets and every e ∈ E be a small subset of [m], |e| \u001c m. In addition\n#{e : i ∈ e} \u001c #{e} for all i ∈ [m]. Let xe := (xi )i∈e , and\nf (x) =\n\nX\n\nfe (xe ).\n\ne∈E\n\nThe gradient map ∇f is nearly-separable.\nAn application of this example arises in wireless communication over a\ngraph of m nodes. Let each xi be the spectrum assignment to node i, each e be\na neighborhood of nodes, and each fe be a utility function. The input of fe is\nxe since the utility depends on the spectra assignments in the neighborhood.\nIn machine learning, if each observation only involves a few features,\nthen each function of the optimization objective will depend on a small\nnumber of components of x. This is the case when graphical models are\nused [64, 9].\n\n\fCoordinate friendly structures, algorithms, and applications\n\n15\n\nExample 9 (squared hinge loss function). Consider for a, x ∈ Rm ,\nf (x) :=\n\n\u00012\n1\nmax(0, 1 − βa> x) ,\n2\n\nwhich is known as the squared hinge loss function. Consider the operator\n(11)\n\nT x := ∇f (x) = −β max(0, 1 − βa> x)a.\n\nLet us maintain M(x) = a> x. For arbitrary x and i, let\n>\nx+\ni := (T x)i = −β max(0, 1 − βa x)ai\n+\n>\nand x+\nj := xj , ∀j 6= i. Then, computing xi from x and a x takes O(1) (as\n>\na> x is maintained), and computing a> x+ from x+\ni − xi and a x costs O(1).\nFormally, we have\nh\ni\nM {x, a> x} 7→ {x+ , a> x+ }\ni\ni\nh\nh\n> +\n−\nx\n}\n→\n7\na\nx\n≤M {x, a> x} 7→ x+ + M {a> x, x+\ni\ni\n\n=O(1) + O(1) = O(1).\nOn the other hand, M [x 7→ T x] = O(m). Therefore, (10) holds, and T\ndefined in (11) is CF.\n\n3. Composite Coordinate Friendly Operators\nCompositions of two or more operators arise in algorithms for problems that\nhave composite functions, as well as algorithms that are derived from operator splitting methods. To update the variable xk to xk+1 , two or more\noperators are sequentially applied, and therefore the structures of all operators determine whether the update is CF. This is where CF structures\nbecome less trivial but more interesting. This section studies composite CF\noperators. The exposition leads to the recovery of existing algorithms, as\nwell as powerful new algorithms.\n3.1. Combinations of Operators\nWe start by an example with numerous applications. It is a generalization\nof Example 9.\n\n\f16\nExample 10 (scalar map pre-composing affine function). Let aj ∈ Rm , bj ∈\nR, and φj : R → R be differentiable functions, j ∈ [p]. Let\nf (x) =\n\np\nX\n\nφj (a>\nj x + bj ).\n\nj=1\n\nAssume that evaluating φ0j costs O(1) for each j. Then, ∇f is CF. Indeed,\nlet\nT1 y := A> y, T2 y := [φ01 (y1 ); . . . ; φ0p (yp )], T3 x := Ax + b,\n>\n>\np×m and b = [b ; b ; . . . ; b ] ∈ Rp×1 . Then we\nwhere A = [a>\n1 2\np\n1 ; a2 ; . . . ; ap ] ∈ R\nhave ∇f (x) = T1 ◦ T2 ◦ T3 x. For any x and i ∈ [m], let x+\ni = ∇i f (x) and\nx+\n=\nx\n,\n∀j\n=\n6\ni,\nand\nlet\nM(x)\n:=\nT\nx.\nWe\ncan\nfirst\ncompute\nT2 ◦ T3 x from\nj\n3\nj\n+\nT3 x for O(p) operations, then compute ∇i f (x) and thus x from {x, T2 ◦T3 x}\nfor O(p) operations, and finally update the maintained T3 x to T3 x+ from\n{x, x+ , T3 x} for another O(p) operations. Formally,\n\u0002\n\u0003\nM {x, T3 x} 7→ {x+ , T3 x+ }\n\u0003\n\u0002\n\u0003\n\u0002\n≤M [T3 x 7→ T2 ◦ T3 x] + M {x, T2 ◦ T3 x} 7→ x+ + M {x, T3 x, x+ } 7→ {T3 x+ }\n=O(p) + O(p) + O(p) = O(p).\n\nSince M [x 7→ ∇f (x)] = O(pm), therefore ∇f = T1 ◦ T2 ◦ T3 is CF.\nIf p = m, T1 , T2 , T3 all map from Rm to Rm . Then, it is easy to check\nthat T1 is Type-I CF, T2 is separable, and T3 is Type-II CF. The last one is\ncrucial since not maintaining T3 x would disqualify T from CF. Indeed, to\nobtain (T x)i , we must multiply A>\ni to all the entries of T2 ◦ T3 x, which in\nturn needs all the entries of T3 x, computing which from scratch would cost\nO(pm).\nThere are general rules to preserve Type-I and Type-II CF. For example, T1 ◦ T2 is still Type-I CF, and T2 ◦ T3 is still CF, but there are counter\nexamples where T2 ◦ T3 can be neither Type-I nor Type-II CF. Such properties are important for developing efficient coordinate update algorithms for\ncomplicated problems; we will formalize them in the following.\nThe operators T2 and T3 in the above example are prototypes of cheap\nand easy-to-maintain operators from H to G that arise in operator compositions.\nDefinition 6 (cheap operator). For a composite operator T = T1 ◦ · · · ◦ Tp ,\nan operator Ti : H → G is cheap if M [x 7→ Ti x] is less than or equal to the\nnumber of remaining coordinate-update operations, in order of magnitude.\n\n\fCoordinate friendly structures, algorithms, and applications\nCase\n1\n2\n3\n4\n\nT1\nC1\nC2\nC2\nC3\n\n∈\n(separable)\n(nearly-sep.)\n(non-sep.)\n\nT2 ∈\nC1 , C2 , C3\nC1 , C3\nC2\nC1 ∪ C 2 ∪ C 3\n\n17\n\n(T1 ◦ T2 ) ∈\nC1 , C2 , C3 , respectively\nC2 , C3 , resp.\nC2 or C3 , case by case\nC3\n\nTable 1: T1 ◦ T2 inherits the weaker separability property from those of T1\nand T2 .\nCase\n5\n6\n7\n8\n9\n\nT1 ∈\nC1 ∪ C 2\nF, F2\nF1\ncheap\nF1\n\nT2 ∈\nF, F1\nC1\nF2\nF2\ncheap\n\n(T1 ◦ T2 ) ∈\nF, F1 , resp.\nF, F2 , resp.\nF\nF\nF1\n\nExample\nExamples 11 and 13\nExample 10\nExample 12\nExample 13\nExamples 10 and 13\n\nTable 2: Summary of how T1 ◦ T2 inherits CF properties from those of T1\nand T2 .\nDefinition 7 (easy-to-maintain operator). For a composite operator T =\nT1 ◦ · · · ◦ Tp , the operator Tp : H → G is easy-to-maintain, if for any x, i, x+\nsatisfying (6), M [{x, Tp x, x+ } 7→ Tp x+ ] is less than or equal to the number\nof remaining coordinate-update operations, in order of magnitude, or belongs\n1\n+\n+\nto O( dim\nG M [x 7→ T x ]).\nThe splitting schemes in §3.2 below will be based on T1 + T2 or T1 ◦ T2 ,\nas well as a sequence of such combinations. If T1 and T2 are both CF, T1 + T2\nremains CF, but T1 ◦ T2 is not necessarily so. This subsection discusses how\nT1 ◦ T2 inherits the properties from T1 and T2 . Our results are summarized\nin Tables 1 and 2 and explained in detail below.\nThe combination T1 ◦ T2 generally inherits the weaker property from T1\nand T2 .\nThe separability (C1 ) property is preserved by composition. If T1 , . . . , Tn\nare separable, then T1 ◦ · · · ◦ Tn is separable. However, combining nearlyseparable (C2 ) operators may not yield a nearly-separable operator since\ncomposition introduces more dependence among the input entries. Therefore, composition of nearly-separable operators can be either nearly-separable\nor non-separable.\nNext, we discuss how T1 ◦ T2 inherits the CF properties from T1 and\nT2 . For simplicity, we only use matrix-vector multiplication as examples to\nillustrate the ideas; more interesting examples will be given later.\n\n\f18\n• If T1 is separable or nearly-separable (C1 ∪ C2 ), then as long as T2 is\nCF (F), T1 ◦ T2 remains CF. In addition, if T2 is Type-I CF (F1 ), so\nis T1 ◦ T2 .\nExample 11. Let A ∈ Rm×m be sparse and B ∈ Rm×m dense. Then\nT1 x = Ax is nearly-separable and T2 x = Bx is Type-I CF4 . For any\ni, let Ii index the set of nonzeros on the ith row of A. We first compute (Bx)Ii , which costs O(|Ii |m), and then ai,Ii (Bx)Ii , which costs\nO(|Ii |), where ai,Ii is formed by the nonzero entries on the ith row of\nA. Assume O(|Ii |) = O(1), ∀i. We have, from the above discussion,\nthat M [x 7→ (T1 ◦ T2 x)i ] = O(m), while M [x 7→ T1 ◦ T2 x] = O(m2 ).\nHence, T1 ◦ T2 is Type-I CF.\n• Assume that T2 is separable (C1 ). It is easy to see that if T1 is CF (F),\nthen T1 ◦ T2 remains CF. In addition if T1 is Type-II CF (F2 ), so is\nT1 ◦ T2 ; see Example 10.\nNote that, if T2 is nearly-separable, we do not always have CF properties for T1 ◦ T2 . This is because T2 x and T2 x+ can be totally different\n(so updating T2 x is expensive) even if x and x+ only differ over one\ncoordinate; see the footnote 3 on Page 13.\n• Assume that T1 is Type-I CF (F1 ). If T2 is Type-II CF (F2 ), then\nT1 ◦ T2 is CF (F).\nExample 12. Let A, B ∈ Rm×m be dense. Then T1 x = Ax is Type-I\nCF and T2 x = Bx Type-II CF (by maintaining Bx; see Example 2).\nFor any x and i, let x+ satisfy (6). Maintaining T2 x, we can compute\n(T1 ◦ T2 x)j for O(m) operations for any j and update T2 x+ for O(m)\noperations. On the other hand, computing T1 ◦ T2 x+ without maintaining T2 x takes O(m2 ) operations.\n• Assume that one of T1 and T2 is cheap. If T2 is cheap, then as long as\nT1 is Type-I CF (F1 ), T1 ◦ T2 is Type-I CF. If T1 is cheap, then as long\nas T2 is Type-II CF (F2 ), T1 ◦ T2 is CF (F); see Example 13.\nWe will see more examples of the above cases in the rest of the paper.\n3.2. Operator Splitting Schemes\nWe will apply our discussions above to operator splitting and obtain new\nalgorithms. But first, we review several major operator splitting schemes and\n4\n\nFor this example, one can of course pre-compute AB and claim that (T1 ◦ T2 )\nis Type-I CF. Our arguments keep A and B separate and only use the nearlyseparability of T1 and Type-I CF property of T2 , so our result holds for any such\ncomposition even when T1 and T2 are nonlinear.\n\n\fCoordinate friendly structures, algorithms, and applications\n\n19\n\ndiscuss their CF properties. We will encounter important concepts such as\n(maximum) monotonicity and cocoercivity, which are given in Appendix A.\nFor a monotone operator A, the resolvent operator JA and the reflectiveresolvent operator RA are also defined there, in (67) and (68), respectively.\nConsider the following problem: given three operators A, B, C, possibly\nset-valued,\n(12)\n\nfind x ∈ H\n\nsuch that\n\n0 ∈ Ax + Bx + Cx,\n\nwhere “+” is the Minkowski sum. This is a high-level abstraction of many\nproblems or their optimality conditions. The study began in the 1960s, followed by a large number of algorithms and applications over the last fifty\nyears. Next, we review a few basic methods for solving (12).\nWhen A, B are maximally monotone (think it as the subdifferential ∂f\nof a proper convex function f ) and C is β-cocoercive (think it as the gradient\n∇f of a 1/β-Lipschitz differentiable function f ), a solution can be found by\nthe iteration (2) with T = T3S , introduced recently in [24], where\n(13)\n\nT3S := I − JγB + JγA ◦ (2JγB − I − γC ◦ JγB ).\n\n2β\nIndeed, by setting γ ∈ (0, 2β), T3S is ( 4β−γ\n)-averaged (think it as a property\nweaker than the Picard contraction; in particular, T may not have a fixed\npoint). Following the standard convergence result (cf. textbook [5]), provided\nthat T has a fixed point, the sequence from (2) converges to a fixed-point\nx∗ of T . Note that, instead of x∗ , JγB (x∗ ) is a solution to (12).\nFollowing §3.1, T3S is CF if JγA is separable (C1 ), JγB is Type-II CF\n(F2 ), and C is Type-I CF (F1 ).\nWe give a few special cases of T3S below, which have much longer history.\nThey all converge to a fixed point x∗ whenever a solution exists and γ is\nproperly chosen. If B 6= 0, then JγB (x∗ ), instead of x∗ , is a solution to (12).\nForward-Backward Splitting (FBS): Letting B = 0 yields JγB = I.\nThen, T3S reduces to FBS [52]:\n\n(14)\n\nTFBS := JγA ◦ (I − γC)\n\nfor solving the problem 0 ∈ Ax + Cx.\nBackward-Forward Splitting (BFS): Letting A = 0 yields JγA = I.\nThen, T3S reduces to BFS:\n(15)\n\nTBFS := (I − γC) ◦ JγB\n\n\f20\nfor solving the problem 0 ∈ Bx + Cx. When A = B, TFBS and TBFS apply\nthe same pair of operators in the opposite orders, and they solve the same\nproblem. Iterations based on TBFS are rarely used in the literature because\nthey need an extra application of JγB to return the solution, so TBFS is\nseemingly an unnecessary variant of TFBS . However, they become different\nfor coordinate update; in particular, TBFS is CF (but TFBS is generally not)\nwhen JγB is Type-II CF (F2 ) and C is Type-I CF (F1 ). Therefore, TBFS is\nworth discussing alone.\nDouglas-Rachford Splitting (DRS): Letting C = 0, T3S reduces to\n(16)\n\n1\nTDRS := I − JγB + JγA ◦ (2JγB − I) = (I + RγA ◦ RγB )\n2\n\nintroduced in [26] for solving the problem 0 ∈ Ax+Bx. A more general splitting is the Relaxed Peaceman-Rachford Splitting (RPRS) with λ ∈ [0, 1]:\n(17)\n\nTRPRS = (1 − λ) I + λ RγA ◦ RγB ,\n\nwhich recovers TDRS by setting λ = 12 and Peaceman-Rachford Splitting\n(PRS) [53] by letting λ = 1.\nForward-Douglas-Rachford Splitting (FDRS): Let V be a linear\nsubspace, and NV and PV be its normal cone and projection operator, respectively. The FDRS [15]\nTFDRS = I − PV + JγA ◦ (2PV − I − γPV ◦ C˜ ◦ PV ),\naims at finding a point x such that 0 ∈ A x+C˜ x+NV x. If an optimal x exists,\nwe have x ∈ V and NV x is the orthogonal complement of V . Therefore, the\nproblem is equivalent to finding x such that 0 ∈ A x + PV ◦ C˜ ◦ PV x + NV x.\nThus, T3S recovers TFDRS by letting B = NV and C = PV ◦ C˜ ◦ PV .\nForward-Backward-Forward Splitting (FBFS): Composing TFBS\nwith one more forward step gives TFBFS introduced in [71]:\n(18)\n\nTFBFS = −γC + (I − γC)JγA (I − γC).\n\nTFBFS is not a special case of T3S . At the expense of one more application\nof (I − γC), TFBFS relaxes the convergence condition of TFBS from the cocoercivity of C to its monotonicity. (For example, a nonzero skew symmetric\nmatrix is monotonic but not cocoercive.) From Table 2, we know that TFBFS\nis CF if both C and JγA are separable.\n\n\fCoordinate friendly structures, algorithms, and applications\n3.2.1. Examples in Optimization.\n(19)\n\n21\n\nConsider the optimization problem\n\nminimize f (x) + g(x),\nx∈X\n\nwhere X is the feasible set and f and g are objective functions. We present\nexamples of operator splitting methods discussed above.\nExample 13 (proximal gradient method). Let X = Rm , f be differentiable,\nand g be proximable in (19). Setting A = ∂g and C = ∇f in (14) gives\nJγA = proxγg and reduces xk+1 = TFBS (xk ) to prox-gradient iteration:\nxk+1 = proxγg (xk − γ∇f (xk )).\n\n(20)\n\nA special case of (20) with g = ιX is the projected gradient iteration:\nxk+1 = PX (xk − γ∇f (xk )).\n\n(21)\n\nIf ∇f is CF and proxγg is (nearly-)separable (e.g., g(x) = kxk1 or the\nindicator function of a box constraint) or if ∇f is Type-II CF and proxγg\nis cheap (e.g., ∇f (x) = Ax − b and g = kxk2 ), then the FBS iteration (20)\nis CF. In the latter case, we can also apply the BFS iteration (15) (i.e,\ncompute proxγg and then perform the gradient update), which is also CF.\nExample 14 (ADMM). Setting X = Rm simplifies (19) to\n(22)\n\nminimize f (x) + g(y),\nx,y\n\nsubject to x − y = 0.\n\nThe ADMM method iterates:\n(23a)\n\nxk+1 = proxγf (y k − γsk ),\n\n(23b)\n\ny k+1 = proxγg (xk+1 + γsk ),\n1\nsk+1 = sk + (xk+1 − y k+1 ).\nγ\n\n(23c)\n\n(The iteration can be generalized to handle the constraint Ax − By = b.)\nThe dual problem of (22) is mins f ∗ (−s) + g ∗ (s), where f ∗ is the convex\nconjugate of f . Letting A = −∂f ∗ (−·) and B = ∂g ∗ in (16) recovers the\niteration (23) through (see the derivation in Appendix B)\ntk+1 = TDRS (tk ) = tk − JγB (tk ) + JγA ◦ (2JγB − I)(tk ).\nFrom the results in §3.1, a sufficient condition for the above iteration to be\nCF is that JγA is (nearly-)separable and JγB being CF.\n\n\f22\nThe above abstract operators and their CF properties will be applied\nin §5 to give interesting algorithms for several applications.\n\n4. Primal-dual Coordinate Friendly Operators\nWe study how to solve the problem\n(24)\n\nminimize f (x) + g(x) + h(Ax),\nx∈H\n\nwith primal-dual splitting algorithms, as well as their coordinate update\nversions. Here, f is differentiable and A is a “p-by-m” linear operator from\nH = H1 × · · · × Hm to G = G1 × · · · × Gp . Problem (24) abstracts many\napplications in image processing and machine learning.\nExample 15 (image deblurring/denoising). Let u0 be an image, where\nu0i ∈ [0, 255], and B be the blurring linear operator. Let k∇uk1 be the\nanisotropic5 total variation of u (see (49) for definition). Suppose that b\nis a noisy observation of Bu0 . Then, we can try to recover u0 by solving\n(25)\n\nminimize\nu\n\n1\nkBu − bk2 + ι[0,255] (u) + λk∇uk1 ,\n2\n\nwhich can be written in the form of (24) with f = 21 kB · −bk2 , g = ι[0,255] ,\nA = ∇, and h = λk · k1 .\nMore examples with the formulation (24) will be given in §4.2. In general,\nprimal-dual methods are capable of solving complicated problems involving\nconstraints and the compositions of proximable and linear maps like k∇uk1 .\nIn many applications, although h is proximable, h ◦ A is generally nonproximable and non-differentiable. To avoid using slow subgradient methods,\nwe can consider the primal-dual splitting approaches to separate h and A so\nthat proxh can be applied. We derive that the equivalent form (for convex\ncases) of (24) is to find x such that\n(26)\n\n0 ∈ (∇f + ∂g + A> ◦ ∂h ◦ A)(x).\n\nIntroducing the dual variable s ∈ G and applying the biconjugation prop5\n\nGeneralization to the isotropic case is straightforward by grouping variables\nproperly.\n\n\fCoordinate friendly structures, algorithms, and applications\n\n23\n\nerty: s ∈ ∂h(Ax) ⇔ Ax ∈ ∂h∗ (s), yields the equivalent condition\n\u0015 \u0014\n\u0015\u0013 \u0014 \u0015\n\u0012 \u0014\n\u0015 \u0014\nx\n∂g 0\n∇f 0\n0 A>\n+\n+\n(27)\n0∈\n,\n∗\n0 ∂h\n0 0\ns\n−A 0\n| {z } |\n{z\n} |{z}\nz\noperator A\noperator B\nwhich we shorten as 0 ∈ Az + Bz, with z ∈ H × G =: F.\nProblem (27) can be solved by the Condat-Vũ algorithm [20, 76]:\n\u001a k+1\ns\n= proxγh∗ (sk + γAxk ),\n(28)\nk+1\nx\n= proxηg (xk − η(∇f (xk ) + A> (2sk+1 − sk ))),\nwhich explicitly applies A and A> and updates s, x in a Gauss-Seidel style 6 .\nWe introduce an operator TCV : F → F and write\niteration (28)\n\n⇐⇒\n\nz k+1 = TCV (z k ).\n\nSwitching the orders of x and s yields the following algorithm:\n\u001a k+1\nx\n= proxηg (xk − η(∇f (xk ) + A> sk )),\n0\n(29)\nas z k+1 = TCV\nzk .\nk+1\ns\n= proxγh∗ (sk + γA(2xk+1 − xk )),\nIt is known from [18, 23] that both (28) and (29) reduce to iterations of\nnonexpansive operators (under a special metric), i.e., TCV is nonexpansive;\nsee Appendix C for the reasoning.\nRemark 1. Similar primal-dual algorithms can be used to solve other problems such as saddle point problems [40, 46, 16] and variational inequalities [68]. Our coordinate update algorithms below apply to these problems as\nwell.\n4.1. Primal-dual Coordinate Update Algorithms\nIn this subsection, we make the following assumption.\nAssumption 1. Functions g and h∗ in the problem (24) are separable and\nproximable. Specifically,\ng(x) =\n\nm\nX\ni=1\n\ngi (xi )\n\nand\n\n∗\n\nh (y) =\n\np\nX\n\nh∗i (yi ).\n\nj=1\n\nBy the Moreau identity: proxγh∗ = I − γprox γ1 h ( γ· ), one can compute prox γ1 h\ninstead of proxγh∗ , which inherits the same separability properties from prox γ1 h .\n6\n\n\f24\nFurthermore, ∇f is CF.\nProposition 1. Under Assumption 1, the followings hold:\n(a) when p = O(m), the Condat-Vu operator TCV in (28) is CF, more specifically,\n\nh\n\nk\n\n+\n\n+\n\ni\n\n\u0012\n\nM {z , Ax} 7→ {z , Ax } = O\n\nh\ni\u0013\n1\nk\nk\n;\nM z 7→ TCV z\nm+p\n\n0\n(b) when m \u001c p and M [x 7→ ∇f (x)] = O(m), the Condat-Vu operator TCV\nin (29) is CF, more specifically,\n\n\u0012\nh\ni\nM {z k , A> s} 7→ {z + , A> s+ } = O\n\nh\ni\u0013\n1\n0\nM z k 7→ TCV\nzk .\nm+p\n\nProof. Computing z k+1 = TCV z k involves evaluating ∇f , proxg , and proxh∗ ,\n\u0002\n\u0003\napplying A and A> , and adding vectors. It is easy to see M z k 7→ TCV z k =\n\u0002 k\n\u0003\n0 z k is the same.\nO(mp + m + p) + M[x → ∇f (x)], and M z 7→ TCV\n(a) We assume ∇f ∈ F1 for simplicity, and other cases are similar.\n1. If (TCV z k )j = sk+1\n, computing it involves: adding ski and γ(Axk )i , and\ni\n\u0002\n\u0003\nevaluating proxγh∗i . In this case M {z k , Ax} 7→ {z + , Ax+ } = O(1).\n2. If (TCV z k )j = xk+1\n, computing it involves evaluating: the entire sk+1\ni\nfor O(p) operations, (A> (2sk+1 − sk ))i for O(p) operations, proxηgi\n1\nfor O(1) operations, ∇i f (xk ) for O( m\nM [x 7→ ∇f (x)]) operations, as\n+\nwell as updating Ax for O(p) operations. In this case\n\u0002\n\u0003\n1\nM {z k , Ax} 7→ {z + , Ax+ } = O(p + m\nM [x 7→ ∇f (x)]).\n\n\u0002\n\u0003\nTherefore, M {z k , Ax} 7→ {z + , Ax+ } = O\n\n1\nm+p M\n\n\u0002 k\n\u0003\u0001\nz 7→ TCV z k .\n\n(b) When m \u001c p and M [x 7→ ∇f (x)] = O(m), following arguments similar\nto the above, we have\n\u0002\n\u0003\n0 z k ) = xk+1 ;\nM {z k , A> s} 7→ {z + , A> s+ } = O(1)+ M [x 7→ ∇i f (x)] if (TCV\nj\ni\n\u0002 k >\n\u0003\n+\n>\n+\n0 zk ) =\nand M {z , A s} 7→ {z , A s } = O(m) + M [x 7→ ∇f (x)] if (TCV\nj\nsk+1\n.\ni\n\u0002\n\u0003\n\u0002\n\u0003\n1\n0 z k ).\nM z k 7→ TCV\nIn both cases M {z k , A> s} 7→ {z + , A> s+ } = O( m+p\n\n\fCoordinate friendly structures, algorithms, and applications\n\n25\n\n4.2. Extended Monotropic Programming\nWe develop a primal-dual coordinate update algorithm for the extended\nmonotropic program:\n(30)\n\nminimize\n\ng1 (x1 ) + g2 (x2 ) + · · · + gm (xm ) + f (x),\n\nsubject to\n\nA1 x1 + A2 x2 + · · · + Am xm = b,\n\nx∈H\n\nwhere x = (x1 , . . . , xm ) ∈ H = H1 ×. . .× Hm with Hi being Euclidean spaces.\nIt generalizes linear, quadratic, second-order cone, semi-definite programs by\nallowing extended-valued objective functions gi and f . It is a special case\nm\nX\nof (24) by letting g(x) =\ngi (xi ), A = [A1 , · · · , Am ] and h = ι{b} .\ni=1\n\nExample 16 (quadratic programming). Consider the quadratic program\n(31)\n\nminimize\nm\nx∈R\n\n1 >\nx U x + c> x, subject to Ax = b, x ∈ X,\n2\n\nwhere U is a symmetric positive semidefinite matrix and X = {x : xi ≥\n0 ∀i}. Then, (31) is a special case of (30) with gi (xi ) = ι·≥0 (xi ), f (x) =\n1 >\n>\n2 x U x + c x and h = ι{b} .\nExample 17 (Second Order Cone Programming (SOCP)). The SOCP\nminimize\nc> x,\nm\nx∈R\n\nsubject to Ax = b,\nx ∈ X = Q1 × · · · × Qn ,\n\n(where the number of cones n may not be equal to the number of blocks m,)\ncan be written in the form of (30): minimizex∈Rm ιX (x) + c> x + ι{b} (Ax).\nApplying iteration (28) to problem (30) and eliminating sk+1 from the\nsecond row yield the Jacobi-style update (denoted as Temp ):\n\n\u001a\n(32)\n\nsk+1 = sk + γ(Axk − b),\nxk+1 = proxηg (xk − η(∇f (xk ) + A> sk + 2γA> Axk − 2γA> b)).\n\nTo the best of our knowledge, this update is never found in the literature.\nNote that xk+1 no longer depends on sk+1 , making it more convenient to\nperform coordinate updates.\n\n\f26\nRemark 2. In general, when the s update is affine, we can decouple sk+1\nand xk+1 by plugging the s update into the x update. It is the case when h\nis affine or quadratic in problem (24).\nA sufficient condition for Temp to be CF is proxg ∈ C1 i.e., separable.\nIndeed, we have Temp = T1 ◦ T2 , where\n\u0014\n\u0015\n\u0014 \u0015 \u0014\n\u0015\nI\n0\ns + γ(Ax − b)\ns\nT1 =\n, T2\n=\n.\n0 proxηg\nx\nx − η(∇f (x) + A> s + 2γA> Ax − 2γA> b)\nFollowing Case 5 of Table 2, Temp is CF. When m = Θ(p), the separability\ncondition on proxg can be relaxed to proxg ∈ F1 since in this case T2 ∈ F2 ,\nand we can apply Case 7 of Table 2 (by maintaining ∇f (x), A> s, Ax and\nA> Ax.)\n4.3. Overlapping-Block Coordinate Updates\nIn the coordinate update scheme based on (28), if we select xi to update then\nwe must first compute sk+1 , because the variables xi ’s and sj ’s are coupled\nthrough the matrix A. However, once xk+1\nis obtained, sk+1 is discarded. It\ni\nis not used to update s or cached for further use. This subsection introduces\nways to utilize the otherwise wasted computation.\nWe define, for each i, J(i) ⊂ [p] as the set of indices j such that A>\ni,j 6= 0,\nand, for each j, I(j) ⊂ [m] as the set of indices of i such that A>\n=\n6\n0. We\ni,j\nalso let mj := |I(j)|, and assume mj 6= 0, ∀j ∈ [p] without loss of generality.\nWe arrange the coordinates of z = [x; s] into m overlapping blocks. The\nith block consists of the coordinate xi and all sj ’s for j ∈ J(i). This way, each\nsj may appear in more than one block. We propose a block coordinate update\nscheme based on (28). Because the blocks overlap, each sj may be updated\nin multiple blocks, so the sj P\nupdate is relaxed with parameters ρi,j ≥ 0\n(see (33) below) that satisfy\ni∈I(j) ρi,j = 1, ∀j ∈ [p]. The aggregated\neffect is to update sj without scaling. (Following the KM iteration [39], we\ncan also assign a relaxation parameter ηk for the xi update; then, the sj\nupdate should be relaxed with ρi,j ηk .)\nWe propose the following update scheme:\n\nselect i ∈ [m], and then compute\n\n\n\nk+1\nk\nk\n\nall j ∈ J(i),\n\n s̃j = proxγh∗j (sj + γ(Ax )j ), forP\nk+1\nk+1\nk\nk\nx̃i = proxηgi (xi − η(∇i f (x ) + j∈J(i) A>\n− skj ))),\n(33)\ni,j (2s̃j\n\n\n\nupdate xk+1\n= xki + (x̃k+1\n− xki ),\n\ni\ni\n\n\nk+1\nk+1\nk\nupdate sj = sj + ρi,j (s̃j − skj ), for all j ∈ J(i).\n\n\fCoordinate friendly structures, algorithms, and applications\n\n27\n\nRemark 3. The use of relaxation parameters ρi,j makes our scheme different from that in [56].\nFollowing the assumptions and arguments in §4.1, if we maintain Ax,\nthe cost for each block coordinate update is O(p) + M [x 7→ ∇i f (x)], which\n1\nis O( m\nM [z 7→ TCV z]). Therefore the coordinate update scheme (33) is computationally worthy.\nTypical choices of ρi,j include: (1) one of the ρi,j ’s is 1 for each j, others\nall equal to 0. This can be viewed as assigning the update of sj solely to a\nblock containing xi . (2) ρi,j = m1j for all i ∈ I(j). This approach spreads the\nupdate of sj over all the related blocks.\nRemark 4. The recent paper [28] proposes a different primal-dual coordinate update algorithm. The authors produce a new matrix Ā based on A,\nwith only one nonzero entry in each row, i.e. mj = 1 for each j. They also\nmodify h to h̄ so that the problem\n(34)\n\nminimize f (x) + g(x) + h̄(Āx)\nx∈H\n\nhas the same solution as (24). Then they solve (34) by the scheme (33).\nBecause they have mj = 1, every dual variable coordinate is only associated\nwith one primal variable coordinate. They create non-overlapping blocks of\nz by duplicating each dual variable coordinate sj multiple times. The computation cost for each block coordinate update of their algorithm is the same\nas (33), but more memory is needed for the duplicated copies of each sj .\n\n4.4. Async-Parallel Primal-Dual Coordinate Update Algorithms\nand Their Convergence\n\nIn this subsection, we propose two async-parallel primal-dual coordinate\nupdate algorithms using the algorithmic framework of [54] and state their\nconvergence results. When there is only one agent, all algorithms proposed in\nthis section reduce to stochastic coordinate update algorithms [19], and their\nconvergence is a direct consequence of Theorem 1. Moreover, our convergence\nanalysis also applies to sync-parallel algorithms.\n\n\f28\nThe two algorithms are based on §4.1 and §4.3, respectively.\nAlgorithm 1: Async-parallel primal-dual coordinate update algorithm using TCV\nInput :P\nz 0 ∈ F, K > 0, a discrete distribution (q1 , . . . , qm+p ) with\nm+p\ni=1 qi = 1 and qi > 0, ∀i,\nset global iteration counter k = 0;\nwhile k < K, every agent asynchronously and continuously do\nselect ik ∈ [m + p] with Prob(ik = i) = qi ;\nperform an update to zik according to (35);\nupdate the global counter k ← k + 1;\n\nWhenever an agent updates a coordinate, the global iteration number k\nincreases by one. The kth update is applied to zik , with ik being independent\nrandom variables: zi = xi when i ≤ m and zi = si−m when i > m. Each\ncoordinate update has the form:\n\n(\n(35)\n\n= zikk −\nzik+1\nk\nzik+1 = zik ,\n\nηk\n(m+p)qik\n\n(ẑikk − (TCV ẑ k )ik ),\n\n∀i 6= ik ,\n\nwhere ηk is the step size, z k denotes the state of z in global memory just\nbefore the update (35) is applied, and ẑ k is the result that z in global memory\nis read by an agent to its local cache (see [54, §1.2] for both consistent and\ninconsistent cases). While (ẑikk −(TCV ẑ k )ik ) is being computed, asynchronous\nparallel computing allows other agents to make updates to z, introducing\nso-called asynchronous delays. Therefore, ẑ k can be different from z k . We\nrefer the reader to [54, §1.2] for more details.\nThe async-parallel algorithm using the overlapping-block coordinate update (33) is in Algorithm 2 (recall that the overlapping-block coordinate\n\n\fCoordinate friendly structures, algorithms, and applications\n\n29\n\nupdate is introduced to save computation).\nAlgorithm 2: Async-parallel primal-dual overlapping-block coordinate update algorithm using TCV\nInput :P\nz 0 ∈ F, K > 0, a discrete distribution (q1 , . . . , qm ) with\nm\ni=1 qi = 1 and qi > 0, ∀i,\nset global iteration counter k = 0;\nwhile k < K, every agent asynchronously and continuously do\nselect ik ∈ [m] with Prob(ik = i) = qi ;\nCompute s̃k+1\n, ∀j ∈ J(ik ) and x̃k+1\naccording to (33);\nj\nik\nηk\nk+1\nk+1\nk\nk\nupdate xik = xik + mqi (x̃ik − xik );\nk\n\nlet xk+1\n= xki for i 6= ik ;\ni\ni,j ηk\nupdate sk+1\n= skj + ρmq\n(s̃k+1\n− skj ), for all j ∈ J(ik );\nj\nj\ni\nk\n\n/ J(ik );\nlet sk+1\n= skj , for all j ∈\nj\nupdate the global counter k ← k + 1;\nHere we still allow asynchronous delays, so x̃ik and s̃k+1\nare computed\nj\nk\nusing some ẑ .\nRemark 5. If shared memory is used, it is recommended to set all but one\nρi,j ’s to 0 for each i.\nTheorem 1. Let Z ∗ be the set of solutions to problem (24) and (z k )k≥0 ⊂ F\nbe the sequence generated by Algorithm 1 or Algorithm 2 under the following\nconditions:\n(i) f, g, h∗ are closed proper convex functions, f is differentiable, and ∇f\nis Lipschitz continuous with constant β;\n(ii) the delay for every coordinate is bounded by a positive number τ , i.e.\nk−dk\nfor every 1 ≤ i ≤ m + p, ẑik = zi i for some 0 ≤ dki ≤ τ ;\n(iii) ηk ∈ [ηmin , ηmax ] for certain ηmin , ηmax > 0.\nThen (z k )k≥0 converges to a Z ∗ -valued random variable with probability 1.\nThe formulas for ηmin and ηmax , as well as the proof of Theorem 1, are\ngiven in Appendix D along with additional remarks. The algorithms can be\napplied to solve problem (24). A variety of examples are provided in §5.1\nand §5.2.\n\n\f30\n\n5. Applications\nIn this section, we provide examples to illustrate how to develop coordinate\nupdate algorithms based on CF operators. The applications are categorized\ninto five different areas. The first subsection discusses three well-known machine learning problems: empirical risk minimization, Support Vector Machine (SVM), and group Lasso. The second subsection discusses image processing problems including image deblurring, image denoising, and Computed Tomography (CT) image recovery. The remaining subsections provide\napplications in finance, distributed computing as well as certain stylized optimization models. Several applications are treated with coordinate update\nalgorithms for the first time.\nFor each problem, we describe the operator T and how to efficiently\ncalculate (T x)i . The final algorithm is obtained after plugging the update\nin a coordinate update framework in §1.1 along with parameter initialization,\nan index selection rule, as well as some termination criteria.\n5.1. Machine Learning\n5.1.1. Empirical Risk Minimization (ERM). We consider the following regularized empirical risk minimization problem\np\n\n(36)\n\nminimize\nm\nx∈R\n\n1X\nφj (a>\nj x) + f (x) + g(x),\np\nj=1\n\nwhere aj ’s are sample vectors, φj ’s are loss functions, and f + g is a regularization function. We assume that f is differentiable and g is proximable.\nExamples of (36) include linear SVM, regularized logistic regression, ridge\nregression, and Lasso. Further information on ERM can be found in [34]. The\nneed for coordinate update algorithms arises in many applications of (36)\nwhere the number of samples or the dimension of x is large.\n1 Pp\n>\n>\nWe define A = [a>\n1 ; a2 ; . . . ; ap ] and h(y) := p\nj=1 φj (yj ). Hence,\n1 Pp\n>\nh(Ax) = p j=1 φj (aj x), and problem (36) reduces to form (24). We can\napply the primal-dual update scheme to solve this problem, for which we introduce the dual variable s = (s1 , ..., sp )> . We use p + 1 coordinates, where\nthe 0th coordinate is x ∈ Rm and the jth coordinate is sj , j ∈ [p]. The\n\n\fCoordinate friendly structures, algorithms, and applications\n\n31\n\noperator T is given in (29). At each iteration, a coordinate is updated:\n\n(37)\n\n\nif x is chosen (the index 0), then compute\n\n\n\n\nxk+1 = proxηg (xk − η(∇f (xk ) + A> sk )),\n\n\n if s is chosen (an index j ∈ [p]), then compute\nj\nx̃k+1 = proxηg (xk − η(∇f (xk ) + A> sk )),\n\n\n\n\nand\n\n\n\nk+1 − xk )).\nsk+1\n= p1 proxpγφ∗j (pskj + pγa>\nj (2x̃\nj\n\nWe maintain A> s ∈ Rm in the memory. Depending on the structure of\n∇f , we can compute it each time or maintain it. When proxg ∈ F1 , we\ncan consider breaking x into coordinates xi ’s and also select an index i to\nupdate xi at each time.\n5.1.2. Support Vector Machine. Given the training data {(ai , βi )}m\ni=1\nwith βi ∈ {+1, −1}, ∀i, the kernel support vector machine [65] is\nminimize\n(38)\n\nx,ξ,y\n\nsubject to\n\nPm\n1\n2\ni=1 ξi ,\n2 kxk2 + C\nβi (x> φ(ai ) − y) ≥ 1\n\n− ξi , ξi ≥ 0, ∀i ∈ [m],\n\nwhere φ is a vector-to-vector map, mapping each data ai to a point in a\n(possibly) higher-dimensional space. If φ(a) = a, then (38) reduces to the\nlinear support vector machine. The model (38) can be interpreted as finding\na hyperplane {w : x> w − y = 0} to separate two sets of points {φ(ai ) : βi =\n1} and {φ(ai ) : βi = −1}.\nThe dual problem of (38) is\n(39)\n\nX\n1\nminimize s> Qs − e> s, subject to 0 ≤ si ≤ C, ∀i,\nβi si = 0,\ns\n2\ni\n\nwhere Qij = βi βj k(ai , aj ), k(·, ·) is a so-called kernel function, and e =\n(1, ..., 1)> . If φ(a) = a, then k(ai , aj ) = a>\ni aj .\nUnbiased case. If y = 0 is enforced in (38), then the solution hyperplane\n{w : x> w = 0} passes through the origin and is called unbiased. Consequently, the dual problem (39) will no longer have the linear constraint\nP\ni βi si = 0, leaving it with the coordinate-wise separable box constraints\n0 ≤ si ≤ C. To solve (39), we can apply the FBS operator T defined by (14).\n\n\f32\nLet d(s) := 12 s> Qs − e> s, A = prox[0,C] , and C = ∇d. The coordinate update based on FBS is\nsk+1\n= proj[0,C] (ski − γi ∇i d(sk )),\ni\nwhere we can take γi =\n\n1\nQii .\n\nBiased (general) case. In this case, the mode (38) has y ∈ R, so the\nhyperplane {w : x> w − y = 0} may not pass the origin and\nP is called biased.\nThen, the dual problem (39) retains the linear constraint i βi si = 0. In this\ncase, we apply the primal-dual splitting scheme (28) or the three-operator\nsplitting scheme (13).\nThe coordinate update based on the full primal-dual splitting scheme (28)\nis:\n(40a)\n\ntk+1 = tk + γ\n\n(40b)\n\nsk+1\ni\n\nm\nX\n\nβi ski ,\n\ni=1\n\n\u0010\n\u0001\u0011\n= proj[0,C] ski − η ∇i d(sk ) + βi (2tk+1 − tk ) ,\n\nwhere t, sPare the primal and dual variables, respectively. Note that we can\nlet w := m\ni=1 βi si and maintain it. With variable w and substituting (40a)\ninto (40b), we can equivalently write (40) into\n\n(41)\n\n\nif t is chosen (the index 0), then compute\n\n\n\n\ntk+1 = tk + γwk ,\n\nif si is chosen (an index i ∈ [m]), then compute\n\u0001\u0001\n\nk+1\nk − η q > sk − 1 + β (2γw k + tk )\n\ns\ns\n=\nproj\n\ni\n[0,C]\ni\ni\ni\n\n\nwk+1 = wk + βi (sk+1\n− ski ).\ni\n\nWe can also apply the\nP three-operator splitting (13) as follows. Let D1 :=\n[0, C]m and D2 := {s : m\ni=1 βi si = 0}. Let A = projD2 , B = projD1 , and\nC(x) = Qx − e, The full update corresponding to T = (I − ηk )I + ηk T3S is\n(42a)\n(42b)\n\nsk+1 =projD2 (uk ),\n\u0010\n\u0011\n\u0001\nuk+1 =uk + ηk projD1 2sk+1 − uk − γ(Qsk+1 − e) − sk+1 ,\n\nwhere s is just an intermediate variable. Let β̃ :=\n\nβ\nkβk2\n\nand w := β̃ > u. Then\n\nprojD2 (u) = (I − β̃ β̃ > )u. Hence, sk+1 = uk − wk β̃. Plugging it into (42b)\n\n\fCoordinate friendly structures, algorithms, and applications\n\n33\n\nyields the following coordinate update scheme:\n\n\nif i ∈ [m] is chosen, then compute\n\n\n\n sk+1 = uk − wk β̃i ,\ni\ni\n\u0010\n\u0010\n\u0011\n\u0001\u0011\nk+1\nk+1\nk+1\nk+η\nk − γ q > uk − w k (q > β̃) − 1\nu\n=\nu\nproj\n2s\n−\nu\n−\ns\n\nk\n[0,C]\ni\ni\ni\ni\ni\ni\ni\n\n\n\nwk+1 = wk + β̃i (uk+1\n− uki ),\ni\nwhere wk is the maintained variable and sk is the intermediate variable.\n5.1.3. Group Lasso.\n(43)\n\nThe group Lasso regression problem [84] is\nminimize\nf (x) +\nn\nx∈R\n\nm\nX\n\nλi kxi k2 ,\n\ni=1\n\nwhere f is a differentiable convex function, often bearing the form 12 kAx −\nbk22 , and xi ∈ Rni is a subvector of x ∈ Rn supported on Ii ⊂ [n], and\n∪i Ii = [n]. If Ii ∩ Ij = ∅, ∀i 6= j, it is called non-overlapping group Lasso,\nand if there are two different groups Ii and Ij with a non-empty intersection,\nit is called overlapping group Lasso. The model finds a coefficient vector x\nthat minimizes the fitting (or loss) function f (x) and that is group sparse:\nall but a few xi ’s are zero.\nLet Ui be formed by the columns of the identity matrix I corresponding\n> ] ∈ R(Σi ni )×n . Then, x = U > x.\nto the indices in Ii , and let U = [U1> ; . . . ; Um\ni\nPm i\nLet hi (yi ) = λi kyi k2 , yi ∈ Rni for i ∈ [m], and h(y) =\nh\n(y\n)\nfor\ni\ni\ni=1\ny = [y1 ; . . . ; ym ] ∈ RΣi ni . In this way, (43) becomes\n(44)\n\nminimize f (x) + h(U x).\nx\n\nNon-overlapping case [84]. In this case, we have Ii ∩ Ij = ∅, ∀i 6= j,\nand can apply the FBS scheme (14) to (44). Specifically, let T1 = ∂(h ◦ U )\nand T2 = ∇f . The FBS full update is\nxk+1 = JγT1 ◦ (I − γT2 )(xk ).\nThe corresponding coordinate update is the following\n\n\u001a\n(45)\n\nif i ∈ [m] is chosen, then compute\nxk+1\n= arg minxi 21 kxi − xki + γi ∇i f (xk )k22 + γi hi (xi ),\ni\n\n\f34\nwhere ∇i f (xk ) is the partial derivative of f with respect to xi and the step\n1\nsize can be taken to be γi = kA:,i\nk2 . When ∇f is either cheap or easy-tomaintain, the coordinate update in (45) is inexpensive.\nOverlapping case [38]. This case allows Ii ∩ Ij 6= ∅ for some i 6= j,\ncausing the evaluation of JγT1 to be generally difficult. However, we can\napply the primal-dual update (28) to this problem as\n(46a)\n\nsk+1 = proxγh∗ (sk + γU xk ),\n\n(46b)\n\nxk+1 = xk − η(∇f (xk ) + U > (2sk+1 − sk )),\n\nwhere s is the dual variable. Note that\n\u001a\n0,\nif ksi k2 ≤ λi , ∀i,\n∗\nh (s) =\n+∞, otherwise,\nis cheap. Hence, the corresponding coordinate update of (46) is\n(47)\n\nif si is chosen for some i ∈ [m], then compute\n\n\n\n sk+1 = projB (sk + γxk )\ni\ni\ni\nλi\nif\nx\nis\nchosen\nfor\nsome\ni\n∈\n[m], then compute\ni\n\n\u0010\n\u0011\n\n\n xk+1 = xk − η U T ∇f (xk ) + U T P T\nk + γxk ) − sk ) ,\nU\n(2proj\n(s\nBλj j\ni\ni\ni\nj\nj\nj,Ui Uj 6=0 j\ni\nwhere Bλ is the Euclidean ball of radius λ. When ∇f is easy-to-maintain,\nthe coordinate update in (47) is inexpensive. To the best of our knowledge,\nthe coordinate update method (47) is new.\n5.2. Imaging\n5.2.1. DRS for Image Processing in the Primal-dual Form [50].\nMany convex image processing problems have the general form\nminimize f (x) + g(Ax),\nx\n\nwhere A is a matrix such as a dictionary, sampling operator, or finite difference operator. We can reduce the problem to the system: 0 ∈ A(z) + B(z),\nwhere z = [x; s],\n\u0014\n\u0015\n\u0014\n\u0015\u0014 \u0015\n∂f (x)\n0\nA> x\nA(z) :=\n, and B(z) :=\n.\n∂g ∗ (s)\ns\n−A\n0\n\n\fCoordinate friendly structures, algorithms, and applications\n\n35\n\n(see Appendix C for the reduction.) The work [50] gives their resolvents\nJγA\nJγB\n\n\u0014\nproxγf\n=\n0\n\n0\nproxγg∗\n\u0014\n0\n−1\n= (I + γB) =\n0\n\n\u0015\n,\n\n\u0015 \u0014 \u0015\n\u0014\n\u0015>\n0\nI\nI\n2 >\n−1\n+\n(I + γ A A)\n,\nI\nγA\n−γA\n\nwhere JγA is often cheap or separable and we can explicitly form JγB as\na matrix or implement it based on a fast transform. With the defined JγA\nand JγB , we can apply the RPRS method as z k+1 = TRPRS z k . The resulting RPRS operator is CF when JγB is CF. Hence, we can derive a new\nRPRS coordinate update algorithm. We leave the derivation to the readers. Derivations of coordinate update algorithms for more specific image\nprocessing problems are shown in the following subsections.\n5.2.2. Total Variation Image Processing.\nTotal Variation (TV) image processing model\n(48)\n\nWe consider the following\n\n1\nminimize λkxkTV + kA x − bk2 ,\nx\n2\n\nwhere x ∈ Rn is the vector representation of the unknown image, A is an\nm × n matrix describing the transformation from the image to the measurements b. Common A includes sampling matrices in MRI, CT, denoising, deblurring, etc. Let (∇hi , ∇vi ) be the discrete gradient at pixel i and\n∇x = (∇h1 x, ∇v1 x, . . . , ∇hn x, ∇vn x)> . Then the TV semi-norm k · kTV in the\nisotropic and anisotropic fashions are, respectively,\n(49a)\n\nkxkTV =\n\nXq\n\n(∇hi x)2 + (∇vi x)2 ,\n\ni\n\n(49b)\n\nkxkTV = k∇xk1 =\n\nX\u0010\n\n\u0011\n|∇hi x| + |∇vi x| .\n\ni\n\nFor simplicity, we use the anisotropic TV for analysis and in the numerical experiment in § 6.2. It is slightly more complicated for the isotropic TV.\nIntroducing the following notation\n\n\u0012 \u0013\n∇\nB: =\n,\nA\n\n1\nh(p, q): = λkpk1 + kq − bk2 ,\n2\n\n\f36\nwe can reformulate (48) as\nminimize h(B x) = h(∇x, A x),\nx\n\nwhich reduces to the form of (24) with f = g = 0. Based on its definition,\nthe convex conjugate of h(p, q) and its proximal operator are, respectively,\n(50)\n(51)\n\n1\n1\nh∗ (s, t) = ιk·k∞ ≤λ (s) + kt + bk2 − kbk2 ,\n2\n2\n1\nproxγh∗ (s, t) = projk·k∞ ≤λ (s) +\n(t − γb).\n1+γ\n\nLet s, t be the dual variables corresponding to ∇x and Ax respectively, then\nusing (51) and applying (29) give the following full update:\n(52a)\n(52b)\n(52c)\n\nxk+1 = xk − η(∇> sk + A> tk ),\n\u0010\n\u0011\nsk+1 = projk·k∞ ≤λ sk + γ∇(xk − 2η(∇> sk + A> tk )) ,\n\u0011\n1 \u0010k\ntk+1 =\nt + γA(xk − 2η(∇> sk + A> tk )) − γb .\n1+γ\n\nTo perform the coordinate updates as described in §4, we can maintain ∇> sk\nand A> tk . Whenever a coordinate of (s, t) is updated, the corresponding\n∇> sk (or A> tk ) should also be updated. Specifically, we have the following\ncoordinate update algorithm\n\nif xi is chosen for some i ∈ [n], then compute\n\n\n\n\n\n= xki − η(∇> sk + A> tk )i ;\nxk+1\n\ni\n\n\n\n if si is chosen for some i ∈ [2n], then compute\n\u0001\n\n\n= projk·k∞ ≤λ ski + γ∇i (xk − 2η(∇> sk + A> tk ))\nsk+1\ni\n(53)\nand update ∇> sk to ∇> sk+1 ;\n\n\n\n\nif ti is chosen for some i ∈ [m], then compute\n\n\n\u0001\n\n1\n\n\ntki + γAi,: (xk − 2η(∇> sk + A> tk )) − γbi\ntk+1\n= 1+γ\n\ni\n\n\nand update A> tk to A> tk+1 .\n5.2.3. 3D Mesh Denoising. Following an example in [60], we consider\nY\nZ\na 3D mesh described by their nodes x̄i = (x̄X\ni , x̄i , x̄i ), i ∈ [n], and the\nadjacency matrix A ∈ Rn×n , where Aij = 1 if nodes i and j are adjacent,\notherwise Aij = 0. We let Vi be the set of neighbours of node i. Noisy mesh\nnodes zi , i ∈ [n], are observed. We try to recover the original mesh nodes by\n\n\fCoordinate friendly structures, algorithms, and applications\n\n37\n\nsolving the following optimization problem [60]:\n(54)\n\nminimize\nx\n\nn\nX\n\nfi (xi ) +\n\ni=1\n\nn\nX\n\ngi (xi ) +\n\ni=1\n\nXX\ni\n\nhi,j (xi − xj ),\n\nj∈Vi\n\nwhere fi ’s are differentiable data fidelity terms, gi ’s are the indicator funcP P\ntions of box constraints, and i j∈Vi hi,j (xi − xj ) is the total variation on\nthe mesh.\nWe introduce a dual variable s with coordinates si,j , for all ordered\npairs of adjacent nodes (i, j), and, based on the overlapping-block coordinate\nupdating scheme (33), perform coordinate update:\n\n\nselect i from [n], then compute\n\n\n\nk\nk\nk\n\ns̃k+1\n\ni,j = proxγh∗i,j (si,j + γxi − γxj ), ∀j ∈ Vi ,\n\n\n\nk\nk\nk\n\ns̃k+1\n\nj,i = proxγh∗j,i (sj,i + γxj − γxi ), ∀j ∈ Vi ,\nand update\nP\n\nk+1\nk\nk\n\n\nxi k+1 = proxηgi (xki − η(∇fi (xki ) + j∈Vi (2s̃k+1\n\ni,j − 2s̃j,i − si,j + sj,i ))),\n\n\n1 k+1\nk\nk\n\nsk+1\n\ni,j = si,j + 2 (s̃i,j − si,j ), ∀j ∈ Vi ,\n\n\n1 k+1\nk\nk\nsk+1\nj,i = sj,i + 2 (s̃j,i − sj,i ), ∀j ∈ Vi .\n5.3. Finance\n5.3.1. Portfolio Optimization. Assume that we have one unit of capital\nand m assets to invest on. The ith asset has an expected return rate ξi ≥ 0.\nOur goal is to find a portfolio with the minimal risk such that the expected\nreturn is no less than c. This problem can be formulated as\n1 >\nx Qx,\n2\nm\nm\nX\nX\nsubject to x ≥ 0,\nxi ≤ 1,\nξi xi ≥ c,\n\nminimize\nx\n\ni=1\n\ni=1\n\nwhere the objective function is a measure of risk, and the last constraint\n√\n√\nimposes that the expected return is at least c. Let a1 = e/ m, b1 = 1/ m,\na2 = ξ/kξk2 , and b2 = c/kξk2 , where e = (1, . . . , 1)> , ξ = (ξ1 , . . . , ξm )> . The\nabove problem is rewritten as\n(55)\n\n1\n>\nminimize x> Qx, subject to x ≥ 0, a>\n1 x ≤ b1 , a2 x ≥ b2 .\nx\n2\n\n\f38\nWe apply the three-operator splitting scheme (13) to (55). Let f (x) =\n1 >\n2 x Qx,\n{x : a>\n1x\n\n>\nD1 = {x : x ≥ 0}, D2 = {x : a>\n1 x ≤ b1 , a2 x ≥ b2 }, D21 =\n\n= b1 }, and D22 = {x : a>\n2 x = b2 }. Based on (13), the full update is\n\n(56a)\n\ny k+1 =projD2 (xk ),\n\n(56b)\n\n\u0001\nxk+1 =xk + ηk projD1 (2y k+1 − xk − γ∇f (y k+1 )) − y k+1 ,\n\nwhere y is an intermediate variable. As the projection to D1 is simple, we\ndiscuss how to evaluate the projection to D2 . Assume that a1 and a2 are\nneither perpendicular nor co-linear, i.e., a>\n1 a2 6= 0 and a1 6= λa2 for any\nscalar λ. In addition, assume a>\n1 a2 > 0 for simplicity. Let a3 = a2 −\nb3 = b2 −\n\n1\nb1 ,\na>\n1 a2\n\na4 = a1 −\n\n1\na2 ,\na>\n1 a2\n\nand b4 = b1 −\n\n1\nb2 .\na>\n1 a2\n\n1\na1 ,\na>\n1 a2\n\nThen we can\n\npartition the whole space into four areas by the four hyperplanes a>\ni x = bi ,\n>\ni = 1, . . . , 4. Let Pi = {x : a>\ni x ≤ bi , ai+1 x ≥ bi+1 }, i = 1, 2, 3 and P4 = {x :\n>\na>\n4 x ≤ b4 , a1 x ≥ b1 }. Then\n\n\nx,\n\n\n\nprojD22 (x),\nprojD2 (x) =\nprojD21 ∩D22 (x),\n\n\n\nprojD21 (x),\n\nif\nif\nif\nif\n\nx ∈ P1 ,\nx ∈ P2 ,\nx ∈ P3 ,\nx ∈ P4 .\n\nLet wi = a>\ni x − bi , i = 1, 2, and maintain w1 , w2 . Let ã2 =\nã1 =\n\na1 −a2 (a>\n1 a2 )\n2 .\n1−(a>\n1 a2 )\n\nThen\n\nprojD21 (x) = x − w1 a1 ,\nprojD22 (x) = x − w2 a2 ,\nprojD21 ∩D22 (x) = x − w1 ã1 − w2 ã2 ,\n\na2 −a1 (a>\n1 a2 )\n2 ,\n1−(a>\n1 a2 )\n\n\fCoordinate friendly structures, algorithms, and applications\n\n39\n\nHence, the coordinate update of (56) is\n(57a)\nxk ∈ P1 : xk+1\n=(1 − ηk )xki + ηk max(0, xki − γqi> xk ),\ni\nxk ∈ P2 : xk+1\n=(1 − ηk )xki + ηk w2k (a2 )i + ηk max (0,\ni\n\u0001\nxki − γqi> xk − w2k (2(a2 )i − γqi> a2 ) ,\n(57b)\n\u0001\nxk ∈ P3 : xk+1\n=(1 − ηk )xki + ηk w1k (ã1 )i + w2k (ã2 )i + ηk max (0,\ni\n\n\u0001\nxki − γqi> xk − w1k (2(ã1 )i − γqi> ã1 ) − w2k (2(ã2 )i − γqi> ã2 ) ,\n\n(57c)\n\nxk ∈ P4 : xk+1\n=(1 − ηk )xki + ηk w1k (a1 )i + ηk max (0,\ni\n\u0001\n(57d)\nxki − γqi> xk − w1k (2(a1 )i − γqi> a1 ) ,\nwhere qi is the ith column of Q. At each iteration, we select i ∈ [m], and\nperform an update to xi according to (57) based on where xk is. We then\nrenew wjk+1 = wjk +aij (xk+1\n−xki ), j = 1, 2. Note that checking xk in some Pj\ni\nrequires only O(1) operations by using w1 and w2 , so the coordinate update\nin (57) is inexpensive.\n5.4. Distributed Computing\n5.4.1. Network. Consider that m worker agents and one master agent\nform a star-shaped network, where the master agent at the center connects\nto each of the worker agents. The m + 1 agents collaboratively solve the\nconsensus problem:\nm\nX\nfi (x),\nminimize\nx\n\ni=1\n\nRd\n\nwhere x ∈\nis the common variable and each proximable function fi is\nheld privately by agent i. The problem can be reformulated as\n(58)\n\nminimize d F (x) :=\n\nx1 ,...,xm ,y∈R\n\nm\nX\n\nfi (xi ),\n\nsubject to xi = y, ∀i ∈ [m],\n\ni=1\n\nwhich has the KKT condition\n \n\n  \n0 0\nI\nx\nx\n∂F 0 0\n>\n\n\n\n\n\n\n\ny + 0 0 −e\ny ,\n(59)\n0∈ 0 0 0\ns\ns\n0 0 0\nI −e\n0\n{z\n}\n|\n|\n{z\n}\noperator A\noperator C\n\n\f40\nwhere s is the dual variable.\nApplying the FBFS scheme (18) to (59) yields the following full update:\n\n(60a)\n(60b)\n\nxk+1\n= proxγfi (xki − γski ) + γ 2 xki − γ 2 y k − 2γski ,\ni\nm\nm\nX\nX\nk+1\n2 k\nk\n2\ny\n= (1 + mγ )y + 3γ\nsj − γ\nxkj ,\nj=1\n\n(60c)\n\nj=1\n\nsk+1\n= ski − 2γxki − γproxγfi (xki − γski ) + 3γy k + γ 2\ni\n\nm\nX\n\nskj ,\n\nj=1\n\nwhere (60a) and (60c) are applied to all i ∈ [m]. Hence, for each i, we\ngroup xi and si together and assign them on agent i. We let the master\nP\nP\nagent maintain j sj and j xj . Therefore, in the FBFS coordinate update,\nP\nupdating any (xi , si ) needs only y and j sj from the master agent, and\nupdating y is done on the master agent. In synchronous parallel setting, at\n, then the master\n, xk+1\neach iteration, each worker agent i computes sk+1\ni\ni\nagent collects the updates from all of the worker agents and then updates\nP\ny and j sj . The above update can be relaxed to be asynchronous. In this\ncase, the master and worker agents work concurrently, the master agent\nP\nupdates y and j sj as soon as it receives the updated si and xi from any\nof the worker agents. It also periodically broadcasts y back to the worker\nagents.\n5.5. Dimension Reduction\n5.5.1. Nonnegative Matrix Factorization. Nonnegative Matrix Factorization (NMF) is an important dimension reduction method for nonnegative data. It was proposed by Paatero and his coworkers in [51]. Given a\nnonnegative matrix A ∈ Rp×n\n+ , NMF aims at finding two nonnegative matrip×r\nn×r\nces W ∈ R+ and H ∈ R+ such that W H > ≈ A, where r is user-specified\ndepending on the applications, and usually r \u001c min(p, n). A widely used\nmodel is\n\n(61)\n\n1\nminimize F (W, H) := kW H > − Ak2F ,\nW,H\n2\nn×r\nsubject to W ∈ Rp×r\n+ , H ∈ R+ .\n\n\fCoordinate friendly structures, algorithms, and applications\n\n41\n\nApplying the projected gradient method (21) to (61), we have\n\u0001\n(62a)\nW k+1 = max 0, W k − ηk ∇W F (W k , H k ) ,\n\u0001\n(62b)\nH k+1 = max 0, H k − ηk ∇H F (W k , H k ) .\nIn general, we do not know the Lipschitz constant of ∇F , so we have to\nchoose ηk by line search such that the Armijo condition is satisfied.\nPartitioning the variables into 2r block coordinates: (w1 , . . . , wr , h1 , . . . , hr )\nwhere wi and hi are the ith columns of W and H, respectively, we can apply\nthe coordinate update based on the projected-gradient method:\n\n(63)\n\n\nif wik is chosen for some ik ∈ [r], then compute\n\n\u0001\n\n\n= max 0, wikk − ηk ∇wik F (W k , H k ) ;\nwik+1\nk\nif hik −r is chosen for some ik ∈ {r + 1, ..., 2r}, then\n\n\n\u0001 compute\n\nk\nk, Hk) .\nhk+1\n=\nmax\n0,\nh\n−\nη\n∇\nF\n(W\nk hik −r\nik −r\nik −r\n\nIt is easy to see that ∇wi F (W k , H k ) and ∇hi F (W k , H k ) are both Lipschitz\ncontinuous with constants khki k22 and kwik k22 respectively. Hence, we can set\n\n(\nηk =\n\n1\n,\nkhkik k22\n1\n,\nkwikk −r k22\n\nif 1 ≤ ik ≤ r,\nif r + 1 ≤ ik ≤ 2r.\n\nHowever, it is possible to have wik = 0 or hki = 0 for some i and k, and\nthus the setting in the above formula may have trouble of being divided\nby zero. To overcome this problem, one can first modify the problem (61)\nby restricting W to have unit-norm columns and then apply the coordinate\nupdate method in (63). Note that the modification does not change the\noptimal value since W H > = (W D)(HD−1 )> for any r×r invertible diagonal\nmatrix D. We refer the readers to [82] for more details.\nNote that ∇W F (W, H) = (W H > −A)H, ∇H F (W, H) = (W H > −A)> W\nand ∇wi F (W, H) = (W H > − A)hi , ∇hi F (W, H) = (W H > − A)> wi , ∀i.\nTherefore, the coordinate updates given in (63) are computationally worthy\n(by maintaining the residual W k (H k )> − A).\n5.6. Stylized Optimization\n5.6.1. Second-Order Cone Programming (SOCP). SOCP extends\nLP by incorporating second-order cones. A second-order cone in Rn is\n\b\nQ = (x1 , x2 , . . . , xn ) ∈ Rn : k(x2 , . . . , xn )k2 ≤ x1 .\n\n\f42\nGiven a point v ∈ Rn , let ρv1 := k(v2 , . . . , vn )k2 and ρv2 := 12 (v1 + ρv1 ). Then,\nthe projectionv of v to Q returns 0 if v1 < −ρv1 , returns v if v1 ≥ ρv1 , and\nreturns (ρv2 , ρρ2v · (v2 , . . . , vn )) otherwise. Therefore, if we define the scalar\n1\ncouple:\n\nv1 < −ρv1 ,\n\n(0, 0),\n(ξ1v , ξ2v ) = (1, 1),\nv1 ≥ ρv1 ,\n\n v ρv2 \u0001\notherwise,\nρ2 , ρv ,\n1\n\u0001\nthen we have u = projQ (v) = ξ1v v1 , ξ2v · (v2 , . . . , vn ) . Based on this, we\nhave\nProposition 2.\n1. Let v ∈ Rn and v + := v + νei for any ν ∈ R. Then,\ngiven ρv1 , ρv2 , ξ1v , ξ2v defined above, it takes O(1) operations to obtain\n+\n+\n+\n+\nρv1 , ρv2 , ξ1v , ξ2v .\n2. Let v ∈ Rn and A = [a1 A2 ] ∈ Rm×n , where a1 ∈ Rm , A2 ∈ Rm×(n−1) .\nGiven ρv1 , ρv2 , ξ1v , ξ2v , we have\nA(2 · projQ (v) − v) = ((2ξ1v − 1)v1 ) · a1 + (2ξ2v − 1) · A2 (v2 , . . . , vn )> .\nBy the proposition, if T1 is an affine operator, then in the composition\nT1 ◦ projQ , the computation of projQ is cheap as long as we maintain\nρv1 , ρv2 , ξ1v , ξ2v .\nGiven x, c ∈ Rn , b ∈ Rm , and A ∈ Rm×n , the standard form of SOCP is\n(64a)\n\nminimize c> x, subject to Ax = b,\nx\n\nx ∈ X = Q1 × · · · × Qn̄ ,\n\n(64b)\n\nwhere each Qi is a second-order cone, and n̄ 6= n in general. The problem (64)\nis equivalent to\n\u0001\nminimize c> x + ιA·=b (x) + ιX (x),\nx\n\nto which we can apply the DRS iteration z k+1 = TDRS (z k ) (see (16)), in\nwhich JγA = projX and TγB is a linear operator given by\nJγB (x) = arg min c> y +\ny\n\n1\nky − xk2\n2γ\n\nsubject to Ay = b.\n\nAssume that the matrix A has full row-rank (otherwise, Ax = b has either\nredundant rows or no solution). Then, in (16), we have RγB (x) = Bx + d,\nwhere B := I − 2A> (AA> )−1 A and d := 2A> (AA> )−1 (b + γAc) − 2γc.\n\n\fCoordinate friendly structures, algorithms, and applications\n\n43\n\nIt is easy to apply coordinate updates to z k+1 = TDRS (z k ) following\nProposition 2. Specifically, by maintaining the scalars ρv1 , ρv2 , ξ1v , ξ2v for each\nv = xi ∈ Qi during coordinate updates, the computation of the projection\ncan be completely avoided. We pre-compute (AA> )−1 and cache the matrix\nB and vector d. Then, TDRS is CF, and we have the following coordinate\nupdate method\n\n(65)\n\n\n select i ∈ [n̄], then compute\nyik+1 = Bi xk + di\n\nxk+1\n= projQi (yik+1 ) + 12 (xki − yik+1 ),\ni\n\nwhere Bi ∈ Rni ×n is the ith row block submatrix of B, and yik+1 is the\nintermediate variable.\nIt is trivial to extend this method for SOCPs with a quadratic objective:\n1\nminimize c> x + x> Cx, subject to Ax = b, x ∈ X = Q1 × · · · × Qn̄ ,\nx\n2\nbecause J2 is still linear. Clearly, this method applies to linear programs as\nthey are special SOCPs.\nNote that many LPs and SOCPs have sparse matrices A, which deserve\nfurther investigation. In particular, we may prefer not to form (AA> )−1 and\nuse the results in §4.2 instead.\n\n6. Numerical Experiments\nWe illustrate the behavior of coordinate update algorithms for solving portfolio optimization, image processing, and sparse logistic regression problems.\nOur primary goal is to show the efficiency of coordinate update algorithms\ncompared to the corresponding full update algorithms. We will also illustrate\nthat asynchronous parallel coordinate update algorithms are more scalable\nthan their synchronous parallel counterparts.\nOur first two experiments run on Mac OSX 10.9 with 2.4 GHz Intel Core\ni5 and 8 Gigabytes of RAM. The experiments were coded in Matlab. The\nsparse logistic regression experiment runs on 1 to 16 threads on a machine\nwith two 2.5 Ghz 10-core Intel Xeon E5-2670v2 (20 cores in total) and\n64 Gigabytes of RAM. The experiment was coded in C++ with OpenMP\nenabled. We use the Eigen library7 for sparse matrix operations.\n7\n\nhttp://eigen.tuxfamily.org\n\n\f44\n6.1. Portfolio Optimization\nIn this subsection, we compare the performance of the 3S splitting scheme (56)\nwith the corresponding coordinate update algorithm (57) for solving the\nportfolio optimization problem (55). In this problem, our goal is to distribute our investment resources to all the assets so that the investment\nrisk is minimized and the expected return is greater than c. This test uses\ntwo datasets, which are summarized in Table 3. The NASDAQ dataset is\ncollected through Yahoo! Finance. We collected one year (from 10/31/2014\nto 10/31/2015) of historical closing prices for 2730 stocks.\n\nNumber of assets (N)\nExpected return rate\nAsset return rate\nRisk\n\nSynthetic data\n\nNASDAQ data\n\n1000\n0.02\n3 * rand(N, 1) - 1\ncovariance matrix + 0.01 · I\n\n2730\n0.02\nmean of 30 days return rate\npositive definite matrix\n\nTable 3: Two datasets for portfolio optimization\nIn our numerical experiments, for comparison purposes, we first obtain a\nhigh accurate solution by solving (55) with an interior point solver. For both\nfull update and coordinate update, ηk is set to 0.8. However, we use different\n2\n, and for 3S\nγ. For 3S full update, we used the step size parameter γ1 = kQk\n2\n2\ncoordinate update, γ2 = max{Q11 ,...,QN N } . In general, coordinate update can\nbenefit from more relaxed parameters. The results are reported in Figure 3.\nWe can observe that the coordinate update method converges much faster\nthan the 3S method for the synthetic data. This is due to the fact that γ2\nis much larger than γ1 . However, for the NASDAQ dataset, γ1 ≈ γ2 , so 3S\ncoordinate update is only moderately faster than 3S full update.\n6.2. Computed Tomography Image Reconstruction\nWe compare the performance of algorithm (52) and its corresponding coordinate version on Computed Tomography (CT) image reconstruction. We\ngenerate a thorax phantom of size 284 × 284 to simulate spectral CT measurements. We then apply the Siddon’s algorithm [66] to form the sinogram\ndata. There are 90 parallel beam projections and, for each projection, there\nare 362 measurements. Then the sinogram data is corrupted with Gaussian\nnoise. We formulate the image reconstruction problem in the form of (48).\nThe primal-dual full update corresponds to (52). For coordinate update, the\n\n\fCoordinate friendly structures, algorithms, and applications\n3S full up date vs 3S coordinate up date\n\n−2\n\n3S full up date vs 3S coordinate up date\n\n−2\n\n10\n\n45\n\n10\n\n−4\n\n10\n\n−3\n\n10\n\n−6\n\n10\n\nfk − f∗\n\nfk − f∗\n\n−8\n\n10\n\n−10\n\n10\n\n−4\n\n10\n\n−12\n\n10\n\n−5\n\n10\n\n−14\n\n10\n\n3S full update\n3S coordinate update\n\n−16\n\n10\n\n0\n\n20\n\n40\n\n60\nEp ochs\n\n3S full update\n3S coordinate update\n\n−6\n\n10\n80\n\n100\n\n(a) Synthesis dataset\n\n0\n\n20\n\n40\n\n60\nEp ochs\n\n80\n\n100\n\n(b) NASDAQ dataset\n\nFigure 3: Compare the convergence of 3S full update with 3S coordinate\nupdate algorithms.\n\nblock size for x is set to 284, which corresponds to a column of the image.\nThe dual variables s, t are also partitioned into 284 blocks accordingly. A\nblock of x and the corresponding blocks of s and t are bundled together as\na single block. In each iteration, a bundled block is randomly chosen and\nupdated. The reconstruction results are shown in Figure 4. After 100 epochs,\nthe image recovered by the coordinate version is better than that by (52).\nAs shown in Figure 4d, the coordinate version converges faster than (52).\n6.3. `1 Regularized Logistic Regression\nIn this subsection, we compare the performance of sync-parallel coordinate\nupdate and async-parallel coordinate update for solving the sparse logistic\nregression problem\n(66)\n\nminimize\nλkxk1 +\nn\nx∈R\n\nN\n\u0001\n1 X\nlog 1 + exp(−bj · a>\nj x) ,\nN\nj=1\n\nwhere {(aj , bj )}N\nj=1 is the set of sample-label pairs with bj ∈ {1, −1}, λ =\n0.0001, and n and N represent the numbers of features and samples, respec-\n\n\f46\n\n(a) Phantom image\n\n(b) Recovered by PDS\n10\n\n6\nPDS full update\nPDS coordinate update\n\n10 5\n\n10 4\n\n10 3\n\n10 2\n0\n\n(c) Recovered by PDS coord\n\n20\n\n40\n\n60\n\n80\n\n100\n\n(d) Objective function value\n\nFigure 4: CT image reconstruction.\n\ntively. This test uses the datasets8 : real-sim and news20, which are summarized in Table 4.\nWe let each coordinate hold roughly 50 features. Since the total number\nof features is not divisible by 50, some coordinates have 51 features. We let\neach thread draw a coordinate uniformly at random at each iteration. We\nstop all the tests after 10 epochs since they have nearly identical progress\nper epoch. The step size is set to ηk = 0.9, ∀k. Let A = [a1 , . . . , aN ]> and\nb = [b1 , ..., bN ]> . In global memory, we store A, b and x. We also store the\nproduct Ax in global memory so that the forward step can be efficiently\ncomputed. Whenever a coordinate of x gets updated, Ax is immediately\n8\n\nhttp://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/\n\n\fCoordinate friendly structures, algorithms, and applications\nName\nreal-sim\nnews20\n\n# samples\n72, 309\n19,996\n\n47\n\n# features\n20, 958\n1,355,191\n\nTable 4: Two datasets for sparse logistic regression.\nupdated at a low cost. Note that if Ax is not stored in global memory, every\ncoordinate update will have to compute Ax from scratch, which involves the\nentire x and will be very expensive.\nTable 5 gives the running times of the sync-parallel and async-parallel\nimplementations on the two datasets. We can observe that async-parallel\nachieves almost-linear speedup, but sync-parallel scales very poorly as we\nexplain below.\nIn the sync-parallel implementation, all the running threads have to\nwait for the last thread to finish an iteration, and therefore if a thread has a\nlarge load, it slows down the iteration. Although every thread is (randomly)\nassigned to roughly the same number of features (either 50 or 51 components\nof x) at each iteration, their ai ’s have very different numbers of nonzeros,\nand the thread with the largest number of nonzeros is the slowest. (Sparse\nmatrix computation is used for both datasets, which are very large.) As\nmore threads are used, despite that they altogether do more work at each\niteration, the per-iteration time may increase as the slowest thread tends\nto be slower. On the other hand, async-parallel coordinate update does not\nsuffer from the load imbalance. Its performance grows nearly linear with the\nnumber of threads.\nFinally, we have observed that the progress toward solving (66) is mainly\na function of the number of epochs and does not change appreciably when\nthe number of threads increases or between sync-parallel and async-parallel.\nTherefore, we always stop at 10 epochs.\n\n7. Conclusions\nWe have presented a coordinate update method for fixed-point iterations,\nwhich updates one coordinate (or a few variables) at every iteration and can\nbe applied to solve linear systems, optimization problems, saddle point problems, variational inequalities, and so on. We proposed a new concept called\nCF operator. When an operator is CF, its coordinate update is computationally worthy and often preferable over the full update method, in particular in\na parallel computing setting. We gave examples of CF operators and also discussed how the properties can be preserved by composing two or more such\noperators such as in operator splitting and primal-dual splitting schemes. In\n\n\f48\n\n# threads\n1\n2\n4\n8\n16\n\ntime\nasync\n81.6\n45.9\n21.6\n16.1\n7.1\n\nreal-sim\n(s)\nspeedup\nsync async sync\n82.1\n1.0\n1.0\n80.6\n1.8\n1.0\n63.0\n3.8\n1.3\n61.4\n5.1\n1.3\n46.4\n11.5\n1.8\n\ntime\nasync\n591.1\n304.2\n150.4\n78.3\n41.6\n\nnews20\n(s)\nspeedup\nsync async sync\n591.3\n1.0\n1.0\n590.1\n1.9\n1.0\n557.0\n3.9\n1.1\n525.1\n7.5\n1.1\n493.2\n14.2\n1.2\n\nTable 5: Running times of async-parallel and sync-parallel FBS implementations for `1 regularized logistic regression on two datasets. Sync-parallel\nhas very poor speedup due to the large distribution of coordinate sparsity\nand thus the large load imbalance across threads.\n\naddition, we have developed CF algorithms for problems arising in several\ndifferent areas including machine learning, imaging, finance, and distributed\ncomputing. Numerical experiments on portfolio optimization, CT imaging,\nand logistic regression have been provided to demonstrate the superiority\nof CF methods over their counterparts that update all coordinates at every\niteration.\n\nReferences\n[1] Attouch, H., Bolte, J., Redont, P., Soubeyran, A.: Proximal alternating minimization and projection methods for nonconvex problems: An\napproach based on the Kurdyka-Lojasiewicz inequality. Mathematics\nof Operations Research 35(2), 438–457 (2010)\n[2] Bahi, J., Miellou, J.C., Rhofir, K.: Asynchronous multisplitting methods for nonlinear fixed point problems. Numerical Algorithms 15(3-4),\n315–345 (1997)\n[3] Baudet, G.M.: Asynchronous iterative methods for multiprocessors. J.\nACM 25(2), 226–244 (1978).\n[4] Bauschke, H.H., Borwein, J.M.: On the convergence of von Neumann’s\nalternating projection algorithm for two sets. Set-Valued Analysis 1(2),\n185–212 (1993)\n[5] Bauschke, H.H., Combettes, P.L.: Convex analysis and monotone operator theory in Hilbert spaces. Springer Science & Business Media\n(2011)\n\n\fCoordinate friendly structures, algorithms, and applications\n\n49\n\n[6] Baz, D.E., Frommer, A., Spiteri, P.: Asynchronous iterations with flexible communication: contracting operators. Journal of Computational\nand Applied Mathematics 176(1), 91 – 103 (2005)\n[7] Baz, D.E., Gazen, D., Jarraya, M., Spiteri, P., Miellou, J.: Flexible\ncommunication for parallel asynchronous methods with application to\na nonlinear optimization problem. In: E. D’Hollander, F. Peters, G. Joubert, U. Trottenberg, R. Volpel (eds.) Parallel Computing Fundamentals, Applications and New Directions, Advances in Parallel Computing,\nvol. 12, pp. 429 – 436. North-Holland (1998)\n[8] Beck, A., Tetruashvili, L.: On the convergence of block coordinate descent type methods. SIAM Journal on Optimization 23(4), 2037–2060\n(2013)\n[9] Bengio, Y., Delalleau, O., Le Roux, N.: Label propagation and\nquadratic criterion. In: Semi-Supervised Learning, pp. 193–216. MIT\nPress (2006)\n[10] Bertsekas, D.P.: Distributed asynchronous computation of fixed points.\nMathematical Programming 27(1), 107–120 (1983)\n[11] Bertsekas, D.P.: Nonlinear programming. Athena Scientific (1999)\n[12] Bertsekas, D.P., Tsitsiklis, J.N.: Parallel and distributed computation:\nnumerical methods. Prentice hall Englewood Cliffs, NJ (1989)\n[13] Bolte, J., Sabach, S., Teboulle, M.: Proximal alternating linearized minimization for nonconvex and nonsmooth problems. Mathematical Programming 146(1-2), 459–494 (2014)\n[14] Bradley, J.K., Kyrola, A., Bickson, D., Guestrin, C.: Parallel coordinate\ndescent for l1-regularized loss minimization. In: Proceedings of the 28th\nInternational Conference on Machine Learning (ICML-11), pp. 321–328\n(2011)\n[15] Briceño-Arias, L.M.: Forward-Douglas–Rachford splitting and forwardpartial inverse method for solving monotone inclusions. Optimization\n64(5), 1239–1261 (2015)\n[16] Briceno-Arias, L.M., Combettes, P.L.: Monotone operator methods for\nNash equilibria in non-potential games. In: Computational and Analytical Mathematics, pp. 143–159. Springer (2013)\n[17] Chazan, D., Miranker, W.: Chaotic relaxation. Linear Algebra and its\nApplications 2(2), 199–222 (1969)\n\n\f50\n[18] Combettes, P.L., Condat, L., Pesquet, J.C., Vu, B.C.: A forwardbackward view of some primal-dual optimization methods in image recovery. In: Proceedings of the 2014 IEEE International Conference on\nImage Processing (ICIP), pp. 4141–4145 (2014)\n[19] Combettes, P.L., Pesquet, J.C.: Stochastic quasi-Fejér block-coordinate\nfixed point iterations with random sweeping. SIAM Journal on Optimization 25(2), 1221–1248 (2015).\n[20] Condat, L.: A primal–dual splitting method for convex optimization\ninvolving Lipschitzian, proximable and linear composite terms. Journal\nof Optimization Theory and Applications 158(2), 460–479 (2013)\n[21] Dang, C.D., Lan, G.: Stochastic block mirror descent methods for nonsmooth and stochastic optimization. SIAM Journal on Optimization\n25(2), 856–881 (2015).\n[22] Davis, D.: An o(n log(n)) algorithm for projecting onto the ordered\nweighted `1 norm ball. arXiv preprint arXiv:1505.00870 (2015)\n[23] Davis, D.: Convergence rate analysis of primal-dual splitting schemes.\nSIAM Journal on Optimization 25(3), 1912–1943 (2015)\n[24] Davis, D., Yin, W.: A three-operator splitting scheme and its optimization applications. arXiv preprint arXiv:1504.01032 (2015)\n[25] Dhillon, I.S., Ravikumar, P.K., Tewari, A.: Nearest neighbor based\ngreedy coordinate descent. In: Advances in Neural Information Processing Systems, pp. 2160–2168 (2011)\n[26] Douglas, J., Rachford, H.H.: On the numerical solution of heat conduction problems in two and three space variables. Transactions of the\nAmerican Mathematical Society 82(2), 421–439 (1956)\n[27] El Baz, D., Gazen, D., Jarraya, M., Spiteri, P., Miellou, J.C.: Flexible\ncommunication for parallel asynchronous methods with application to\na nonlinear optimization problem. Advances in Parallel Computing 12,\n429–436 (1998)\n[28] Fercoq, O., Bianchi, P.: A coordinate descent primal-dual algorithm\nwith large step size and possibly non separable functions. arXiv preprint\narXiv:1508.04625 (2015)\n[29] Frommer, A., Szyld, D.B.: On asynchronous iterations. Journal of Computational and Applied Mathematics 123(1-2), 201–216 (2000)\n\n\fCoordinate friendly structures, algorithms, and applications\n\n51\n\n[30] Gabay, D., Mercier, B.: A dual algorithm for the solution of nonlinear\nvariational problems via finite element approximation. Computers &\nMathematics with Applications 2(1), 17–40 (1976)\n[31] Glowinski, R., Marroco, A.: Sur l’approximation, par éléments finis\nd’ordre un, et la résolution, par pénalisation-dualité d’une classe de\nproblèmes de dirichlet non linéaires. Revue française d’automatique,\ninformatique, recherche opérationnelle. Analyse numérique 9(2), 41–76\n(1975)\n[32] Grippo, L., Sciandrone, M.: On the convergence of the block nonlinear\nGauss-Seidel method under convex constraints. Operations Research\nLetters 26(3), 127–136 (2000)\n[33] Han, S.: A successive projection method. Mathematical Programming\n40(1), 1–14 (1988)\n[34] Hastie, T., Tibshirani, R., Friedman, J., Franklin, J.: The elements of\nstatistical learning: data mining, inference and prediction. The Mathematical Intelligencer 27(2), 83–85 (2005)\n[35] Hildreth, C.: A quadratic programming procedure. Naval Research Logistics Quarterly 4(1), 79–85 (1957)\n[36] Hong, M., Wang, X., Razaviyayn, M., Luo, Z.Q.: Iteration complexity analysis of block coordinate descent methods. arXiv preprint\narXiv:1310.6957v2 (2015)\n[37] Hsieh, C.j., Yu, H.f., Dhillon, I.: PASSCoDe: Parallel asynchronous\nstochastic dual co-ordinate descent. In: Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pp. 2370–2379\n(2015)\n[38] Jacob, L., Obozinski, G., Vert, J.P.: Group lasso with overlap and graph\nlasso. In: Proceedings of the 26th International Conference on Machine\nLearning (ICML-09), pp. 433–440. ACM (2009)\n[39] Krasnosel’skii, M.A.: Two remarks on the method of successive approximations. Uspekhi Matematicheskikh Nauk 10(1), 123–127 (1955)\n[40] Lebedev, V., Tynjanskiı, N.: Duality theory of concave-convex games.\nIn: Soviet Math. Dokl, vol. 8, pp. 752–756 (1967)\n[41] Li, Y., Osher, S.: Coordinate descent optimization for `1 minimization\nwith application to compressed sensing; a greedy algorithm. Inverse\nProblems and Imaging 3(3), 487–503 (2009)\n\n\f52\n[42] Liu, J., Wright, S.J.: Asynchronous stochastic coordinate descent: Parallelism and convergence properties. SIAM Journal on Optimization\n25(1), 351–376 (2015)\n[43] Liu, J., Wright, S.J., Ré, C., Bittorf, V., Sridhar, S.: An asynchronous\nparallel stochastic coordinate descent algorithm. Journal of Machine\nLearning Research 16, 285–322 (2015)\n[44] Lu, Z., Xiao, L.: On the complexity analysis of randomized blockcoordinate descent methods. Mathematical Programming 152(1-2),\n615–642 (2015).\n[45] Luo, Z.Q., Tseng, P.: On the convergence of the coordinate descent\nmethod for convex differentiable minimization. Journal of Optimization\nTheory and Applications 72(1), 7–35 (1992)\n[46] McLinden, L.: An extension of Fenchel’s duality theorem to saddle\nfunctions and dual minimax problems. Pacific Journal of Mathematics\n50(1), 135–158 (1974)\n[47] Nedić, A., Bertsekas, D.P., Borkar, V.S.: Distributed asynchronous incremental subgradient methods. Studies in Computational Mathematics 8, 381–407 (2001)\n[48] Nesterov, Y.: Efficiency of coordinate descent methods on huge-scale\noptimization problems. SIAM Journal on Optimization 22(2), 341–362\n(2012)\n[49] Nutini, J., Schmidt, M., Laradji, I., Friedlander, M., Koepke, H.: Coordinate descent converges faster with the Gauss-Southwell rule than\nrandom selection. In: Proceedings of the 32nd International Conference\non Machine Learning (ICML-15), pp. 1632–1641 (2015)\n[50] O’Connor, D., Vandenberghe, L.: Primal-dual decomposition by operator splitting and applications to image deblurring. SIAM Journal on\nImaging Sciences 7(3), 1724–1754 (2014)\n[51] Paatero, P., Tapper, U.: Positive matrix factorization: A non-negative\nfactor model with optimal utilization of error estimates of data values.\nEnvironmetrics 5(2), 111–126 (1994)\n[52] Passty, G.B.: Ergodic convergence to a zero of the sum of monotone\noperators in Hilbert space. Journal of Mathematical Analysis and Applications 72(2), 383–390 (1979)\n\n\fCoordinate friendly structures, algorithms, and applications\n\n53\n\n[53] Peaceman, D.W., Rachford Jr, H.H.: The numerical solution of\nparabolic and elliptic differential equations. Journal of the Society for\nIndustrial and Applied Mathematics 3(1), 28–41 (1955)\n[54] Peng, Z., Xu, Y., Yan, M., Yin, W.: ARock: an algorithmic framework for asynchronous parallel coordinate updates. ArXiv e-prints\narXiv:1506.02396 (2015)\n[55] Peng, Z., Yan, M., Yin, W.: Parallel and distributed sparse optimization. In: Proceedings of the 2013 Asilomar Conference on Signals, Systems and Computers, pp. 659–646 (2013)\n[56] Pesquet, J.C., Repetti, A.: A class of randomized primal-dual algorithms for distributed optimization. Journal of Nonlinear and Convex\nAnalysis 16(12), 2453–2490 (2015)\n[57] Polak, E., Sargent, R., Sebastian, D.: On the convergence of sequential\nminimization algorithms. Journal of Optimization Theory and Applications 12(6), 567–575 (1973)\n[58] Razaviyayn, M., Hong, M., Luo, Z.Q.: A unified convergence analysis\nof block successive minimization methods for nonsmooth optimization.\nSIAM Journal on Optimization 23(2), 1126–1153 (2013)\n[59] Recht, B., Re, C., Wright, S., Niu, F.: Hogwild: A lock-free approach\nto parallelizing stochastic gradient descent. In: Advances in Neural\nInformation Processing Systems, pp. 693–701 (2011)\n[60] Repetti, A., Chouzenoux, E., Pesquet, J.C.: A random block-coordinate\nprimal-dual proximal algorithm with application to 3d mesh denoising.\nIn: Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3561–3565 (2015)\n[61] Richtárik, P., Takáč, M.: Iteration complexity of randomized blockcoordinate descent methods for minimizing a composite function. Mathematical Programming 144(1-2), 1–38 (2014)\n[62] Richtárik, P., Takáč, M.: Parallel coordinate descent methods for\nbig data optimization. Mathematical Programming 156(1), 433–484\n(2016).\n[63] Rockafellar, R.T.: Convex analysis. Princeton University Press (1997)\n[64] Rue, H., Held, L.: Gaussian Markov random fields: theory and applications. CRC Press (2005)\n\n\f54\n[65] Scholkopf, B., Smola, A.J.: Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press (2001)\n[66] Siddon, R.L.: Fast calculation of the exact radiological path for a threedimensional CT array. Medical Physics 12(2), 252–255 (1985)\n[67] Strikwerda, J.C.: A probabilistic analysis of asynchronous iteration.\nLinear Algebra and its Applications 349(1), 125–154 (2002)\n[68] Tseng, P.: Applications of a splitting algorithm to decomposition in\nconvex programming and variational inequalities. SIAM Journal on\nControl and Optimization 29(1), 119–138 (1991)\n[69] Tseng, P.: On the rate of convergence of a partially asynchronous gradient projection algorithm. SIAM Journal on Optimization 1(4), 603–619\n(1991)\n[70] Tseng, P.: Dual coordinate ascent methods for non-strictly convex minimization. Mathematical Programming 59(1), 231–247 (1993)\n[71] Tseng, P.: A modified forward-backward splitting method for maximal\nmonotone mappings. SIAM J. Control and Optimization 38(2), 431–\n446 (2000).\n[72] Tseng, P.: Convergence of a block coordinate descent method for nondifferentiable minimization. Journal of Optimization Theory and Applications 109(3), 475–494 (2001)\n[73] Tseng, P., Yun, S.: Block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization. Journal of Optimization Theory and Applications 140(3), 513–535 (2009)\n[74] Tseng, P., Yun, S.: A coordinate gradient descent method for nonsmooth separable minimization. Mathematical Programming 117(1-2),\n387–423 (2009)\n[75] Von Neumann, J.: On rings of operators. reduction theory. Annals of\nMathematics 50(2), 401–485 (1949)\n[76] Vũ, B.C.: A splitting algorithm for dual monotone inclusions involving\ncocoercive operators. Advances in Computational Mathematics 38(3),\n667–681 (2013)\n[77] Warga, J.: Minimizing certain convex functions. Journal of the Society\nfor Industrial and Applied Mathematics 11(3), 588–593 (1963)\n[78] Wright, S.J.: Coordinate descent algorithms. Mathematical Programming 151(1), 3–34 (2015)\n\n\fCoordinate friendly structures, algorithms, and applications\n\n55\n\n[79] Wu, T.T., Lange, K.: Coordinate descent algorithms for lasso penalized\nregression. The Annals of Applied Statistics 2(1), 224–244 (2008)\n[80] Xu, Y.: Alternating proximal gradient method for sparse nonnegative\nTucker decomposition. Mathematical Programming Computation 7(1),\n39–70 (2015).\n[81] Xu, Y., Yin, W.: A block coordinate descent method for regularized\nmulticonvex optimization with applications to nonnegative tensor factorization and completion. SIAM Journal on Imaging Sciences 6(3),\n1758–1789 (2013)\n[82] Xu, Y., Yin, W.: A globally convergent algorithm for nonconvex\noptimization based on block coordinate update.\narXiv preprint\narXiv:1410.1386 (2014)\n[83] Xu, Y., Yin, W.: Block stochastic gradient iteration for convex and\nnonconvex optimization. SIAM Journal on Optimization 25(3), 1686–\n1716 (2015).\n[84] Yuan, M., Lin, Y.: Model selection and estimation in regression with\ngrouped variables. Journal of the Royal Statistical Society: Series B\n(Statistical Methodology) 68(1), 49–67 (2006)\n[85] Zadeh, N.: A note on the cyclic coordinate ascent method. Management\nScience 16(9), 642–644 (1970)\n\nAppendix A. Some Key Concepts of Operators\nIn this section, we go over a few key concepts in monotone operator theory\nand operator splitting theory.\nDefinition 8 (monotone operator). A set-valued operator T : H ⇒ H is\nmonotone if hx − y, u − vi ≥ 0, ∀x, y ∈ H, u ∈ T x, v ∈ T y. Furthermore, T\nis maximally monotone if its graph Grph(T ) = {(x, u) ∈ H × H : u ∈ T x}\nis not strictly contained in the graph of any other monotone operator.\nExample 18. An important maximally monotone operator is the subdifferential ∂f of a closed proper convex function f .\nDefinition 9 (nonexpansive operator). An operator T : H → H is nonexpansive if kT x − T yk ≤ kx − yk, ∀x, y ∈ H. We say T is averaged,\nor α-averaged, if there is one nonexpansive operator R such that T =\n(1 − α)I + αR for some 0 < α < 1. A 21 -averaged operator T is also called\nfirmly-nonexpansive.\n\n\f56\nBy definition, a nonexpansive operator is single-valued. Let T be averaged. If T has a fixed point, the iteration (2) converges to a fixed point;\notherwise, the iteration diverges unboundedly. Now let T be nonexpansive.\nThe damped update of T : xk+1 = xk −η(xk −T xk ), is equivalent to applying\nthe averaged operator (1 − η)I + ηT .\nExample 19. A common firmly-nonexpansive operator is the resolvent of\na maximally monotone map T , written as\n(67)\n\nJA := (I + A)−1 .\n\nGiven x ∈ H, JA (x) = {y : x ∈ y + Ay}. (By monotonicity of A, JA is a\nsingleton, and by maximality of A, JA (x) is well defined for all x ∈ H. ) A\nreflective resolvent is\n(68)\n\nRA := 2JA − I.\n\nDefinition 10 (proximal map). The proximal map for a function f is a\nspecial resolvent defined as:\n(69)\n\n\b\n1\nproxγf (y) = arg min f (x) +\nkx − yk2 ,\n2γ\nx\n\nwhere γ > 0. The first-order variational condition of the minimization yields\nproxγf (y) = (I + γ∂f )−1 ; hence, proxγf is firmly-nonexpansive. When\nx ∈ Rm and proxγf can be computed in O(m) or O(m log m) operations,\nwe call f proximable.\nExamples of proximable functions include `1 , `2 , `∞ -norms, several matrix norms, the owl-norm [22], (piece-wise) linear functions, certain quadratic\nfunctions, and many more.\nExample 20. A special proximal map is the projection map. Let X be a\nnonempty closed convex set, and ιS be its indicator function. Minimizing\nιS (x) enforces x ∈ S, so proxγιS reduces to the projection map projS for\nany γ > 0. Therefore, projS is also firmly nonexpansive.\nDefinition 11 (β-cocoercive operator). An operator T : H → H is βcocoercive if hx − y, T x − T yi ≥ βkT x − T yk2 , ∀x, y ∈ H.\nExample 21. A special example of cocoercive operator is the gradient of a\nsmooth function. Let f be a differentiable function. Then ∇f is β-Lipschitz\ncontinuous if and only if ∇f is β1 -cocoercive [5, Corollary 18.16].\n\n\fCoordinate friendly structures, algorithms, and applications\n\n57\n\nAppendix B. Derivation of ADMM from the DRS Update\nWe derive the ADMM update in (23) from the DRS update\n(70a)\n(70b)\n\nsk = JηB (tk ),\n\u0012\n\u0013\n1\n1\nk+1\nt\n=\n(2JηA − I) ◦ (2JηB − I) + I (tk ),\n2\n2\n\nwhere A = −∂f ∗ (−·) and B = ∂g ∗ .\nNote (70a) is equivalent to tk ∈ sk +η∂g ∗ (sk ), i.e., there is a y k ∈ ∂g ∗ (sk )\nsuch that tk = sk + ηy k , so\ntk − ηy k = sk ∈ ∂g(y k ).\n\n(71)\n\nIn addition, (70b) can be written as\ntk+1 = JηA (2sk − tk ) + tk − sk\n= sk + (JηA − I)(2sk − tk )\n= sk + (I − (I + η∂f ∗ )−1 )(tk − 2sk )\n= sk + η(ηI + ∂f )−1 (tk − 2sk )\n= sk + η(ηI + ∂f )−1 (ηy k − sk ),\n\n(72)\n\nwhere in the fourth equality, we have used the Moreau’s Identity [63]: (I +\n∂h)−1 + (I + ∂h∗ )−1 = I for any closed convex function h. Let\n(73)\n\n1\n1\nxk+1 = (ηI + ∂f )−1 (ηy k − sk ) = (I + ∂f )−1 (y k − sk ).\nη\nη\n\nThen (72) becomes\ntk+1 = sk + ηxk+1 ,\nand\n(74)\n\n(71)\n\nsk+1 = tk+1 − ηy k+1 = sk + ηxk+1 − ηy k+1 ,\n\nwhich together with sk+1 ∈ ∂g(y k+1 ) gives\n(75)\n\n1\n1\ny k+1 = (ηI + ∂g)−1 (sk + ηxk+1 ) = (I + ∂g)−1 (xk+1 + sk ).\nη\nη\n\nHence, from (73), (74), and (75), the ADMM update in (23) is equivalent\nto the DRS update in (70) with η = γ1 .\n\n\f58\n\nAppendix C. Representing the Condat-Vũ Algorithm as a\nNonexpansive Operator\nWe show how to derive the Condat-Vũ algorithm (28) by applying a forwardbackward operator to the optimality condition (27):\n\n\u0015 \u0014\n\u0015 \u0014\n\u0015\u0013 \u0014 \u0015\n∇f 0\n∂g 0\nx\n0 A>\n,\n+\n+\n0 0\n0 ∂h∗\ns\n−A 0\n| {z } |\n{z\n} |{z}\nz\noperator A\noperator B\n\n\u0012 \u0014\n(76)\n\n0∈\n\n\u0014 \u0015\nx\nIt can be written as 0 ∈ Az + Bz after we define z =\n. Let M be a\ns\nsymmetric positive definite matrix, we have\n0 ∈ Az + Bz\n⇔M z − Az ∈ M z + Bz\n⇔z − M −1 Az ∈ z + M −1 Bz\n⇔z = (I + M −1 B)−1 ◦ (I − M −1 A)z.\nConvergence and other results can be found in [23]. The last equivalent\nrelation is due to M −1 B being a maximally monotone operator under the\nnorm induced by M . We let\n\"\n#\n1\n>\nηI A\nM=\n\u001f0\nA γ1 I\nand iterate\nz k+1 = T z k = (I + M −1 B)−1 ◦ (I − M −1 A)z k .\nWe have M z k+1 + Bz k+1 = M z k − Az k :\n(\n1 k\n1 k+1\n> k\nk\n+ A> sk+1 + A> sk+1 + ∂g(xk+1 ),\nη x + A s − ∇f (x ) ∈ η x\n1 k\n1 k+1\nk\n∈ γs\n+ A xk+1 − A xk+1 + ∂h∗ (sk+1 ),\nγs + A x\nwhich is equivalent to\n\u001a k+1\ns\n= proxγh∗ (sk + γAxk ),\nxk+1 = proxηg (xk − η(∇f (xk ) + A> (2sk+1 − sk ))).\n\n\fCoordinate friendly structures, algorithms, and applications\n\n59\n\nNow we derived the Condat-Vũ algorithm. With proper choices of η and γ,\nthe forward-backward operator T = (I + M −1 B)−1 ◦ (I − M −1 A) can be\nshown to be α-averaged if we use the inner product\nhz1 , z2 iM = z1> M z2 and\n\u0014 \u0015\n√\nx\nnorm kzkM = z > M z on the space of z =\n. More details can be found\ns\nin [23].\n\"\n#\n1\n>\nI\n−A\nIf we change the matrix M to η\n, the other algorithm (29)\n1\n−A\nγI\ncan be derived similarly.\n\nAppendix D. Proof of Convergence for Async-parallel\nPrimal-dual Coordinate Update Algorithms\nAlgorithms 1 and 2 differ from that in [54] in the following aspects:\n1. the operator TCV is nonexpansive under a norm induced by a symmetric positive definite matrix M (see Appendix C), instead of the\nstandard Euclidean norm;\n2. the coordinate updates are no longer orthogonal to each other under\nthe norm induced by M ;\n3. the block coordinates may overlap each other.\nBecause of these differences, we make two major modifications to the proof\nin [54, Section 3]: (i) adjusting parameters in [54, Lemma 2] and modify\nits proof to accommodate for the new norm; (2) modify the inner product\nand induced norm used in [54, Theorem 2] and adjust the constants in [54,\nTheorems 2 and 3].\nWe assume the same inconsistent case as in [54], i.e., the relationship\nbetween ẑ k and z k is\nX\n(77)\nẑ k = z k +\n(z d − z d+1 ),\nd∈J(k)\n\nwhere J(k) ⊆ {k − 1, ..., k − τ } and τ is the maximum number of other\nupdates to z during the computation of the update. Let S = I − TCV .\nηk\nThen the coordinate update can be rewritten as z k+1 = z k − (m+p)q\nSik ẑ k ,\ni\nk\n\nk , (S ẑ k ) , ẑ k , . . . , ẑ k\nwhere Si ẑ k = (ẑ1k , . . . , ẑi−1\ni i+1\nm+p ) for Algorithm 1. For Algorithm 2, the update is\n\n(78)\n\nz k+1 = z k −\n\nηk\nSi ẑ k ,\nmqik k\n\n\f60\nwhere\n\n\n0\n ..\n\n.\n\n\n0\n\n\nIHi\n\n\n\nSi ẑ k = \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n k\n S ẑ .\n\n\n\n\n\n\n\n\n\n0\n..\n\n.\n0\nρi,1 IG1\n..\n\n.\nρi,p IGp\n\nLet λmax and λmin be the maximal and minimal eigenvalues of the matrix\nbe the condition number. Then we have the\nM , respectively, and κ = λλmax\nmin\nfollowing lemma.\nLemma 1. For both Algorithms 1 and 2,\nX\n(79)\nSi ẑ k = S ẑ k ,\ni\n\nX\n\n(80)\n\nkSi ẑ k k2M ≤ κkS ẑ k k2M ,\n\ni\n\nwhere i runs from 1 to m + p for Algorithm 1 and 1 to m for Algorithm 2.\nProof. The first part comes immediately from the definition of S for both\nalgorithms. For the second part, we have\n(81)\n\nX\n\nkSi ẑ k k2M ≤\n\ni\n\nX\n\nλmax kSi ẑ k k2 = λmax kS ẑ k k2 ≤\n\ni\n\nλmax\nkS ẑ k k2M ,\nλmin\n\nfor Algorithm 1. For Algorithm 2, the equality is replaced by “≤”.\nAt last we define\n(82)\n\nz̄ k+1 := z k − ηk S ẑ k ,\n\nqmin = mini qi > 0, and |J(k)| be the number of elements in J(k). It is shown\nin [23] that with proper choices of η and γ, TCV is nonexpansive under the\nnorm induced by M . Then Lemma 2 shows that S is 1/2-cocoercive under\nthe same norm.\n\n\fCoordinate friendly structures, algorithms, and applications\n\n61\n\nLemma 2. An operator T : F → F is nonexpansive under the induced norm\nby M if and only if S = I − T is 1/2-cocoercive under the same norm, i.e.,\n(83)\n\n1\nhz − z̃, Sz − S z̃iM ≥ kSz − S z̃k2M ,\n2\n\n∀ z, z̃ ∈ F.\n\nThe proof is the same as that of [5, Proposition 4.33].\nWe state the complete theorem for Algorithm 2. The theorem for Algorithm 1 is similar (we need to change m to m + p when necessary).\nTheorem 2. Let Z ∗ be the set of optimal solutions of (24) and (z k )k≥0 ⊂ F\nbe the sequence generated by Algorithm 2 (with proper choices of η and γ\nsuch that TCV is nonexpansive under the norm induced by M ), under the\nfollowing conditions:\n(i) f, g, h∗ are closed proper convex functions. In addition, f is differentiable and ∇f is Lipschitz continuous with β;\nmin\n(ii) ηk ∈ [ηmin , ηmax ] for certain 0 < ηmax < 2τ √mq\nκqmin +κ and any 0 < ηmin ≤\nηmax .\nThen (z k )k≥0 converges to a Z ∗ -valued random variable with probability 1.\nThe proof directly follows [54, Section 3]. Here we only present the key\nmodifications. Interested readers are referred to [54] for the complete procedure.\nThe next lemma shows that the conditional expectation of the distance\nbetween z k+1 and any z ∗ ∈ FixTCV = Z ∗ for given Z k = {z 0 , z 1 , · · · , z k }\nhas an upper bound that depends on Z k and z ∗ only.\nLemma 3. Let (z k )k≥0 be the sequence generated by Algorithm 2. Then for\nany z ∗ ∈ FixTCV , we have\n\n(84)\n\n\u0001\nσ X\nE kz k+1 − z ∗ k2M Z k ≤kz k − z ∗ k2M +\nkz d − z d+1 k2M\nm\nd∈J(k)\n\u0012\n\u0013\nκ\n1\n1 |J(k)|\n+\n+\n−\nkz k − z̄ k+1 k2M\nm\nσ\nmqmin ηk\n\nwhere E(· | Z k ) denotes conditional expectation on Z k and σ > 0 (to be\noptimized later).\n\n\f62\nProof. We have\n\n(85)\n\n\u0010\n\u0011\nE kz k+1 − z ∗ k2M | Z k\n\u0010\n\u0011\n(78)\nηk\nk − z ∗ k2 | Z k\n= E kz k − mq\nẑ\nS\ni\nk\nM\nik\n\u0010\n2ηk\n=kz k − z ∗ k2M + E mq\nSik ẑ k , z ∗ − z k\nik\nPm\nk ∗\nk\nk\n=kz k − z ∗ k2M + 2η\ni=1 Si ẑ , z − z\nm\n=kz k − z ∗ k2M +\n\n2ηk\nm\n\nS ẑ k , z ∗ − z k\n\nM\n\n+\n\nM\n\n+\n\nηk2\nk 2\nm2 qi2k kSik ẑ kM\n2\n\nM\n\nηk2\nm2\n\nηk\n+m\n2\nPm\n\nZk\n\n\u0011\n\nPm\n\n1\nk 2\ni=1 qi kSi ẑ kM\n\n1\nk 2\ni=1 qi kSi ẑ kM ,\n\nwhere the third equality holds because the probability of choosing i is qi .\nNote that\n\nPm\n\n1\nk 2\ni=1 qi kSi ẑ kM\n\n(86)\n\n≤\n\nm\n1 X\n\nqmin\n\nkSi ẑ k k2M\n\ni=1\n\nm\nκ X\n\n(80)\n\n≤\n\nqmin\n\nkS ẑ k k2M\n\ni=1\n\nκ\nkz k − z̄ k+1 k2M ,\n= 2\nηk qmin\n\n(82)\n\nand\n(87)\nhS ẑ k , z ∗ − z k iM\n=hS ẑ k , z ∗ − ẑ k +\n\nP\n\nd∈J(k) (z\n\n(82)\n\nd\n\n− z d+1 )iM\n\n= hS ẑ k , z ∗ − ẑ k iM +\n≤hS ẑ k − Sz ∗ , z ∗ −\n\n(83)\n\n≤ − 21 kS ẑ k k2M +\n\n(82)\n\n= −\n\n1\nk\n2ηk2 kz\n\n1 P\nk\nk+1 , z d − z d+1 i\nM\nd∈J(k) hz − z̄\nηk\n1 P\n1\nk\nk+1\nk\nk2M\nẑ iM + 2ηk d∈J(k) σ kz − z̄\n\n1\n2ηk\n\n+ σkz d − z d+1 k2M\n\n\u0001\n\n1\nk\nd∈J(k) ( σ kz\n\n− z̄ k+1 k2M + σkz d − z d+1 k2M )\nP\nk\nk+1 k2 + σ\nd\nd+1 k2 ,\n+ |J(k)|\nM\nM\nd∈J(k) kz − z\n2σηk kz − z̄\n2ηk\n\nP\n\n− z̄ k+1 k2M\n\nwhere the first inequality follows from the Young’s inequality. Plugging (86)\nand (87) into (85) gives the desired result.\nLet Fτ +1 =\nproduct:\n\nQτ\n\ni=0 F\n\nbe a product space and h· | ·i be the induced inner\n\nh(z 0 , . . . , z τ ) | (z̃ 0 , . . . , z̃ τ )i =\n\nτ\nX\ni=0\n\nhz i , z̃ i iM ,\n\n∀(z 0 , . . . , z τ ), (z̃ 0 , . . . , z̃ τ ) ∈ Fτ +1 .\n\n\fCoordinate friendly structures, algorithms, and applications\n\n63\n\nDefine a (τ + 1) × (τ + 1) matrix U 0 by\n\n\n\n1 0 ···\n0 0 · · ·\n\nU 0 :=  . . .\n..\n .. ..\n0 0 ···\n\n\n0\nr\n0\nqmin\n\n..  +\nκ\n\n.\n0\n\nτ\n−τ\n\n\n\n\n\n\n\n\n−τ\n2τ − 1 1 − τ\n1 − τ 2τ − 3 2 − τ\n..\n..\n.\n.\n−2\n\n\n\n\n\n\n,\n..\n\n.\n\n3 −1\n−1 1\n\nand let U = U 0 ⊗ IF . Here ⊗ represents the Kronecker product. For a given\n(y 0 , · · · , y τ ) ∈ Fτ +1 , (z 0 , · · · , z τ ) = U (y 0 , · · · , y τ ) is given by:\n\np\n0\n1\nz 0 = y 0 + τ qmin\nκ (y − y ),\np\ni−1 + (2τ − 2i + 1)y i + (i − τ )y i+1 ), if 1 ≤ i ≤ τ − 1,\nz i = qmin\nκ ((i − τ − 1)y\np\nτ\nτ −1 ).\nz τ = qmin\nκ (y − y\nThen U is a self-adjoint and positive definite linear operator since U 0 is\nsymmetric and positive definite, and we define h· | ·iU = h· | U ·i as the U weighted inner product and k · kU the induced norm.\nLet\nzk = (z k , z k−1 , . . . , z k−τ ) ∈ Fτ +1 , k ≥ 0, z∗ = (z ∗ , . . . , z ∗ ) ∈ Z∗ ⊆ Fτ +1 ,\nwhere z k = z 0 for k < 0. With\n(88)\np\nPk−1\ni\ni+1 k2 ,\nξk (z∗ ) := kzk −z∗ k2U = kz k −z ∗ k2M + qmin\nM\ni=k−τ (i−(k−τ )+1)kz −z\nκ\nwe have the following fundamental inequality:\nTheorem 3 (fundamental inequality). Let (z k )k≥0 be the sequence generated\nby Algorithm 2. Then for any z∗ ∈ Z∗ , it holds that\n\n\u0010\n\n∗\n\nE ξk+1 (z ) Z\n\nk\n\n\u0011\n\n1\n≤ ξk (z ) +\nm\n∗\n\n\u0012\n\n\u0013\n√\n2τ κ\nκ\n1\n+\n−\nkz̄ k+1 − z k k2M .\n√\nm qmin mqmin ηk\n\n\f64\n\np\nProof. Let σ = m qmin\nκ . We have\nE(ξk+1 (z∗ )|Z k )\n(88)\n\n= E(kz k+1 − z ∗ k2M |Z k ) + σ\n\nPk\n\ni−(k−τ )\nE(kz i\nm\n\n− z i+1 k2M |Z k )\nPk−1\n(78)\nηk2\ni−(k−τ )\nk 2\nk\nkz i − z i+1 k2M\n= E(kz k+1 − z ∗ k2M |Z k ) + στ\ni=k+1−τ\nm E( m2 qi2k kSik ẑ kM |Z ) + σ\nm\nP\ni−(k−τ )\nκ\nkz k − z̄ k+1 k2M + σ k−1\n≤E(kz k+1 − z ∗ k2M |Z k ) + mστ\nkz i − z i+1 k2M\n3q\ni=k+1−τ\nm\nmin\n\u0010\n\u0011\n(84)\n|J(k)|\nκ\n1\nστ κ\n1\nk\nk+1 k2\n+\n−\n+\n≤ kz k − z ∗ k2M + m\n2\nM\nσ\nm qmin\nmqmin\nηk kz − z̄\nPk−1\ni−(k−τ )\nσ P\nd\nd+1\n2\ni\ni+1\nkz − z k2M\n+ m d∈J(k) kz − z kM + σ i=k+1−τ\nm\n\u0011\n\u0010\n1\nτ\nστ κ\nκ\n1\nk\nk+1 k2\n≤kz k − z ∗ k2M + m\n+\n+\n−\n2\nM\nσ\nm qmin\nmqmin\nηk kz − z̄\nPk−1\ni−(k−τ )\nσ Pk−1\ni\ni+1\n2\ni\n+ m i=k−τ kz − z kM + σ i=k+1−τ\nkz − z i+1 k2M\nm\n\u0011\n\u0010 √\n(88)\n2τ κ\nκ\n1\n1\nk\nk+1 k2 .\n√\n+\n−\n= ξk (x∗ ) + m\nM\nm qmin\nmqmin\nηk kz − z̄\ni=k+1−τ\n\nThe first inequality follows from the computation of the conditional expectation on Z k and (86), the third inequality holds because J(k) ⊂ {k − 1, k −\np\n2, · · · , k − τ }, and the last equality uses σ = m qmin\nκ , which minimizes\nστ κ\nτ\nσ + m2 qmin over σ > 0. Hence, the desired inequality holds.\n\nZhimin Peng\nPO Box 951555\nUCLA Math Department\nLos Angeles, CA 90095\nE-mail address: zhimin.peng@math.ucla.edu\nTianyu Wu\nPO Box 951555\nUCLA Math Department\nLos Angeles, CA 90095\nE-mail address: wuty11@math.ucla.edu\nYangyang Xu\n207 Church St SE\nUniversity of Minnesota, Twin Cities\nMinneapolis, MN 55455\nE-mail address: yangyang@ima.umn.edu\n\n\fCoordinate friendly structures, algorithms, and applications\n\n65\n\nMing Yan\nDepartment of Computational Mathematics, Science and Engineering\nDepartment of Mathematics\nMichigan State University\nEast Lansing, MI 48824\nE-mail address: yanm@math.msu.edu\nWotao Yin\nPO Box 951555\nUCLA Math Department\nLos Angeles, CA 90095\nE-mail address: wotaoyin@math.ucla.edu\nReceived January 1, 2016\n\n\f"
        ],
        [
         "49",
         "49",
         "cs.CE",
         "Computational Engineering",
         "1308.5906v1.pdf",
         "Biological effects and equivalent doses in radiotherapy: a software solution\n\nCyril Voyant,1,2* Daniel Julian,3 Rudy Roustit,4 Katia Biffi,2 and Céline Lantieri2\n\n1-University of Corsica, CNRS UMR SPE 6134, (Campus Grimaldi, 20250 Corte), France\n2-Hospital of Castelluccio, Radiotherapy Unit, BP 85, 20177 Ajaccio, France\n3-Joseph Fourier University, 38000 Grenoble, France\n4- Centre de la république, Radiotherapy Unit, 63000 Clermont-Ferrand\n\n*Corresponding author: Cyril Voyant\nEmail: voyant@univ-corse.fr; Tel.: +33 495293666; Fax: +33 495293797\n\n\fAbstract\n\nThe limits of TDF (time, dose, and fractionation) and linear quadratic models have been known\nfor a long time. Medical physicists and physicians are required to provide fast and reliable\ninterpretations regarding the delivered doses or any future prescriptions relating to treatment changes.\nWe therefore propose a calculation interface under the GNU license to be used for equivalent doses,\nbiological doses, and normal tumor complication probability (Lyman model). The methodology used\ndraws from several sources: the linear-quadratic-linear model of Astrahan, the repopulation effects of\nDale, and the prediction of multi-fractionated treatments of Thames. The results are obtained from an\nalgorithm that minimizes an ad-hoc cost function, and then compared to the equivalent dose computed\nusing standard calculators in seven French radiotherapy centers.\n\nKeywords\nRadiobiology, fractionation, equivalent, dose, time effect, linear quadratic, repopulation\n\n\fNomenclature\n\nfitting parameters of the linear quadratic\nmodel of cell survival (Gy² et Gy)\nadjustment parameter of the occurrence\nmodel of cancer radio-induced (Gy-1)\nHeaviside function\nparameter of the LQL model\nparameter adjustment necessary to take\ninto account the poly-fractionation in the model\nLQ (hours-1)\nbiological equivalent dose (Gy)\nD\nphysical dose (Gy)\ndt\ndose per fraction from which the curve\nof cell survival becomes linear (Gy)\nproliferation dose (Gy/day)\nequivalent doses for the treatment 1\nand 2 (Gy)\nequivalent dose for a 2 Gy/Fraction\ntreatment (Gy)\nEUD\n\nEquivalent uniform Dose (Gy)\n\nEUD for an equivalent dose related to a\nreference of 2 Gy per fraction\n\ncost function to minimize by the\nalgorithm\nnumber of day-offs\nLQ model correction taking account the\npoly-fractionation\noccurrences probability of radio\ninduced cancer (%)\nfraction number and slope factor of the\nNTCP model\nnombre de fraction\ncomplications rate of post radiation (%)\nparameter related to the occurrence of\nradiation-induced cancers (Gy-1)\nduration between two irradiations\n(heures)\nNTCP\n\noverall time (day)\nTD50 dose at which there is a 50% complication\n(Gy)\ntime at which repopulation begins after\nstart of treatment (day)\npotential doubling time (day)\nTstop\ndays off during the treatment\nboundary used in the NCTP calculus (Gy)\n\n\fI.\n\nIntroduction: Problems of the biologically equivalent dose\n\nIt has long been known that radiation biology plays an important role and is necessary for\nradiotherapy treatments. The radiation effects on normal and malignant tissues after exposure range\nfrom a femtosecond to months and years thereafter [1,2]. Therefore, to optimize treatment, it is crucial\nto explain and understand these mechanisms [3-5]. Providing a conceptual basis for radiotherapy and\nidentifying the mechanisms and processes that underlie the tumor and normal tissue responses to\nirradiation can help to explain the observed phenomena [6]. Examples include understanding hypoxia,\nreoxygenation, tumor cell repopulation, or the mechanisms of repair of DNA damage [3,7,8]. The\ndifferent biological effects of radiation should be divided into several phases: the physical phase\n(interaction between charged particles and tissue atoms), chemical phase (the period during which the\ndamaged atoms and molecules react with other cellular components in rapid chemical reactions), and\nbiological phase (impact of the generated lesions on the biological tissue [4]). The following section\ndescribes the models most often used in radiotherapy. These are simplistic models that actual\ntreatments are based, and that are validated and approved [9-12].\n\n1. Reference models\nNumerous models exist to evaluate the biological equivalent dose, but the two most common are\nthe nominal standard dose (NSD [13]) and linear quadratic (LQ [9]) models. The NSD uses the power\nlaw described in equation 1 below (\n\nis the tolerance dose of the tissue, NSD is a constant, n and t\n\n, N the number of fractions, and T the overall treatment time). However, this model has been\noften criticized [14]. In short, some researchers consider and have even shown that the NSD formula is\nnot a valid description for all tumors and normal tissues; instead, they maintain that the model\nincorrectly describes the effects of fraction number and treatment duration.\nEq 1\n\n\fThe LQ model is most frequently used in the radiotherapy units. It allows the equivalent dose to be\neasily evaluated for different fractionations. This concept involves the\n\nratio, as shown in equation 2\n\nbelow (D is the total dose for a fraction size of d gray).\n\nEq 2\n\nis the dose obtained using a 2Gy fraction dose, which is biologically equivalent to the total dose\nD given with a fraction dose of d gray. The values of\n\nmay be added in separate parts in the\n\ntreatment plan. This formula may be adapted to fraction doses other than 2Gy.\n\n2. Limitations of the LQ model\n\nThe LQ model is frequently used for modeling the effects of radiotherapy at low and medium\ndoses per fraction for which it appears to fit clinical data reasonably well. The main disadvantage of\nthe LQ approach is that the overall time factor is not taken into account, because in radiotherapy it is\nregarded to be more complex than previously supposed [3]. It is indeed very difficult to include this\nparameter in the LQ equation. However, a technique may be used to integrate a penalty term in\nEquation 2. Thus, for Tstop days off treatment, the dose recovered would be Tstop.Dprol, where Dprol is the\nproliferation factor (in Gy/day; for example, 0.22 for laryngeal edema or 0.15 for rectosigmoid\ncomplications). This methodology is essentially validated for discontinuation during treatment. As a\ngeneral rule, the main limitations of using the LQ model are linked to repopulation (LQ doesn’t take\ninto account the dose protraction), bi-fractionated treatments and high-dose fractions (continuously\nbending survival curve versus linear behavior observed at least in some cell lines). Other more\nsophisticated models, however, take into account these weaknesses. We will later see that the LQ\nmodel requires further theoretical investigation, especially in terms of the biologically effective dose\n(BED).\nGiven the difficulty of computing the BED, we conducted a study in seven radiotherapy centers in\nFrance: CHD Castelluccio (Ajaccio; two classical calculators used), Centre de Cancérologie du Grand\n\n\fMontpellier (Montpellier), CRLCC Paul Lamarque (Montpellier), Clinique Saint-Pierre (Perpignan),\nCentre de la République (Clermont Ferrand), CHU of Grenoble, and CHU of Nîmes. A questionnaire\nwas sent to medical physicists working at these centers, with the aim of comparing the results of\nequivalence (for standard radiotherapy planning). Table 1 presents the results of this survey, which\nindicate that not all of the operators obtained the same results. The 95 % confidence interval was often\nvery large. Moreover, the relative standard deviation (also known as the dispersion coefficient) was\nfrequently greater than 5% (13 times out of 24). This dispersion was larger in the case of target\nvolumes; the maximal volume (close to 40%) was related to high doses per fraction with a gap\nbetween two radiotherapy cycles. In Table 1, it is evident that all of the users did not estimate dose\nequivalence in the same manner. Only for a dose per fraction approaching 2Gy and standard overall\ntime were the results equivalent. Note that in the multi-fractionated treatments, only 50% of the\ncenters were able to give an equivalent dose, as this kind of treatment was not computable.\nTreatements\n10x3Gy\n\n10x3Gy\n\n1x8Gy\n\n10x3Gy\n\n1x8Gy\n(1 month gap time)\n1x8Gy\n\n5x4Gy\n\nMedian\naverage ± 95% CI\nstand dev\nmedian\naverage ± 95% CI\nstand dev\nmedian\naverage ± 95% CI\nstand dev\nmedian\naverage ± 95% CI\nstand dev\nmedian\naverage ± 95% CI\nstand dev\nmedian\naverage ± 95% CI\nstand dev\n\nOrgans at risk\nSpinal cord\n37.50\n37.8 ± 1.2\n1.71 / 4.5%\nSpinal cord\n37.50\n37.8 ± 1.2\n1.71 / 4.5%\nSpinal cord\n20.00\n19.1 ± 2.3\n3.38 / 17.7%\nBrain\n37.50\n37.5 ± 0.9\n1.26 / 3.4%\nSpinal cord\n33.30\n33.6 ± 4.0\n5.78 / 17.2%\nPericardium\n30.90\n33.7 ± 5.9\n8.47 / 25.1%\n\nTarget volumes\nProstate (metastasis)\n36.65\n36.6 ± 1.9\n2.87 / 7.8%\nBreast (metastasis)\n35.57\n35.97 ± 1.4\n2.06 / 5.7%\nProstate (metastasis)\n14.90\n15.9 ± 3.2\n4.72 / 29.6%\nBreast (metastasis)\n35.57\n35.9 ± 1.4\n2.03 / 5.6%\nProstate (metastasis)\n21.50\n24.04 ± 6.8\n9.76 / 40.6%\nLung (metastasis)\n27.07\n28.9 ± 3.6\n5.14 / 17.8%\n\n\f20x2Gy (1 week\ngap time) 10x2Gy\n\nmedian\naverage ± 95% CI\nstand dev\n\n22x1.8Gy (bifractionated)\n\nmedian\naverage ± 95% CI\nstand dev\n\n25x1.8Gy then\n15x2Gy\n\nmedian\naverage ± 95% CI\nstand dev\n\n20x2.5Gy (4\nfraction/week)\n\nmedian\naverage ± 95% CI\nstand dev\n\n4x4.5Gy (2 week\ngap time) 4x4Gy\n\nmedian\naverage ± 95% CI\nstand dev\n\n28x1.8Gy (1 week\ngap time)\n\nmedian\naverage ± 95% CI\nstand dev\n\nOral mucosa\n57.95\n58.1 ± 0.5\n0.66 / 1.14%\nOral mucosa\n41.95\n41.0 ± 1.6\n2.32 / 5.6%\nRectum\n72.58\n72.6 ± 0.4\n0.54 / 0.7%\nLung\n55.58\n55.0 ± 1.1\n1.59 / 2.9%\nOptic chiasma\n50.25\n49.9 ± 2.7\n3.96 / 7.9%\nSkin (early)\n46.65\n46.7 ± 0.6\n0.90 / 1.9%\n\nOropharynx\n57.95\n57.6 ± 1.9\n2.76 / 4.8%\nOropharynx\n41.70\n41.7 ± 2.2\n3.12 / 7.5%\nProstate\n72.75\n72.6 ± 0.7\n1.00 / 1.37%\nBreast\n53.50\n53.5 ± 1.2\n1.71 / 3.2%\nGlioblastoma\n42.15\n43.1 ± 2.8\n4.07 / 9.4%\nBreast\n46.36\n46.5 ± 0.8\n1.13 / 2.4%\n\nTable 1: Methodologies for computing the equivalent dose used in eight clinical calculators from seven radiotherapy\ncenters. The median dose, average dose, and standard deviation are given in Gy. For the standard deviation, absolute\nand relative modes (/average) were used. Bold font is used to represent values >5%\n\nThe numbers of centers included in this study was low, and there was no consensus among the\ncenters in terms of their methods for computing the doses. If we look more closely, the biological\nequivalent dose was the only process that was calculated using non-official software. All of the other\nsteps in treatment planning followed an official protocol. It is thus legitimate to ask why centers use\nin-vivo dosimeters or try to achieve a global error of 2% throughout treatment, if the prospective\ncalculation of the equivalent dose (and prescription) is greater than 20%. In order to address the\nquestion of responsibility, the following section of this article is targeted at medical physicists,\nknowing that the prescriber is the physician. In case of equivalent computations, the optimal operation\nwould be for the technical work to be performed by the physicist and validations by the physician\n\n\f(while taking into account the clinical scenario). This methodology allows for a double checking of the\ncalculation results.\nThe next section describes the theoretical methodology that we propose to compute the BED.\n\nII.\n\nMaterials and methods: the developed models\n\nThe BED (introduced by Fowler [9]) is a mathematical concept used to illustrate the biological\neffects observed after irradiation. In addition to being easily computable (BED = physical dose x\nrelative efficiency), this notion is interesting because two irradiations with the same BED generate the\nsame radiobiological effects. For this reason, it is easy to compare treatments with different doses,\nfractionations, and overall times. The following section introduces the BED-based models that we\nadvocate as well as the rules and guidelines for using the LQL_equiv software.\n\n1. Target volume models\nLet us examine two different treatment cases separately. The first involves treatments with a highdose fraction (one treatment per day, the fraction size d is greater than the dt limit; [15]), which\nrequires a linear quadratic linear (LQL) model. The second case relates to other treatments (d < dt),\nwhere the LQ model is applicable to daily multi-fractionation [16].\n\na. The d > dt case\nWhen the dose per fraction (d) is greater than the LQL threshold (dt ~ 2 ⁄ ), the BED is\ncomputed using Equation 3 below (one fraction permitted per day). This template regroups Astrahan’s\nhigh-dose model [17] and Dale’s repopulation model [18] (n is the number of fraction,\nHeaviside function, the parameter of the LQL model and\n\n(\n\n(\n\n⁄\n\n)\n\n)\n\nthe potential doubling time in day).\n\nEq 3\n\nthe\n\n\fThe second term used in this equation is useful only when the overall time T is greater than the Tk\nvalue (kick-off time). If this threshold is not achieved, the tumor is considered to be non-proliferative\n(early hypoxia).\n\nb. The d ≤ dt case\n\nWhen the fraction dose is low, it is possible to use the standard BED equations while considering\none or more fractions per day (Equation 4). This methodology follows the model of Thames [19], who\nintroduces the repair factor Hm related to the amount of unrepaired damage (Equation 5). If the interfraction interval is reduced below the full repair interval (between 6 hours and 1 day), the overall\ndamage from the whole treatment is increased because the repair of damage due to one radiation dose\nmay not be complete before the next fraction is given (\npoly-fractionation, m the number of fraction per day,\n\nis LQ model correction taking account the\nthe incomplete repair and\n\nthe parameter\n\nadjustment necessary to take into account the poly-fractionation in the model LQ in hours-1).\n(\n( ) (\n\n⁄\n\n) (\n\n)\n\n)\n\nEq 4\nEq 5\n\nNote that in the case of mono-fractionation, the Hm factor is null. These equations only relate to the\ntarget volume calculation. For the organs at risk, the kick-off time is not relevant, meaning that it is\nnecessary to use a repopulation-specific approach.\n\n2. Models for organs at risk\n\nAs in the precedent section on target volumes, this section similarly separates high and low doses\nper fraction. The BED formulae are almost equivalent to the target volume model; only the terms\nrelating to the lack of dose by proliferation are modified.\n\n\fa. The d > dt case\n\nTo understand this methodology, it is necessary to consult Van Dyk’s law [20]. The kick-off time\nis no longer considered, with the recovered dose (\n\nin Gy/day) instead being added. The\n\nglobal model is described in Equation 6 below.\n\n(\n\n(\n\n⁄\n\n)\n\n)\n\nEq 6\n\nb. The d ≤ dt case\n\nIn the case of low doses per fraction, the methodology is similar to the target volume model: the\nparameter (Equation 5) is nonetheless required, which allows us to take into account more than\none fraction per day. As seen in the Equation 7 below, the recovered dose is used as in the previous\ncase.\n(\n\n⁄\n\n)\n\nEq 7\n\n3. Computational methods for the equivalent dose\n\nThe standard models used for the equivalent dose as based on the LQ approach are easily\nexploitable. The main formulation of the model (Equation 2) can be obtained by considering the\ngeneral formula described in the Equation 8 as follows.\n⁄\n⁄\n\nEq 8\n\n\fThis equation may be validated using BED methodology. Considering the BED of two treatments to\nbe equal, it appears that a simple relation links the two overall doses,\n\nand\n\n.\n\nThe detail of this procedure is shown in the Equation 9 below.\n(\n\n⁄\n\n)\n\n(\n\n⁄\n\n)\n\nEq 9\n\nIn the case of more sophisticated BED formulations, it is not easy to determine a simple formula\nlinking the\n\ndoses, as recovery and repopulation significantly complicate the computational\n\nprinciple. Most of the existing software that uses the overall time correction does not calculate the\nequivalent dose; instead, it only provides the BED for the chosen treatments. In clinical use, it is more\nvaluable for the physician or physicist to work with the equivalent dose in standard fractionation. In\nthis context, the methodology used in the LQL_Equiv software is based on an innovative algorithm,\nwhich allows a cost function extremum to be determined based on BED modeling. To explain this\nmethodology, it is necessary to consider two irradiations (Indices 1 and 2), which are defined by a\nfraction number (n), dose per fraction (d), and days of discontinuation (ja). The corresponding BED is\nnoted as BED1 (n1,d1,ja1) and BED2 (n2,d2,ja2), while the cost function f is defined in Equation 10 as\nfollows.\n|\n\n|\n\nEq 10\n\nIn clinical use, it is desirable to compare a radiotherapy trial with one that is performed in a\nconventional manner (generally with 2 Gy per fraction without interruption). This concept of a\nreference dose simplifies the issue, as it is thus possible to dispense with the days off treatment and\nmulti-fractionation per day in relation to the reference treatment. The following example concerns a\ntumor case with a dose per fraction less than dt (second part of the target volume model), while the\ncost function, f, is given in Equation 11. Concerning the three other cases examined in previous\nsections, a similar relationship is found.\n\n(\n\n)\n\n|\n\n(\n\n⁄\n⁄\n\n)\n)\n\n(\n\n(\n\n)\n| Eq 11\n\n\fThe global treatment duration can be seen to be directly associated with the fraction number and days\noff during radiotherapy. Following Equation 11, the 2Gy-per-fraction equivalent dose (EQD2) for\nstandard treatment with the characteristics\n(\n\n{\n\nis given by the algorithm shown in Equation 12.\n\n)\n\nEq 12\n\nAll of the results obtained in this section were implemented using a Matlab® standalone application\nknown as LQL-equiv. The characteristics of this software, its limitations, and guidelines for its use are\ndiscussed in the following section.\n\nIII.\n\nResults: LQL_Equiv software\n\nThe LQL_Equiv software was developed in collaboration by the CHD Castelluccio radiotherapy\nunit in Ajaccio and the University of Corsica. It is a free software released under the GNU license. The\nsource codes, executable file, help files, and license terms are available at http://cyril-voyant.univcorse.fr/LQL-Equiv_a34.html. Before installing this software, it is advisable to refer to the installation\nguide and to download and execute Matlab Component Runtime (MCR 32 bits, version 7.15 or later).\nThis latter step is necessary since the application was programmed using the GUI Matlab® software\n(32 bits, v. 7.12) and deployed with the Matlab Compiler® (v. 4.12), which use MCR (a standalone set\nof shared libraries enabling the execution of Matlab® applications on a computer without an installed\nversion of Matlab®). Users of the LQL_Equiv software are advised to provide us with comments on\nthe software, its libraries (biological parameters for each organ or tumor type), or any bugs so as to\nallow us to develop the software. Note that the application requires Microsoft Windows® (the\nresolution and colors are for Vista or later versions).\n\n1. Software\n\nThe graphical interface of the LQL_Equiv software software is presented in the Figure 1, divided\ninto five sections: demographical zone, tissue choice (organs at risk and target volumes), reference\n\n\fzone (characteristics for computing the equivalent dose), treatment planning zone (three juxtaposed\nand independent treatments), and finally, the equivalent dose under the reference conditions. Prior to\nusing the software, it is important to understand that repopulation or a high dose per fraction can\nconsiderably alter the standard equivalent results. Therefore, it is recommended for each user to verify\nthe results obtained and validate them during an initial test phase. The results must be consistent with\nroutine procedures as well as the data in the literature. The detail of the instructions allowing to use the\nsoftware is available in the annex part.\n\nFigure 1: Graphical interface for the LQL_Equiv software\n\nThe ideal scenario would be to compare these results with other software and obtain a mean score\nfor the two outputs or for the outputs that minimize the physical dose. We recommend using this\nsoftware as a secondary BED calculator. It aims to provide assistance, but cannot be used as a\nsubstitute for routine calculations made by a professional. The creators of the LQL_Equiv software\ncannot be held responsible for any errors caused by the misuse of the results obtained.\n\n2. Comparison with standard models\n\n\fThis section compares the results of the LQL_Equiv software with the available clinical models.\nHowever,, it is important to note that all of the parameters used for calculating the equivalence are\navailable on the graphical interface. Using Matlab™ and the downloadable source codes, it is easy to\nmodify or complete these parameters. It is also possible to contact the software authors to assist in\ndeveloping the software. LQL_Equiv is in direct competition with TDF Plan developed by Eye\nPhysics LLC, which proposes a multitude of parameters. However, the software is dedicated to the\ncalculation of BED and is not really consistent with the reference equivalent dose. Moreover, we\naimed to develop ergonomic software with minimum of adjustable parameters, which ultimately\ncomplicate the interpretation of the output. These two approaches are nevertheless complementary; for\nmore\n\ninformation about the different models used, refer to the TDF Plan website\n\n(http://www.eyephysics.com/tdf/Index.htm). Table 2 presents a comparison between outputs of the\nstandard calculation models described in section II (LQ without proliferation and ⁄\n\n=10 for oral\n\nmucosa and 2 for others) and the LQL-equiv software. The difference between the two approaches is\nsubstantial. The overall time effect and unusual doses per fraction result in completely different\noutputs. The maximum difference is close to 25%; this value is linked to the cell repopulation of\nprostate cancer. In this case, the non-specific methods are certainly not usable.\nTreatements\n10x3Gy\n\n10x3Gy\n\n1x8Gy\n\n10x3Gy\n1x8Gy\n(1 month gap\n\nclassical output (Gy)\nLQL-equiv output (Gy)\ndifference (Gy / %)\nclassical output (Gy)\nLQL-equiv output (Gy)\ndifference (Gy / %)\nclassical output (Gy)\nLQL-equiv output (Gy)\ndifference (Gy / %)\nclassical output (Gy)\nLQL-equiv output (Gy)\ndifference (Gy / %)\nclassical output (Gy))\n\nOrgans at risk\nSpinal cord\n37.5\n37.5\n-0 / -0%\nSpinal cord\n37.5\n37.5\n-0 / -0%\nSpinal cord\n20\n16\n-4 / -20%\nBrain\n37.5\n43.5\n6 / 16%\nSpinal cord\n40\n\nTarget volums\nProstate (metastasis)\n37.5\n36\n-1.5 / -4%\nBreast (metastasis)\n37.5\n38.2\n0.7 / 1.9%\nProstate (metastasis)\n20\n16.8\n-3.2 / -16%\nBreast (metastasis)\n37.5\n38.2\n0.7 / 1.9%\nProstate (metastasis)\n40\n\n\ftime) 1x8Gy\n\n5x4Gy\n\nLQL-equiv output (Gy)\ndifference (Gy / %)\nclassical output (Gy)\nLQL-equiv output (Gy)\ndifference (Gy / %)\n\n20x2Gy (1 week\ngap time) 10x2Gy\n\nclassical output (Gy)\nLQL-equiv output (Gy)\ndifference (Gy / %)\n\n22x1.8Gy (bifractionated)\n\nclassical output (Gy)\nLQL-equiv output (Gy)\ndifference (Gy / %)\n\n25x1.8Gy then\n15x2Gy\n\nclassical output (Gy)\nLQL-equiv output (Gy)\ndifference (Gy / %)\n\n20x2.5Gy (4\nfraction/week)\n\nclassical output (Gy)\nLQL-equiv output (Gy)\ndifference (Gy / %)\n\n4x4.5Gy (2 week\ngap time) 4x4Gy\n\nclassical output (Gy)\nLQL-equiv output (Gy)\ndifference (Gy / %)\n\n28x1.8Gy (1 week\ngap time)\n\nclassical output (Gy)\nLQL-equiv output (Gy)\ndifference (Gy / %)\n\n32\n-8 / -4.63%\nPericardium\n30\n37.5\n7.5 / 25%\nOral mucosa\n60\n54.4\n-5.6 / -9.3%\nOral mucosa\n38.9\n45\n6.1 / 15.7%\nRectum\n72.7\n71\n-1.7 / -2.3%\nLung\n56.2\n62.9\n6.7 / 11.9%\nOptic chiasma\n53.2\n42.8\n-10.4 / -19.5%\nSkin (early)\n47.9\n47.6\n-0.3 / 0.6%\n\n33.3\n-6.7 / 16.7%\nLung (metastasis)\n30\n23.3\n-6.7 / -22.3%\nOropharynx\n60\n53\n-7 / -11.7%\nOropharynx\n38.9\n36\n-2.9 /-7.4%\nProstate\n72.7\n73\n0.3 / 0.4%\nBreast\n56.2\n56.8\n0.2 / 0.3%\nGlioblastoma\n53.2\n47.4\n-5.8 / -10.9%\nBreast\n47.9\n42.3\n-5.6 / -11.7%\n\nTable 2 : Comparison between the outputs of the LQL_Equiv and standard calculation models (LQ without proliferation\nand with ⁄ =10 for oral mucosa and 2 for others). Bold font is used to show differences >5%.\n\nIn addition, for the BED and equivalent calculations, the LQL_Equiv software allows two other\nparameters to be obtained, which may be useful in clinical practice: the normal tumor complication\nprobability (NTCP; [22]) and the ratio of radiation-induced cancer after irradiation.\n\n3. Others elements computed by the software\nIn the LQL_Equiv software, the bottom the interface is dedicated to the calculation of the NTCP\nand ratio of radiation-induced cancer. For the first parameter, the formula for its computation (only for\n\n\fnormal tissues) is based on the Lyman model [22] as presented in the Equation 12 below (TD50 is the\ndose at which there is a 50% complication in Gy,\n\nthe boundary used in the NCTP calculus in Gy and\n\nm the slope factor). To use this formula, it is necessary to first compute the EUD (Niemerko [21]).\nHowever, in practice, this quantity is not feasible. It is instead possible to use the equivalent dose\nrelated to a reference dose of 2 Gy per fraction (\n\n. However, the NTCP formalism is\n\nvalid for 2±0.2 Gy/fraction. Moreover, the DVH must be used, in which case the equivalent dose\nrefers to the average dose for the parallel organs or the maximal dose (D5%) for the serial organs.\n\n√\n\n{\n\n∫\nEq 13\n\nThe second add-on in the software concerns the estimation of radiation-induced cancer. The theory\nused was developed by the United Nations Scientific Committee on the Effects of Atomic Radiation\n(UNSCEAR; http://www.unscear.org/unscear/fr/publications.html). The different meta-analyses of\nprevious radiological incidents are used in this model. The ratio of radiation-induced cancer (in %)\nrelating to normal tissue is provided in Equation 14 as follows (\nthe occurrence model of cancer radio-induced in Gy-1,\n\nis the adjustment parameter of\n\nthe UNSCEAR probability and\n\nthe\n\nequivalent dose for a 2 Gy/Fraction treatment in Gy).\n\nEq 14\nNote that methods used to compute NTCP and Kincidence are simplified; it is evident that interested\nreaders must identify more specialized documents. These parameters are given as additional\ninformation.\n\nIV.\n\nConclusion\n\nIn this article, we have exposed the compiling results of various published LQ model\nmodifications, which have been modified to be better suited for specialized radiotherapy techniques\n\n\fsuch as hypo- or hyperfractionation. The LQ model was modified to take into account multifractionation, repopulation, high-dose fractions, and overall time. Moreover, we propose a software\nprogram (LQL_equiv), integrating all of these concepts regarding the main organs at risk or target\nvolumes. Moreover, this free and easy-to-use software allows the NTCP to be calculated. Finally, this\nsoftware permits the obtained results to be compared and validated against other “homemade” models,\nwith the purpose of harmonizing practices in interested centers. However, it is essential to don’t\nconsider models as “general biological rules”, parameters and outputs uncertainties can be very large;\nthis phenomenon is related to the number of regression parameters (parsimony principle) and to the\ndata snooping (e.g. failure to adjust existing statistical models when applying them to new datasets).\n\nV.\n\nAcknowledgments\n\nWe would like to thank the following people for their contribution: Stéphane Muraro (Centre de\nCancérologie du Grand Montpellier), Norbert Aillères and Sébastien Siméon (CRLCC Paul Lamarque;\nMontpellier), Vincent Plagnol (Clinique Saint-Pierre; Perpignan), Nicolas Docquière and Jean-Yves\nGiraud (CHU de Grenoble), and Bérengère Piron (CHU de Nîmes).\n\n\fVI.\n\nAnnex 1: Instructions for use\n\nThe number of modifiable parameters in the LQL_Equiv software is minimal, while the items\nrequired to complete a dose equivalent calculation are limited. Only the white boxes can be modified.\nThe upper left part of the interface is dedicated to patient demography (identity and pathology)\nand operator traceability. These parameters are not essential for initiating the calculation. Below this,\nthe reference dose per fraction should be provided; by default, the dose is 2 Gy/fraction.\nIn the top-right of the interface, there are two dropdown menus related to the organs at risk and\ntarget volumes chosen by the operator to obtain the equivalent dose. Once these steps are completed, it\nis necessary to define the desired treatment plans. Only three plans are proposed, but the software is\nable to test more by integrating the overall results in a single treatment plan, such as the EQ1 (dose,\ndays off, and number of fractions must be adjusted). The overall time must be verified or else there\nmay be some imprecision in the final calculation. A null number of fractions or doses results in\ncancelling the calculation of the equivalent dose (the duration of the sequence does not contribute to\nthe final output).\nAfter selecting the treatment plan and clicking on the calculation button, the BED and equivalent\ndoses are given. The page may be printed, or otherwise, there is the digital archiving solution based on\nthe Windows™ print screen button.\nWhen taking into account the days off, the weekend should not be considered; only\ndiscontinuations that occur during weekdays should be included. Beyond 20 days off from treatment,\nthe algorithms are no longer valid. In the first approximation, the side of caution indicates that healthy\ntissues do not recover during the gap time. For the second cycle of radiotherapy that occurs a long\ntime after the first, we must be vigilant with regard to the treated organs. In the case of the skin, for\nexample, we may consider a duration of 2 to 5 years to be sufficient to negate any effects from the\nprevious treatment (this is, however, invalid if the effects are already visible at the time of irradiation),\nwhile for the spinal cord, it must be considered, where possible, that there exists a dose memory, with\n\n\fthe effects of gray radiation always being present. In this regard, the software takes into account that\ncertain organs, such as spinal cord, have a low Drec in order to limit the consequences to the most\ncritical organs. Moreover, it is necessary to consider all of the treatment phases if a dose equivalent is\nrequired for the second stage of a prostate disease. In this case, the first phase of the treatment must be\nconsidered, or otherwise, the kick-off time will not be correctly taken into account.\nTo avoid the dose overestimation, we recommend first calculating the dose equivalent for the\norgan, i.e., the limiting factor, and then estimating the fractionation effect on the target volume.\nFor organs at risk, it is possible to use the nominal dose. Thus, in the case of the pelvis, for the\nfirst 45 Gy given in 25 fractions, the dose received by the rectum may be considered equal to 45 Gy.\nHowever, in order to optimize the methodology, it seems more reasonable to utilize a more detailed\nanalysis. If the validation criterion is D30, the software should be completed according to the dose per\nfraction and number of fractions for the dose received by 30% of the rectum. It is also possible to use\nthe average dose for parallel organs, maximal dose for serial organs, or simply the equivalent uniform\ndose (EUD) [21]. Another example illustrating the difference between the critical dose and nominal\nstandard dose is based on spinal irradiation. If doses of 30 Gy in 10 fractions are delivered, this does\nnot necessary mean that the spinal cord has received the entire dose. Dose volume histogram (DVH)\nanalysis allows us to observe that the spinal cord received 32 Gy after the 10 fractions, which means\nthat the equivalent dose is 10 fractions of 3.2 Gy, which significantly changes the results.\nFurthermore, it should be added that in this software, as is often the case, the time between two\nirradiations in bi-fractionated radiotherapy must be greater than 6 hours.\n\n\fVII. References\n1. Spotheim-Maurizot M, Mostafavi M, Belloni J, Douki T. Radiation chemistry: from\nbasics to applications in material and life sciences. L’Editeur : EDP Sciences; 2008.\n2. Deutsch\n\nE,\n\nVozenin\n\nM-C.\n\nLa\n\nradiobiologie,\n\nune\n\ndiscipline\n\nd’interface.\n\nCancer/Radiothérapie.15(5):347. 2011.\n3. Steel GG. Basic Clinical Radiobiology. 3rd Revised edition. Hodder Arnold; 2002.\n4. Joubert A, Vogin G, Devic C, Granzotto A, Viau M, Maalouf M, et al. Biologie des\nradiations :\n\navancées\n\nmajeures\n\net\n\nperspectives\n\npour\n\nla\n\nradiothérapie.\n\nCancer/Radiothérapie. 15(5):348–54. 2011.\n5. Foray N. Aspects radiobiologiques des traitements anticancéreux par rayonnement\nsynchrotron : bilan et perspectives. Cancer/Radiothérapie. 14(2):145–54. 2010.\n6. Bourgier C, Heymann S, Vielh P, Azria D. Implications radiobiologiques de la\nclassification moléculaire des cancers du sein : présent ou avenir ? Cancer/Radiothérapie\n16(1), 29-33. 2012.\n7. Vogin G. Radiosensibilité, radiocurabilité et réparation. Cancer/Radiothérapie.15(4):294–\n306. 2011.\n8. Favaudon V. La radiobiologie. Cancer/Radiothérapie. 10;4(5):333. 2000.\n9. Fowler JF. The linear-quadratic formula and progress in fractionated radiotherapy. Br J\nRadiol 62, 679:694. 1989.\n10. Bruzzaniti V, Abate A, Pedrini M, Benassi M, Strigari L: IsoBED: a tool for automatic\ncalculation of biologically equivalent fractionation schedules in radiotherapy using IMRT\nwith a simultaneous integrated boost (SIB) technique. J Exp Clin Cancer Res, 30:52. 2011\n11. Bibault J-E, Fumagalli I, Diaz O, Faivre J-C, Leroy T, Pichon B, et al. The French Society\nof Young Radiation Oncologists: History, goals and perspective. Reports of Practical\nOncology & Radiotherapy;17:255–8. 2012\n\n\f12. Gomez-Iturriaga A, Bilbao P, Casquero F, Cacicedo J, Crook J. Smartphones and tablets:\nReshaping\n\nradiation\n\noncologists’\n\nlives.\n\nReports\n\nof\n\nPractical\n\nOncology\n\n&\n\nRadiotherapy;17:276–80. 2012\n13. Ellis P. Dose, Time and fractionation. A clinical hypothesis. Clinical Radiology, 20:1-6.\n1969.\n14. Liversage WE. A critical look at the ret. British Journal of Radiology, 44:91-100. 1971.\n15. Cosset JM. Hypofractionnement en radiothérapie : le retour ? Cancer/Radiothérapie. 9(6–\n7):366–73. 2005.\n16. Laszlo A, Rosset A, Hermann F, Ozsahin M, Zouhair A, Mirimanoff R. Radiothérapie\ntrifractionnée accélérée seule ou alternée avec la chimiothérapie chez des patients\nsouffrant d’un cancer localement évolué de la sphère ORL : analyse de la toxicité tardive.\nCancer/Radiothérapie. 5(2):130–7. 2001.\n17. Melvin Astrahan. Some implications of linear-quadratic-linear radiation dose-response\nwith regard to hypofractionation. Med. Phys. 35 (9), 4161-73. 2008.\n18. Dale RG. Radiobiological assessment of permanent implants using tumor repopulation\nfactors in the linear quadratic model. Brit J Radiol 62, 241-244. 1989.\n19. Thames HD. An incomplete-repair model for survival after fractionated and continuous\nirradiations. Int J Radiat Biol 47 (3), 319-39. 1985.\n20. Van Dyk J. Radiation induced lung damage: dose time fractionation consideration. Radiot\nOnc 14(1), 55-69. 1989.\n21. Niemerko, A. Reporting and analyzing dose distributions: a concept of equivalent uniform\ndose. Med.Phys. 24:103-110. 1997.\n22. Lyman J T. Complication probability as assessed from dose-volume histograms. Radiat.\nRes. Suppl 8, S13-9. 1985\n\n\fVIII. Conflicts of interest\nThe authors declare to have no conflicts of interest.\n\n\f"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 33388
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>cls_label</th>\n",
       "      <th>description</th>\n",
       "      <th>file_id</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cs.CE</td>\n",
       "      <td>Computational Engineering</td>\n",
       "      <td>0902.0763v1.pdf</td>\n",
       "      <td>Genetic algorithm based optimization and post\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>cs.CE</td>\n",
       "      <td>Computational Engineering</td>\n",
       "      <td>1708.08551v1.pdf</td>\n",
       "      <td>Deep Learning for Accelerated Reliability Anal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>cs.CE</td>\n",
       "      <td>Computational Engineering</td>\n",
       "      <td>0705.1759v1.pdf</td>\n",
       "      <td>FINITE ELEMENT MODEL UPDATING USING RESPONSE S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>cs.CE</td>\n",
       "      <td>Computational Engineering</td>\n",
       "      <td>1801.03018v1.pdf</td>\n",
       "      <td>Predict Forex Trend via Convolutional Neural N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>cs.CE</td>\n",
       "      <td>Computational Engineering</td>\n",
       "      <td>0901.2665v1.pdf</td>\n",
       "      <td>A Density Matrix-based Algorithm for Solving E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33383</th>\n",
       "      <td>33383</td>\n",
       "      <td>cs.IT</td>\n",
       "      <td>Information Theory</td>\n",
       "      <td>1702.03692v3.pdf</td>\n",
       "      <td>SUBMITTED TO IEEE TRANSACTIONS ON WIRELESS COM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33384</th>\n",
       "      <td>33384</td>\n",
       "      <td>cs.IT</td>\n",
       "      <td>Information Theory</td>\n",
       "      <td>1710.08671v2.pdf</td>\n",
       "      <td>Linear State Estimation via 5G C-RAN Cellular\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33385</th>\n",
       "      <td>33385</td>\n",
       "      <td>cs.IT</td>\n",
       "      <td>Information Theory</td>\n",
       "      <td>1707.00421v2.pdf</td>\n",
       "      <td>On Binary Matroid Minors and Applications to\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33386</th>\n",
       "      <td>33386</td>\n",
       "      <td>cs.IT</td>\n",
       "      <td>Information Theory</td>\n",
       "      <td>1712.06804v1.pdf</td>\n",
       "      <td>1\\n\\nAsymptotic Coupling and Its Applications ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33387</th>\n",
       "      <td>33387</td>\n",
       "      <td>cs.IT</td>\n",
       "      <td>Information Theory</td>\n",
       "      <td>1710.07617v1.pdf</td>\n",
       "      <td>1\\n\\nAsymptotically Optimal Resource Block All...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33388 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index cls_label                description           file_id  \\\n",
       "0          0     cs.CE  Computational Engineering   0902.0763v1.pdf   \n",
       "1          1     cs.CE  Computational Engineering  1708.08551v1.pdf   \n",
       "2          2     cs.CE  Computational Engineering   0705.1759v1.pdf   \n",
       "3          3     cs.CE  Computational Engineering  1801.03018v1.pdf   \n",
       "4          4     cs.CE  Computational Engineering   0901.2665v1.pdf   \n",
       "...      ...       ...                        ...               ...   \n",
       "33383  33383     cs.IT         Information Theory  1702.03692v3.pdf   \n",
       "33384  33384     cs.IT         Information Theory  1710.08671v2.pdf   \n",
       "33385  33385     cs.IT         Information Theory  1707.00421v2.pdf   \n",
       "33386  33386     cs.IT         Information Theory  1712.06804v1.pdf   \n",
       "33387  33387     cs.IT         Information Theory  1710.07617v1.pdf   \n",
       "\n",
       "                                                 content  \n",
       "0      Genetic algorithm based optimization and post\\...  \n",
       "1      Deep Learning for Accelerated Reliability Anal...  \n",
       "2      FINITE ELEMENT MODEL UPDATING USING RESPONSE S...  \n",
       "3      Predict Forex Trend via Convolutional Neural N...  \n",
       "4      A Density Matrix-based Algorithm for Solving E...  \n",
       "...                                                  ...  \n",
       "33383  SUBMITTED TO IEEE TRANSACTIONS ON WIRELESS COM...  \n",
       "33384  Linear State Estimation via 5G C-RAN Cellular\\...  \n",
       "33385  On Binary Matroid Minors and Applications to\\n...  \n",
       "33386  1\\n\\nAsymptotic Coupling and Its Applications ...  \n",
       "33387  1\\n\\nAsymptotically Optimal Resource Block All...  \n",
       "\n",
       "[33388 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cls_label  description              \n",
       "cs.DS      Data Structures              4136\n",
       "cs.IT      Information Theory           3233\n",
       "cs.SY      Systems and Control          3106\n",
       "math.GR    Group Theory                 3065\n",
       "math.ST    Statistics Theory            3025\n",
       "cs.NE      Neural and Evolutionary      3012\n",
       "cs.AI      Artificial Intelligence      2995\n",
       "cs.PL      Programming Languages        2901\n",
       "math.AC    Commutative Algebra          2885\n",
       "cs.CV      Computer Vision              2525\n",
       "cs.CE      Computational Engineering    2505\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df[[\"cls_label\",\"description\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset_df.drop_duplicates(subset=[\"file_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cls_label  description              \n",
       "cs.DS      Data Structures              4011\n",
       "math.GR    Group Theory                 3064\n",
       "cs.SY      Systems and Control          3061\n",
       "math.ST    Statistics Theory            3016\n",
       "cs.IT      Information Theory           2954\n",
       "math.AC    Commutative Algebra          2857\n",
       "cs.AI      Artificial Intelligence      2758\n",
       "cs.NE      Neural and Evolutionary      2625\n",
       "cs.PL      Programming Languages        2585\n",
       "cs.CV      Computer Vision              2525\n",
       "cs.CE      Computational Engineering    2505\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df[[\"cls_label\",\"description\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 31961\n",
      "Training documents: 22366\n",
      "Validation documents: 9595\n",
      "\n",
      "Split ratio for each class:\n",
      "split      train  validation\n",
      "cls_label                   \n",
      "cs.AI       1930         828\n",
      "cs.CE       1753         752\n",
      "cs.CV       1767         758\n",
      "cs.DS       2807        1204\n",
      "cs.IT       2067         887\n",
      "cs.NE       1837         788\n",
      "cs.PL       1809         776\n",
      "cs.SY       2142         919\n",
      "math.AC     1999         858\n",
      "math.GR     2144         920\n",
      "math.ST     2111         905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_561245/3566951953.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['split'] = 'train'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_dataset(df, train_ratio=0.7, random_state=42):\n",
    "    # Initialize the 'split' column with 'train'\n",
    "    df['split'] = 'train'\n",
    "    \n",
    "    # Get unique class labels\n",
    "    unique_labels = df['cls_label'].unique()\n",
    "    \n",
    "    # For each class label, split the data\n",
    "    for label in unique_labels:\n",
    "        label_indices = df[df['cls_label'] == label].index\n",
    "        train_indices, val_indices = train_test_split(\n",
    "            label_indices,\n",
    "            train_size=train_ratio,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Mark validation samples\n",
    "        df.loc[val_indices, 'split'] = 'validation'\n",
    "    \n",
    "    return df\n",
    "\n",
    "dataset_df = split_dataset(dataset_df)\n",
    "\n",
    "print(f\"Total documents: {len(dataset_df)}\")\n",
    "print(f\"Training documents: {len(dataset_df[dataset_df['split'] == 'train'])}\")\n",
    "print(f\"Validation documents: {len(dataset_df[dataset_df['split'] == 'validation'])}\")\n",
    "\n",
    "# Verify the split ratio for each class\n",
    "print(\"\\nSplit ratio for each class:\")\n",
    "print(dataset_df.groupby(['cls_label', 'split']).size().unstack(fill_value=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_561245/3471668958.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_df['content_length'] = dataset_df['content'].apply(len)\n"
     ]
    }
   ],
   "source": [
    "dataset_df['content_length'] = dataset_df['content'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=dataset_df, x='content_length', kde=True)\n",
    "plt.title('Distribution of Content Length')\n",
    "plt.xlabel('Content Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df.to_json(\"/home/snt/projects_lujun/agentCLS/assets/LDD_split.json\", lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read EURLEX57K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "EURLEX57K_train = pd.read_json(\"/home/snt/projects_lujun/agentCLS/assets/EURLEX57K/train.jsonl\", lines=True)\n",
    "EURLEX57K_train[\"split\"] = \"train\"\n",
    "\n",
    "EURLEX57K_dev = pd.read_json(\"/home/snt/projects_lujun/agentCLS/assets/EURLEX57K/dev.jsonl\", lines=True)\n",
    "EURLEX57K_dev[\"split\"] = \"dev\"\n",
    "\n",
    "EURLEX57K_test = pd.read_json(\"/home/snt/projects_lujun/agentCLS/assets/EURLEX57K/test.jsonl\", lines=True)\n",
    "EURLEX57K_test[\"split\"] = \"test\"\n",
    "\n",
    "\n",
    "EURLEX57K = pd.concat([EURLEX57K_train, EURLEX57K_dev, EURLEX57K_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "EURLEX57K[\"content\"] = (\n",
    "    EURLEX57K[\"title\"].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x)).fillna('') + \"\\n\" +\n",
    "    EURLEX57K[\"recitals\"].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x)).fillna('') + \"\\n\" +\n",
    "    EURLEX57K[\"main_body\"].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x)).fillna('')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "EURLEX57K[\"content_length\"] = EURLEX57K[\"content\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=EURLEX57K, x='content_length', kde=True)\n",
    "plt.title('Distribution of Content Length')\n",
    "plt.xlabel('Content Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "EURLEX57K[\"cls_label\"] = EURLEX57K[\"document_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "EURLEX57K[\"split\"] = EURLEX57K[\"split\"].replace({\"dev\": \"validation\", \"test\": \"validation\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split\n",
       "train         45000\n",
       "validation    12000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EURLEX57K[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "EURLEX57K.to_json(\"/home/snt/projects_lujun/agentCLS/assets/EURLEX57K_split.json\", lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "EURLEX57K = pd.read_json(\"/home/snt/projects_lujun/agentCLS/assets/EURLEX57K_split.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Class Distribution'}, xlabel='cls_label'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EURLEX57K.cls_label.value_counts().plot(kind='bar', figsize=(12, 6), title='Class Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "df= pd.read_json(\"/home/snt/projects_lujun/agentCLS/assets/EURLEX57K_split.json\", lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"description\"] = df[\"cls_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json(\"/home/snt/projects_lujun/agentCLS/assets/EURLEX57K_split.json\", lines=True, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataset More Feasible Like 10, 100, 500, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "LDD_split = pd.read_json(\"/home/snt/projects_lujun/agentCLS/assets/LDD_split.json\", lines=True)\n",
    "EURLEX57K_split = pd.read_json(\"/home/snt/projects_lujun/agentCLS/assets/EURLEX57K_split.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cls_label\n",
       "cs.DS      4011\n",
       "math.GR    3064\n",
       "cs.SY      3061\n",
       "math.ST    3016\n",
       "cs.IT      2954\n",
       "math.AC    2857\n",
       "cs.AI      2758\n",
       "cs.NE      2625\n",
       "cs.PL      2585\n",
       "cs.CV      2525\n",
       "cs.CE      2505\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDD_split.cls_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cls_label\n",
       "Regulation    37531\n",
       "Decision      17239\n",
       "Directive      2230\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EURLEX57K_split.cls_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_label\n",
      "Regulation    50\n",
      "Decision      45\n",
      "Directive      5\n",
      "Name: count, dtype: int64\n",
      "cls_label\n",
      "Decision      300\n",
      "Directive     300\n",
      "Regulation    300\n",
      "Name: count, dtype: int64\n",
      "cls_label\n",
      "Regulation    350\n",
      "Decision      345\n",
      "Directive     305\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226418/4284767600.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('cls_label', group_keys=False).apply(downsample)\n",
      "/tmp/ipykernel_1226418/4284767600.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('cls_label', group_keys=False).apply(downsample)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def downsample_proportional(df, max_samples_per_class, random_seed=42):\n",
    "    counts = df['cls_label'].value_counts()\n",
    "    total_target_samples = max_samples_per_class * len(counts)\n",
    "    total_current_samples = counts.sum() \n",
    "    ratio = total_target_samples / total_current_samples \n",
    "\n",
    "    def downsample(group):\n",
    "        n_samples = min(max_samples_per_class, max(1, int(len(group) * ratio)))\n",
    "        return group.sample(n=n_samples, random_state=random_seed)\n",
    "\n",
    "    return df.groupby('cls_label', group_keys=False).apply(downsample)\n",
    "\n",
    "\n",
    "def downsample_equal(df, max_samples_per_class, random_seed=42):\n",
    "\n",
    "    def downsample(group):\n",
    "        n_samples = min(max_samples_per_class, len(group))\n",
    "        return group.sample(n=n_samples, random_state=random_seed)\n",
    "\n",
    "    return df.groupby('cls_label', group_keys=False).apply(downsample)\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "train_max_num = 50\n",
    "val_max_num = 300\n",
    "\n",
    "input_path  = \"/home/snt/projects_lujun/agentCLS/assets/EURLEX57K_split.json\"\n",
    "dataset_name = input_path.split(\"/\")[-1].split(\".\")[0]\n",
    "input_dataset = pd.read_json(input_path, lines=True)\n",
    "df_sampled_val = downsample_equal(input_dataset[input_dataset[\"split\"]==\"validation\"], max_samples_per_class=val_max_num)\n",
    "df_sampled_train = downsample_proportional(input_dataset[input_dataset[\"split\"]==\"train\"], max_samples_per_class=train_max_num)\n",
    "\n",
    "print (df_sampled_train.cls_label.value_counts().sort_values(ascending=False))\n",
    "print (df_sampled_val.cls_label.value_counts().sort_values(ascending=False))\n",
    "df_sampled = pd.concat([df_sampled_train, df_sampled_val], axis=0)\n",
    "print (df_sampled.cls_label.value_counts().sort_values(ascending=False))\n",
    "\n",
    "df_sampled.to_json(f\"/home/snt/projects_lujun/agentCLS/assets/{dataset_name}_proportional_train_{train_max_num}_val_{val_max_num}.json\", lines=True, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1226418/4284767600.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('cls_label', group_keys=False).apply(downsample)\n",
      "/tmp/ipykernel_1226418/4284767600.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby('cls_label', group_keys=False).apply(downsample)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_label\n",
      "Regulation    1500\n",
      "Decision      1362\n",
      "Directive      177\n",
      "Name: count, dtype: int64\n",
      "cls_label\n",
      "Decision      300\n",
      "Directive     300\n",
      "Regulation    300\n",
      "Name: count, dtype: int64\n",
      "cls_label\n",
      "Regulation    1800\n",
      "Decision      1662\n",
      "Directive      477\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "train_max_num = 1500\n",
    "val_max_num = 300\n",
    "\n",
    "input_path  = \"/home/snt/projects_lujun/agentCLS/assets/EURLEX57K_split.json\"\n",
    "dataset_name = input_path.split(\"/\")[-1].split(\".\")[0]\n",
    "input_dataset = pd.read_json(input_path, lines=True)\n",
    "df_sampled_val = downsample_equal(input_dataset[input_dataset[\"split\"]==\"validation\"], max_samples_per_class=val_max_num)\n",
    "df_sampled_train = downsample_proportional(input_dataset[input_dataset[\"split\"]==\"train\"], max_samples_per_class=train_max_num)\n",
    "\n",
    "print (df_sampled_train.cls_label.value_counts().sort_values(ascending=False))\n",
    "print (df_sampled_val.cls_label.value_counts().sort_values(ascending=False))\n",
    "df_sampled = pd.concat([df_sampled_train, df_sampled_val], axis=0)\n",
    "print (df_sampled.cls_label.value_counts().sort_values(ascending=False))\n",
    "\n",
    "df_sampled.to_json(f\"/home/snt/projects_lujun/agentCLS/assets/{dataset_name}_proportional_train_{train_max_num}_val_{val_max_num}.json\", lines=True, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File /home/snt/projects_lujun/agentCLS/assets/EURLEX57K_proportional_train_1500_val_300.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m EURLEX57K \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/snt/projects_lujun/agentCLS/assets/EURLEX57K_proportional_train_1500_val_300.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcls_label\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/env_classification/lib/python3.9/site-packages/pandas/io/json/_json.py:791\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    789\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[0;32m~/miniconda3/envs/env_classification/lib/python3.9/site-packages/pandas/io/json/_json.py:904\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[0m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m filepath_or_buffer\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mujson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 904\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/env_classification/lib/python3.9/site-packages/pandas/io/json/_json.py:960\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    952\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[1;32m    959\u001b[0m ):\n\u001b[0;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal json to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    967\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    968\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File /home/snt/projects_lujun/agentCLS/assets/EURLEX57K_proportional_train_1500_val_300.json does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "EURLEX57K = pd.read_json(\"/home/snt/projects_lujun/agentCLS/assets/EURLEX57K_split_proportional_train_1500_val_300.json\", lines=True).cls_label.value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDD_150 = pd.read_json(\"/home/snt/projects_lujun/agentCLS/assets/EURLEX57K_split_proportional_train_150_val_300.json\", lines=True).cls_label.value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
