{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import requests\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "from utils.prompts import (\n",
    "    prompt_EUR_BASE,\n",
    "    prompt_EUR_COT,\n",
    "    prompt_EUR_COD,\n",
    "    prompt_EUR_FEW_SHOT,\n",
    "    prompt_LDD_BASE,\n",
    "    prompt_LDD_COT,\n",
    "    prompt_LDD_COD,\n",
    "    prompt_LDD_FEW_SHOT,\n",
    "    prompt_IE_BASE,\n",
    "    prompt_IE_COT,\n",
    "    prompt_IE_COD,\n",
    "    prompt_IE_FEW_SHOT,\n",
    "    prompt_SELF_CONSIS,\n",
    ")\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Train a model using CPT (Continual Pretraining Training)\")\n",
    "\n",
    "    parser.add_argument(\"--project_root\", type=str, default=\"/Users/lujun.li/projects/mt_luxembourgish\", help=\"Path to project root\")\n",
    "    parser.add_argument(\"--training_dataset_path\", type=str, default=\"data/processed/dataset_merged_llama_fake_targets.jsonl\", help=\"Path to training dataset\")\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"/home/llama/Personal_Directories/srb/binary_classfication/Llama-3.2-3B-Instruct\", help=\"Path to model\")\n",
    "    parser.add_argument(\"--vllm_url\", type=str, default=\"VLLM_API_URL\", help=\"URL to VLLM API\")\n",
    "    parser.add_argument(\"--is_BASE\", type=bool, default=True, help=\"Whether to run BASE\")\n",
    "    parser.add_argument(\"--is_COT\", type=bool, default=True, help=\"Whether to run COT\")\n",
    "    parser.add_argument(\"--is_COD\", type=bool, default=True, help=\"Whether to run COD\")\n",
    "    parser.add_argument(\"--is_FEW_SHOT\", type=bool, default=True, help=\"Whether to run FEW_SHOT\")\n",
    "    parser.add_argument(\"--is_SELF_CONSIS\", type=bool, default=True, help=\"Whether to run SELF_CONSIS\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# project_root = \"/home/snt/projects_lujun/agentCLS\"\n",
    "# training_dataset_path = \"assets/training_dataset/EURLEX57K_split_proportional_train_1500_val_300.jsonl\"\n",
    "# model_path = \"/home/snt/projects_lujun/base_models/Llama-3.2-1B-Instruct\"\n",
    "# VLLM_API_URL = \"http://0.0.0.0:8000/v1/chat/completions\"\n",
    "# is_BASE = True\n",
    "# is_COT = True\n",
    "# is_COD = True\n",
    "# is_FEW_SHOT = True\n",
    "# is_SELF_CONSIS = True\n",
    "\n",
    "project_root = args.project_root\n",
    "training_dataset_path = args.training_dataset_path\n",
    "model_path = args.model_path\n",
    "VLLM_API_URL = args.vllm_url\n",
    "is_BASE = args.is_BASE\n",
    "is_COT = args.is_COT\n",
    "is_COD = args.is_COD\n",
    "is_FEW_SHOT = args.is_FEW_SHOT\n",
    "is_SELF_CONSIS = args.is_SELF_CONSIS\n",
    "\n",
    "print(f\"project_root: {project_root}\")\n",
    "print(f\"training_dataset_path: {training_dataset_path}\")\n",
    "print(f\"model_path: {model_path}\")\n",
    "print(f\"VLLM_API_URL: {VLLM_API_URL}\")\n",
    "print(f\"is_BASE: {is_BASE}\")\n",
    "print(f\"is_COT: {is_COT}\")\n",
    "print(f\"is_COD: {is_COD}\")\n",
    "print(f\"is_FEW_SHOT: {is_FEW_SHOT}\")\n",
    "print(f\"is_SELF_CONSIS: {is_SELF_CONSIS}\")\n",
    "\n",
    "\n",
    "MAX_LEN = 512\n",
    "temperature = 0.8\n",
    "repetition_penalty = 1.1\n",
    "top_k = 10\n",
    "top_p= 0.7\n",
    "prompt_templates = []\n",
    "\n",
    "model_name = model_path.split(\"/\")[-1]\n",
    "train_dataset_path = os.path.abspath(os.path.join(project_root, training_dataset_path))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "current_time = datetime.now()\n",
    "formatted_time = current_time.strftime('%m_%d_%H_%M')\n",
    "\n",
    "\n",
    "if \"EURLEX\" in training_dataset_path:\n",
    "    prompt_templates = [prompt_EUR_BASE, prompt_EUR_COT, prompt_EUR_COD, prompt_EUR_FEW_SHOT, prompt_SELF_CONSIS]\n",
    "    is_EURLEX = True\n",
    "elif \"LDD\" in training_dataset_path:\n",
    "    prompt_templates = [prompt_LDD_BASE, prompt_LDD_COT, prompt_LDD_COD, prompt_LDD_FEW_SHOT, prompt_SELF_CONSIS]\n",
    "    is_LDD = True\n",
    "elif \"FOYER\" in training_dataset_path:\n",
    "    prompt_templates = [prompt_IE_BASE, prompt_IE_COT, prompt_IE_COD, prompt_IE_FEW_SHOT, prompt_SELF_CONSIS]\n",
    "    is_IE = True\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset: {training_dataset_path}\")\n",
    "\n",
    "\n",
    "sample_size = 10\n",
    "dataset = pd.read_json(training_dataset_path, lines=True)\n",
    "start_path = project_root + \"/\" + \"assets/prompt_testing/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(sample, prompt_template, model_name):\n",
    "    prompt = prompt_template.format(input=sample[\"content\"])\n",
    "    system_message = \"You are a helpful AI assistant.\"\n",
    "    if \"gemma\" in model_name:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    \n",
    "    return messages\n",
    "\n",
    "\n",
    "def call_vllm(messages, vllm_api_url, max_len=100, temperature=1.0):\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = {\n",
    "        \"model\": model_path,\n",
    "        \"messages\": messages,\n",
    "        \"max_tokens\": max_len,\n",
    "        \"temperature\": temperature,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "        \"top_k\": top_k\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(vllm_api_url, headers=headers, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            print(f\"Error: Received status code {response.status_code}, response: {response.text}\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Exception while calling VLLM API: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# Self-consistency prompt\n",
    "def create_SIS_prompt(sample, prompt_template, path1, path2, path3, model_name):\n",
    "    prompt = prompt_template.format(question_prompt=sample[\"content\"], path_1=path1, path_2=path2, path_3=path3)\n",
    "    system_message = \"You are a helpful AI assistant.\"\n",
    "    if \"gemma\" in model_name:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_classification(prompt_output, list_category):\n",
    "    default_category = list_category[0]\n",
    "    prompt_output = prompt_output.strip()\n",
    "    parts = prompt_output.split('####')\n",
    "    \n",
    "    if len(parts) > 1:\n",
    "        string_last = parts[-1].strip()\n",
    "        if string_last:\n",
    "            for category in list_category:\n",
    "                if category.lower() in string_last.lower():\n",
    "                    return category\n",
    "            else:\n",
    "                category_counts = {}\n",
    "                for category in list_category:\n",
    "                    count = string_last.lower().count(category.lower())\n",
    "                    category_counts[category] = count\n",
    "                max_count = max(category_counts.values())\n",
    "                if max_count == 0:\n",
    "                    category_counts = {}\n",
    "                    for category in list_category:\n",
    "                        count = prompt_output.lower().count(category.lower())\n",
    "                        category_counts[category] = count\n",
    "                    max_count = max(category_counts.values())\n",
    "                    max_categories = [k for k, v in category_counts.items() if v == max_count]\n",
    "                    if max_count == 0:\n",
    "                        return default_category\n",
    "                    else:\n",
    "                        return max_categories[0]\n",
    "                max_categories = [k for k, v in category_counts.items() if v == max_count]\n",
    "                return max_categories[0]\n",
    "    \n",
    "        else:\n",
    "            return default_category\n",
    "    else:\n",
    "        category_counts = {}\n",
    "        for category in list_category:\n",
    "            count = prompt_output.lower().count(category.lower())\n",
    "            category_counts[category] = count\n",
    "        max_count = max(category_counts.values())\n",
    "\n",
    "        if max_count == 0:\n",
    "            return default_category\n",
    "            \n",
    "        max_categories = [k for k, v in category_counts.items() if v == max_count]\n",
    "        return max_categories[0]\n",
    "    \n",
    "\n",
    "def generate_dataset_responses(dataset,prompt_template, eval_output_path):\n",
    "    list_categories = dataset[\"cls_label\"].unique().tolist()\n",
    "    df_results = pd.DataFrame()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for index, sample in tqdm(dataset.iterrows(), total=len(dataset), desc=\"Generating responses\"):\n",
    "        start_time = datetime.now()\n",
    "        messages = create_prompt(sample, prompt_template=prompt_template, model_name=model_name)\n",
    "        llm_response = call_vllm(messages = messages, vllm_api_url= VLLM_API_URL, max_len=MAX_LEN, temperature=temperature).strip()\n",
    "        ground_truth_cls = sample[\"cls_label\"]\n",
    "        parsed_cls = parse_classification(llm_response, list_categories)\n",
    "        y_true.append(ground_truth_cls)\n",
    "        y_pred.append(parsed_cls)\n",
    "        if parsed_cls is None:\n",
    "            is_correct = False\n",
    "        else:\n",
    "            is_correct = ground_truth_cls.lower() == parsed_cls.lower()\n",
    "        end_time = datetime.now()\n",
    "        time_diff = (end_time - start_time).total_seconds()\n",
    "        result = {\n",
    "            \"LLM_Input\": messages,\n",
    "            \"LLM_Output\": llm_response,\n",
    "            \"LLM_Prediciton\": parsed_cls,\n",
    "            \"Ground_Truth\": ground_truth_cls,\n",
    "            \"Iscorrect\": is_correct,\n",
    "            \"Time_Passed\": time_diff,\n",
    "        }\n",
    "        updated_dataframe = pd.DataFrame([result])\n",
    "        updated_dataframe.to_json(eval_output_path, orient=\"records\", lines=True, mode=\"a\")\n",
    "        df_results = pd.concat([df_results, updated_dataframe], axis=0)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"weighted\")  # Use 'weighted' for imbalanced classes\n",
    "    try:\n",
    "        auc = roc_auc_score(pd.get_dummies(y_true), pd.get_dummies(y_pred), multi_class=\"ovr\")\n",
    "    except ValueError:\n",
    "        auc = \"N/A (AUC is not applicable for single-class cases)\"\n",
    "\n",
    "    print(f\"ACC: {acc:.4f}\\tF1: {f1:.4f}\\tAUC: {auc}\")\n",
    "    return df_results\n",
    "\n",
    "def generate_dataset_responses_SIS(dataset, prompt_template_SIS , prompt_template_COT, eval_output_path):\n",
    "    list_categories = dataset[\"cls_label\"].unique().tolist()\n",
    "    df_results = pd.DataFrame()\n",
    "    rep = 3\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for index, sample in tqdm(dataset.iterrows(), total=len(dataset), desc=\"Generating responses\"):\n",
    "        ## Generate Three Paths\n",
    "        start_time = datetime.now()\n",
    "        messages_COT = create_prompt(sample, prompt_template=prompt_template_COT, model_name=model_name)\n",
    "        paths = [call_vllm(messages = messages_COT, vllm_api_url= VLLM_API_URL, max_len=MAX_LEN, temperature=temperature).strip() for i in range(rep)]\n",
    "        messages_SIS = create_SIS_prompt(sample, prompt_template=prompt_template_SIS, path1=paths[0], path2=paths[1], path3=paths[2], model_name=model_name)\n",
    "        llm_response = call_vllm(messages = messages_SIS, vllm_api_url= VLLM_API_URL, max_len=MAX_LEN, temperature=temperature).strip()\n",
    "        ground_truth_cls = sample[\"cls_label\"]\n",
    "        parsed_cls = parse_classification(llm_response, list_categories)\n",
    "        y_true.append(ground_truth_cls)\n",
    "        y_pred.append(parsed_cls)\n",
    "        if parsed_cls is None:\n",
    "            is_correct = False\n",
    "        else:\n",
    "            is_correct = ground_truth_cls.lower() == parsed_cls.lower()\n",
    "        end_time = datetime.now()\n",
    "        time_diff = (end_time - start_time).total_seconds()\n",
    "        messages = {\"messages_COT\": messages_COT, \"messages_SIS\": messages_SIS}\n",
    "        result = {\n",
    "            \"LLM_Input\": messages,\n",
    "            \"LLM_Output\": llm_response,\n",
    "            \"LLM_Prediciton\": parsed_cls,\n",
    "            \"Ground_Truth\": ground_truth_cls,\n",
    "            \"Iscorrect\": is_correct,\n",
    "            \"Time_Passed\": time_diff,\n",
    "            \"Paths\": paths\n",
    "        }\n",
    "        updated_dataframe = pd.DataFrame([result])\n",
    "        updated_dataframe.to_json(eval_output_path, orient=\"records\", lines=True, mode=\"a\")\n",
    "        df_results = pd.concat([df_results, updated_dataframe], axis=0)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"weighted\")  # Use 'weighted' for imbalanced classes\n",
    "    try:\n",
    "        auc = roc_auc_score(pd.get_dummies(y_true), pd.get_dummies(y_pred), multi_class=\"ovr\")\n",
    "    except ValueError:\n",
    "        auc = \"N/A (AUC is not applicable for single-class cases)\"\n",
    "\n",
    "    print(f\"ACC: {acc:.4f}\\tF1: {f1:.4f}\\tAUC: {auc}\")\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting base prompt testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 10/10 [00:00<00:00, 11.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.5000\tF1: 0.4733\tAUC: 0.6180555555555556\n",
      "[INFO] Starting COT prompt testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 10/10 [00:06<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.4000\tF1: 0.3810\tAUC: 0.513888888888889\n",
      "[INFO] Starting COD prompt testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 10/10 [00:02<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.3000\tF1: 0.2500\tAUC: 0.3925925925925926\n",
      "[INFO] Starting Few-shot prompt testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 10/10 [00:00<00:00, 12.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.4000\tF1: 0.3848\tAUC: 0.5101851851851852\n",
      "[INFO] Starting Self-consistency prompt testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating responses: 100%|██████████| 10/10 [00:28<00:00,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.4000\tF1: 0.3500\tAUC: 0.5436507936507936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Starting base prompt testing...\")\n",
    "if is_BASE:\n",
    "    eval_output_path = start_path + training_dataset_path.split(\"/\")[-1].replace(\".jsonl\", f\"_{formatted_time}_is_base_{model_name}.jsonl\")\n",
    "    testset = dataset[dataset[\"split\"]==\"validation\"].sample(sample_size)\n",
    "    generated_outputs_df = generate_dataset_responses(testset, prompt_templates[0], eval_output_path)\n",
    "\n",
    "print(\"[INFO] Starting COT prompt testing...\")\n",
    "if is_COT:\n",
    "    eval_output_path = start_path + training_dataset_path.split(\"/\")[-1].replace(\".jsonl\", f\"_{formatted_time}_is_cot_{model_name}.jsonl\")\n",
    "    testset = dataset[dataset[\"split\"]==\"validation\"].sample(sample_size)\n",
    "    generated_outputs_df = generate_dataset_responses(testset, prompt_templates[1], eval_output_path)\n",
    "\n",
    "print(\"[INFO] Starting COD prompt testing...\")\n",
    "if is_COD:\n",
    "    eval_output_path = start_path + training_dataset_path.split(\"/\")[-1].replace(\".jsonl\", f\"_{formatted_time}_is_cod_{model_name}.jsonl\")\n",
    "    testset = dataset[dataset[\"split\"]==\"validation\"].sample(sample_size)\n",
    "    generated_outputs_df = generate_dataset_responses(testset, prompt_templates[2], eval_output_path)\n",
    "\n",
    "print(\"[INFO] Starting Few-shot prompt testing...\")\n",
    "if is_FEW_SHOT:\n",
    "    eval_output_path = start_path + training_dataset_path.split(\"/\")[-1].replace(\".jsonl\", f\"_{formatted_time}_is_few_shot_{model_name}.jsonl\")\n",
    "    testset = dataset[dataset[\"split\"]==\"validation\"].sample(sample_size)\n",
    "    generated_outputs_df = generate_dataset_responses(testset, prompt_templates[3], eval_output_path)\n",
    "\n",
    "print(\"[INFO] Starting Self-consistency prompt testing...\")\n",
    "if is_SELF_CONSIS:\n",
    "    eval_output_path = start_path + training_dataset_path.split(\"/\")[-1].replace(\".jsonl\", f\"_{formatted_time}_is_self_consis_{model_name}.jsonl\")\n",
    "    testset = dataset[dataset[\"split\"]==\"validation\"].sample(sample_size)\n",
    "    generated_outputs_df = generate_dataset_responses_SIS(testset, prompt_templates[4],prompt_templates[1], eval_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
