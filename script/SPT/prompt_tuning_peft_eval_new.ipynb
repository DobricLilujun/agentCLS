{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/snt/miniconda3/envs/env_classification/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "from datasets.arrow_dataset import Dataset\n",
        "from peft import PeftModel, PeftConfig\n",
        "import os\n",
        "import json\n",
        "\n",
        "from utils.prompts import (\n",
        "    prompt_EUR_BASE,\n",
        "    prompt_EUR_COT,\n",
        "    prompt_EUR_COD,\n",
        "    prompt_EUR_FEW_SHOT,\n",
        "    prompt_LDD_BASE,\n",
        "    prompt_LDD_COT,\n",
        "    prompt_LDD_COD,\n",
        "    prompt_LDD_FEW_SHOT,\n",
        "    prompt_IE_BASE,\n",
        "    prompt_IE_COT,\n",
        "    prompt_IE_COD,\n",
        "    prompt_IE_FEW_SHOT,\n",
        "    prompt_SELF_CONSIS,\n",
        ")\n",
        "\n",
        "GPU_HOURS_DICT = {\n",
        "    \"Llama-3.2-1B-Instruct\": {\n",
        "        \"train\": 27.360,\n",
        "        \"inference\": 25.782\n",
        "    },\n",
        "    \"gemma-2-2b-it\": {\n",
        "        \"train\": 51.496,\n",
        "        \"inference\": 32.664\n",
        "    },\n",
        "    \"Llama-3.2-3B-Instruct\": {\n",
        "        \"train\": 65.522,\n",
        "        \"inference\": 39.556\n",
        "    },\n",
        "    \"ModernBERT-base\": {\n",
        "        \"train\": 27.006,\n",
        "        \"inference\": 1.528\n",
        "    }\n",
        "}\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# ========================== Constants ==========================\n",
        "per_device_train_batch_size = 1\n",
        "per_device_eval_batch_size = 1\n",
        "num_train_epochs = 10\n",
        "learning_rate = 1e-6\n",
        "project_root = \"/home/snt/projects_lujun/agentCLS\"\n",
        "\n",
        "training_dataset_path = \"assets/training_dataset/EURLEX57K_split_equal_train_1000_val_300.jsonl\"\n",
        "model_path = \"/home/snt/projects_lujun/base_models/Llama-3.2-1B-Instruct\"\n",
        "output_dir = \"/home/snt/projects_lujun/agentCLS/assets/logs/prompt_tuning/EURLEX57K_split_equal_train_1000_val_300_0.005_Llama-3.2-1B-Instruct_output_03_17_17_53_45_PROMPT_TUNING_128\"\n",
        "\n",
        "def find_file(filename, search_dir):\n",
        "    for root, dirs, files in os.walk(search_dir):\n",
        "        if filename in files:\n",
        "            return os.path.join(root, filename)\n",
        "    return None\n",
        "\n",
        "json_file_path = find_file(\"adapter_labels.json\", output_dir)\n",
        "max_length = 4096"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_2353518/1562197831.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  filtered_train_data = dataset.groupby('labels', group_keys=False).apply(\n",
            "/tmp/ipykernel_2353518/1562197831.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  filtered_validation_data = dataset.groupby('labels', group_keys=False).apply(\n",
            "Map: 100%|██████████| 3000/3000 [00:00<00:00, 8810.34 examples/s]\n",
            "Map: 100%|██████████| 900/900 [00:00<00:00, 9258.74 examples/s]\n",
            "Map: 100%|██████████| 3000/3000 [00:08<00:00, 339.08 examples/s]\n",
            "Map: 100%|██████████| 900/900 [00:02<00:00, 361.36 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "dict_keys(['celex_id', 'document_type', 'title', 'header', 'recitals', 'main_body', 'eurovoc_concepts', 'split', 'content', 'content_length', 'labels', 'description'])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "is_prefix_tuning = True\n",
        "is_prompt_tuning = False\n",
        "num_virtual_tokens = 128\n",
        "\n",
        "sys.path.append(os.path.abspath(project_root))\n",
        "train_dataset_path = os.path.abspath(os.path.join(project_root, training_dataset_path))\n",
        "\n",
        "peft_config = None\n",
        "\n",
        "train_ratio = 1.0\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "input_dataset_name = train_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
        "model_name = model_path.split(\"/\")[-1]\n",
        "\n",
        "\n",
        "if \"EURLEX\" in training_dataset_path:\n",
        "    prompt_templates = [prompt_EUR_BASE, prompt_EUR_COT, prompt_EUR_COD, prompt_EUR_FEW_SHOT, prompt_SELF_CONSIS]\n",
        "    is_EURLEX = True\n",
        "elif \"LDD\" in training_dataset_path:\n",
        "    prompt_templates = [prompt_LDD_BASE, prompt_LDD_COT, prompt_LDD_COD, prompt_LDD_FEW_SHOT, prompt_SELF_CONSIS]\n",
        "    is_LDD = True\n",
        "elif \"FOYER\" in training_dataset_path:\n",
        "    prompt_templates = [prompt_IE_BASE, prompt_IE_COT, prompt_IE_COD, prompt_IE_FEW_SHOT, prompt_SELF_CONSIS]\n",
        "    is_IE = True\n",
        "else:\n",
        "    raise ValueError(f\"Unknown dataset: {training_dataset_path}\")\n",
        "    \n",
        "dataset = pd.read_json(train_dataset_path, lines=True)\n",
        "dataset.rename(columns={\"cls_label\": \"labels\"}, inplace=True)\n",
        "\n",
        "# Compute sample counts for each group based on 'labels' and 'split'\n",
        "sample_counts = dataset.groupby(['labels', 'split']).size() * train_ratio\n",
        "\n",
        "filtered_train_data = dataset.groupby('labels', group_keys=False).apply(\n",
        "    lambda x: x[x['split'] == 'train'].iloc[:int(sample_counts.loc[x.name, 'train'])]\n",
        ")\n",
        "\n",
        "filtered_validation_data = dataset.groupby('labels', group_keys=False).apply(\n",
        "    lambda x: x[x['split'] == 'validation'].iloc[:int(sample_counts.loc[x.name, 'validation'])]\n",
        ")\n",
        "\n",
        "filtered_train = filtered_train_data.reset_index(drop=True)\n",
        "filtered_validation = filtered_validation_data.reset_index(drop=True)\n",
        "\n",
        "\n",
        "train_dataset = Dataset.from_pandas(filtered_train)\n",
        "val_dataset = Dataset.from_pandas(filtered_validation)\n",
        "\n",
        "# Tokenization\n",
        "def tokenize(examples):\n",
        "    return tokenizer(examples[\"content\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
        "\n",
        "with open(json_file_path, \"r\") as json_file:\n",
        "    loaded_data = json.load(json_file)\n",
        "\n",
        "label2id = loaded_data[\"label2id\"]\n",
        "id2label = loaded_data[\"id2label\"]\n",
        "label2id = {label: int(i) for label, i in label2id.items()}\n",
        "id2label = {int(i): label for i, label in id2label.items()}\n",
        "\n",
        "train_dataset = train_dataset.map(lambda x: {\"labels\": label2id[x[\"labels\"]]})\n",
        "val_dataset = val_dataset.map(lambda x: {\"labels\": label2id[x[\"labels\"]]})\n",
        "\n",
        "\n",
        "keep_columns = [\"labels\", \"input_ids\", \"attention_mask\"]\n",
        "tokenized_train_dataset = train_dataset.map(tokenize, batched=True, remove_columns=[col for col in train_dataset.column_names if col not in keep_columns])\n",
        "tokenized_val_dataset = val_dataset.map(tokenize, batched=True, remove_columns=[col for col in val_dataset.column_names if col not in keep_columns])\n",
        "train_dataset.features.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_last_checkpoints(output_dir):\n",
        "    checkpoints = os.listdir(output_dir)\n",
        "    checkpoints = [c for c in checkpoints if \"checkpoint\" in c]\n",
        "    checkpoints = [int(c.split(\"-\")[-1]) for c in checkpoints]\n",
        "    last_checkpoint = max(checkpoints)\n",
        "    return f\"{output_dir}/checkpoint-{last_checkpoint}\"\n",
        "\n",
        "def get_all_checkpoints(output_dir):\n",
        "    # List all checkpoint directories in output_dir\n",
        "    checkpoints = os.listdir(output_dir)\n",
        "    # Filter for directories that include 'checkpoint' in their name\n",
        "    checkpoints = [c for c in checkpoints if \"checkpoint\" in c]\n",
        "    # Return the full paths to all the checkpoint directories\n",
        "    return [os.path.join(output_dir, checkpoint) for checkpoint in checkpoints]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating models in: /home/snt/projects_lujun/agentCLS/assets/logs/prompt_tuning/EURLEX57K_split_equal_train_1000_val_300_0.005_Llama-3.2-1B-Instruct_output_03_17_17_53_45_PROMPT_TUNING_128\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/snt/projects_lujun/base_models/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Processing validation data:   0%|          | 0/900 [00:00<?, ?sample/s]LlamaForSequenceClassification will not detect padding tokens in `inputs_embeds`. Results may be unexpected if using padding tokens in conjunction with `inputs_embeds.`\n",
            "Processing validation data:  13%|█▎        | 117/900 [01:33<10:26,  1.25sample/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 229\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# Execute evaluation\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating models in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 229\u001b[0m \u001b[43mevaluate_multiple\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[4], line 133\u001b[0m, in \u001b[0;36mevaluate_multiple\u001b[0;34m(output_dir, val_dataset)\u001b[0m\n\u001b[1;32m    131\u001b[0m logits \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    132\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m predicted_class_idx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Convert indices to labels\u001b[39;00m\n\u001b[1;32m    136\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m id2label[predicted_class_idx]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from tensorboard.backend.event_processing import event_accumulator\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import os\n",
        "import math \n",
        "# GPU resource consumption dictionary for different models\n",
        "GPU_HOURS_DICT = {\n",
        "    \"Llama-3.2-1B-Instruct\": {\n",
        "        \"train\": 27.360,\n",
        "        \"inference\": 25.782\n",
        "    },\n",
        "    \"gemma-2-2b-it\": {\n",
        "        \"train\": 51.496,\n",
        "        \"inference\": 32.664\n",
        "    },\n",
        "    \"Llama-3.2-3B-Instruct\": {\n",
        "        \"train\": 65.522,\n",
        "        \"inference\": 39.556\n",
        "    },\n",
        "    \"ModernBERT-base\": {\n",
        "        \"train\": 27.006,\n",
        "        \"inference\": 1.528\n",
        "    }\n",
        "}\n",
        "\n",
        "# Weighting parameters for efficiency metrics\n",
        "ALPHA = 0.5  # Training weight\n",
        "BETA = 0.5   # Inference weight\n",
        "\n",
        "def extract_training_time(log_dir, scalar_name=\"train/loss\"):\n",
        "    \"\"\"\n",
        "    Extract training duration from TensorBoard event files.\n",
        "    \n",
        "    Args:\n",
        "        log_dir (str): Directory containing TensorBoard logs\n",
        "        scalar_name (str): Name of scalar to track for timing\n",
        "        \n",
        "    Returns:\n",
        "        tuple: Start time, end time, and total duration in seconds\n",
        "    \"\"\"\n",
        "    # Find all event files in the directory\n",
        "    event_files = [\n",
        "        os.path.join(root, file)\n",
        "        for root, _, files in os.walk(log_dir)\n",
        "        for file in files\n",
        "        if \"events\" in file\n",
        "    ]\n",
        "\n",
        "    if not event_files:\n",
        "        raise FileNotFoundError(f\"No TensorBoard event files found in {log_dir}\")\n",
        "    \n",
        "    # Use the first event file found\n",
        "    event_file = event_files[0] \n",
        "    \n",
        "    # Load the event file\n",
        "    ea = event_accumulator.EventAccumulator(event_file)\n",
        "    ea.Reload()\n",
        "    \n",
        "    # Verify the scalar exists\n",
        "    available_keys = ea.scalars.Keys()\n",
        "    if scalar_name not in available_keys:\n",
        "        raise ValueError(f\"Scalar '{scalar_name}' not found. Available keys: {available_keys}\")\n",
        "    \n",
        "    # Extract wall times from the scalar events\n",
        "    wall_times = ea.Scalars(scalar_name)\n",
        "    start_time = wall_times[0].wall_time\n",
        "    end_time = wall_times[-1].wall_time\n",
        "    \n",
        "    return start_time, end_time, end_time - start_time\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_multiple(output_dir, val_dataset):\n",
        "    \"\"\"\n",
        "    Evaluate model performance and efficiency metrics across multiple checkpoints.\n",
        "    \n",
        "    Args:\n",
        "        output_dir (str): Directory containing model checkpoints\n",
        "        val_dataset: Validation dataset for evaluation\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    \n",
        "    # Get all checkpoint paths\n",
        "    checkpoint_paths = get_all_checkpoints(output_dir)\n",
        "    \n",
        "    for checkpoint_path in checkpoint_paths:\n",
        "        config = PeftConfig.from_pretrained(checkpoint_path)\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path, label2id=label2id, id2label=id2label, ).to(device)\n",
        "        model = PeftModel.from_pretrained(model, checkpoint_path)\n",
        "        model.config.pad_token_id = tokenizer.pad_token_id\n",
        "        model.config.use_cache = False\n",
        "        model.config.pretraining_tp = 1\n",
        "        model.gradient_checkpointing_enable()\n",
        "        model.eval()\n",
        "        \n",
        "        # Initialize data structures for evaluation\n",
        "        validation_results = []\n",
        "        true_label_one_hot_list = []\n",
        "        true_labels = []\n",
        "        predicted_labels = []\n",
        "        all_probs = []\n",
        "        \n",
        "        # Start inference timing\n",
        "        start_time = time.time()\n",
        "        \n",
        "        # Process validation data\n",
        "        for input_sample in tqdm(val_dataset, desc=\"Processing validation data\", unit=\"sample\"):\n",
        "            sample = input_sample['content']\n",
        "            true_label_idx = int(input_sample['labels'])\n",
        "            \n",
        "            # Tokenize and predict\n",
        "            tokenized_input = tokenizer(\n",
        "                sample, \n",
        "                padding=\"max_length\", \n",
        "                max_length=max_length, \n",
        "                truncation=True, \n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                model_output = model(**tokenized_input)\n",
        "                \n",
        "            # Process prediction results\n",
        "            logits = model_output.logits\n",
        "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "            predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
        "            \n",
        "            # Convert indices to labels\n",
        "            predicted_label = id2label[predicted_class_idx]\n",
        "            true_label = id2label[true_label_idx]\n",
        "            \n",
        "            # Create one-hot encoding of true label\n",
        "            true_label_one_hot = np.zeros(probabilities.size(-1))\n",
        "            true_label_one_hot[true_label_idx] = 1\n",
        "            \n",
        "            # Store results\n",
        "            true_labels.append(true_label)\n",
        "            true_label_one_hot_list.append(true_label_one_hot)\n",
        "            predicted_labels.append(predicted_label)\n",
        "            all_probs.append(probabilities.detach().cpu().numpy())\n",
        "            \n",
        "            # Store detailed results for each sample\n",
        "            result = {\n",
        "                'content': sample,\n",
        "                'true_label': true_label,\n",
        "                'predicted_label': predicted_label,\n",
        "                'true_label_one_hot': true_label_one_hot.tolist(),\n",
        "                'predicted_class_idx': predicted_class_idx,\n",
        "                'probabilities': probabilities.detach().cpu().numpy().tolist()\n",
        "            }\n",
        "            validation_results.append(result)\n",
        "        \n",
        "        # Calculate inference time\n",
        "        end_time = time.time()\n",
        "        inference_time = end_time - start_time\n",
        "        inference_time_hours = round(inference_time / 3600, 4)\n",
        "        \n",
        "        # Get training time\n",
        "        start_time, end_time, training_duration = extract_training_time(output_dir)\n",
        "        training_time_hours = round(training_duration / 3600, 4)\n",
        "        \n",
        "        # Save validation results\n",
        "        timestamp = datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
        "        df_validation_results = pd.DataFrame(validation_results)\n",
        "        jsonl_file_path = os.path.join(checkpoint_path, f'validation_results_{timestamp}.jsonl')\n",
        "        df_validation_results.to_json(jsonl_file_path, orient='records', lines=True)\n",
        "        \n",
        "        # Calculate performance metrics\n",
        "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "        f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
        "        \n",
        "        # Find GPU resource consumption for the model\n",
        "        model_name = None\n",
        "        for name in GPU_HOURS_DICT:\n",
        "            if name in output_dir:\n",
        "                model_name = name\n",
        "                gpu_ram_train = GPU_HOURS_DICT[name][\"train\"]\n",
        "                gpu_ram_inference = GPU_HOURS_DICT[name][\"inference\"]\n",
        "                break\n",
        "        \n",
        "        if not model_name:\n",
        "            raise ValueError(f\"Model name not found in GPU_HOURS_DICT: {output_dir}\")\n",
        "        \n",
        "        # Calculate efficiency metrics\n",
        "        training_gpu_hours_ram = gpu_ram_train * training_time_hours\n",
        "        inference_gpu_hours_ram = gpu_ram_inference * inference_time_hours\n",
        "        total_gpu_hours_ram = training_gpu_hours_ram + inference_gpu_hours_ram\n",
        "        \n",
        "        # Calculate composite metrics\n",
        "        resource_m = f1 /  math.log((ALPHA * training_gpu_hours_ram + BETA * inference_gpu_hours_ram)+1)\n",
        "        time_m = f1 /  math.log((ALPHA * training_time_hours + BETA * inference_time_hours+1))\n",
        "        \n",
        "        # Calculate time and resource ratios\n",
        "        ratio_time = inference_time_hours / (inference_time_hours + training_time_hours)\n",
        "        ratio_time_ram = inference_gpu_hours_ram / (inference_gpu_hours_ram + training_gpu_hours_ram)\n",
        "        \n",
        "        # Print evaluation results\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"Evaluation Results for {model_name or 'Unknown Model'}\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Performance Metrics:\")\n",
        "        print(f\"  Accuracy:    {accuracy:.4f}\")\n",
        "        print(f\"  F1 Score:    {f1:.4f}\")\n",
        "        # print(f\"  AUC:         {auc:.4f}\")\n",
        "        \n",
        "        print(\"\\nEfficiency Metrics:\")\n",
        "        print(f\"  Resource_M:  {resource_m:.4f}\")\n",
        "        print(f\"  Time_M:      {time_m:.4f}\")\n",
        "        \n",
        "        print(\"\\nTime Distribution:\")\n",
        "        print(f\"  Inference/Total Time Ratio: {ratio_time:.4f}\")\n",
        "        print(f\"  Inference/Total RAM Ratio:  {ratio_time_ram:.4f}\")\n",
        "        \n",
        "        print(\"\\nGPU Resource Usage (GPU hours × RAM):\")\n",
        "        print(f\"  Total:       {total_gpu_hours_ram:.4f}\")\n",
        "        print(f\"  Training:    {training_gpu_hours_ram:.4f}\")\n",
        "        print(f\"  Inference:   {inference_gpu_hours_ram:.4f}\")\n",
        "        print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# Execute evaluation\n",
        "print(f\"Evaluating models in: {output_dir}\")\n",
        "evaluate_multiple(output_dir=output_dir, val_dataset=val_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "LLM 02 - Prompt Tuning with PEFT",
      "widgets": {}
    },
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env_classification",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
