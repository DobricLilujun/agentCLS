{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "31738463-c9b0-431d-869e-1735e1e2f5c7",
          "showTitle": false,
          "title": ""
        },
        "id": "KWOEt-yOVaTB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/snt/miniconda3/envs/env_classification/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
        "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\"\n",
        "model_name_or_path = \"bigscience/bloomz-560m\"\n",
        "tokenizer_name_or_path = \"bigscience/bloomz-560m\"\n",
        "peft_config = PromptTuningConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
        "    num_virtual_tokens=8,\n",
        "    prompt_tuning_init_text=\"Classify if the tweet is a complaint or not:\",\n",
        "    tokenizer_name_or_path=model_name_or_path,\n",
        ")\n",
        "\n",
        "dataset_name = \"twitter_complaints\"\n",
        "checkpoint_name = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}_v1.pt\".replace(\n",
        "    \"/\", \"_\"\n",
        ")\n",
        "text_column = \"Tweet text\"\n",
        "label_column = \"text_label\"\n",
        "max_length = 64\n",
        "lr = 3e-2\n",
        "num_epochs = 50\n",
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xhm3jZMVaTD",
        "outputId": "305f0137-6a02-4e43-9c9d-2b4ecd377937"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Tweet text': '@HMRCcustomers No this is my first job',\n",
              " 'ID': 0,\n",
              " 'Label': 2,\n",
              " 'text_label': 'no complaint'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"ought/raft\", dataset_name)\n",
        "\n",
        "classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n",
        "dataset = dataset.map(\n",
        "    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n",
        "    batched=True,\n",
        "    num_proc=1,\n",
        ")\n",
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n",
        "print(target_max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    batch_size = len(examples[text_column])\n",
        "    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n",
        "    targets = [str(x) for x in examples[label_column]]\n",
        "    model_inputs = tokenizer(inputs)\n",
        "    labels = tokenizer(targets)\n",
        "    for i in range(batch_size):\n",
        "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
        "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
        "        # print(i, sample_input_ids, label_input_ids)\n",
        "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
        "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
        "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
        "    # print(model_inputs)\n",
        "    for i in range(batch_size):\n",
        "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
        "        label_input_ids = labels[\"input_ids\"][i]\n",
        "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
        "            max_length - len(sample_input_ids)\n",
        "        ) + sample_input_ids\n",
        "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
        "            \"attention_mask\"\n",
        "        ][i]\n",
        "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
        "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
        "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
        "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 3531.09 examples/s]\n",
            "Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3399/3399 [00:00<00:00, 5593.64 examples/s]\n"
          ]
        }
      ],
      "source": [
        "processed_datasets = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=1,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        "    load_from_cache_file=False,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = processed_datasets[\"train\"]\n",
        "eval_dataset = processed_datasets[\"test\"]\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
        ")\n",
        "eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 8,192 || all params: 559,222,784 || trainable%: 0.0015\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
        "model = get_peft_model(model, peft_config)\n",
        "print(model.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "lr_scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/snt/miniconda3/envs/env_classification/lib/python3.9/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "/tmp/ipykernel_532688/1976273226.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [350/350 19:55, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>38.256229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>46.194800</td>\n",
              "      <td>37.542969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>44.774700</td>\n",
              "      <td>36.719383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>44.774700</td>\n",
              "      <td>35.852859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>45.172500</td>\n",
              "      <td>35.049133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>42.038000</td>\n",
              "      <td>34.302338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>42.038000</td>\n",
              "      <td>33.809715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>37.850700</td>\n",
              "      <td>33.287140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>40.841600</td>\n",
              "      <td>32.852196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>37.891400</td>\n",
              "      <td>32.383926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>37.891400</td>\n",
              "      <td>31.916389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>37.771300</td>\n",
              "      <td>31.447659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>37.036800</td>\n",
              "      <td>31.000729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>37.036800</td>\n",
              "      <td>30.569267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>31.027500</td>\n",
              "      <td>30.149023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>36.585300</td>\n",
              "      <td>29.754211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>36.585300</td>\n",
              "      <td>29.373613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>34.422400</td>\n",
              "      <td>29.010187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>33.408700</td>\n",
              "      <td>28.664736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>31.973900</td>\n",
              "      <td>28.332489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>31.973900</td>\n",
              "      <td>28.019783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>30.653200</td>\n",
              "      <td>27.724251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>29.192500</td>\n",
              "      <td>27.439085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>29.192500</td>\n",
              "      <td>27.166393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>29.580100</td>\n",
              "      <td>26.913177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>29.009000</td>\n",
              "      <td>26.674086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>29.009000</td>\n",
              "      <td>26.441813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>27.740600</td>\n",
              "      <td>26.221106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>27.663500</td>\n",
              "      <td>26.014215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>26.277000</td>\n",
              "      <td>25.822287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>26.277000</td>\n",
              "      <td>25.640099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>26.071900</td>\n",
              "      <td>25.465580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>27.909200</td>\n",
              "      <td>25.307283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>27.909200</td>\n",
              "      <td>25.158020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>25.855400</td>\n",
              "      <td>25.021711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>27.969900</td>\n",
              "      <td>24.894981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>27.969900</td>\n",
              "      <td>24.776035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>23.685000</td>\n",
              "      <td>24.667540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>27.270000</td>\n",
              "      <td>24.567934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>25.422000</td>\n",
              "      <td>24.480326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>25.422000</td>\n",
              "      <td>24.403936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>24.989100</td>\n",
              "      <td>24.335068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>25.226100</td>\n",
              "      <td>24.271229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>25.226100</td>\n",
              "      <td>24.214691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>26.845800</td>\n",
              "      <td>24.169455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>24.562600</td>\n",
              "      <td>24.132097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>24.562600</td>\n",
              "      <td>24.103273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>24.090300</td>\n",
              "      <td>24.082497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>24.205700</td>\n",
              "      <td>24.069542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>23.068800</td>\n",
              "      <td>24.064772</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='425' max='425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [425/425 00:22]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 24.06477165222168,\n",
              " 'eval_runtime': 22.6262,\n",
              " 'eval_samples_per_second': 150.224,\n",
              " 'eval_steps_per_second': 18.784,\n",
              " 'epoch': 50.0}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataloader.dataset,\n",
        "    eval_dataset=eval_dataloader.dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_id = \"/home/snt/projects_lujun/agentCLS/peft_outputs_sentences/checkpoint-24\"\n",
        "\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = tokenizer(\n",
        "    f'{text_column} : {\"@nationalgridus I have no water and the bill is current and paid. Can you do something about this?\"} Label : ',\n",
        "    return_tensors=\"pt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Tweet text : @nationalgridus I have no water and the bill is current and paid. Can you do something about this? Label :  @nationalgridusThe United States of America']\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n",
        "    )\n",
        "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "LLM 02 - Prompt Tuning with PEFT",
      "widgets": {}
    },
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env_classification",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
