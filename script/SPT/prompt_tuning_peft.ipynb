{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "31738463-c9b0-431d-869e-1735e1e2f5c7",
          "showTitle": false,
          "title": ""
        },
        "id": "KWOEt-yOVaTB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/snt/miniconda3/envs/env_classification/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "/tmp/ipykernel_2144361/2050568447.py:161: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  filtered_train_data = dataset.groupby('labels', group_keys=False).apply(\n",
            "/tmp/ipykernel_2144361/2050568447.py:165: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  filtered_validation_data = dataset.groupby('labels', group_keys=False).apply(\n",
            "Map: 100%|██████████| 15/15 [00:00<00:00, 2281.83 examples/s]\n",
            "Map: 100%|██████████| 3/3 [00:00<00:00, 631.10 examples/s]\n",
            "Casting the dataset: 100%|██████████| 15/15 [00:00<00:00, 3726.06 examples/s]\n",
            "Casting the dataset: 100%|██████████| 3/3 [00:00<00:00, 819.84 examples/s]\n",
            "Map: 100%|██████████| 15/15 [00:00<00:00, 298.62 examples/s]\n",
            "Map: 100%|██████████| 3/3 [00:00<00:00, 196.87 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "dict_keys(['celex_id', 'document_type', 'title', 'header', 'recitals', 'main_body', 'eurovoc_concepts', 'split', 'content', 'content_length', 'labels', 'description'])"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType, PrefixTuningConfig\n",
        "import torch\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from datasets.arrow_dataset import Dataset\n",
        "from datasets import ClassLabel\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from transformers import EarlyStoppingCallback\n",
        "from peft import PeftModel, PeftConfig\n",
        "import time\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "# ========================== CMD Argument Parser ==========================\n",
        "# def parse_args():\n",
        "#     parser = argparse.ArgumentParser(description=\"Train a model using CPT (Continual Pretraining Training)\")\n",
        "#     parser.add_argument(\"--per_device_train_batch_size\", type=int, default=8, help=\"Batch size per device during training\")\n",
        "#     parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=8, help=\"Batch size per device during evaluation\")\n",
        "#     parser.add_argument(\"--num_train_epochs\", type=int, default=1, help=\"Number of training epochs\")\n",
        "#     parser.add_argument(\"--learning_rate\", type=float, default=1e-6, help=\"Learning rate for training\")\n",
        "#     parser.add_argument(\"--project_root\", type=str, default=\"/Users/lujun.li/projects/mt_luxembourgish\", help=\"Path to project root\")\n",
        "#     parser.add_argument(\"--training_dataset_path\", type=str, default=\"data/processed/dataset_merged_llama_fake_targets.jsonl\", help=\"Path to training dataset\")\n",
        "#     parser.add_argument(\"--model_path\", type=str, default=\"/home/llama/Personal_Directories/srb/binary_classfication/Llama-3.2-3B-Instruct\", help=\"Path to model\")\n",
        "#     parser.add_argument(\"--resume_from_checkpoint\", type=bool, default=False, help=\"Resume training from checkpoint\")\n",
        "#     parser.add_argument(\"--resume_checkpoint_path\", type=str, default=None, help=\"Path to checkpoint to resume training from\")\n",
        "#     parser.add_argument(\"--is_prefix_tuning\", type=bool, default=True, help=\"Whether to use PrefixTuning\")\n",
        "#     parser.add_argument(\"--is_prompt_tuning\", type=bool, default=False, help=\"Whether to use PromptTuning\")\n",
        "#     parser.add_argument(\"--num_virtual_tokens\", type=int, default=32, help=\"Number of virtual tokens\")\n",
        "#     return parser.parse_args()\n",
        "\n",
        "# args = parse_args()\n",
        "# per_device_eval_batch_size = args.per_device_eval_batch_size\n",
        "# per_device_train_batch_size = args.per_device_train_batch_size\n",
        "# num_train_epochs = args.num_train_epochs\n",
        "# learning_rate = args.learning_rate\n",
        "# project_root = args.project_root\n",
        "# training_dataset_path = args.training_dataset_path\n",
        "# model_path = args.model_path\n",
        "# resume_from_checkpoint = args.resume_from_checkpoint\n",
        "# resume_checkpoint_path = args.resume_checkpoint_path\n",
        "# is_prefix_tuning = args.is_prefix_tuning\n",
        "# is_prompt_tuning = args.is_prompt_tuning\n",
        "# num_virtual_tokens = args.num_virtual_tokens\n",
        "\n",
        "\n",
        "# ========================== Constants ==========================\n",
        "per_device_train_batch_size = 1\n",
        "per_device_eval_batch_size = 1\n",
        "num_train_epochs = 10\n",
        "learning_rate = 1e-6\n",
        "project_root = \"/home/snt/projects_lujun/agentCLS\"\n",
        "training_dataset_path = \"assets/training_dataset/EURLEX57K_split_equal_train_1000_val_300.jsonl\"\n",
        "model_path = \"/home/snt/projects_lujun/base_models/Llama-3.2-1B-Instruct\"\n",
        "is_prefix_tuning = True\n",
        "is_prompt_tuning = False\n",
        "num_virtual_tokens = 128\n",
        "\n",
        "sys.path.append(os.path.abspath(project_root))\n",
        "train_dataset_path = os.path.abspath(os.path.join(project_root, training_dataset_path))\n",
        "\n",
        "peft_config = None\n",
        "resume_from_checkpoint = False\n",
        "resume_checkpoint_path = None\n",
        "train_ratio = 0.005\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "input_dataset_name = train_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
        "model_name = model_path.split(\"/\")[-1]\n",
        "max_length = 4096\n",
        "train_seed = 3407\n",
        "logging_steps = 10\n",
        "eval_steps = 100\n",
        "eval_strategy = \"epoch\"\n",
        "save_strategy = \"epoch\"\n",
        "save_total_limit = 2\n",
        "logging_strategy = \"steps\"\n",
        "max_grad_norm = 0.3\n",
        "label_names = \"labels\"\n",
        "\n",
        "\n",
        "\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "\n",
        "if is_prefix_tuning:\n",
        "    peft_config = PrefixTuningConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        num_virtual_tokens=num_virtual_tokens,\n",
        "        base_model_name_or_path=model_path,\n",
        "    )\n",
        "\n",
        "if is_prompt_tuning:\n",
        "    peft_config = PromptTuningConfig(\n",
        "        task_type=TaskType.SEQ_CLS,\n",
        "        prompt_tuning_init=PromptTuningInit.TEXT,\n",
        "        num_virtual_tokens=num_virtual_tokens,\n",
        "        prompt_tuning_init_text=prompt_templates[0],\n",
        "        tokenizer_name_or_path=model_path,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "if resume_from_checkpoint and resume_checkpoint_path is None:\n",
        "    raise ValueError(\"Please provide a checkpoint path to resume training from\")\n",
        "\n",
        "if resume_from_checkpoint:\n",
        "    output_dir = resume_checkpoint_path\n",
        "else:\n",
        "    current_time = datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
        "    if \"PREFIX_TUNING\" in peft_config.peft_type:\n",
        "        output_dir = f\"{project_root}/assets/logs/prompt_tuning/{input_dataset_name}_{train_ratio}_{model_name}_output_{current_time}_PREFIX_TUNING_{peft_config.num_virtual_tokens}\"\n",
        "    elif \"PROMPT_TUNING\" in peft_config.peft_type:\n",
        "        output_dir = f\"{project_root}/assets/logs/prompt_tuning/{input_dataset_name}_{train_ratio}_{model_name}_output_{current_time}_PROMPT_TUNING_{peft_config.num_virtual_tokens}\"\n",
        "    else:\n",
        "        raise ValueError(\"Please provide a valid peft_type\")\n",
        "    \n",
        "dataset = pd.read_json(train_dataset_path, lines=True)\n",
        "dataset.rename(columns={\"cls_label\": \"labels\"}, inplace=True)\n",
        "\n",
        "# Compute sample counts for each group based on 'labels' and 'split'\n",
        "sample_counts = dataset.groupby(['labels', 'split']).size() * train_ratio\n",
        "\n",
        "filtered_train_data = dataset.groupby('labels', group_keys=False).apply(\n",
        "    lambda x: x[x['split'] == 'train'].iloc[:int(sample_counts.loc[x.name, 'train'])]\n",
        ")\n",
        "\n",
        "filtered_validation_data = dataset.groupby('labels', group_keys=False).apply(\n",
        "    lambda x: x[x['split'] == 'validation'].iloc[:int(sample_counts.loc[x.name, 'validation'])]\n",
        ")\n",
        "\n",
        "filtered_train = filtered_train_data.reset_index(drop=True)\n",
        "filtered_validation = filtered_validation_data.reset_index(drop=True)\n",
        "\n",
        "\n",
        "train_dataset = Dataset.from_pandas(filtered_train)\n",
        "val_dataset = Dataset.from_pandas(filtered_validation)\n",
        "\n",
        "\n",
        "# Tokenization\n",
        "def tokenize(examples):\n",
        "    return tokenizer(examples[\"content\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
        "\n",
        "labels =  set(train_dataset['labels'])\n",
        "num_labels = len(labels)\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label\n",
        "\n",
        "train_dataset = train_dataset.map(lambda x: {\"labels\": label2id[x[\"labels\"]]})\n",
        "val_dataset = val_dataset.map(lambda x: {\"labels\": label2id[x[\"labels\"]]})\n",
        "\n",
        "labels_ids = list(set(train_dataset['labels']))\n",
        "class_label = ClassLabel(num_classes=len(labels_ids), names=labels_ids)\n",
        "train_dataset = train_dataset.cast_column(\"labels\", class_label)\n",
        "val_dataset = val_dataset.cast_column(\"labels\", class_label)\n",
        "\n",
        "# def apply_prompt_template(examples):\n",
        "#     # Applying the prompt template to the 'content' column\n",
        "#     return {\n",
        "#         \"content\": prompt_templates[0].format(input=examples[\"content\"])\n",
        "#     }\n",
        "\n",
        "# train_dataset = train_dataset.map(apply_prompt_template)\n",
        "# val_dataset = val_dataset.map(apply_prompt_template)\n",
        "\n",
        "keep_columns = [\"labels\", \"input_ids\", \"attention_mask\"]\n",
        "tokenized_train_dataset = train_dataset.map(tokenize, batched=True, remove_columns=[col for col in train_dataset.column_names if col not in keep_columns])\n",
        "tokenized_val_dataset = val_dataset.map(tokenize, batched=True, remove_columns=[col for col in val_dataset.column_names if col not in keep_columns])\n",
        "train_dataset.features.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/snt/projects_lujun/base_models/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,103,296 || all params: 1,237,923,840 || trainable%: 0.1699\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels, label2id=label2id, id2label=id2label)\n",
        "model = get_peft_model(model, peft_config)\n",
        "print(model.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 01:21, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.893500</td>\n",
              "      <td>1.967448</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.066700</td>\n",
              "      <td>1.932617</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.851900</td>\n",
              "      <td>1.937500</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.140700</td>\n",
              "      <td>1.916016</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.998200</td>\n",
              "      <td>1.917643</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.582800</td>\n",
              "      <td>1.883789</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.040600</td>\n",
              "      <td>1.866536</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.523500</td>\n",
              "      <td>1.883464</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.694700</td>\n",
              "      <td>1.855143</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.156100</td>\n",
              "      <td>1.871419</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished training SFT.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/snt/projects_lujun/base_models/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Processing validation data: 100%|██████████| 3/3 [00:02<00:00,  1.28sample/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.3333\n",
            "F1 Score: 0.1667\n",
            "AUC: 0.5000\n",
            "Average Inference Time per Sample: 0.1570 seconds\n",
            "Finished training and evaluation.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json \n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    acc = accuracy_score(labels, np.argmax(predictions, axis=-1))\n",
        "    f1 = f1_score(labels, np.argmax(predictions, axis=-1), average=\"weighted\")\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "def train():\n",
        "    # Define training args\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir= output_dir,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        bf16=True,\n",
        "        optim=\"adamw_torch_fused\", \n",
        "        logging_strategy=logging_strategy,\n",
        "        logging_steps=logging_steps,\n",
        "        eval_strategy=eval_strategy,\n",
        "        eval_steps=eval_steps,\n",
        "        save_strategy=save_strategy,\n",
        "        save_total_limit=save_total_limit,\n",
        "        load_best_model_at_end=True,\n",
        "        max_grad_norm=max_grad_norm,\n",
        "        # group_by_length=True,\n",
        "        # use_mps_device=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        # push to hub parameters\n",
        "        # push_to_hub=True,\n",
        "        # hub_strategy=\"every_save\",\n",
        "        # hub_token=HfFolder.get_token(),\n",
        "        report_to=\"tensorboard\",\n",
        "        disable_tqdm=False,\n",
        "        seed = train_seed,\n",
        "        # label_names=list(id2label.values()),\n",
        "    )\n",
        "    \n",
        "    # Create a Trainer instance\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train_dataset,\n",
        "        eval_dataset=tokenized_val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "    )\n",
        "    trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "    print(\"Finished training SFT.\")\n",
        "    return trainer_stats\n",
        "\n",
        "def get_last_checkpoints(output_dir):\n",
        "    checkpoints = os.listdir(output_dir)\n",
        "    checkpoints = [c for c in checkpoints if \"checkpoint\" in c]\n",
        "    checkpoints = [int(c.split(\"-\")[-1]) for c in checkpoints]\n",
        "    last_checkpoint = max(checkpoints)\n",
        "    return f\"{output_dir}/checkpoint-{last_checkpoint}\"\n",
        "\n",
        "def get_all_checkpoints(output_dir):\n",
        "    # List all checkpoint directories in output_dir\n",
        "    checkpoints = os.listdir(output_dir)\n",
        "    # Filter for directories that include 'checkpoint' in their name\n",
        "    checkpoints = [c for c in checkpoints if \"checkpoint\" in c]\n",
        "    # Return the full paths to all the checkpoint directories\n",
        "    return [os.path.join(output_dir, checkpoint) for checkpoint in checkpoints]\n",
        "\n",
        "\n",
        "def evaluate():\n",
        "    \n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    checkpoints_path  = get_last_checkpoints(output_dir)\n",
        "    config = PeftConfig.from_pretrained(checkpoints_path)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path, label2id=label2id, id2label=id2label, ).to(device)\n",
        "    model = PeftModel.from_pretrained(model, checkpoints_path)\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    validation_results = []\n",
        "\n",
        "    # Initialize lists to store true and predicted labels\n",
        "    true_label_one_hot_list = []\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    # Start time for measuring inference efficiency\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate through validation dataset and make predictions\n",
        "    for input in tqdm(val_dataset, desc=\"Processing validation data\", unit=\"sample\"):\n",
        "        sample = input['content']\n",
        "        true_label_idx = int(input['labels'])\n",
        "        tokenized_input = tokenizer(sample, padding=\"max_length\", max_length=max_length, truncation=True, return_tensors=\"pt\").to(device)\n",
        "        model_output = model(**tokenized_input)\n",
        "        logits = model_output.logits\n",
        "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
        "        predicted_label = id2label[str(predicted_class_idx)]\n",
        "        \n",
        "        true_label = id2label[str(true_label_idx)]\n",
        "        true_label_one_hot = np.zeros(probabilities.size(-1))\n",
        "        true_label_one_hot[true_label_idx] = 1\n",
        "\n",
        "        true_labels.append(true_label)\n",
        "        true_label_one_hot_list.append(true_label_one_hot)\n",
        "        predicted_labels.append(predicted_label)\n",
        "        all_probs.append(probabilities.detach().cpu().numpy())  # Store the raw probabilities for AUC\n",
        "        result = {\n",
        "            'content': sample,\n",
        "            'true_label': true_label,\n",
        "            'predicted_label': predicted_label,\n",
        "            'true_label_one_hot': true_label_one_hot.tolist(),\n",
        "            'predicted_class_idx': predicted_class_idx,\n",
        "            'probabilities': probabilities.detach().cpu().numpy().tolist()  # Convert to list for JSON serialization\n",
        "        }\n",
        "        validation_results.append(result)\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
        "    df_validation_results = pd.DataFrame(validation_results)\n",
        "    jsonl_file_path = os.path.join(checkpoints_path, f'validation_results_{timestamp}.jsonl')\n",
        "    df_validation_results.to_json(jsonl_file_path, orient='records', lines=True)\n",
        "\n",
        "\n",
        "    # Calculate accuracy, F1 score, and AUC\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    f1 = f1_score(true_labels, predicted_labels, average='weighted')  # weighted F1 score\n",
        "    auc = roc_auc_score(np.array(true_label_one_hot_list),  np.squeeze(np.array(all_probs), axis=1), multi_class='ovr', average='weighted')  # for multi-class AUC\n",
        "\n",
        "    # Calculate inference time (average time per sample)\n",
        "    end_time = time.time()\n",
        "    inference_time = (end_time - start_time) / len(train_dataset)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(f\"AUC: {auc:.4f}\")\n",
        "    print(f\"Average Inference Time per Sample: {inference_time:.4f} seconds\")\n",
        "    return \n",
        "        \n",
        "\n",
        "\n",
        "trainer_stats = None\n",
        "\n",
        "def main():\n",
        "    trainer_stats = train()\n",
        "    return trainer_stats\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trainer_stats = main()\n",
        "    eval_results = evaluate()\n",
        "\n",
        "    print(\"Finished training and evaluation.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Can't find 'adapter_config.json' at '/home/snt/projects_lujun/agentCLS/peft_outputs_sentences/checkpoint-24'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/env_classification/lib/python3.9/site-packages/peft/config.py:195\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhf_hub_download_kwargs\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "File \u001b[0;32m~/miniconda3/envs/env_classification/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:106\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/env_classification/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:154\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m     )\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/snt/projects_lujun/agentCLS/peft_outputs_sentences/checkpoint-24'. Use `repo_type` argument if needed.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel, PeftConfig\n\u001b[1;32m      3\u001b[0m peft_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/snt/projects_lujun/agentCLS/peft_outputs_sentences/checkpoint-24\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mPeftConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path)\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, peft_model_id)\n",
            "File \u001b[0;32m~/miniconda3/envs/env_classification/lib/python3.9/site-packages/peft/config.py:199\u001b[0m, in \u001b[0;36mPeftConfigMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    196\u001b[0m             pretrained_model_name_or_path, CONFIG_NAME, subfolder\u001b[38;5;241m=\u001b[39msubfolder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhf_hub_download_kwargs\n\u001b[1;32m    197\u001b[0m         )\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 199\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    201\u001b[0m loaded_attributes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_json_file(config_file)\n\u001b[1;32m    202\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclass_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloaded_attributes}\n",
            "\u001b[0;31mValueError\u001b[0m: Can't find 'adapter_config.json' at '/home/snt/projects_lujun/agentCLS/peft_outputs_sentences/checkpoint-24'"
          ]
        }
      ],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_id = \"/home/snt/projects_lujun/agentCLS/peft_outputs_sentences/checkpoint-24\"\n",
        "\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
        "model = PeftModel.from_pretrained(model, peft_model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = tokenizer(\n",
        "    f'{text_column} : {\"@nationalgridus I have no water and the bill is current and paid. Can you do something about this?\"} Label : ',\n",
        "    return_tensors=\"pt\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Tweet text : @nationalgridus I have no water and the bill is current and paid. Can you do something about this? Label :  @nationalgridusThe United States of America']\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n",
        "    )\n",
        "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PeftType.PREFIX_TUNING: 'PREFIX_TUNING'>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "peft_config.peft_type\n",
        "\n",
        "if PRE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "dashboards": [],
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "LLM 02 - Prompt Tuning with PEFT",
      "widgets": {}
    },
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env_classification",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
