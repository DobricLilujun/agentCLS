{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/miniconda3/envs/env_classification/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_1252450/1081927266.py:156: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_train_data = dataset.groupby('labels', group_keys=False).apply(\n",
      "/tmp/ipykernel_1252450/1081927266.py:160: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_validation_data = dataset.groupby('labels', group_keys=False).apply(\n",
      "Map: 100%|██████████| 106/106 [00:00<00:00, 5668.20 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:00<00:00, 5997.37 examples/s]\n",
      "Casting the dataset: 100%|██████████| 106/106 [00:00<00:00, 20339.28 examples/s]\n",
      "Casting the dataset: 100%|██████████| 42/42 [00:00<00:00, 11493.49 examples/s]\n",
      "Map: 100%|██████████| 106/106 [00:03<00:00, 28.20 examples/s]\n",
      "Map: 100%|██████████| 42/42 [00:02<00:00, 18.27 examples/s]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/snt/projects_lujun/base_models/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 01:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.704400</td>\n",
       "      <td>1.806699</td>\n",
       "      <td>0.415969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 348\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_stats\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 348\u001b[0m     trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m     eval_results \u001b[38;5;241m=\u001b[39m evaluate()\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training and evaluation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 344\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m--> 344\u001b[0m     trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_stats\n",
      "Cell \u001b[0;32mIn[1], line 257\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Create a Trainer instance\u001b[39;00m\n\u001b[1;32m    249\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    250\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    251\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    255\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)]\n\u001b[1;32m    256\u001b[0m )\n\u001b[0;32m--> 257\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training SFT.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer_stats\n",
      "File \u001b[0;32m~/miniconda3/envs/env_classification/lib/python3.9/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/env_classification/lib/python3.9/site-packages/transformers/trainer.py:2639\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2636\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2638\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2639\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2642\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2643\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/env_classification/lib/python3.9/site-packages/transformers/trainer.py:3092\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3089\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;241m=\u001b[39m is_new_best_metric\n\u001b[1;32m   3091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3092\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3093\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/miniconda3/envs/env_classification/lib/python3.9/site-packages/transformers/trainer.py:3194\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial)\u001b[0m\n\u001b[1;32m   3190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(output_dir, _internal_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   3193\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[0;32m-> 3194\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_scaler(output_dir)\n\u001b[1;32m   3196\u001b[0m     \u001b[38;5;66;03m# Save RNG state\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/env_classification/lib/python3.9/site-packages/transformers/trainer.py:3316\u001b[0m, in \u001b[0;36mTrainer._save_optimizer_and_scheduler\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m   3311\u001b[0m     save_fsdp_optimizer(\n\u001b[1;32m   3312\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfsdp_plugin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, output_dir\n\u001b[1;32m   3313\u001b[0m     )\n\u001b[1;32m   3314\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[0;32m-> 3316\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3318\u001b[0m \u001b[38;5;66;03m# Save SCHEDULER & SCALER\u001b[39;00m\n\u001b[1;32m   3319\u001b[0m is_deepspeed_custom_scheduler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_deepspeed_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   3320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler, DeepSpeedSchedulerWrapper\n\u001b[1;32m   3321\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/env_classification/lib/python3.9/site-packages/torch/serialization.py:944\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 944\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/env_classification/lib/python3.9/site-packages/torch/serialization.py:1216\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     storage \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m-> 1216\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, LoraConfig\n",
    "import json\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from datasets.dataset_dict import DatasetDict, IterableDatasetDict\n",
    "from datasets.iterable_dataset import IterableDataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from datasets import ClassLabel\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "# Data preparation\n",
    "import torch\n",
    "from transformers import EarlyStoppingCallback\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# \n",
    "# ========================== CMD Argument Parser ==========================\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Train a model using CPT (Continual Pretraining Training)\")\n",
    "#     parser.add_argument(\"--per_device_train_batch_size\", type=int, default=8, help=\"Batch size per device during training\")\n",
    "#     parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=8, help=\"Batch size per device during evaluation\")\n",
    "#     parser.add_argument(\"--num_train_epochs\", type=int, default=1, help=\"Number of training epochs\")\n",
    "#     parser.add_argument(\"--learning_rate\", type=float, default=1e-6, help=\"Learning rate for training\")\n",
    "#     parser.add_argument(\"--project_root\", type=str, default=\"/Users/lujun.li/projects/mt_luxembourgish\", help=\"Path to project root\")\n",
    "#     parser.add_argument(\"--training_dataset_path\", type=str, default=\"data/processed/dataset_merged_llama_fake_targets.jsonl\", help=\"Path to training dataset\")\n",
    "#     parser.add_argument(\"--model_path\", type=str, default=\"/home/llama/Personal_Directories/srb/binary_classfication/Llama-3.2-3B-Instruct\", help=\"Path to model\")\n",
    "#     parser.add_argument(\"--resume_from_checkpoint\", type=bool, default=False, help=\"Resume training from checkpoint\")\n",
    "#     parser.add_argument(\"--resume_checkpoint_path\", type=str, default=None, help=\"Path to checkpoint to resume training from\")\n",
    "#     parser.add_argument(\"--qlora\", type=bool, default=False, help=\"Use QLoRA\")\n",
    "#     parser.add_argument(\"--r\", type=int, default=16, help=\"Rank for LoRA\")\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# args = parse_args()\n",
    "\n",
    "\n",
    "# print(\"Arguments passed:\")\n",
    "# print(f\"Train Batch Size: {args.per_device_train_batch_size}\")\n",
    "# print(f\"Eval Batch Size: {args.per_device_eval_batch_size}\")\n",
    "# print(f\"Number of Epochs: {args.num_train_epochs}\")\n",
    "# print(f\"Learning Rate: {args.learning_rate}\")\n",
    "# print(f\"Project Root: {args.project_root}\")\n",
    "# print(f\"Training Dataset Path: {args.training_dataset_path}\")\n",
    "# print(f\"Model path: {args.model_path}\")\n",
    "# print(f\"Resume from checkpoint: {args.resume_from_checkpoint}\")\n",
    "# print(f\"Resume checkpoint path: {args.resume_checkpoint_path}\")\n",
    "# print(f\"Qlora: {args.qlora}\")\n",
    "# print(f\"Rank: {args.r}\")\n",
    "\n",
    "# per_device_train_batch_size = args.per_device_train_batch_size  # Batch size for training per device\n",
    "# per_device_eval_batch_size = args.per_device_eval_batch_size  # Batch size for evaluation per device\n",
    "# num_train_epochs = args.num_train_epochs  # Number of epochs for training\n",
    "# learning_rate = args.learning_rate # Learning rate for the optimizer\n",
    "# project_root = args.project_root\n",
    "# training_dataset_path = args.training_dataset_path\n",
    "# model_path = args.model_path\n",
    "# resume_from_checkpoint = args.resume_from_checkpoint\n",
    "# resume_checkpoint_path = args.resume_checkpoint_path\n",
    "# qlora = args.qlora\n",
    "# r = args.r\n",
    "\n",
    "\n",
    "## Data preparation\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "num_train_epochs = 1\n",
    "learning_rate = 5e-5\n",
    "project_root = \"/home/snt/projects_lujun/agentCLS\"\n",
    "training_dataset_path = \"assets/training_dataset/LDD_split.json\"\n",
    "model_path = \"/home/snt/projects_lujun/base_models/Llama-3.2-1B-Instruct\"\n",
    "resume_from_checkpoint = False\n",
    "resume_checkpoint_path = None\n",
    "qlora = True\n",
    "r = 16\n",
    "\n",
    "train_dataset_path = os.path.abspath(os.path.join(project_root, training_dataset_path))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "\n",
    "# Default Parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_seed = 3407\n",
    "train_ratio = 0.01\n",
    "logging_steps = 10\n",
    "eval_steps = 100\n",
    "eval_strategy = \"steps\"\n",
    "save_strategy = \"epoch\"\n",
    "save_total_limit = 2\n",
    "logging_strategy = \"steps\"\n",
    "max_grad_norm = 0.3\n",
    "input_dataset_name = train_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
    "model_name = model_path.split(\"/\")[-1]\n",
    "max_length = 4096\n",
    "load_in_4bit = True\n",
    "bnb_4bit_quant_type = 'nf4'\n",
    "quantization_config = None\n",
    "\n",
    "\n",
    "if qlora:\n",
    "# Quantization with Lora\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit, # enable 4-bit quantization\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type, # information theoretically optimal dtype for normally distributed weights\n",
    "        bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
    "        bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
    "    )\n",
    "\n",
    "    # Lora\n",
    "    lora_config = LoraConfig(\n",
    "        r = r, # the dimension of the low-rank matrices\n",
    "        lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "        target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "        lora_dropout = 0.05, # dropout probability of the LoRA layers\n",
    "        bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n",
    "        task_type = 'SEQ_CLS',\n",
    "    )\n",
    "\n",
    "\n",
    "if resume_from_checkpoint and resume_checkpoint_path is None:\n",
    "    raise ValueError(\"Please provide a checkpoint path to resume training from\")\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    output_dir = resume_checkpoint_path\n",
    "else:\n",
    "    current_time = datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
    "    output_dir = f\"{project_root}/assets/logs/{input_dataset_name}_{train_ratio}_{model_name}_output_{current_time}\"\n",
    "\n",
    "dataset = pd.read_json(train_dataset_path, lines=True)\n",
    "dataset.rename(columns={\"cls_label\": \"labels\"}, inplace=True)\n",
    "\n",
    "# Compute sample counts for each group based on 'labels' and 'split'\n",
    "sample_counts = dataset.groupby(['labels', 'split']).size() * train_ratio\n",
    "\n",
    "filtered_train_data = dataset.groupby('labels', group_keys=False).apply(\n",
    "    lambda x: x[x['split'] == 'train'].iloc[:int(sample_counts.loc[x.name, 'train'])]\n",
    ")\n",
    "\n",
    "filtered_validation_data = dataset.groupby('labels', group_keys=False).apply(\n",
    "    lambda x: x[x['split'] == 'validation'].iloc[:int(sample_counts.loc[x.name, 'validation'])]\n",
    ")\n",
    "\n",
    "filtered_train = filtered_train_data.reset_index(drop=True)\n",
    "filtered_validation = filtered_validation_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(filtered_train)\n",
    "val_dataset = Dataset.from_pandas(filtered_validation)\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"content\"], padding=True, truncation=True, max_length=max_length)\n",
    "\n",
    "labels =  set(train_dataset['labels'])\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x: {\"labels\": label2id[x[\"labels\"]]})\n",
    "val_dataset = val_dataset.map(lambda x: {\"labels\": label2id[x[\"labels\"]]})\n",
    "\n",
    "labels_ids = list(set(train_dataset['labels']))\n",
    "class_label = ClassLabel(num_classes=len(labels_ids), names=labels_ids)\n",
    "train_dataset = train_dataset.cast_column(\"labels\", class_label)\n",
    "val_dataset = val_dataset.cast_column(\"labels\", class_label)\n",
    "\n",
    "\n",
    "keep_columns = [\"labels\", \"input_ids\", \"attention_mask\"]\n",
    "tokenized_train_dataset = train_dataset.map(tokenize, batched=True, remove_columns=[col for col in train_dataset.column_names if col not in keep_columns])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize, batched=True, remove_columns=[col for col in val_dataset.column_names if col not in keep_columns])\n",
    "train_dataset.features.keys()\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels, label2id=label2id, id2label=id2label, quantization_config=quantization_config,)\n",
    "if qlora:\n",
    "    model = get_peft_model(prepare_model_for_kbit_training(model), lora_config)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "train_dataset.features.keys()\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy_score(labels, np.argmax(predictions, axis=-1))\n",
    "    f1 = f1_score(labels, np.argmax(predictions, axis=-1), average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "def train():\n",
    "    # Define training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir= output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        bf16=True,\n",
    "        optim=\"adamw_torch_fused\", \n",
    "        logging_strategy=logging_strategy,\n",
    "        logging_steps=logging_steps,\n",
    "        eval_strategy=eval_strategy,\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=save_strategy,\n",
    "        save_total_limit=save_total_limit,\n",
    "        load_best_model_at_end=False,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        # group_by_length=True,\n",
    "        # use_mps_device=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        # push to hub parameters\n",
    "        # push_to_hub=True,\n",
    "        # hub_strategy=\"every_save\",\n",
    "        # hub_token=HfFolder.get_token(),\n",
    "        report_to=\"tensorboard\",\n",
    "        disable_tqdm=False,\n",
    "        seed = train_seed,\n",
    "    )\n",
    "    \n",
    "    # Create a Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    print(\"Finished training SFT.\")\n",
    "    return trainer_stats\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    def get_last_checkpoints(output_dir):\n",
    "        checkpoints = os.listdir(output_dir)\n",
    "        checkpoints = [c for c in checkpoints if \"checkpoint\" in c]\n",
    "        checkpoints = [int(c.split(\"-\")[-1]) for c in checkpoints]\n",
    "        last_checkpoint = max(checkpoints)\n",
    "        return f\"{output_dir}/checkpoint-{last_checkpoint}\"\n",
    "    \n",
    "    checkpoints_path  = get_last_checkpoints(output_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels, label2id=label2id, id2label=id2label, quantization_config=quantization_config,).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    validation_results = []\n",
    "\n",
    "    # Initialize lists to store true and predicted labels\n",
    "    true_label_one_hot_list = []\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    # Start time for measuring inference efficiency\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate through validation dataset and make predictions\n",
    "    for input in tqdm(val_dataset, desc=\"Processing validation data\", unit=\"sample\"):\n",
    "        sample = input['content']\n",
    "        true_label_idx = int(input['labels'])\n",
    "        tokenized_input = tokenizer(sample, padding=True, max_length=max_length, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        model_output = model(**tokenized_input)\n",
    "        logits = model_output.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
    "        predicted_label = id2label[str(predicted_class_idx)]\n",
    "        \n",
    "        true_label = id2label[str(true_label_idx)]\n",
    "        true_label_one_hot = np.zeros(probabilities.size(-1))\n",
    "        true_label_one_hot[true_label_idx] = 1\n",
    "\n",
    "        true_labels.append(true_label)\n",
    "        true_label_one_hot_list.append(true_label_one_hot)\n",
    "        predicted_labels.append(predicted_label)\n",
    "        all_probs.append(probabilities.detach().cpu().numpy())  # Store the raw probabilities for AUC\n",
    "        result = {\n",
    "            'content': sample,\n",
    "            'true_label': true_label,\n",
    "            'predicted_label': predicted_label,\n",
    "            'true_label_one_hot': true_label_one_hot.tolist(),\n",
    "            'predicted_class_idx': predicted_class_idx,\n",
    "            'probabilities': probabilities.detach().cpu().numpy().tolist()  # Convert to list for JSON serialization\n",
    "        }\n",
    "        validation_results.append(result)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
    "    df_validation_results = pd.DataFrame(validation_results)\n",
    "    jsonl_file_path = os.path.join(checkpoints_path, f'validation_results_{timestamp}.jsonl')\n",
    "    df_validation_results.to_json(jsonl_file_path, orient='records', lines=True)\n",
    "\n",
    "\n",
    "    # Calculate accuracy, F1 score, and AUC\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')  # weighted F1 score\n",
    "    auc = roc_auc_score(np.array(true_label_one_hot_list),  np.squeeze(np.array(all_probs), axis=1), multi_class='ovr', average='weighted')  # for multi-class AUC\n",
    "\n",
    "    # Calculate inference time (average time per sample)\n",
    "    end_time = time.time()\n",
    "    inference_time = (end_time - start_time) / len(train_dataset)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"Average Inference Time per Sample: {inference_time:.4f} seconds\")\n",
    "    return \n",
    "        \n",
    "\n",
    "\n",
    "trainer_stats = None\n",
    "\n",
    "def main():\n",
    "    trainer_stats = train()\n",
    "    return trainer_stats\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     trainer_stats = main()\n",
    "#     eval_results = evaluate()\n",
    "\n",
    "#     print(\"Finished training and evaluation.\")\n",
    "trainer_stats = train()\n",
    "eval_results = evaluate()\n",
    "# python llama3_FT.py \\\n",
    "# --per_device_train_batch_size 8 \\\n",
    "# --per_device_eval_batch_size 8 \\\n",
    "# --num_train_epochs 10 \\\n",
    "# --learning_rate 1e-6 \\\n",
    "# --project_root /home/llama/Personal_Directories/srb/agentCLS \\\n",
    "# --training_dataset_path assets/training_dataset/LDD_split_equal_train_1000_val_300.jsonl \\\n",
    "# --model_path /home/llama/Personal_Directories/srb/binary_classfication/Llama-3.2-3B-Instruct \\\n",
    "# --resume_from_checkpoint \"False\" \\\n",
    "# --resume_checkpoint_path \"\" \\\n",
    "# --qlora False \\\n",
    "# --r 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    def get_last_checkpoints(output_dir):\n",
    "        checkpoints = os.listdir(output_dir)\n",
    "        checkpoints = [c for c in checkpoints if \"checkpoint\" in c]\n",
    "        checkpoints = [int(c.split(\"-\")[-1]) for c in checkpoints]\n",
    "        last_checkpoint = max(checkpoints)\n",
    "        return f\"{output_dir}/checkpoint-{last_checkpoint}\"\n",
    "    \n",
    "    checkpoints_path  = get_last_checkpoints(output_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels, label2id=label2id, id2label=id2label, quantization_config=quantization_config,).to(device)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.gradient_checkpointing_enable()\n",
    "    validation_results = []\n",
    "\n",
    "    # Initialize lists to store true and predicted labels\n",
    "    true_label_one_hot_list = []\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    # Start time for measuring inference efficiency\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate through validation dataset and make predictions\n",
    "    for input in tqdm(val_dataset, desc=\"Processing validation data\", unit=\"sample\"):\n",
    "        sample = input['content']\n",
    "        true_label_idx = int(input['labels'])\n",
    "        tokenized_input = tokenizer(sample, padding=True, max_length=max_length, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        model_output = model(**tokenized_input)\n",
    "        logits = model_output.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
    "        predicted_label = id2label[str(predicted_class_idx)]\n",
    "        \n",
    "        true_label = id2label[str(true_label_idx)]\n",
    "        true_label_one_hot = np.zeros(probabilities.size(-1))\n",
    "        true_label_one_hot[true_label_idx] = 1\n",
    "\n",
    "        true_labels.append(true_label)\n",
    "        true_label_one_hot_list.append(true_label_one_hot)\n",
    "        predicted_labels.append(predicted_label)\n",
    "        all_probs.append(probabilities.detach().cpu().numpy())  # Store the raw probabilities for AUC\n",
    "        result = {\n",
    "            'content': sample,\n",
    "            'true_label': true_label,\n",
    "            'predicted_label': predicted_label,\n",
    "            'true_label_one_hot': true_label_one_hot.tolist(),\n",
    "            'predicted_class_idx': predicted_class_idx,\n",
    "            'probabilities': probabilities.detach().cpu().numpy().tolist()  # Convert to list for JSON serialization\n",
    "        }\n",
    "        validation_results.append(result)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
    "    df_validation_results = pd.DataFrame(validation_results)\n",
    "    jsonl_file_path = os.path.join(checkpoints_path, f'validation_results_{timestamp}.jsonl')\n",
    "    df_validation_results.to_json(jsonl_file_path, orient='records', lines=True)\n",
    "\n",
    "\n",
    "    # Calculate accuracy, F1 score, and AUC\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')  # weighted F1 score\n",
    "    auc = roc_auc_score(np.array(true_label_one_hot_list),  np.squeeze(np.array(all_probs), axis=1), multi_class='ovr', average='weighted')  # for multi-class AUC\n",
    "\n",
    "    # Calculate inference time (average time per sample)\n",
    "    end_time = time.time()\n",
    "    inference_time = (end_time - start_time) / len(train_dataset)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"Average Inference Time per Sample: {inference_time:.4f} seconds\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/snt/projects_lujun/base_models/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Processing validation data:   0%|          | 0/91 [01:14<?, ?sample/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m predicted_class_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(probabilities, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 37\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m \u001b[43mid2label\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredicted_class_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     39\u001b[0m true_label \u001b[38;5;241m=\u001b[39m id2label[\u001b[38;5;28mstr\u001b[39m(true_label_idx)]\n\u001b[1;32m     40\u001b[0m true_label_one_hot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(probabilities\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mKeyError\u001b[0m: '5'"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
