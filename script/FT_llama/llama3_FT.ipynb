{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/miniconda3/envs/env_classification/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_2251580/575281757.py:158: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_train_data = dataset.groupby('labels', group_keys=False).apply(\n",
      "/tmp/ipykernel_2251580/575281757.py:162: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_validation_data = dataset.groupby('labels', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "from datasets.arrow_dataset import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from transformers import EarlyStoppingCallback\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# \n",
    "# ========================== CMD Argument Parser ==========================\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Train a model using CPT (Continual Pretraining Training)\")\n",
    "#     parser.add_argument(\"--per_device_train_batch_size\", type=int, default=8, help=\"Batch size per device during training\")\n",
    "#     parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=8, help=\"Batch size per device during evaluation\")\n",
    "#     parser.add_argument(\"--num_train_epochs\", type=int, default=1, help=\"Number of training epochs\")\n",
    "#     parser.add_argument(\"--learning_rate\", type=float, default=1e-6, help=\"Learning rate for training\")\n",
    "#     parser.add_argument(\"--project_root\", type=str, default=\"/Users/lujun.li/projects/mt_luxembourgish\", help=\"Path to project root\")\n",
    "#     parser.add_argument(\"--training_dataset_path\", type=str, default=\"data/processed/dataset_merged_llama_fake_targets.jsonl\", help=\"Path to training dataset\")\n",
    "#     parser.add_argument(\"--model_path\", type=str, default=\"/home/llama/Personal_Directories/srb/binary_classfication/Llama-3.2-3B-Instruct\", help=\"Path to model\")\n",
    "#     parser.add_argument(\"--resume_from_checkpoint\", type=bool, default=False, help=\"Resume training from checkpoint\")\n",
    "#     parser.add_argument(\"--resume_checkpoint_path\", type=str, default=None, help=\"Path to checkpoint to resume training from\")\n",
    "#     parser.add_argument(\"--qlora\", type=bool, default=False, help=\"Use QLoRA\")\n",
    "#     parser.add_argument(\"--r\", type=int, default=16, help=\"Rank for LoRA\")\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# args = parse_args()\n",
    "\n",
    "\n",
    "# print(\"Arguments passed:\")\n",
    "# print(f\"Train Batch Size: {args.per_device_train_batch_size}\")\n",
    "# print(f\"Eval Batch Size: {args.per_device_eval_batch_size}\")\n",
    "# print(f\"Number of Epochs: {args.num_train_epochs}\")\n",
    "# print(f\"Learning Rate: {args.learning_rate}\")\n",
    "# print(f\"Project Root: {args.project_root}\")\n",
    "# print(f\"Training Dataset Path: {args.training_dataset_path}\")\n",
    "# print(f\"Model path: {args.model_path}\")\n",
    "# print(f\"Resume from checkpoint: {args.resume_from_checkpoint}\")\n",
    "# print(f\"Resume checkpoint path: {args.resume_checkpoint_path}\")\n",
    "# print(f\"Qlora: {args.qlora}\")\n",
    "# print(f\"Rank: {args.r}\")\n",
    "\n",
    "# per_device_train_batch_size = args.per_device_train_batch_size  # Batch size for training per device\n",
    "# per_device_eval_batch_size = args.per_device_eval_batch_size  # Batch size for evaluation per device\n",
    "# num_train_epochs = args.num_train_epochs  # Number of epochs for training\n",
    "# learning_rate = args.learning_rate # Learning rate for the optimizer\n",
    "# project_root = args.project_root\n",
    "# training_dataset_path = args.training_dataset_path\n",
    "# model_path = args.model_path\n",
    "# resume_from_checkpoint = args.resume_from_checkpoint\n",
    "# resume_checkpoint_path = args.resume_checkpoint_path\n",
    "# qlora = args.qlora\n",
    "# r = args.r\n",
    "\n",
    "\n",
    "## Data preparation\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "num_train_epochs = 10\n",
    "learning_rate = 1e-6\n",
    "project_root = \"/home/snt/projects_lujun/agentCLS\"\n",
    "training_dataset_path = \"assets/training_dataset/EURLEX57K_split_proportional_train_1500_val_300.jsonl\"\n",
    "model_path = \"/home/snt/projects_lujun/base_models/Llama-3.2-1B-Instruct\"\n",
    "resume_from_checkpoint = False\n",
    "resume_checkpoint_path = None\n",
    "qlora = False\n",
    "r = 16\n",
    "\n",
    "train_dataset_path = os.path.abspath(os.path.join(project_root, training_dataset_path))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "\n",
    "# Default Parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_seed = 3407\n",
    "train_ratio = 0.1\n",
    "logging_steps = 10\n",
    "eval_steps = 100\n",
    "eval_strategy = \"epoch\"\n",
    "save_strategy = \"epoch\"\n",
    "save_total_limit = 2\n",
    "logging_strategy = \"steps\"\n",
    "max_grad_norm = 0.3\n",
    "input_dataset_name = train_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
    "model_name = model_path.split(\"/\")[-1]\n",
    "max_length = 4096\n",
    "load_in_4bit = True\n",
    "bnb_4bit_quant_type = 'nf4'\n",
    "quantization_config = None\n",
    "\n",
    "\n",
    "if qlora:\n",
    "# Quantization with Lora\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit, # enable 4-bit quantization\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type, # information theoretically optimal dtype for normally distributed weights\n",
    "        bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
    "        bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
    "    )\n",
    "\n",
    "    # Lora\n",
    "    lora_config = LoraConfig(\n",
    "        r = r, # the dimension of the low-rank matrices\n",
    "        lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "        target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "        lora_dropout = 0.05, # dropout probability of the LoRA layers\n",
    "        bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n",
    "        task_type = 'SEQ_CLS',\n",
    "    )\n",
    "\n",
    "\n",
    "if resume_from_checkpoint and resume_checkpoint_path is None:\n",
    "    raise ValueError(\"Please provide a checkpoint path to resume training from\")\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    output_dir = resume_checkpoint_path\n",
    "else:\n",
    "    current_time = datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
    "    output_dir = f\"{project_root}/assets/logs/SFT/{input_dataset_name}_{train_ratio}_{model_name}_output_{current_time}\"\n",
    "\n",
    "dataset = pd.read_json(train_dataset_path, lines=True)\n",
    "dataset.rename(columns={\"cls_label\": \"labels\"}, inplace=True)\n",
    "\n",
    "# Compute sample counts for each group based on 'labels' and 'split'\n",
    "sample_counts = dataset.groupby(['labels', 'split']).size() * train_ratio\n",
    "\n",
    "filtered_train_data = dataset.groupby('labels', group_keys=False).apply(\n",
    "    lambda x: x[x['split'] == 'train'].iloc[:int(sample_counts.loc[x.name, 'train'])]\n",
    ")\n",
    "\n",
    "filtered_validation_data = dataset.groupby('labels', group_keys=False).apply(\n",
    "    lambda x: x[x['split'] == 'validation'].iloc[:int(sample_counts.loc[x.name, 'validation'])]\n",
    ")\n",
    "\n",
    "filtered_train = filtered_train_data.reset_index(drop=True)\n",
    "filtered_validation = filtered_validation_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(filtered_train)\n",
    "val_dataset = Dataset.from_pandas(filtered_validation)\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"content\"], padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "\n",
    "labels =  set(train_dataset['labels'])\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map( lambda x: {\"labels\":label2id[x[\"labels\"]]} )\n",
    "val_dataset = val_dataset.map( lambda x: {\"labels\": label2id[x[\"labels\"]]})\n",
    "\n",
    "keep_columns = [\"labels\", \"input_ids\", \"attention_mask\"]\n",
    "tokenized_train_dataset = train_dataset.map(tokenize, batched=True, remove_columns=[col for col in train_dataset.column_names if col not in keep_columns])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize, batched=True, remove_columns=[col for col in val_dataset.column_names if col not in keep_columns])\n",
    "train_dataset.features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/303 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 303/303 [00:00<00:00, 7546.85 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 6727.75 examples/s]\n",
      "Map: 100%|██████████| 303/303 [00:00<00:00, 373.14 examples/s]\n",
      "Map: 100%|██████████| 90/90 [00:00<00:00, 344.35 examples/s]\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/snt/projects_lujun/base_models/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels, label2id=label2id, id2label=id2label, quantization_config=quantization_config,)\n",
    "\n",
    "if qlora:\n",
    "    model = get_peft_model(prepare_model_for_kbit_training(model), lora_config)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()\n",
    "train_dataset.features.keys()\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    acc = accuracy_score(labels, np.argmax(predictions, axis=-1))\n",
    "    f1 = f1_score(labels, np.argmax(predictions, axis=-1), average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "def train():\n",
    "    # Define training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir= output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        bf16=True,\n",
    "        optim=\"adamw_torch_fused\", \n",
    "        logging_strategy=logging_strategy,\n",
    "        logging_steps=logging_steps,\n",
    "        eval_strategy=eval_strategy,\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=save_strategy,\n",
    "        save_total_limit=save_total_limit,\n",
    "        load_best_model_at_end=True,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        # group_by_length=True,\n",
    "        # use_mps_device=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        # push to hub parameters\n",
    "        # push_to_hub=True,\n",
    "        # hub_strategy=\"every_save\",\n",
    "        # hub_token=HfFolder.get_token(),\n",
    "        report_to=\"tensorboard\",\n",
    "        disable_tqdm=False,\n",
    "        seed = train_seed,\n",
    "    )\n",
    "    \n",
    "    # Create a Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    print(\"Finished training SFT.\")\n",
    "    return trainer_stats\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    def get_last_checkpoints(output_dir):\n",
    "        checkpoints = os.listdir(output_dir)\n",
    "        checkpoints = [c for c in checkpoints if \"checkpoint\" in c]\n",
    "        checkpoints = [int(c.split(\"-\")[-1]) for c in checkpoints]\n",
    "        last_checkpoint = max(checkpoints)\n",
    "        return f\"{output_dir}/checkpoint-{last_checkpoint}\"\n",
    "    \n",
    "    checkpoints_path  = get_last_checkpoints(output_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoints_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path) \n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    validation_results = []\n",
    "\n",
    "    # Initialize lists to store true and predicted labels\n",
    "    true_label_one_hot_list = []\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    # Start time for measuring inference efficiency\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate through validation dataset and make predictions\n",
    "    for input in tqdm(val_dataset, desc=\"Processing validation data\", unit=\"sample\"):\n",
    "        sample = input['content']\n",
    "        true_label_idx = int(input['labels'])\n",
    "        tokenized_input = tokenizer(sample, padding=\"max_length\", max_length=max_length, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        model_output = model(**tokenized_input)\n",
    "        logits = model_output.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
    "        predicted_label = id2label[str(predicted_class_idx)]\n",
    "        \n",
    "        true_label = id2label[str(true_label_idx)]\n",
    "        true_label_one_hot = np.zeros(probabilities.size(-1))\n",
    "        true_label_one_hot[true_label_idx] = 1\n",
    "\n",
    "        true_labels.append(true_label)\n",
    "        true_label_one_hot_list.append(true_label_one_hot)\n",
    "        predicted_labels.append(predicted_label)\n",
    "        all_probs.append(probabilities.detach().cpu().numpy())  # Store the raw probabilities for AUC\n",
    "        result = {\n",
    "            'content': sample,\n",
    "            'true_label': true_label,\n",
    "            'predicted_label': predicted_label,\n",
    "            'true_label_one_hot': true_label_one_hot.tolist(),\n",
    "            'predicted_class_idx': predicted_class_idx,\n",
    "            'probabilities': probabilities.detach().cpu().numpy().tolist()  # Convert to list for JSON serialization\n",
    "        }\n",
    "        validation_results.append(result)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
    "    df_validation_results = pd.DataFrame(validation_results)\n",
    "    jsonl_file_path = os.path.join(checkpoints_path, f'validation_results_{timestamp}.jsonl')\n",
    "    df_validation_results.to_json(jsonl_file_path, orient='records', lines=True)\n",
    "\n",
    "\n",
    "    # Calculate accuracy, F1 score, and AUC\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')  # weighted F1 score\n",
    "    auc = roc_auc_score(np.array(true_label_one_hot_list),  np.squeeze(np.array(all_probs), axis=1), multi_class='ovr', average='weighted')  # for multi-class AUC\n",
    "\n",
    "    # Calculate inference time (average time per sample)\n",
    "    end_time = time.time()\n",
    "    inference_time = (end_time - start_time) / len(train_dataset)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"Average Inference Time per Sample: {inference_time:.4f} seconds\")\n",
    "    return \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# python llama3_FT.py \\\n",
    "# --per_device_train_batch_size 8 \\\n",
    "# --per_device_eval_batch_size 8 \\\n",
    "# --num_train_epochs 10 \\\n",
    "# --learning_rate 1e-6 \\\n",
    "# --project_root /home/llama/Personal_Directories/srb/agentCLS \\\n",
    "# --training_dataset_path assets/training_dataset/LDD_split_equal_train_1000_val_300.jsonl \\\n",
    "# --model_path /home/llama/Personal_Directories/srb/binary_classfication/Llama-3.2-3B-Instruct \\\n",
    "# --resume_from_checkpoint \"False\" \\\n",
    "# --resume_checkpoint_path \"\" \\\n",
    "# --qlora False \\\n",
    "# --r 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = None\n",
    "\n",
    "def main():\n",
    "    trainer_stats = train()\n",
    "    return trainer_stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainer_stats = main()\n",
    "    eval_results = evaluate()\n",
    "\n",
    "    print(\"Finished training and evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/miniconda3/envs/env_classification/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets.arrow_dataset import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "project_root = \"/home/snt/projects_lujun/agentCLS\"\n",
    "model_path = \"/home/snt/llm_models/ModernBERT-base\"\n",
    "# model_path = \"/home/snt/projects_lujun/base_models/Llama-3.2-1B-Instruct\"\n",
    "# training_dataset_path = \"/home/snt/projects_lujun/agentCLS/assets/training_dataset/EURLEX57K_split_proportional_train_1500_val_300.jsonl\"\n",
    "training_dataset_path = \"/home/snt/projects_lujun/agentCLS/assets/training_dataset/LDD_split_proportional_train_1500_val_300.jsonl\"\n",
    "\n",
    "output_dir = \"/home/snt/projects_lujun/agentCLS/assets/logs/SFT/LDD_split_equal_train_1000_val_300_1.0_ModernBERT-base_output_03_14_23_08_21\"\n",
    "max_length = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def find_right_label2id(model, tokenizer, train_dataset, max_length=512, device='cuda'):\n",
    "    train_ratio = 0.01\n",
    "    description_counts = defaultdict(int)\n",
    "    for sample in train_dataset:\n",
    "        description_counts[sample['labels']] += 1\n",
    "    \n",
    "    label2id = {}\n",
    "    id2label = {}\n",
    "    model.to(device)\n",
    "    for description, count in description_counts.items():\n",
    "        output_ids = []\n",
    "        \n",
    "        subset = [sample for sample in train_dataset if sample['labels'] == description]\n",
    "        sampled_subset = random.sample(subset, max(1, int(len(subset) * train_ratio)))\n",
    "        \n",
    "        for sample in tqdm(sampled_subset, desc=f\"Processing {description}\", unit=\"sample\"):\n",
    "            tokenized_input = tokenizer(sample['content'], padding=\"max_length\", max_length = max_length, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                model_output = model(**tokenized_input)\n",
    "            \n",
    "            logits = model_output.logits\n",
    "            predicted_class_idx = torch.argmax(torch.nn.functional.softmax(logits, dim=-1), dim=-1).item()\n",
    "            output_ids.append(predicted_class_idx)\n",
    "        \n",
    "        most_common_id = Counter(output_ids).most_common(1)[0][0]\n",
    "        label2id[description] = most_common_id\n",
    "        id2label[most_common_id] = description\n",
    "    \n",
    "    return label2id, id2label\n",
    "\n",
    "def get_last_checkpoints(output_dir):\n",
    "    checkpoints = os.listdir(output_dir)\n",
    "    checkpoints = [c for c in checkpoints if \"checkpoint\" in c]\n",
    "    checkpoints = [int(c.split(\"-\")[-1]) for c in checkpoints]\n",
    "    last_checkpoint = max(checkpoints)\n",
    "    return f\"{output_dir}/checkpoint-{last_checkpoint}\"\n",
    "\n",
    "def get_all_checkpoints(output_dir):\n",
    "    checkpoints = os.listdir(output_dir)\n",
    "    checkpoints = [c for c in checkpoints if c.startswith(\"checkpoint-\") and c.split(\"-\")[-1].isdigit()]\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    return [os.path.join(output_dir, c) for c in checkpoints]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2262220/92108369.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_train_data = dataset.groupby('labels', group_keys=False).apply(lambda x: x[x['split'] == 'train'].iloc[:int(sample_counts.loc[x.name, 'train'])])\n",
      "/tmp/ipykernel_2262220/92108369.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_validation_data = dataset.groupby('labels', group_keys=False).apply(lambda x: x[x['split'] == 'validation'].iloc[:int(sample_counts.loc[x.name, 'validation'])])\n",
      "Processing cs.AI: 100%|██████████| 14/14 [00:07<00:00,  1.93sample/s]\n",
      "Processing cs.CE: 100%|██████████| 12/12 [00:03<00:00,  3.71sample/s]\n",
      "Processing cs.CV: 100%|██████████| 13/13 [00:03<00:00,  3.75sample/s]\n",
      "Processing cs.DS: 100%|██████████| 15/15 [00:04<00:00,  3.14sample/s]\n",
      "Processing cs.IT: 100%|██████████| 15/15 [00:04<00:00,  3.53sample/s]\n",
      "Processing cs.NE: 100%|██████████| 13/13 [00:03<00:00,  3.53sample/s]\n",
      "Processing cs.PL: 100%|██████████| 13/13 [00:03<00:00,  3.58sample/s]\n",
      "Processing cs.SY: 100%|██████████| 15/15 [00:04<00:00,  3.64sample/s]\n",
      "Processing math.AC: 100%|██████████| 14/14 [00:04<00:00,  3.43sample/s]\n",
      "Processing math.GR: 100%|██████████| 15/15 [00:04<00:00,  3.73sample/s]\n",
      "Processing math.ST: 100%|██████████| 15/15 [00:04<00:00,  3.49sample/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = get_all_checkpoints(output_dir)[0]\n",
    "train_dataset_path = os.path.abspath(os.path.join(project_root, training_dataset_path))\n",
    "sys.path.append(project_root)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if \"BERT\" not in model_path:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "\n",
    "dataset = pd.read_json(train_dataset_path, lines=True)\n",
    "dataset.rename(columns={\"cls_label\": \"labels\"}, inplace=True)\n",
    "train_ratio = 1.0\n",
    "# Compute sample counts for each group based on 'labels' and 'split'\n",
    "sample_counts = dataset.groupby(['labels', 'split']).size() \n",
    "filtered_train_data = dataset.groupby('labels', group_keys=False).apply(lambda x: x[x['split'] == 'train'].iloc[:int(sample_counts.loc[x.name, 'train'])])\n",
    "filtered_validation_data = dataset.groupby('labels', group_keys=False).apply(lambda x: x[x['split'] == 'validation'].iloc[:int(sample_counts.loc[x.name, 'validation'])])\n",
    "filtered_train = filtered_train_data.reset_index(drop=True)\n",
    "filtered_validation = filtered_validation_data.reset_index(drop=True)\n",
    "train_dataset = Dataset.from_pandas(filtered_train)\n",
    "val_dataset = Dataset.from_pandas(filtered_validation)\n",
    "input_dataset_name = train_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
    "model_name = model_path.split(\"/\")[-1]\n",
    "\n",
    "label2id_n, id2label_n = find_right_label2id(model, tokenizer, train_dataset, max_length=4096, device='cuda')\n",
    "num_labels = len(label2id_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/15682 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15682/15682 [00:03<00:00, 5161.85 examples/s]\n",
      "Map: 100%|██████████| 3300/3300 [00:00<00:00, 3546.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map( lambda x: {\"labels\":label2id_n[x[\"labels\"]]} )\n",
    "val_dataset = val_dataset.map( lambda x: {\"labels\": label2id_n[x[\"labels\"]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation data: 100%|██████████| 3300/3300 [15:31<00:00,  3.54sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8197\n",
      "F1 Score: 0.8181\n",
      "AUC: 0.9788\n",
      "Resource_M: 0.0058\n",
      "Time_M: 0.1525\n",
      "ratio_time: 0.0242\n",
      "ratio_time_ram: 0.0014\n",
      "total GPU hours (RAM): 283.2029\n",
      "training GPU hours (RAM): 282.8068\n",
      "inference GPU hours (RAM): 0.3961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation data: 100%|██████████| 3300/3300 [15:29<00:00,  3.55sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8170\n",
      "F1 Score: 0.8144\n",
      "AUC: 0.9790\n",
      "Resource_M: 0.0058\n",
      "Time_M: 0.1518\n",
      "ratio_time: 0.0241\n",
      "ratio_time_ram: 0.0014\n",
      "total GPU hours (RAM): 283.2021\n",
      "training GPU hours (RAM): 282.8068\n",
      "inference GPU hours (RAM): 0.3953\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import os\n",
    "import math \n",
    "\n",
    "max_length = 4096\n",
    "gpu_per_hours_dict = {\n",
    "    \"Llama-3.2-1B-Instruct\":{\n",
    "        \"train\": 27.360,\n",
    "        \"inference\": 25.782\n",
    "    },\n",
    "    \"gemma-2-2b-it\":{\n",
    "        \"train\": 51.496,\n",
    "        \"inference\": 32.664\n",
    "    },\n",
    "    \"Llama-3.2-3B-Instruct\":{\n",
    "        \"train\": 65.522,\n",
    "        \"inference\": 39.556\n",
    "    },\n",
    "    \"ModernBERT-base\":{\n",
    "        \"train\": 27.006,\n",
    "        \"inference\": 1.528\n",
    "    }\n",
    "}\n",
    "\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "\n",
    "def extract_training_time(log_dir, scalar_name=\"train/loss\"):\n",
    "\n",
    "    event_files = [\n",
    "        os.path.join(root, file)\n",
    "        for root, _, files in os.walk(log_dir)\n",
    "        for file in files\n",
    "        if \"events\" in file\n",
    "    ]\n",
    "\n",
    "    if not event_files:\n",
    "        raise FileNotFoundError(\"Directory does not contain any event files.\")\n",
    "    \n",
    "    event_file = event_files[0] \n",
    "\n",
    "    ea = event_accumulator.EventAccumulator(event_file)\n",
    "    ea.Reload() \n",
    "\n",
    "    available_keys = ea.scalars.Keys()\n",
    "    if scalar_name not in available_keys:\n",
    "        raise ValueError(f\"Scalar '{scalar_name}' doesn't exist: {available_keys}\")\n",
    "    \n",
    "\n",
    "    wall_times = ea.Scalars(scalar_name)\n",
    "    start_time = wall_times[0].wall_time\n",
    "    end_time = wall_times[-1].wall_time \n",
    "\n",
    "    training_duration = end_time - start_time\n",
    "    return start_time, end_time, training_duration\n",
    "\n",
    "def evaluate_multiple(output_dir, val_dataset):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    checkpoints_paths= get_all_checkpoints(output_dir)\n",
    "    for checkpoints_path in checkpoints_paths:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(checkpoints_path).to(device)\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.use_cache = False\n",
    "        model.config.pretraining_tp = 1\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.eval()\n",
    "        validation_results = []\n",
    "\n",
    "        # Initialize lists to store true and predicted labels\n",
    "        true_label_one_hot_list = []\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        all_probs = []\n",
    "\n",
    "        # Start time for measuring inference efficiency\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Iterate through validation dataset and make predictions\n",
    "        for input in tqdm(val_dataset, desc=\"Processing validation data\", unit=\"sample\"):\n",
    "            sample = input['content']\n",
    "            true_label_idx = int(input['labels'])\n",
    "            tokenized_input = tokenizer(sample, padding=\"max_length\", max_length=max_length, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            model_output = model(**tokenized_input)\n",
    "            logits = model_output.logits\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
    "            predicted_label = id2label_n[predicted_class_idx]\n",
    "            \n",
    "            true_label = id2label_n[true_label_idx]\n",
    "            true_label_one_hot = np.zeros(probabilities.size(-1))\n",
    "            true_label_one_hot[true_label_idx] = 1\n",
    "\n",
    "            true_labels.append(true_label)\n",
    "            true_label_one_hot_list.append(true_label_one_hot)\n",
    "            predicted_labels.append(predicted_label)\n",
    "            all_probs.append(probabilities.detach().cpu().numpy())  # Store the raw probabilities for AUC\n",
    "            result = {\n",
    "                'content': sample,\n",
    "                'true_label': true_label,\n",
    "                'predicted_label': predicted_label,\n",
    "                'true_label_one_hot': true_label_one_hot.tolist(),\n",
    "                'predicted_class_idx': predicted_class_idx,\n",
    "                'probabilities': probabilities.detach().cpu().numpy().tolist()  # Convert to list for JSON serialization\n",
    "            }\n",
    "            validation_results.append(result)\n",
    "\n",
    "        timestamp = datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
    "        df_validation_results = pd.DataFrame(validation_results)\n",
    "        jsonl_file_path = os.path.join(checkpoints_path, f'validation_results_{timestamp}.jsonl')\n",
    "        df_validation_results.to_json(jsonl_file_path, orient='records', lines=True)\n",
    "\n",
    "\n",
    "        # Calculate accuracy, F1 score, and AUC\n",
    "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='weighted')  # weighted F1 score\n",
    "        auc = roc_auc_score(np.array(true_label_one_hot_list),  np.squeeze(np.array(all_probs), axis=1), multi_class='ovr', average='weighted')  # for multi-class AUC\n",
    "\n",
    "        # Calculate inference time (average time per sample)\n",
    "        end_time = time.time()\n",
    "        inference_time = (end_time - start_time) \n",
    "        inference_time_hours = round(inference_time / 3600, 4)\n",
    "        start_time, end_time, training_duration = extract_training_time(output_dir)\n",
    "        training_time_hours = round(training_duration / 3600, 4)\n",
    "\n",
    "        for k,v in gpu_per_hours_dict.items():\n",
    "            if k in output_dir:\n",
    "                gpu_ram_train = v[\"train\"]\n",
    "                gpu_ram_inference = v[\"inference\"]\n",
    "                break\n",
    "\n",
    "\n",
    "        training_gpu_hours_ram = gpu_ram_train * training_time_hours\n",
    "        inference_gpu_hours_ram = gpu_ram_inference * inference_time_hours\n",
    "        total_gpu_hours_ram = training_gpu_hours_ram + inference_gpu_hours_ram\n",
    "        Resource_M = f1 / math.log((alpha * training_gpu_hours_ram + beta * inference_gpu_hours_ram)+1)\n",
    "        Time_M = f1 / math.log((alpha * training_time_hours + beta * inference_time_hours)+!)\n",
    "        ratio_time = inference_time_hours/(inference_time_hours+training_time_hours)\n",
    "        ratio_time_ram = inference_gpu_hours_ram/(inference_gpu_hours_ram+training_gpu_hours_ram)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        # print(f\"AUC: {auc:.4f}\")\n",
    "        print(f\"Resource_M: {Resource_M:.4f}\")\n",
    "        print(f\"Time_M: {Time_M:.4f}\")\n",
    "        print(f\"\"\"ratio_time: {ratio_time:.4f}\"\"\")\n",
    "        print(f\"\"\"ratio_time_ram: {ratio_time_ram:.4f}\"\"\")\n",
    "        print(f\"total GPU hours (RAM): {total_gpu_hours_ram:.4f}\")\n",
    "        print(f\"training GPU hours (RAM): {training_gpu_hours_ram:.4f}\")\n",
    "        print(f\"inference GPU hours (RAM): {inference_gpu_hours_ram:.4f}\")\n",
    "\n",
    "\n",
    "        # print(f\"Training started at: {round(start_time / 3600, 2)} hours\")\n",
    "        # print(f\"Training ended at: {round(end_time / 3600, 2)} hours\")\n",
    "        # print(f\"Training duration: {round(training_duration / 3600, 2)} hours\")\n",
    "\n",
    "\n",
    "    return \n",
    "\n",
    "evaluate_multiple(output_dir=output_dir, val_dataset=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models in: /home/snt/projects_lujun/agentCLS/assets/logs/SFT/LDD_split_proportional_train_1500_val_300_1.0_ModernBERT-base_output_03_15_15_03_05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation data:   0%|          | 0/330 [00:00<?, ?sample/s]/home/snt/miniconda3/envs/env_classification/lib/python3.9/site-packages/torch/_inductor/compile_fx.py:194: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "Processing validation data: 100%|██████████| 330/330 [01:37<00:00,  3.40sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Evaluation Results for ModernBERT-base\n",
      "==================================================\n",
      "Performance Metrics:\n",
      "  Accuracy:    0.1182\n",
      "  F1 Score:    0.1204\n",
      "  AUC:         0.4936\n",
      "\n",
      "Efficiency Metrics:\n",
      "  Resource_M:  0.0011\n",
      "  Time_M:      0.0296\n",
      "\n",
      "Time Distribution:\n",
      "  Inference/Total Time Ratio: 0.0033\n",
      "  Inference/Total RAM Ratio:  0.0002\n",
      "\n",
      "GPU Resource Usage (GPU hours × RAM):\n",
      "  Total:       218.9735\n",
      "  Training:    218.9322\n",
      "  Inference:   0.0413\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation data: 100%|██████████| 330/330 [01:36<00:00,  3.41sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Evaluation Results for ModernBERT-base\n",
      "==================================================\n",
      "Performance Metrics:\n",
      "  Accuracy:    0.1212\n",
      "  F1 Score:    0.1230\n",
      "  AUC:         0.4950\n",
      "\n",
      "Efficiency Metrics:\n",
      "  Resource_M:  0.0011\n",
      "  Time_M:      0.0302\n",
      "\n",
      "Time Distribution:\n",
      "  Inference/Total Time Ratio: 0.0033\n",
      "  Inference/Total RAM Ratio:  0.0002\n",
      "\n",
      "GPU Resource Usage (GPU hours × RAM):\n",
      "  Total:       218.9733\n",
      "  Training:    218.9322\n",
      "  Inference:   0.0411\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# GPU resource consumption dictionary for different models\n",
    "GPU_HOURS_DICT = {\n",
    "    \"Llama-3.2-1B-Instruct\": {\n",
    "        \"train\": 27.360,\n",
    "        \"inference\": 25.782\n",
    "    },\n",
    "    \"gemma-2-2b-it\": {\n",
    "        \"train\": 51.496,\n",
    "        \"inference\": 32.664\n",
    "    },\n",
    "    \"Llama-3.2-3B-Instruct\": {\n",
    "        \"train\": 65.522,\n",
    "        \"inference\": 39.556\n",
    "    },\n",
    "    \"ModernBERT-base\": {\n",
    "        \"train\": 27.006,\n",
    "        \"inference\": 1.528\n",
    "    }\n",
    "}\n",
    "\n",
    "# Weighting parameters for efficiency metrics\n",
    "ALPHA = 0.5  # Training weight\n",
    "BETA = 0.5   # Inference weight\n",
    "\n",
    "def extract_training_time(log_dir, scalar_name=\"train/loss\"):\n",
    "    \"\"\"\n",
    "    Extract training duration from TensorBoard event files.\n",
    "    \n",
    "    Args:\n",
    "        log_dir (str): Directory containing TensorBoard logs\n",
    "        scalar_name (str): Name of scalar to track for timing\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Start time, end time, and total duration in seconds\n",
    "    \"\"\"\n",
    "    # Find all event files in the directory\n",
    "    event_files = [\n",
    "        os.path.join(root, file)\n",
    "        for root, _, files in os.walk(log_dir)\n",
    "        for file in files\n",
    "        if \"events\" in file\n",
    "    ]\n",
    "\n",
    "    if not event_files:\n",
    "        raise FileNotFoundError(f\"No TensorBoard event files found in {log_dir}\")\n",
    "    \n",
    "    # Use the first event file found\n",
    "    event_file = event_files[0] \n",
    "    \n",
    "    # Load the event file\n",
    "    ea = event_accumulator.EventAccumulator(event_file)\n",
    "    ea.Reload()\n",
    "    \n",
    "    # Verify the scalar exists\n",
    "    available_keys = ea.scalars.Keys()\n",
    "    if scalar_name not in available_keys:\n",
    "        raise ValueError(f\"Scalar '{scalar_name}' not found. Available keys: {available_keys}\")\n",
    "    \n",
    "    # Extract wall times from the scalar events\n",
    "    wall_times = ea.Scalars(scalar_name)\n",
    "    start_time = wall_times[0].wall_time\n",
    "    end_time = wall_times[-1].wall_time\n",
    "    \n",
    "    return start_time, end_time, end_time - start_time\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_multiple(output_dir, val_dataset):\n",
    "    \"\"\"\n",
    "    Evaluate model performance and efficiency metrics across multiple checkpoints.\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): Directory containing model checkpoints\n",
    "        val_dataset: Validation dataset for evaluation\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Get all checkpoint paths\n",
    "    checkpoint_paths = get_all_checkpoints(output_dir)\n",
    "    \n",
    "    for checkpoint_path in checkpoint_paths:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(checkpoints_path).to(device)\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.use_cache = False\n",
    "        model.config.pretraining_tp = 1\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.eval()\n",
    "        \n",
    "        # Initialize data structures for evaluation\n",
    "        validation_results = []\n",
    "        true_label_one_hot_list = []\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        all_probs = []\n",
    "        \n",
    "        # Start inference timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Process validation data\n",
    "        for input_sample in tqdm(val_dataset, desc=\"Processing validation data\", unit=\"sample\"):\n",
    "            sample = input_sample['content']\n",
    "            true_label_idx = int(input_sample['labels'])\n",
    "            \n",
    "            # Tokenize and predict\n",
    "            tokenized_input = tokenizer(\n",
    "                sample, \n",
    "                padding=\"max_length\", \n",
    "                max_length=max_length, \n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model_output = model(**tokenized_input)\n",
    "                \n",
    "            # Process prediction results\n",
    "            logits = model_output.logits\n",
    "            probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
    "            \n",
    "            # Convert indices to labels\n",
    "            predicted_label = id2label_n[predicted_class_idx]\n",
    "            true_label = id2label_n[true_label_idx]\n",
    "            \n",
    "            # Create one-hot encoding of true label\n",
    "            true_label_one_hot = np.zeros(probabilities.size(-1))\n",
    "            true_label_one_hot[true_label_idx] = 1\n",
    "            \n",
    "            # Store results\n",
    "            true_labels.append(true_label)\n",
    "            true_label_one_hot_list.append(true_label_one_hot)\n",
    "            predicted_labels.append(predicted_label)\n",
    "            all_probs.append(probabilities.detach().cpu().numpy())\n",
    "            \n",
    "            # Store detailed results for each sample\n",
    "            result = {\n",
    "                'content': sample,\n",
    "                'true_label': true_label,\n",
    "                'predicted_label': predicted_label,\n",
    "                'true_label_one_hot': true_label_one_hot.tolist(),\n",
    "                'predicted_class_idx': predicted_class_idx,\n",
    "                'probabilities': probabilities.detach().cpu().numpy().tolist()\n",
    "            }\n",
    "            validation_results.append(result)\n",
    "        \n",
    "        # Calculate inference time\n",
    "        end_time = time.time()\n",
    "        inference_time = end_time - start_time\n",
    "        inference_time_hours = round(inference_time / 3600, 4)\n",
    "        \n",
    "        # Get training time\n",
    "        start_time, end_time, training_duration = extract_training_time(output_dir)\n",
    "        training_time_hours = round(training_duration / 3600, 4)\n",
    "        \n",
    "        # Save validation results\n",
    "        timestamp = datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
    "        df_validation_results = pd.DataFrame(validation_results)\n",
    "        jsonl_file_path = os.path.join(checkpoint_path, f'validation_results_{timestamp}.jsonl')\n",
    "        df_validation_results.to_json(jsonl_file_path, orient='records', lines=True)\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "        auc = roc_auc_score(\n",
    "            np.array(true_label_one_hot_list),\n",
    "            np.squeeze(np.array(all_probs), axis=1),\n",
    "            multi_class='ovr',\n",
    "            average='weighted'\n",
    "        )\n",
    "        \n",
    "        # Find GPU resource consumption for the model\n",
    "        model_name = None\n",
    "        for name in GPU_HOURS_DICT:\n",
    "            if name in output_dir:\n",
    "                model_name = name\n",
    "                gpu_ram_train = GPU_HOURS_DICT[name][\"train\"]\n",
    "                gpu_ram_inference = GPU_HOURS_DICT[name][\"inference\"]\n",
    "                break\n",
    "        \n",
    "        if not model_name:\n",
    "            print(f\"Warning: Model not found in GPU_HOURS_DICT. Using default values.\")\n",
    "            gpu_ram_train = 30.0\n",
    "            gpu_ram_inference = 20.0\n",
    "        \n",
    "        # Calculate efficiency metrics\n",
    "        training_gpu_hours_ram = gpu_ram_train * training_time_hours\n",
    "        inference_gpu_hours_ram = gpu_ram_inference * inference_time_hours\n",
    "        total_gpu_hours_ram = training_gpu_hours_ram + inference_gpu_hours_ram\n",
    "        \n",
    "        # Calculate composite metrics\n",
    "        resource_m = f1 / (ALPHA * training_gpu_hours_ram + BETA * inference_gpu_hours_ram)\n",
    "        time_m = f1 / (ALPHA * training_time_hours + BETA * inference_time_hours)\n",
    "        \n",
    "        # Calculate time and resource ratios\n",
    "        ratio_time = inference_time_hours / (inference_time_hours + training_time_hours)\n",
    "        ratio_time_ram = inference_gpu_hours_ram / (inference_gpu_hours_ram + training_gpu_hours_ram)\n",
    "        \n",
    "        # Print evaluation results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Evaluation Results for {model_name or 'Unknown Model'}\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Performance Metrics:\")\n",
    "        print(f\"  Accuracy:    {accuracy:.4f}\")\n",
    "        print(f\"  F1 Score:    {f1:.4f}\")\n",
    "        print(f\"  AUC:         {auc:.4f}\")\n",
    "        \n",
    "        print(\"\\nEfficiency Metrics:\")\n",
    "        print(f\"  Resource_M:  {resource_m:.4f}\")\n",
    "        print(f\"  Time_M:      {time_m:.4f}\")\n",
    "        \n",
    "        print(\"\\nTime Distribution:\")\n",
    "        print(f\"  Inference/Total Time Ratio: {ratio_time:.4f}\")\n",
    "        print(f\"  Inference/Total RAM Ratio:  {ratio_time_ram:.4f}\")\n",
    "        \n",
    "        print(\"\\nGPU Resource Usage (GPU hours × RAM):\")\n",
    "        print(f\"  Total:       {total_gpu_hours_ram:.4f}\")\n",
    "        print(f\"  Training:    {training_gpu_hours_ram:.4f}\")\n",
    "        print(f\"  Inference:   {inference_gpu_hours_ram:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Execute evaluation\n",
    "print(f\"Evaluating models in: {output_dir}\")\n",
    "evaluate_multiple(output_dir=output_dir, val_dataset=val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
