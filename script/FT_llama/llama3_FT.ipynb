{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1242085/3685722755.py:154: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_train_data = dataset.groupby('labels', group_keys=False).apply(\n",
      "/tmp/ipykernel_1242085/3685722755.py:158: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_validation_data = dataset.groupby('labels', group_keys=False).apply(\n",
      "Map: 100%|██████████| 219/219 [00:00<00:00, 7118.58 examples/s]\n",
      "Map: 100%|██████████| 91/91 [00:00<00:00, 7068.18 examples/s]\n",
      "Casting the dataset: 100%|██████████| 219/219 [00:00<00:00, 34486.67 examples/s]\n",
      "Casting the dataset: 100%|██████████| 91/91 [00:00<00:00, 21501.98 examples/s]\n",
      "Map: 100%|██████████| 219/219 [00:08<00:00, 25.37 examples/s]\n",
      "Map: 100%|██████████| 91/91 [00:04<00:00, 22.31 examples/s]\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/snt/projects_lujun/base_models/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28/28 02:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.067600</td>\n",
       "      <td>3.520862</td>\n",
       "      <td>0.084484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.841500</td>\n",
       "      <td>3.448382</td>\n",
       "      <td>0.073767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training SFT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/snt/projects_lujun/base_models/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Processing validation data: 100%|██████████| 91/91 [00:18<00:00,  4.97sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1099\n",
      "F1 Score: 0.0684\n",
      "AUC: 0.4443\n",
      "Average Inference Time per Sample: 0.0840 seconds\n",
      "Finished training and evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, LoraConfig\n",
    "import json\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from datasets.dataset_dict import DatasetDict, IterableDatasetDict\n",
    "from datasets.iterable_dataset import IterableDataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from datasets import ClassLabel\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "# Data preparation\n",
    "import torch\n",
    "from transformers import EarlyStoppingCallback\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# ========================== CMD Argument Parser ==========================\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description=\"Train a model using CPT (Continual Pretraining Training)\")\n",
    "#     parser.add_argument(\"--per_device_train_batch_size\", type=int, default=8, help=\"Batch size per device during training\")\n",
    "#     parser.add_argument(\"--per_device_eval_batch_size\", type=int, default=8, help=\"Batch size per device during evaluation\")\n",
    "#     parser.add_argument(\"--num_train_epochs\", type=int, default=1, help=\"Number of training epochs\")\n",
    "#     parser.add_argument(\"--learning_rate\", type=float, default=1e-6, help=\"Learning rate for training\")\n",
    "#     parser.add_argument(\"--project_root\", type=str, default=\"/Users/lujun.li/projects/mt_luxembourgish\", help=\"Path to project root\")\n",
    "#     parser.add_argument(\"--training_dataset_path\", type=str, default=\"data/processed/dataset_merged_llama_fake_targets.jsonl\", help=\"Path to training dataset\")\n",
    "#     parser.add_argument(\"--model_path\", type=str, default=\"/home/llama/Personal_Directories/srb/binary_classfication/Llama-3.2-3B-Instruct\", help=\"Path to model\")\n",
    "#     parser.add_argument(\"--resume_from_checkpoint\", type=bool, default=False, help=\"Resume training from checkpoint\")\n",
    "#     parser.add_argument(\"--resume_checkpoint_path\", type=str, default=None, help=\"Path to checkpoint to resume training from\")\n",
    "#     parser.add_argument(\"--qlora\", type=bool, default=False, help=\"Use QLoRA\")\n",
    "#     parser.add_argument(\"--r\", type=int, default=16, help=\"Rank for LoRA\")\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# args = parse_args()\n",
    "\n",
    "\n",
    "# print(\"Arguments passed:\")\n",
    "# print(f\"Train Batch Size: {args.per_device_train_batch_size}\")\n",
    "# print(f\"Eval Batch Size: {args.per_device_eval_batch_size}\")\n",
    "# print(f\"Number of Epochs: {args.num_train_epochs}\")\n",
    "# print(f\"Learning Rate: {args.learning_rate}\")\n",
    "# print(f\"Project Root: {args.project_root}\")\n",
    "# print(f\"Training Dataset Path: {args.training_dataset_path}\")\n",
    "# print(f\"Model path: {args.model_path}\")\n",
    "# print(f\"Resume from checkpoint: {args.resume_from_checkpoint}\")\n",
    "# print(f\"Resume checkpoint path: {args.resume_checkpoint_path}\")\n",
    "# print(f\"Qlora: {args.qlora}\")\n",
    "# print(f\"Rank: {args.r}\")\n",
    "\n",
    "# per_device_train_batch_size = args.per_device_train_batch_size  # Batch size for training per device\n",
    "# per_device_eval_batch_size = args.per_device_eval_batch_size  # Batch size for evaluation per device\n",
    "# num_train_epochs = args.num_train_epochs  # Number of epochs for training\n",
    "# learning_rate = args.learning_rate # Learning rate for the optimizer\n",
    "# project_root = args.project_root\n",
    "# training_dataset_path = args.training_dataset_path\n",
    "# model_path = args.model_path\n",
    "# resume_from_checkpoint = args.resume_from_checkpoint\n",
    "# resume_checkpoint_path = args.resume_checkpoint_path\n",
    "# qlora = args.qlora\n",
    "# r = args.r\n",
    "\n",
    "\n",
    "## Data preparation\n",
    "per_device_train_batch_size = 8\n",
    "per_device_eval_batch_size = 8\n",
    "num_train_epochs = 1\n",
    "learning_rate = 5e-5\n",
    "project_root = \"/home/snt/projects_lujun/agentCLS\"\n",
    "training_dataset_path = \"assets/training_dataset/LDD_split.json\"\n",
    "model_path = \"/home/snt/projects_lujun/base_models/Llama-3.2-1B-Instruct\"\n",
    "resume_from_checkpoint = False\n",
    "resume_checkpoint_path = None\n",
    "qlora = True\n",
    "r = 16\n",
    "\n",
    "train_dataset_path = os.path.abspath(os.path.join(project_root, training_dataset_path))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "\n",
    "# Default Parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_seed = 3407\n",
    "train_ratio = 0.01\n",
    "logging_steps = 10\n",
    "eval_steps = 10\n",
    "eval_strategy = \"steps\"\n",
    "save_strategy = \"epoch\"\n",
    "# save_total_limit = 2\n",
    "logging_strategy = \"steps\"\n",
    "max_grad_norm = 0.3\n",
    "input_dataset_name = train_dataset_path.split(\"/\")[-1].split(\".\")[0]\n",
    "model_name = model_path.split(\"/\")[-1]\n",
    "max_length = 4096\n",
    "load_in_4bit = True\n",
    "bnb_4bit_quant_type = 'nf4'\n",
    "quantization_config = None\n",
    "\n",
    "\n",
    "if qlora:\n",
    "# Quantization with Lora\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = load_in_4bit, # enable 4-bit quantization\n",
    "        bnb_4bit_quant_type = bnb_4bit_quant_type, # information theoretically optimal dtype for normally distributed weights\n",
    "        bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
    "        bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
    "    )\n",
    "\n",
    "    # Lora\n",
    "    lora_config = LoraConfig(\n",
    "        r = r, # the dimension of the low-rank matrices\n",
    "        lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "        target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "        lora_dropout = 0.05, # dropout probability of the LoRA layers\n",
    "        bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n",
    "        task_type = 'SEQ_CLS',\n",
    "    )\n",
    "\n",
    "\n",
    "if resume_from_checkpoint and resume_checkpoint_path is None:\n",
    "    raise ValueError(\"Please provide a checkpoint path to resume training from\")\n",
    "\n",
    "if resume_from_checkpoint:\n",
    "    output_dir = resume_checkpoint_path\n",
    "else:\n",
    "    current_time = datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
    "    output_dir = f\"{project_root}/assets/logs/{input_dataset_name}_{train_ratio}_{model_name}_output_{current_time}\"\n",
    "\n",
    "dataset = pd.read_json(train_dataset_path, lines=True)\n",
    "dataset.rename(columns={\"cls_label\": \"labels\"}, inplace=True)\n",
    "\n",
    "# Compute sample counts for each group based on 'labels' and 'split'\n",
    "sample_counts = dataset.groupby(['labels', 'split']).size() * train_ratio\n",
    "\n",
    "filtered_train_data = dataset.groupby('labels', group_keys=False).apply(\n",
    "    lambda x: x[x['split'] == 'train'].iloc[:int(sample_counts.loc[x.name, 'train'])]\n",
    ")\n",
    "\n",
    "filtered_validation_data = dataset.groupby('labels', group_keys=False).apply(\n",
    "    lambda x: x[x['split'] == 'validation'].iloc[:int(sample_counts.loc[x.name, 'validation'])]\n",
    ")\n",
    "\n",
    "filtered_train = filtered_train_data.reset_index(drop=True)\n",
    "filtered_validation = filtered_validation_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(filtered_train)\n",
    "val_dataset = Dataset.from_pandas(filtered_validation)\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"content\"], padding=True, truncation=True, max_length=max_length)\n",
    "\n",
    "labels =  set(train_dataset['labels'])\n",
    "num_labels = len(labels)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x: {\"labels\": label2id[x[\"labels\"]]})\n",
    "val_dataset = val_dataset.map(lambda x: {\"labels\": label2id[x[\"labels\"]]})\n",
    "\n",
    "labels_ids = list(set(train_dataset['labels']))\n",
    "class_label = ClassLabel(num_classes=len(labels_ids), names=labels_ids)\n",
    "train_dataset = train_dataset.cast_column(\"labels\", class_label)\n",
    "val_dataset = val_dataset.cast_column(\"labels\", class_label)\n",
    "\n",
    "\n",
    "keep_columns = [\"labels\", \"input_ids\", \"attention_mask\"]\n",
    "tokenized_train_dataset = train_dataset.map(tokenize, batched=True, remove_columns=[col for col in train_dataset.column_names if col not in keep_columns])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize, batched=True, remove_columns=[col for col in val_dataset.column_names if col not in keep_columns])\n",
    "train_dataset.features.keys()\n",
    "\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels, label2id=label2id, id2label=id2label, quantization_config=quantization_config,)\n",
    "if qlora:\n",
    "    model = get_peft_model(prepare_model_for_kbit_training(model), lora_config)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "train_dataset.features.keys()\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"f1\": f1_score(labels, predictions, average=\"weighted\")}\n",
    "\n",
    "def train():\n",
    "    # Define training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir= output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        learning_rate=5e-5,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        bf16=True,\n",
    "        optim=\"adamw_torch_fused\", \n",
    "        logging_strategy=logging_strategy,\n",
    "        logging_steps=logging_steps,\n",
    "        eval_strategy=eval_strategy,\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=save_strategy,\n",
    "        # save_total_limit=save_total_limit,\n",
    "        load_best_model_at_end=False,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        # group_by_length=True,\n",
    "        # use_mps_device=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        # push to hub parameters\n",
    "        # push_to_hub=True,\n",
    "        # hub_strategy=\"every_save\",\n",
    "        # hub_token=HfFolder.get_token(),\n",
    "        report_to=\"tensorboard\",\n",
    "        disable_tqdm=False,\n",
    "        seed = train_seed,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    # Create a Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer_stats = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    print(\"Finished training SFT.\")\n",
    "    return trainer_stats\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    def get_last_checkpoints(output_dir):\n",
    "        checkpoints = os.listdir(output_dir)\n",
    "        checkpoints = [c for c in checkpoints if \"checkpoint\" in c]\n",
    "        checkpoints = [int(c.split(\"-\")[-1]) for c in checkpoints]\n",
    "        last_checkpoint = max(checkpoints)\n",
    "        return f\"{output_dir}/checkpoint-{last_checkpoint}\"\n",
    "    \n",
    "    checkpoints_path  = get_last_checkpoints(output_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels, label2id=label2id, id2label=id2label, quantization_config=quantization_config,).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    validation_results = []\n",
    "\n",
    "    # Initialize lists to store true and predicted labels\n",
    "    true_label_one_hot_list = []\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    # Start time for measuring inference efficiency\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate through validation dataset and make predictions\n",
    "    for input in tqdm(val_dataset, desc=\"Processing validation data\", unit=\"sample\"):\n",
    "        sample = input['content']\n",
    "        true_label_idx = int(input['labels'])\n",
    "        tokenized_input = tokenizer(sample, padding=True, max_length=max_length, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        model_output = model(**tokenized_input)\n",
    "        logits = model_output.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
    "        predicted_label = id2label[str(predicted_class_idx)]\n",
    "        \n",
    "        true_label = id2label[str(true_label_idx)]\n",
    "        true_label_one_hot = np.zeros(probabilities.size(-1))\n",
    "        true_label_one_hot[true_label_idx] = 1\n",
    "\n",
    "        true_labels.append(true_label)\n",
    "        true_label_one_hot_list.append(true_label_one_hot)\n",
    "        predicted_labels.append(predicted_label)\n",
    "        all_probs.append(probabilities.detach().cpu().numpy())  # Store the raw probabilities for AUC\n",
    "        result = {\n",
    "            'content': sample,\n",
    "            'true_label': true_label,\n",
    "            'predicted_label': predicted_label,\n",
    "            'true_label_one_hot': true_label_one_hot.tolist(),\n",
    "            'predicted_class_idx': predicted_class_idx,\n",
    "            'probabilities': probabilities.detach().cpu().numpy().tolist()  # Convert to list for JSON serialization\n",
    "        }\n",
    "        validation_results.append(result)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
    "    df_validation_results = pd.DataFrame(validation_results)\n",
    "    jsonl_file_path = os.path.join(checkpoints_path, f'validation_results_{timestamp}.jsonl')\n",
    "    df_validation_results.to_json(jsonl_file_path, orient='records', lines=True)\n",
    "\n",
    "\n",
    "    # Calculate accuracy, F1 score, and AUC\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')  # weighted F1 score\n",
    "    auc = roc_auc_score(np.array(true_label_one_hot_list),  np.squeeze(np.array(all_probs), axis=1), multi_class='ovr', average='weighted')  # for multi-class AUC\n",
    "\n",
    "    # Calculate inference time (average time per sample)\n",
    "    end_time = time.time()\n",
    "    inference_time = (end_time - start_time) / len(train_dataset)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"Average Inference Time per Sample: {inference_time:.4f} seconds\")\n",
    "    return \n",
    "        \n",
    "\n",
    "\n",
    "trainer_stats = None\n",
    "\n",
    "def main():\n",
    "    trainer_stats = train()\n",
    "    return trainer_stats\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainer_stats = main()\n",
    "    eval_results = evaluate()\n",
    "\n",
    "    print(\"Finished training and evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    def get_last_checkpoints(output_dir):\n",
    "        checkpoints = os.listdir(output_dir)\n",
    "        checkpoints = [c for c in checkpoints if \"checkpoint\" in c]\n",
    "        checkpoints = [int(c.split(\"-\")[-1]) for c in checkpoints]\n",
    "        last_checkpoint = max(checkpoints)\n",
    "        return f\"{output_dir}/checkpoint-{last_checkpoint}\"\n",
    "    \n",
    "    checkpoints_path  = get_last_checkpoints(output_dir)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=num_labels, label2id=label2id, id2label=id2label, quantization_config=quantization_config,).to(device)\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    model.gradient_checkpointing_enable()\n",
    "    validation_results = []\n",
    "\n",
    "    # Initialize lists to store true and predicted labels\n",
    "    true_label_one_hot_list = []\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    # Start time for measuring inference efficiency\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Iterate through validation dataset and make predictions\n",
    "    for input in tqdm(val_dataset, desc=\"Processing validation data\", unit=\"sample\"):\n",
    "        sample = input['content']\n",
    "        true_label_idx = int(input['labels'])\n",
    "        tokenized_input = tokenizer(sample, padding=True, max_length=max_length, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        model_output = model(**tokenized_input)\n",
    "        logits = model_output.logits\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        predicted_class_idx = torch.argmax(probabilities, dim=-1).item()\n",
    "        predicted_label = id2label[str(predicted_class_idx)]\n",
    "        \n",
    "        true_label = id2label[str(true_label_idx)]\n",
    "        true_label_one_hot = np.zeros(probabilities.size(-1))\n",
    "        true_label_one_hot[true_label_idx] = 1\n",
    "\n",
    "        true_labels.append(true_label)\n",
    "        true_label_one_hot_list.append(true_label_one_hot)\n",
    "        predicted_labels.append(predicted_label)\n",
    "        all_probs.append(probabilities.detach().cpu().numpy())  # Store the raw probabilities for AUC\n",
    "        result = {\n",
    "            'content': sample,\n",
    "            'true_label': true_label,\n",
    "            'predicted_label': predicted_label,\n",
    "            'true_label_one_hot': true_label_one_hot.tolist(),\n",
    "            'predicted_class_idx': predicted_class_idx,\n",
    "            'probabilities': probabilities.detach().cpu().numpy().tolist()  # Convert to list for JSON serialization\n",
    "        }\n",
    "        validation_results.append(result)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%m_%d_%H_%M_%S\")\n",
    "    df_validation_results = pd.DataFrame(validation_results)\n",
    "    jsonl_file_path = os.path.join(checkpoints_path, f'validation_results_{timestamp}.jsonl')\n",
    "    df_validation_results.to_json(jsonl_file_path, orient='records', lines=True)\n",
    "\n",
    "\n",
    "    # Calculate accuracy, F1 score, and AUC\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')  # weighted F1 score\n",
    "    auc = roc_auc_score(np.array(true_label_one_hot_list),  np.squeeze(np.array(all_probs), axis=1), multi_class='ovr', average='weighted')  # for multi-class AUC\n",
    "\n",
    "    # Calculate inference time (average time per sample)\n",
    "    end_time = time.time()\n",
    "    inference_time = (end_time - start_time) / len(train_dataset)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"Average Inference Time per Sample: {inference_time:.4f} seconds\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at /home/snt/projects_lujun/base_models/Llama-3.2-1B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Processing validation data:   0%|          | 0/91 [01:14<?, ?sample/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 37\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m predicted_class_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(probabilities, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 37\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m \u001b[43mid2label\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredicted_class_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     39\u001b[0m true_label \u001b[38;5;241m=\u001b[39m id2label[\u001b[38;5;28mstr\u001b[39m(true_label_idx)]\n\u001b[1;32m     40\u001b[0m true_label_one_hot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(probabilities\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mKeyError\u001b[0m: '5'"
     ]
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
